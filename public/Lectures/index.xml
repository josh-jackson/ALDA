<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lectures | Applied Longitudinal Data Analysis</title>
    <link>/lectures/</link>
      <atom:link href="/lectures/index.xml" rel="self" type="application/rss+xml" />
    <description>Lectures</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 09 Oct 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Lectures</title>
      <link>/lectures/</link>
    </image>
    
    <item>
      <title>Week #7</title>
      <link>/lectures/week-6/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/week-6/</guid>
      <description>


&lt;div id=&#34;longitudinal-structural-equation-modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Longitudinal Structural Equation Modeling&lt;/h1&gt;
&lt;p&gt;SEM is the broader umbrella from the GLM. With it we are able to do two interesting this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Fit a latent measurement model (e.g., CFA)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a structural model (e.g,. path analysis)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two components allow us to address more difficult research questions involving but not limited to: multiple DVs, mediators, varying effects across time, unmeasured variables, constraints, and measurement invariance.&lt;/p&gt;
&lt;p&gt;Compared to MLM, and SEM approach to longitudinal data may be better suited to your research goals. We will see that at one level the two approaches can be equivalent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sem-primer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SEM primer&lt;/h1&gt;
&lt;p&gt;In our studies some of the variables of interest will be measured and others will be latent or unobserved. Measured variables are the observed scores that are typically collected in a research study. That is, your variables in your dataset. This may include responses to survey items, reports of age or income, or performance on a recall task. We will use the general term “indicators” or “items” or “manifest variables” to refer to these measures variables. They will be indicated by boxes in a path diagram.&lt;/p&gt;
&lt;p&gt;Latent variables, in contrast, are not directly observed. A latent variable is something that we assume to exist but we cannot directly measure (see) it. They are typically postulated by theories or inferred from the statistical behavior of measured variables. Sounds like psychological variables! Examples of latent variables include “depression,” “job satisfaction,” and “working memory capacity.” Latent variables will be represented as a circle in path diagrams.&lt;/p&gt;
&lt;p&gt;For example, why does Sally like to go to parties, likes to talk a lot, and always tends to be a in a good mood? Maybe it is because her high levels of extraversion ( a latent variable that we cannot directly measure) is causing these tendencies.&lt;/p&gt;
&lt;p&gt;Key point: the variable/construct itself is not measurable, but the manifestations caused by the variable are measurable/observable.&lt;/p&gt;
&lt;p&gt;Interesting point: because variables are assumed to be causing indicators of the variable, SEM is sometimes referred to as causal modeling. (Also because in path models a directional relationship is hypothesized) Note that we cannot get any closer to causality than we can with regression.&lt;/p&gt;
&lt;div id=&#34;pretty-pictures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pretty pictures&lt;/h2&gt;
&lt;p&gt;Circles = latent variables&lt;/p&gt;
&lt;p&gt;Boxes = observed indicator variables&lt;/p&gt;
&lt;p&gt;two headed arrows = correlations/covariances/variances&lt;/p&gt;
&lt;p&gt;single head arrows = regressions&lt;/p&gt;
&lt;p&gt;triangle = means&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is lavaan 0.6-4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan is BETA software! Please report any bugs.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(semPlot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 methods overwritten by &amp;#39;huge&amp;#39;:
##   method    from   
##   plot.sim  BDgraph
##   print.sim BDgraph&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HolzingerSwineford1939 &amp;lt;- HolzingerSwineford1939

mod.1 &amp;lt;- &amp;#39;visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9&amp;#39;

fit.1 &amp;lt;- cfa(mod.1, data=HolzingerSwineford1939,meanstructure = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.1,  intercepts = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.1, &amp;quot;std&amp;quot;, intercepts = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measurement model&lt;/h2&gt;
&lt;p&gt;The first step of an SEM model with latent variables is to define them. This is called specifying the measurement model. It is up to you to specify how you think the latent variable is created. It is then up to you to compare that measurement model against alternative measurement models to see if it meaningful.&lt;/p&gt;
&lt;p&gt;The key components are the a) factor loadings, b) residuals, and c) variance of latent variable.&lt;/p&gt;
&lt;div id=&#34;classical-test-theory-interpretation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Classical test theory interpretation&lt;/h3&gt;
&lt;p&gt;How can we think of a latent construct:&lt;/p&gt;
&lt;p&gt;Latent construct = what the indicators share in common&lt;/p&gt;
&lt;p&gt;The indicators represent the sum of True Score variance + Item specific variance + Random error&lt;/p&gt;
&lt;p&gt;The variance of the latent variable represents the amount of common information in the latent variable. If your indicators are haphazardly chosen then there will be low variance. We want to maximize variance here as variance suggests meaningful differences can be made between people.&lt;/p&gt;
&lt;p&gt;The residual errors (sometimes referred to as disturbances) represent the amount of information unique to each indicator. A combination of error and item-specific variance.&lt;/p&gt;
&lt;p&gt;The extent of the connection between the latent variable and the indicators is represented as a factor loading.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generizability-interpretation-of-latent-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generizability interpretation of latent variables&lt;/h3&gt;
&lt;p&gt;Same as above, but…&lt;/p&gt;
&lt;p&gt;True score variance can be thought of as consisting as a combination of
1. Construct variance- this is the truest true score variance
2. Method variance- see Campbell and Fiske or sludge factor of Meehl.
3. Occasion- important for longitudinal, aging, and cohort analyses–and for this class.&lt;/p&gt;
&lt;p&gt;For longitudinal models, occasion specific variance can lead to biased estimates. We want to separate the time specific variance from the overall construct variance. Or, we want to make sure that the time specific variance doesn’t make it appear that a construct is changing when really it is not.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;formative-indicators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Formative indicators&lt;/h3&gt;
&lt;p&gt;These pretty pictures imply that the latent variables “cause” the indicators. This is the standard view and are referred to as reflexive indicators. However, there is another approach, formative indicators, were indicators “cause” the latent variable. Or, in other words, the latent variable doesn’t actually exist. It is not real, only a combination of variables. An example of this is SES. SES does not ‘exist’ but is a socially constructed idea. Some people have argued that psychological constructs are of this kind, and that reflexive indiators are innapropriate. The arguments are mainly philisophpical and thus is beyond the scope of our discussion. Suffice to say, that most of the time there are few analytic differences. You can think of this as similar to the factor analysis vs principal component analysis debates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Measurement error&lt;/h3&gt;
&lt;p&gt;A major advantage is that each latent variable does not contain measurement error. It is as is if we measured our variable with an alpha = 1.&lt;/p&gt;
&lt;p&gt;What does that do? Well, ideally that gets us closer to the population model, which could yield higher R2 and parameter estimates.&lt;/p&gt;
&lt;p&gt;How does this happen? It is a direct result of capturing what is shared among the indicators. The measurement error associated with each indicator is uncorrelated with the latent variable.&lt;/p&gt;
&lt;p&gt;Think about how this situation differs from creating a composite among variables. Think about how this differs from creating a factor score among variables within a simple factor analyses approach. How are all three different and similar? What does it mean if the error variances are correlated with one another?&lt;/p&gt;
&lt;p&gt;Remember however, that it is theoretically error free. The latent variable is not only filled with true score variance. Instead it could have method and occasion variance. Unless you have multiple methods and occasions it is hard to parce them apart.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regarding-means&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regarding means&lt;/h3&gt;
&lt;p&gt;SEM is also known as covariance structure analysis. You can do SEM using only variance-covariance matrices. These do not necessarily involve any direct information about their means. Means in SEM are optional. This is cool because you can technically reproduce the analyses of a paper if they give you a correlation matrix of study variables.&lt;/p&gt;
&lt;p&gt;Given we are interested in change across time, however, we will be intersted in means. Latent variables by themselves do not have any inherent metric, it is up to us to choose the scale they are on. We can standardize them, use the original metric, and more! More later on how we define the mean of a latent variable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;path-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Path model&lt;/h2&gt;
&lt;p&gt;The path model component can be in addition to a measurement model or seperate from them. You have already worked with path models as a simple regression is a path model, so is a standard mediation. You can make the path models more complex than these though, by specifying relationships among many variables.&lt;/p&gt;
&lt;p&gt;An example with no measurement model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate data dataset:
X &amp;lt;- rnorm(100)
M &amp;lt;- rnorm(100)
Y &amp;lt;- rnorm(100) 
data &amp;lt;- data.frame(X, Y, M)

# Two regressions:
res1 &amp;lt;- lm(M ~ X, data = data)
res2 &amp;lt;- lm(Y ~ X + M, data = data)

# Plot mediation
semPaths(res1 + res2, &amp;quot;model&amp;quot;, &amp;quot;est&amp;quot;, intercepts = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;mediation model with measurement model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2 &amp;lt;- &amp;#39;visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9

speed ~ textual + visual
textual ~ visual&amp;#39;

fit.2 &amp;lt;- cfa(mod.2, data=HolzingerSwineford1939)
semPaths(fit.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-an-sem-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating an SEM model&lt;/h2&gt;
&lt;p&gt;Our goal when doing SEM is a creation of a model that specifies certain relationship among variables. This is done by creating a measurement or path model that we think is driven by the data generating process we are trying to study. In addition to setting the measurement model and paths we may want to put apriori constraining parameters (variances/covarainces/regressions) to reflect how we think variables are related.&lt;/p&gt;
&lt;p&gt;E.g., Should these two variables be correlated or not?&lt;/p&gt;
&lt;p&gt;Then we use or ML algorithm to get our model implied covariances/means as close as possible to the observed covariances/means.&lt;/p&gt;
&lt;p&gt;Of note, SEM can handle any time of measured DV/IV or construct/indicators. If you have categorical indicators you can do SEM. However, it is hard to measure change using categorical indicators. But, categorical indicators are used for many latent variable models such as in measuring psychopathology.&lt;/p&gt;
&lt;p&gt;If you have a categorical construct you can also do SEM. Here it is called latent transition analysis (if you also had categorical indicators) or latent class / latent mixture modeling if you had continuous indicators (i am counting ordinal as continuous).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-the-scale-and-defining-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting the scale and defining variables&lt;/h2&gt;
&lt;p&gt;We are trying to measure clouds. How can we do this given that they are always moving? Need to define the scale of a latent variable because there is no inherent scale of measurement. Largely irrelevant as to what scale is chosen just as centering or standardizing yeild no substantive changes. Instead, scaling serves to establish a point of reference so as to interpret other parameters (much like the justifications for centering and standardizing).&lt;/p&gt;
&lt;p&gt;3 options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Fixed factor. Here you fix the variance of the latent variable to 1 (standardized).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Marker variable. Here you fix one factor loading to 1. All other loadings are relative to this loading. The variance of the latent variable can thus be anything. This is often the default of software programs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Effect coding. Here you constrain loading to average to 1. This will be helpful for us as we can then put the scale of measurement into our original metric. For longitudinal models this is helpful in terms of how to interpret the amount of change.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;identification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Identification&lt;/h2&gt;
&lt;p&gt;Note that we have multiple parameters we are trying to estimate. Paths (regression coefficients), means, variances (of manifest variables and latent variables). This makes estiamting a little more tricky as you cannot have more unknowns than knowns. There are some tricks and rules of hand about what you can estimate or not. Typically however, it is suggested that you have 3 indicators per latent variable.&lt;/p&gt;
&lt;p&gt;If you are asking too much for a model you can also constrain parameters to be the same. For example, you may assume that all residual variances are the same. This assumtpion can be built into your model and reduces the number of parameters you have to estimate.&lt;/p&gt;
&lt;p&gt;More specifically, you need to compare the number of knows (variances and covariances) to the unknowns (model parameters).&lt;/p&gt;
&lt;p&gt;Foe example, a three indicator latent variable has 7 unknowns. 3 Loadings, 3 error variances and the variance of the latent variable&lt;/p&gt;
&lt;p&gt;The covariance matrix has 6 data points. Thus we need to add in one more known, in this case a fixed factor or a marker variable.&lt;/p&gt;
&lt;p&gt;Knowns - unknowns = df. Note that df in this case df will not directly relate to sample size, so it is a little different than typical degree of freedom concepts.&lt;/p&gt;
&lt;p&gt;However, we will use this version of a df because it corresponds to how we will test the fit of these models. Specifically, the difference in dfs between models is distributed as a chi-square.&lt;/p&gt;
&lt;div id=&#34;types-of-identification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Types of identification&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Just identified is where the number of knowns equal unknowns. Also known as saturated model. When you evaluate the fit of the model these will be perfect. So while these will estimate, we cannot examine whether or not our model is a good representation of the world, as we are simply recreating the observed covariance matrix (data). For some instances this may not be a problem, for others…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Over identified is when you have more knowns than unknowns. This is good as we can fit a model that is more parsiminous than our data. Moreover, we can examine fit stats.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Under identified is when you have problems and have more unknowns than knowns. this is because there is more than one solution available and the algorithm cannot decide e.g,. 2 + X = Y. If we add a constraint or a known value then it becomes manageable 2 + X = 12&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-indices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit Indices&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;residuals. Good to check.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;modification indices. Check to see if missing parameters that residuals may suggest you didn’t include or should include. Can test with more advanced techniques. But eh… makes your models non-theoretical, could be over fitting, relying too much on sig tests…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;chi-square. (Statistical fit) Implied versus observed data, tests to see if model are exact fit with data. But eh…too impacted by sample size&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;RMSEA or SRMR (Absolute fit). Does not compare to a null model. Judges distance from perfect fit.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Above .10 poor fit
Below .08 acceptable&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;CFI, TFI (Relative fit models). Compares relative to a null model. Judges distance from the worse fit ie a null model. Null models have no covariance among observed and latent variables.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;range from 0-1. Indicate % of improvement from the null model to a saturated i.e. just identified model.&lt;/p&gt;
&lt;p&gt;Usually &amp;gt;.9 is okay. Some care about &amp;gt; .95&lt;/p&gt;
&lt;p&gt;Minor changes to the model can improve fit.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Check the model parameters. Are they wonky? Easy to get negative variances or correlations above 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;parcels&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parcels&lt;/h3&gt;
&lt;p&gt;It is often necessary to simplify your model. One option to do so is with parcels where you combine indicators into a composite. This simplifies the model in that you have fewer parameters to fit. In addition to being a way to get a model identified, it also has benefits in terms of the assumptions of the indicator variables.&lt;/p&gt;
&lt;p&gt;To do so, you can combine items however you want into 3 or 4 groups or parcels, averaging them together. You may balance highly loading with less highly loading items (item to construct technique) or you may pair pos and negatively keyed items together. It is up to you.&lt;/p&gt;
&lt;p&gt;Some dislike it because you are aggregating without taking into account the association between the indicators; it is a blind procedure based on theory/assumptions rather than maths. ¯_(ツ)_/¯&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;types-of-longitudinal-models-other-than-growth-models-brief-intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Types of longitudinal models other than growth models (brief intro)&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long &amp;lt;- read.csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/SEM_workshop/longitudinal.csv&amp;quot;)

summary(long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     PosAFF11        PosAFF21         PosAFF31        NegAFF11      
##  Min.   :1.365   Min.   :0.4152   Min.   :1.140   Min.   :-0.8584  
##  1st Qu.:2.739   1st Qu.:2.6343   1st Qu.:2.797   1st Qu.: 1.1035  
##  Median :3.209   Median :3.1143   Median :3.204   Median : 1.5075  
##  Mean   :3.212   Mean   :3.1050   Mean   :3.248   Mean   : 1.5220  
##  3rd Qu.:3.688   3rd Qu.:3.6216   3rd Qu.:3.775   3rd Qu.: 1.9815  
##  Max.   :5.804   Max.   :6.1970   Max.   :6.048   Max.   : 3.2403  
##     NegAFF21          NegAFF31          PosAFF12        PosAFF22     
##  Min.   :-0.3991   Min.   :-0.5606   Min.   :1.528   Min.   :0.6575  
##  1st Qu.: 1.0229   1st Qu.: 1.0100   1st Qu.:2.852   1st Qu.:2.6571  
##  Median : 1.3718   Median : 1.4335   Median :3.215   Median :3.1206  
##  Mean   : 1.3971   Mean   : 1.3981   Mean   :3.253   Mean   :3.1256  
##  3rd Qu.: 1.7566   3rd Qu.: 1.8101   3rd Qu.:3.637   3rd Qu.:3.5467  
##  Max.   : 2.9844   Max.   : 2.7674   Max.   :5.413   Max.   :5.4420  
##     PosAFF32         NegAFF12         NegAFF22         NegAFF32       
##  Min.   :0.7369   Min.   :0.1797   Min.   :0.1784   Min.   :-0.03494  
##  1st Qu.:2.8484   1st Qu.:1.1464   1st Qu.:0.9963   1st Qu.: 1.02027  
##  Median :3.2692   Median :1.3818   Median :1.3172   Median : 1.31692  
##  Mean   :3.2737   Mean   :1.4115   Mean   :1.3237   Mean   : 1.30002  
##  3rd Qu.:3.7170   3rd Qu.:1.7251   3rd Qu.:1.6382   3rd Qu.: 1.56441  
##  Max.   :5.9676   Max.   :2.5033   Max.   :2.5587   Max.   : 2.44236  
##     PosAFF13        PosAFF23         PosAFF33        NegAFF13       
##  Min.   :1.307   Min.   :0.8057   Min.   :1.629   Min.   :-0.01837  
##  1st Qu.:2.979   1st Qu.:2.7147   1st Qu.:2.858   1st Qu.: 1.15739  
##  Median :3.299   Median :3.0832   Median :3.325   Median : 1.43937  
##  Mean   :3.302   Mean   :3.0945   Mean   :3.280   Mean   : 1.43015  
##  3rd Qu.:3.683   3rd Qu.:3.5296   3rd Qu.:3.698   3rd Qu.: 1.73650  
##  Max.   :4.712   Max.   :4.8007   Max.   :5.014   Max.   : 2.75085  
##     NegAFF23        NegAFF33     
##  Min.   :0.147   Min.   :0.3145  
##  1st Qu.:1.009   1st Qu.:1.0261  
##  Median :1.294   Median :1.3154  
##  Mean   :1.281   Mean   :1.2974  
##  3rd Qu.:1.560   3rd Qu.:1.5583  
##  Max.   :2.447   Max.   :2.6385&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;longitudinal-cfa&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Longitudinal CFA&lt;/h3&gt;
&lt;p&gt;key concerns:
1. Should the correlations be the same across time?
2. Should the error variances be correlated?
3. Are the loadings the same across time? (more on this later)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long.cfa &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ PosAFF11 + PosAFF21 + PosAFF31
Pos2 =~ PosAFF12 + PosAFF22 + PosAFF32
Pos3 =~ PosAFF13 + PosAFF23 + PosAFF33
Neg1 =~ NegAFF11 + NegAFF21 + NegAFF31
Neg2 =~ NegAFF12 + NegAFF22 + NegAFF32
Neg3 =~ NegAFF13 + NegAFF23 + NegAFF33

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33

&amp;#39;

fit.long.cfa &amp;lt;- cfa(long.cfa, data=long, std.lv=TRUE)

summary(fit.long.cfa, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 128 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         69
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     119.443
##   Degrees of freedom                               102
##   P-value (Chi-square)                           0.114
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             5253.085
##   Degrees of freedom                               153
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.997
##   Tucker-Lewis Index (TLI)                       0.995
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3060.353
##   Loglikelihood unrestricted model (H1)      -3000.632
## 
##   Number of free parameters                         69
##   Akaike (AIC)                                6258.707
##   Bayesian (BIC)                              6528.365
##   Sample-size adjusted Bayesian (BIC)         6309.453
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.022
##   90 Percent Confidence Interval          0.000  0.036
##   P-value RMSEA &amp;lt;= 0.05                          1.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.028
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11          0.654    0.030   21.936    0.000    0.654    0.903
##     PosAFF21          0.651    0.031   20.864    0.000    0.651    0.875
##     PosAFF31          0.685    0.031   22.361    0.000    0.685    0.912
##   Pos2 =~                                                               
##     PosAFF12          0.556    0.026   21.256    0.000    0.556    0.883
##     PosAFF22          0.638    0.030   21.448    0.000    0.638    0.887
##     PosAFF32          0.644    0.027   23.567    0.000    0.644    0.940
##   Pos3 =~                                                               
##     PosAFF13          0.508    0.024   21.028    0.000    0.508    0.887
##     PosAFF23          0.545    0.027   20.347    0.000    0.545    0.867
##     PosAFF33          0.538    0.026   20.827    0.000    0.538    0.879
##   Neg1 =~                                                               
##     NegAFF11          0.563    0.028   20.465    0.000    0.563    0.868
##     NegAFF21          0.479    0.024   19.856    0.000    0.479    0.847
##     NegAFF31          0.555    0.025   22.373    0.000    0.555    0.920
##   Neg2 =~                                                               
##     NegAFF12          0.365    0.019   18.989    0.000    0.365    0.826
##     NegAFF22          0.375    0.017   21.452    0.000    0.375    0.889
##     NegAFF32          0.368    0.017   21.383    0.000    0.368    0.896
##   Neg3 =~                                                               
##     NegAFF13          0.363    0.021   17.128    0.000    0.363    0.782
##     NegAFF23          0.341    0.017   19.493    0.000    0.341    0.855
##     NegAFF33          0.344    0.017   19.700    0.000    0.344    0.869
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.004    0.007    0.578    0.563    0.004    0.043
##    .PosAFF13          0.000    0.007    0.037    0.971    0.000    0.003
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.004    0.006    0.674    0.500    0.004    0.050
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.008    0.008    1.020    0.308    0.008    0.071
##    .PosAFF23          0.008    0.008    0.991    0.322    0.008    0.070
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.011    0.007    1.470    0.142    0.011    0.104
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.004    0.007    0.616    0.538    0.004    0.057
##    .PosAFF33          0.016    0.007    2.182    0.029    0.016    0.177
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.004    0.006    0.690    0.490    0.004    0.061
##  .NegAFF11 ~~                                                           
##    .NegAFF12          0.005    0.005    0.966    0.334    0.005    0.065
##    .NegAFF13          0.006    0.006    1.036    0.300    0.006    0.070
##  .NegAFF12 ~~                                                           
##    .NegAFF13          0.007    0.005    1.528    0.126    0.007    0.099
##  .NegAFF21 ~~                                                           
##    .NegAFF22          0.015    0.004    3.605    0.000    0.015    0.267
##    .NegAFF23          0.011    0.005    2.387    0.017    0.011    0.173
##  .NegAFF22 ~~                                                           
##    .NegAFF23          0.010    0.003    3.145    0.002    0.010    0.253
##  .NegAFF31 ~~                                                           
##    .NegAFF32         -0.006    0.004   -1.607    0.108   -0.006   -0.147
##    .NegAFF33         -0.008    0.004   -1.778    0.075   -0.008   -0.163
##  .NegAFF32 ~~                                                           
##    .NegAFF33         -0.001    0.003   -0.481    0.630   -0.001   -0.041
##   Pos1 ~~                                                               
##     Pos2              0.473    0.044   10.663    0.000    0.473    0.473
##     Pos3              0.399    0.048    8.228    0.000    0.399    0.399
##     Neg1             -0.436    0.047   -9.358    0.000   -0.436   -0.436
##     Neg2             -0.297    0.052   -5.706    0.000   -0.297   -0.297
##     Neg3             -0.169    0.056   -3.003    0.003   -0.169   -0.169
##   Pos2 ~~                                                               
##     Pos3              0.449    0.046    9.777    0.000    0.449    0.449
##     Neg1             -0.179    0.054   -3.279    0.001   -0.179   -0.179
##     Neg2             -0.543    0.041  -13.203    0.000   -0.543   -0.543
##     Neg3             -0.198    0.055   -3.578    0.000   -0.198   -0.198
##   Pos3 ~~                                                               
##     Neg1             -0.074    0.057   -1.304    0.192   -0.074   -0.074
##     Neg2             -0.167    0.056   -2.989    0.003   -0.167   -0.167
##     Neg3             -0.292    0.054   -5.442    0.000   -0.292   -0.292
##   Neg1 ~~                                                               
##     Neg2              0.526    0.043   12.317    0.000    0.526    0.526
##     Neg3              0.351    0.052    6.778    0.000    0.351    0.351
##   Neg2 ~~                                                               
##     Neg3              0.435    0.048    9.006    0.000    0.435    0.435
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .PosAFF11          0.096    0.011    8.497    0.000    0.096    0.184
##    .PosAFF21          0.130    0.013    9.956    0.000    0.130    0.235
##    .PosAFF31          0.095    0.012    7.944    0.000    0.095    0.168
##    .PosAFF12          0.087    0.009   10.044    0.000    0.087    0.220
##    .PosAFF22          0.110    0.011    9.883    0.000    0.110    0.213
##    .PosAFF32          0.055    0.009    6.438    0.000    0.055    0.117
##    .PosAFF13          0.070    0.008    8.319    0.000    0.070    0.214
##    .PosAFF23          0.098    0.011    9.317    0.000    0.098    0.249
##    .PosAFF33          0.085    0.010    8.716    0.000    0.085    0.227
##    .NegAFF11          0.104    0.011    9.546    0.000    0.104    0.246
##    .NegAFF21          0.091    0.009   10.363    0.000    0.091    0.283
##    .NegAFF31          0.056    0.009    6.475    0.000    0.056    0.153
##    .NegAFF12          0.062    0.006   10.835    0.000    0.062    0.317
##    .NegAFF22          0.037    0.004    8.445    0.000    0.037    0.209
##    .NegAFF32          0.033    0.004    7.917    0.000    0.033    0.198
##    .NegAFF13          0.084    0.008   10.660    0.000    0.084    0.389
##    .NegAFF23          0.043    0.005    8.170    0.000    0.043    0.270
##    .NegAFF33          0.038    0.005    7.372    0.000    0.038    0.245
##     Pos1              1.000                               1.000    1.000
##     Pos2              1.000                               1.000    1.000
##     Pos3              1.000                               1.000    1.000
##     Neg1              1.000                               1.000    1.000
##     Neg2              1.000                               1.000    1.000
##     Neg3              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.long.cfa)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;longitudinal-path-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Longitudinal Path Model&lt;/h3&gt;
&lt;p&gt;key concerns:
1. Should the regressions be the same across time?
2. Should the error variances be correlated?
3. Are the loadings the same across time? (more on this later)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long.path &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33
Neg1 =~ L4*NegAFF11 + L5*NegAFF21 + L6*NegAFF31
Neg2 =~ L4*NegAFF12 + L5*NegAFF22 + L6*NegAFF32
Neg3 =~ L4*NegAFF13 + L5*NegAFF23 + L6*NegAFF33

## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3
Neg2 ~~ NA*Neg2
Neg3 ~~ NA*Neg3
Pos1 ~~ Neg1
Pos2 ~~ Neg2
Pos3 ~~ Neg3

## directional regression paths
Pos2 ~ Pos1
Pos3 ~ Pos2
Neg2 ~ Neg1
Neg3 ~ Neg2

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33
&amp;#39;

fit.long.path &amp;lt;- sem(long.path, data=long, std.lv=TRUE)

summary(fit.long.path, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 133 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         65
##   Number of equality constraints                    12
##   Row rank of the constraints matrix                12
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     170.843
##   Degrees of freedom                               118
##   P-value (Chi-square)                           0.001
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             5253.085
##   Degrees of freedom                               153
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.990
##   Tucker-Lewis Index (TLI)                       0.987
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3086.053
##   Loglikelihood unrestricted model (H1)      -3000.632
## 
##   Number of free parameters                         53
##   Akaike (AIC)                                6278.107
##   Bayesian (BIC)                              6485.235
##   Sample-size adjusted Bayesian (BIC)         6317.086
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.035
##   90 Percent Confidence Interval          0.023  0.046
##   P-value RMSEA &amp;lt;= 0.05                          0.989
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.055
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11  (L1)    0.630    0.027   23.609    0.000    0.630    0.892
##     PosAFF21  (L2)    0.673    0.029   23.387    0.000    0.673    0.884
##     PosAFF31  (L3)    0.686    0.029   23.966    0.000    0.686    0.913
##   Pos2 =~                                                               
##     PosAFF12  (L1)    0.630    0.027   23.609    0.000    0.575    0.893
##     PosAFF22  (L2)    0.673    0.029   23.387    0.000    0.614    0.878
##     PosAFF32  (L3)    0.686    0.029   23.966    0.000    0.626    0.932
##   Pos3 =~                                                               
##     PosAFF13  (L1)    0.630    0.027   23.609    0.000    0.504    0.884
##     PosAFF23  (L2)    0.673    0.029   23.387    0.000    0.539    0.861
##     PosAFF33  (L3)    0.686    0.029   23.966    0.000    0.549    0.887
##   Neg1 =~                                                               
##     NegAFF11  (L4)    0.546    0.024   22.398    0.000    0.546    0.859
##     NegAFF21  (L5)    0.510    0.023   22.505    0.000    0.510    0.868
##     NegAFF31  (L6)    0.537    0.023   23.717    0.000    0.537    0.908
##   Neg2 =~                                                               
##     NegAFF12  (L4)    0.546    0.024   22.398    0.000    0.384    0.841
##     NegAFF22  (L5)    0.510    0.023   22.505    0.000    0.358    0.871
##     NegAFF32  (L6)    0.537    0.023   23.717    0.000    0.377    0.904
##   Neg3 =~                                                               
##     NegAFF13  (L4)    0.546    0.024   22.398    0.000    0.362    0.780
##     NegAFF23  (L5)    0.510    0.023   22.505    0.000    0.338    0.847
##     NegAFF33  (L6)    0.537    0.023   23.717    0.000    0.356    0.883
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos2 ~                                                                
##     Pos1              0.416    0.042   10.020    0.000    0.456    0.456
##   Pos3 ~                                                                
##     Pos2              0.404    0.044    9.207    0.000    0.460    0.460
##   Neg2 ~                                                                
##     Neg1              0.382    0.031   12.203    0.000    0.544    0.544
##   Neg3 ~                                                                
##     Neg2              0.432    0.049    8.867    0.000    0.457    0.457
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 ~~                                                               
##     Neg1             -0.441    0.046   -9.502    0.000   -0.441   -0.441
##  .Pos2 ~~                                                               
##    .Neg2             -0.269    0.036   -7.544    0.000   -0.561   -0.561
##  .Pos3 ~~                                                               
##    .Neg3             -0.125    0.027   -4.596    0.000   -0.298   -0.298
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.004    0.007    0.536    0.592    0.004    0.039
##    .PosAFF13          0.003    0.007    0.386    0.699    0.003    0.030
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.003    0.006    0.585    0.559    0.003    0.044
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.007    0.008    0.865    0.387    0.007    0.061
##    .PosAFF23          0.008    0.008    1.030    0.303    0.008    0.074
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.011    0.007    1.516    0.130    0.011    0.106
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.005    0.007    0.705    0.481    0.005    0.064
##    .PosAFF33          0.016    0.007    2.173    0.030    0.016    0.180
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.004    0.006    0.580    0.562    0.004    0.051
##  .NegAFF11 ~~                                                           
##    .NegAFF12          0.005    0.005    0.947    0.344    0.005    0.064
##    .NegAFF13          0.007    0.006    1.107    0.268    0.007    0.073
##  .NegAFF12 ~~                                                           
##    .NegAFF13          0.007    0.005    1.539    0.124    0.007    0.100
##  .NegAFF21 ~~                                                           
##    .NegAFF22          0.015    0.004    3.399    0.001    0.015    0.249
##    .NegAFF23          0.010    0.005    2.217    0.027    0.010    0.163
##  .NegAFF22 ~~                                                           
##    .NegAFF23          0.011    0.003    3.430    0.001    0.011    0.259
##  .NegAFF31 ~~                                                           
##    .NegAFF32         -0.007    0.004   -1.724    0.085   -0.007   -0.155
##    .NegAFF33         -0.007    0.004   -1.587    0.113   -0.007   -0.143
##  .NegAFF32 ~~                                                           
##    .NegAFF33         -0.002    0.003   -0.734    0.463   -0.002   -0.066
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .Pos2              0.660    0.075    8.760    0.000    0.792    0.792
##    .Pos3              0.504    0.058    8.628    0.000    0.788    0.788
##    .Neg2              0.347    0.041    8.458    0.000    0.704    0.704
##    .Neg3              0.347    0.041    8.409    0.000    0.791    0.791
##    .PosAFF11          0.102    0.011    9.329    0.000    0.102    0.204
##    .PosAFF21          0.126    0.013    9.699    0.000    0.126    0.218
##    .PosAFF31          0.094    0.012    8.132    0.000    0.094    0.166
##    .PosAFF12          0.084    0.009    9.650    0.000    0.084    0.202
##    .PosAFF22          0.112    0.011   10.307    0.000    0.112    0.229
##    .PosAFF32          0.059    0.008    7.215    0.000    0.059    0.131
##    .PosAFF13          0.071    0.008    8.813    0.000    0.071    0.218
##    .PosAFF23          0.101    0.010    9.833    0.000    0.101    0.259
##    .PosAFF33          0.082    0.009    8.703    0.000    0.082    0.214
##    .NegAFF11          0.106    0.010   10.098    0.000    0.106    0.262
##    .NegAFF21          0.085    0.009    9.768    0.000    0.085    0.247
##    .NegAFF31          0.062    0.008    7.633    0.000    0.062    0.176
##    .NegAFF12          0.061    0.006   10.625    0.000    0.061    0.292
##    .NegAFF22          0.041    0.004    9.619    0.000    0.041    0.242
##    .NegAFF32          0.032    0.004    7.781    0.000    0.032    0.182
##    .NegAFF13          0.084    0.008   11.031    0.000    0.084    0.392
##    .NegAFF23          0.045    0.005    9.102    0.000    0.045    0.282
##    .NegAFF33          0.036    0.005    7.466    0.000    0.036    0.221
##     Pos1              1.000                               1.000    1.000
##     Neg1              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.long.path, layout = &amp;quot;tree3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## layout can also be done manually to get publications worthy plots&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;longitudinal-cross-lagged-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Longitudinal Cross lagged model&lt;/h3&gt;
&lt;p&gt;key concerns:
1. Should the regressions (both cross lagged and autoregressive) be the same across time?
2. Should the indicator error variances be correlated (within time or within construct)?
3. Are the loadings the same across time? (more on this later)
4. Are the latent error variances the same or different?
5. Are the latent error variances correlated the same or different across time?
6. Are there more lagged effects?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long.cross &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33
Neg1 =~ L4*NegAFF11 + L5*NegAFF21 + L6*NegAFF31
Neg2 =~ L4*NegAFF12 + L5*NegAFF22 + L6*NegAFF32
Neg3 =~ L4*NegAFF13 + L5*NegAFF23 + L6*NegAFF33

## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3
Neg2 ~~ NA*Neg2
Neg3 ~~ NA*Neg3

Pos1 ~~ Neg1
Pos2 ~~ Neg2
Pos3 ~~ Neg3

## directional regression paths
Pos2 ~ Pos1 + Neg1
Neg2 ~ Pos1 + Neg1
Pos3 ~ Pos2 + Neg2
Neg3 ~ Pos2 + Neg2

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33
&amp;#39;

fit.long.cross &amp;lt;- sem(long.cross,data=long, std.lv=TRUE)

summary(fit.long.cross, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 137 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         69
##   Number of equality constraints                    12
##   Row rank of the constraints matrix                12
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     163.406
##   Degrees of freedom                               114
##   P-value (Chi-square)                           0.002
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             5253.085
##   Degrees of freedom                               153
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.990
##   Tucker-Lewis Index (TLI)                       0.987
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3082.335
##   Loglikelihood unrestricted model (H1)      -3000.632
## 
##   Number of free parameters                         57
##   Akaike (AIC)                                6278.669
##   Bayesian (BIC)                              6501.430
##   Sample-size adjusted Bayesian (BIC)         6320.590
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.034
##   90 Percent Confidence Interval          0.022  0.046
##   P-value RMSEA &amp;lt;= 0.05                          0.990
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.051
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11  (L1)    0.630    0.027   23.619    0.000    0.630    0.892
##     PosAFF21  (L2)    0.673    0.029   23.393    0.000    0.673    0.884
##     PosAFF31  (L3)    0.686    0.029   23.990    0.000    0.686    0.914
##   Pos2 =~                                                               
##     PosAFF12  (L1)    0.630    0.027   23.619    0.000    0.582    0.896
##     PosAFF22  (L2)    0.673    0.029   23.393    0.000    0.622    0.880
##     PosAFF32  (L3)    0.686    0.029   23.990    0.000    0.634    0.933
##   Pos3 =~                                                               
##     PosAFF13  (L1)    0.630    0.027   23.619    0.000    0.503    0.884
##     PosAFF23  (L2)    0.673    0.029   23.393    0.000    0.537    0.861
##     PosAFF33  (L3)    0.686    0.029   23.990    0.000    0.547    0.886
##   Neg1 =~                                                               
##     NegAFF11  (L4)    0.547    0.024   22.394    0.000    0.547    0.860
##     NegAFF21  (L5)    0.510    0.023   22.488    0.000    0.510    0.868
##     NegAFF31  (L6)    0.538    0.023   23.710    0.000    0.538    0.908
##   Neg2 =~                                                               
##     NegAFF12  (L4)    0.547    0.024   22.394    0.000    0.382    0.840
##     NegAFF22  (L5)    0.510    0.023   22.488    0.000    0.356    0.869
##     NegAFF32  (L6)    0.538    0.023   23.710    0.000    0.375    0.903
##   Neg3 =~                                                               
##     NegAFF13  (L4)    0.547    0.024   22.394    0.000    0.358    0.777
##     NegAFF23  (L5)    0.510    0.023   22.488    0.000    0.334    0.844
##     NegAFF33  (L6)    0.538    0.023   23.710    0.000    0.352    0.881
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos2 ~                                                                
##     Pos1              0.463    0.052    8.829    0.000    0.501    0.501
##     Neg1              0.039    0.053    0.746    0.456    0.042    0.042
##   Neg2 ~                                                                
##     Pos1             -0.057    0.039   -1.454    0.146   -0.081   -0.081
##     Neg1              0.347    0.039    8.812    0.000    0.498    0.498
##   Pos3 ~                                                                
##     Pos2              0.451    0.054    8.307    0.000    0.522    0.522
##     Neg2              0.134    0.073    1.843    0.065    0.117    0.117
##   Neg3 ~                                                                
##     Pos2              0.046    0.046    1.000    0.317    0.065    0.065
##     Neg2              0.446    0.062    7.239    0.000    0.475    0.475
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 ~~                                                               
##     Neg1             -0.437    0.047   -9.375    0.000   -0.437   -0.437
##  .Pos2 ~~                                                               
##    .Neg2             -0.269    0.036   -7.567    0.000   -0.566   -0.566
##  .Pos3 ~~                                                               
##    .Neg3             -0.127    0.027   -4.710    0.000   -0.308   -0.308
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.004    0.007    0.529    0.597    0.004    0.039
##    .PosAFF13          0.002    0.007    0.371    0.711    0.002    0.028
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.003    0.006    0.569    0.569    0.003    0.043
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.007    0.008    0.869    0.385    0.007    0.061
##    .PosAFF23          0.008    0.008    0.964    0.335    0.008    0.069
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.011    0.007    1.416    0.157    0.011    0.099
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.004    0.007    0.649    0.516    0.004    0.059
##    .PosAFF33          0.016    0.007    2.257    0.024    0.016    0.187
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.004    0.006    0.580    0.562    0.004    0.050
##  .NegAFF11 ~~                                                           
##    .NegAFF12          0.005    0.005    0.986    0.324    0.005    0.067
##    .NegAFF13          0.007    0.006    1.088    0.277    0.007    0.072
##  .NegAFF12 ~~                                                           
##    .NegAFF13          0.007    0.005    1.537    0.124    0.007    0.100
##  .NegAFF21 ~~                                                           
##    .NegAFF22          0.015    0.004    3.440    0.001    0.015    0.252
##    .NegAFF23          0.010    0.005    2.238    0.025    0.010    0.165
##  .NegAFF22 ~~                                                           
##    .NegAFF23          0.011    0.003    3.397    0.001    0.011    0.256
##  .NegAFF31 ~~                                                           
##    .NegAFF32         -0.007    0.004   -1.748    0.080   -0.007   -0.157
##    .NegAFF33         -0.007    0.004   -1.602    0.109   -0.007   -0.145
##  .NegAFF32 ~~                                                           
##    .NegAFF33         -0.002    0.003   -0.751    0.453   -0.002   -0.068
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .Pos2              0.653    0.075    8.735    0.000    0.765    0.765
##    .Pos3              0.496    0.058    8.594    0.000    0.780    0.780
##    .Neg2              0.346    0.041    8.475    0.000    0.710    0.710
##    .Neg3              0.345    0.041    8.396    0.000    0.804    0.804
##    .PosAFF11          0.102    0.011    9.359    0.000    0.102    0.205
##    .PosAFF21          0.127    0.013    9.731    0.000    0.127    0.219
##    .PosAFF31          0.093    0.012    8.100    0.000    0.093    0.165
##    .PosAFF12          0.083    0.009    9.656    0.000    0.083    0.198
##    .PosAFF22          0.113    0.011   10.344    0.000    0.113    0.226
##    .PosAFF32          0.060    0.008    7.270    0.000    0.060    0.129
##    .PosAFF13          0.071    0.008    8.814    0.000    0.071    0.219
##    .PosAFF23          0.101    0.010    9.817    0.000    0.101    0.259
##    .PosAFF33          0.082    0.009    8.730    0.000    0.082    0.216
##    .NegAFF11          0.105    0.010   10.063    0.000    0.105    0.260
##    .NegAFF21          0.085    0.009    9.775    0.000    0.085    0.247
##    .NegAFF31          0.062    0.008    7.599    0.000    0.062    0.176
##    .NegAFF12          0.061    0.006   10.647    0.000    0.061    0.295
##    .NegAFF22          0.041    0.004    9.668    0.000    0.041    0.245
##    .NegAFF32          0.032    0.004    7.810    0.000    0.032    0.184
##    .NegAFF13          0.084    0.008   11.023    0.000    0.084    0.396
##    .NegAFF23          0.045    0.005    9.106    0.000    0.045    0.287
##    .NegAFF33          0.036    0.005    7.452    0.000    0.036    0.224
##     Pos1              1.000                               1.000    1.000
##     Neg1              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.long.cross)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.long.cross, layout = &amp;quot;tree3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;longitudinal-mediation-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Longitudinal mediation model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Do Self-Reported Social Experiences Mediate the Effect of Extraversion on Life Satisfaction and Happiness?
#number close friends
library(readr)
TSS_sub &amp;lt;- read_csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/TSS_sub.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Missing column names filled in: &amp;#39;X1&amp;#39; [1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   .default = col_double(),
##   f1acode = col_character(),
##   h1acode = col_character(),
##   h1lbf01 = col_character(),
##   h1dtf01 = col_character(),
##   h1aef01 = col_character(),
##   j1free = col_character(),
##   k1major = col_character(),
##   k1attpar = col_character(),
##   m1major = col_character(),
##   m1attpar = col_character(),
##   i1knwots = col_character(),
##   i2knwots = col_character(),
##   i3knwots = col_character(),
##   i4knwots = col_logical(),
##   n1knwots = col_character(),
##   n2knwots = col_character(),
##   n3knwots = col_character(),
##   n4knwots = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See spec(...) for full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scon.model6&amp;lt;-&amp;#39;
# definine extraversion
bfie =~ a1bfi01 + a1bfi06r + a1bfi11 + a1bfi16 + a1bfi21r + a1bfi26 + a1bfi31r + a1bfi36

# correlated residuals
a1bfi11 ~~  a1bfi16
a1bfi06r ~~ a1bfi21r + a1bfi31r
a1bfi21r ~~ a1bfi31r + a1bfi01

#define social connection at 4 waves
hconnect=~h1clrel + h1satfr + h1sosat + h1ced05
jconnect=~j1clrel + j1satfr + j1sosat + j1ced05
kconnect=~k1clrel + k1satfr + k1sosat + k1ced05
mconnect=~m1clrel + m1satfr + m1sosat + m1ced05

#correlate residuals
h1clrel ~~ j1clrel + k1clrel + m1clrel
j1clrel ~~ k1clrel + m1clrel
k1clrel ~~ m1clrel
h1satfr ~~ j1satfr + k1satfr + m1satfr
j1satfr ~~ k1satfr + m1satfr
k1satfr ~~ m1satfr 
h1sosat ~~ j1sosat + k1sosat + m1sosat 
j1sosat ~~ k1sosat + m1sosat 
k1sosat ~~ m1sosat
h1ced05 ~~ j1ced05 + k1ced05 + m1ced05
j1ced05 ~~ k1ced05 + m1ced05
k1ced05 ~~ m1ced05

# same time covariances between extraversion, connection, satisfaction
bfie~~a1swls
hconnect ~~ h1swls
jconnect ~~ j1swls
kconnect ~~ k1swls

#regressions to calculate indiret effects
hconnect ~ a1*bfie + d1*a1swls 
jconnect ~ a2*bfie + d2*h1swls + m1*hconnect
kconnect ~ a3*bfie + d3*j1swls + m2*jconnect
mconnect ~ a4*bfie + d4*k1swls + m3*kconnect
h1swls ~ y1*a1swls + c1*bfie 
j1swls ~ y2*h1swls + c2*bfie + b1*hconnect
k1swls ~ y3*j1swls + c3*bfie + b2*jconnect 
m1swls ~ y4*k1swls + c4*bfie + b3*kconnect

#effects 
# extraversion -&amp;gt; connect (a)
# connect -&amp;gt;  swb (b)
# extraversion -&amp;gt; swb (c)
# auto-regressive connection (m)
# auto-regressive swb (y)

ind:= a1*b1*y3*y4 + a1*m1*b2*y4 + a1*m1*m2*b3 + a2*b2*y4 + a2*m2*b3 + a3*b3
total:= ind + c4 + c3*y4 + c2*y3*y4 + c1*y2*y3*y4
&amp;#39;
scon62 &amp;lt;- sem(scon.model6, data=TSS_sub, missing = &amp;quot;ml&amp;quot;, fixed.x = FALSE)
summary(scon62, standardized=T, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 113 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                        138
## 
##   Number of observations                           393
##   Number of missing patterns                        30
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     600.051
##   Degrees of freedom                               326
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             4447.119
##   Degrees of freedom                               406
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.932
##   Tucker-Lewis Index (TLI)                       0.916
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -13558.593
##   Loglikelihood unrestricted model (H1)     -13258.568
## 
##   Number of free parameters                        138
##   Akaike (AIC)                               27393.187
##   Bayesian (BIC)                             27941.573
##   Sample-size adjusted Bayesian (BIC)        27503.702
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.046
##   90 Percent Confidence Interval          0.040  0.052
##   P-value RMSEA &amp;lt;= 0.05                          0.854
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.068
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   bfie =~                                                               
##     a1bfi01           1.000                               0.905    0.728
##     a1bfi06r          0.813    0.072   11.336    0.000    0.735    0.601
##     a1bfi11           0.605    0.056   10.808    0.000    0.547    0.592
##     a1bfi16           0.603    0.054   11.084    0.000    0.545    0.604
##     a1bfi21r          0.951    0.063   15.013    0.000    0.860    0.700
##     a1bfi26           0.806    0.067   12.066    0.000    0.729    0.648
##     a1bfi31r          0.823    0.072   11.471    0.000    0.744    0.618
##     a1bfi36           1.064    0.068   15.561    0.000    0.962    0.871
##   hconnect =~                                                           
##     h1clrel           1.000                               1.006    0.681
##     h1satfr           1.014    0.119    8.552    0.000    1.020    0.589
##     h1sosat           1.086    0.116    9.389    0.000    1.093    0.747
##     h1ced05          -0.562    0.066   -8.470    0.000   -0.565   -0.644
##   jconnect =~                                                           
##     j1clrel           1.000                               0.876    0.661
##     j1satfr           1.226    0.126    9.754    0.000    1.074    0.645
##     j1sosat           1.145    0.114   10.055    0.000    1.003    0.754
##     j1ced05          -0.567    0.066   -8.548    0.000   -0.497   -0.597
##   kconnect =~                                                           
##     k1clrel           1.000                               0.830    0.635
##     k1satfr           1.221    0.144    8.485    0.000    1.014    0.611
##     k1sosat           1.097    0.137    7.984    0.000    0.911    0.607
##     k1ced05          -0.612    0.076   -8.028    0.000   -0.508   -0.574
##   mconnect =~                                                           
##     m1clrel           1.000                               0.755    0.662
##     m1satfr           1.172    0.109   10.797    0.000    0.885    0.622
##     m1sosat           1.261    0.120   10.492    0.000    0.952    0.693
##     m1ced05          -0.656    0.068   -9.580    0.000   -0.495   -0.598
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   hconnect ~                                                            
##     bfie      (a1)    0.224    0.082    2.728    0.006    0.202    0.202
##     a1swls    (d1)    0.372    0.062    5.963    0.000    0.370    0.427
##   jconnect ~                                                            
##     bfie      (a2)    0.099    0.071    1.403    0.161    0.102    0.102
##     h1swls    (d2)    0.034    0.065    0.528    0.597    0.039    0.054
##     hconnect  (m1)    0.385    0.113    3.417    0.001    0.443    0.443
##   kconnect ~                                                            
##     bfie      (a3)    0.153    0.062    2.455    0.014    0.167    0.167
##     j1swls    (d3)    0.142    0.060    2.380    0.017    0.172    0.209
##     jconnect  (m2)    0.391    0.101    3.858    0.000    0.412    0.412
##   mconnect ~                                                            
##     bfie      (a4)    0.170    0.051    3.302    0.001    0.204    0.204
##     k1swls    (d4)   -0.070    0.052   -1.352    0.177   -0.093   -0.120
##     kconnect  (m3)    0.671    0.110    6.085    0.000    0.738    0.738
##   h1swls ~                                                              
##     a1swls    (y1)    0.564    0.068    8.289    0.000    0.564    0.468
##     bfie      (c1)    0.070    0.093    0.754    0.451    0.063    0.046
##   j1swls ~                                                              
##     h1swls    (y2)    0.364    0.075    4.839    0.000    0.364    0.415
##     bfie      (c2)    0.079    0.082    0.958    0.338    0.071    0.059
##     hconnect  (b1)    0.097    0.129    0.746    0.455    0.097    0.080
##   k1swls ~                                                              
##     j1swls    (y3)    0.538    0.076    7.091    0.000    0.538    0.507
##     bfie      (c3)    0.271    0.079    3.446    0.001    0.245    0.190
##     jconnect  (b2)    0.009    0.126    0.072    0.943    0.008    0.006
##   m1swls ~                                                              
##     k1swls    (y4)    0.338    0.068    4.944    0.000    0.338    0.362
##     bfie      (c4)    0.005    0.066    0.076    0.940    0.004    0.004
##     kconnect  (b3)    0.562    0.137    4.112    0.000    0.466    0.387
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##  .a1bfi11 ~~                                                            
##    .a1bfi16           0.181    0.032    5.659    0.000    0.181    0.337
##  .a1bfi06r ~~                                                           
##    .a1bfi21r          0.355    0.052    6.900    0.000    0.355    0.414
##    .a1bfi31r          0.325    0.056    5.786    0.000    0.325    0.351
##  .a1bfi21r ~~                                                           
##    .a1bfi31r          0.342    0.050    6.837    0.000    0.342    0.411
##  .a1bfi01 ~~                                                            
##    .a1bfi21r          0.178    0.040    4.518    0.000    0.178    0.238
##  .h1clrel ~~                                                            
##    .j1clrel           0.225    0.090    2.494    0.013    0.225    0.210
##    .k1clrel           0.234    0.090    2.603    0.009    0.234    0.215
##    .m1clrel           0.358    0.073    4.883    0.000    0.358    0.386
##  .j1clrel ~~                                                            
##    .k1clrel           0.334    0.082    4.100    0.000    0.334    0.333
##    .m1clrel           0.257    0.061    4.189    0.000    0.257    0.302
##  .k1clrel ~~                                                            
##    .m1clrel           0.277    0.065    4.259    0.000    0.277    0.321
##  .h1satfr ~~                                                            
##    .j1satfr           0.238    0.136    1.753    0.080    0.238    0.134
##    .k1satfr           0.273    0.145    1.886    0.059    0.273    0.149
##    .m1satfr           0.274    0.108    2.539    0.011    0.274    0.176
##  .j1satfr ~~                                                            
##    .k1satfr           0.471    0.127    3.700    0.000    0.471    0.282
##    .m1satfr           0.417    0.099    4.231    0.000    0.417    0.294
##  .k1satfr ~~                                                            
##    .m1satfr           0.662    0.107    6.207    0.000    0.662    0.452
##  .h1sosat ~~                                                            
##    .j1sosat           0.010    0.078    0.129    0.897    0.010    0.012
##    .k1sosat           0.021    0.107    0.198    0.843    0.021    0.018
##    .m1sosat           0.003    0.077    0.039    0.969    0.003    0.003
##  .j1sosat ~~                                                            
##    .k1sosat           0.135    0.088    1.546    0.122    0.135    0.130
##    .m1sosat           0.070    0.069    1.005    0.315    0.070    0.080
##  .k1sosat ~~                                                            
##    .m1sosat           0.301    0.092    3.256    0.001    0.301    0.254
##  .h1ced05 ~~                                                            
##    .j1ced05           0.122    0.034    3.624    0.000    0.122    0.272
##    .k1ced05           0.094    0.037    2.562    0.010    0.094    0.194
##    .m1ced05           0.069    0.032    2.128    0.033    0.069    0.154
##  .j1ced05 ~~                                                            
##    .k1ced05           0.101    0.034    2.968    0.003    0.101    0.209
##    .m1ced05           0.101    0.029    3.466    0.001    0.101    0.229
##  .k1ced05 ~~                                                            
##    .m1ced05           0.171    0.035    4.898    0.000    0.171    0.355
##   bfie ~~                                                               
##     a1swls            0.379    0.062    6.098    0.000    0.418    0.363
##  .hconnect ~~                                                           
##    .h1swls            0.569    0.091    6.242    0.000    0.668    0.551
##  .jconnect ~~                                                           
##    .j1swls            0.452    0.070    6.470    0.000    0.605    0.571
##  .kconnect ~~                                                           
##    .k1swls            0.378    0.063    6.020    0.000    0.587    0.558
##  .mconnect ~~                                                           
##    .m1swls            0.203    0.041    5.001    0.000    0.406    0.463
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .a1bfi01           3.594    0.063   57.181    0.000    3.594    2.891
##    .a1bfi06r          2.762    0.062   44.674    0.000    2.762    2.259
##    .a1bfi11           3.944    0.047   84.305    0.000    3.944    4.263
##    .a1bfi16           3.706    0.046   81.133    0.000    3.706    4.103
##    .a1bfi21r          3.016    0.062   48.523    0.000    3.016    2.453
##    .a1bfi26           3.604    0.057   63.320    0.000    3.604    3.202
##    .a1bfi31r          2.594    0.061   42.551    0.000    2.594    2.152
##    .a1bfi36           3.663    0.056   65.581    0.000    3.663    3.316
##    .h1clrel           3.756    0.344   10.908    0.000    3.756    2.542
##    .h1satfr           3.575    0.383    9.342    0.000    3.575    2.065
##    .h1sosat           2.697    0.372    7.243    0.000    2.697    1.844
##    .h1ced05           2.987    0.204   14.615    0.000    2.987    3.403
##    .j1clrel           4.785    0.267   17.946    0.000    4.785    3.609
##    .j1satfr           4.346    0.330   13.187    0.000    4.346    2.609
##    .j1sosat           3.799    0.300   12.661    0.000    3.799    2.855
##    .j1ced05           2.587    0.153   16.899    0.000    2.587    3.108
##    .k1clrel           4.744    0.280   16.970    0.000    4.744    3.631
##    .k1satfr           4.190    0.342   12.258    0.000    4.190    2.524
##    .k1sosat           3.367    0.311   10.835    0.000    3.367    2.241
##    .k1ced05           2.733    0.178   15.350    0.000    2.733    3.086
##    .m1clrel           5.734    0.247   23.230    0.000    5.734    5.023
##    .m1satfr           5.387    0.291   18.491    0.000    5.387    3.786
##    .m1sosat           4.568    0.314   14.554    0.000    4.568    3.323
##    .m1ced05           2.252    0.165   13.670    0.000    2.252    2.719
##    .h1swls            2.189    0.371    5.895    0.000    2.189    1.576
##    .j1swls            3.088    0.264   11.692    0.000    3.088    2.541
##    .k1swls            2.369    0.326    7.269    0.000    2.369    1.838
##    .m1swls            2.952    0.272   10.840    0.000    2.952    2.448
##     a1swls            5.341    0.058   91.684    0.000    5.341    4.635
##     bfie              0.000                               0.000    0.000
##    .hconnect          0.000                               0.000    0.000
##    .jconnect          0.000                               0.000    0.000
##    .kconnect          0.000                               0.000    0.000
##    .mconnect          0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .a1bfi01           0.727    0.063   11.517    0.000    0.727    0.470
##    .a1bfi06r          0.955    0.075   12.655    0.000    0.955    0.638
##    .a1bfi11           0.556    0.043   12.809    0.000    0.556    0.650
##    .a1bfi16           0.519    0.041   12.766    0.000    0.519    0.636
##    .a1bfi21r          0.771    0.061   12.625    0.000    0.771    0.510
##    .a1bfi26           0.735    0.059   12.378    0.000    0.735    0.580
##    .a1bfi31r          0.899    0.071   12.650    0.000    0.899    0.619
##    .a1bfi36           0.295    0.040    7.450    0.000    0.295    0.242
##    .h1clrel           1.170    0.138    8.506    0.000    1.170    0.536
##    .h1satfr           1.955    0.203    9.647    0.000    1.955    0.653
##    .h1sosat           0.946    0.119    7.956    0.000    0.946    0.442
##    .h1ced05           0.451    0.047    9.522    0.000    0.451    0.585
##    .j1clrel           0.990    0.102    9.736    0.000    0.990    0.563
##    .j1satfr           1.621    0.164    9.870    0.000    1.621    0.584
##    .j1sosat           0.765    0.094    8.152    0.000    0.765    0.432
##    .j1ced05           0.446    0.042   10.670    0.000    0.446    0.644
##    .k1clrel           1.018    0.108    9.423    0.000    1.018    0.596
##    .k1satfr           1.727    0.174    9.912    0.000    1.727    0.627
##    .k1sosat           1.426    0.148    9.668    0.000    1.426    0.632
##    .k1ced05           0.526    0.052   10.109    0.000    0.526    0.671
##    .m1clrel           0.733    0.068   10.788    0.000    0.733    0.562
##    .m1satfr           1.241    0.109   11.352    0.000    1.241    0.613
##    .m1sosat           0.983    0.095   10.399    0.000    0.983    0.520
##    .m1ced05           0.440    0.037   11.974    0.000    0.440    0.642
##    .h1swls            1.471    0.128   11.501    0.000    1.471    0.763
##    .j1swls            1.123    0.095   11.824    0.000    1.123    0.760
##    .k1swls            1.109    0.095   11.633    0.000    1.109    0.667
##    .m1swls            0.771    0.068   11.279    0.000    0.771    0.530
##     a1swls            1.328    0.095   13.990    0.000    1.328    1.000
##     bfie              0.818    0.104    7.882    0.000    1.000    1.000
##    .hconnect          0.724    0.136    5.331    0.000    0.715    0.715
##    .jconnect          0.556    0.101    5.518    0.000    0.725    0.725
##    .kconnect          0.414    0.085    4.865    0.000    0.600    0.600
##    .mconnect          0.249    0.047    5.346    0.000    0.437    0.437
## 
## Defined Parameters:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     ind               0.131    0.046    2.864    0.004    0.119    0.098
##     total             0.247    0.071    3.479    0.001    0.223    0.185&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# use se = &amp;quot;bootstrap&amp;quot; in the fit function to get bootstrapped se&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;summary-of-panel-sem-models.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Summary of panel SEM models.&lt;/h3&gt;
&lt;p&gt;These models are well suited to address between subjects questions, but does not get at a within subjects questions at all. To do so you need to turn to…&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>week 5 &amp;6 intensive longitudinal</title>
      <link>/lectures/week-5-intenslive-longitudinal/</link>
      <pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/week-5-intenslive-longitudinal/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intensive-longitudinal-designs&#34;&gt;Intensive Longitudinal Designs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#within-versus-between-person-processes&#34;&gt;Within versus between person processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#accounting-for-time&#34;&gt;Accounting for time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-importance-of-centering.&#34;&gt;The importance of centering.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lagged-associations&#34;&gt;Lagged associations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lag-as-moderator&#34;&gt;Lag as moderator?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#within-person-mediation&#34;&gt;Within person mediation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling-residual-correlation&#34;&gt;Modeling Residual Correlation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;intensive-longitudinal-designs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intensive Longitudinal Designs&lt;/h1&gt;
&lt;p&gt;Often we are not interested in looking at trajectories across time. Instead we are interested in using time as a means to look at fluctuations in our DV. Fluctuations are especially helpful to understand within person processes versus between person processes.&lt;/p&gt;
&lt;p&gt;Typically, the focus of these sorts of analyses are with level 1 (time varying) covariates ie variables you have assessed more than once. The questions that can be answered with this design are many, but some examples: Are there associations between variation in X and variation in Y; is X associated with greater levels of Y; does X relate to later levels of Y; Does X relate to later levels of Y after accounting for concurrent levels of X. Notice how all of these have the flavor of standard regression interpretations rather than the models we have been working with. The reason is that that intensive longitudinal designs are less likely to focus on time as a meaningful metric. Instead the focus on accounting for time, or the processes that unfold across time, not time per se.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;within-versus-between-person-processes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Within versus between person processes&lt;/h1&gt;
&lt;p&gt;Get comfortable with playing around between these two levels. Does doing more homework lead to better grades? Can ask this at a between person and a within person level. Between: do people who on average study more tend to get higher grades? Within: When I study more do I get a higher grade? The answers are not necessarily the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1          ✔ purrr   0.3.2     
## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ readr   1.3.1          ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simp&amp;lt;- tribble(
  ~ID, ~group,  ~test.score, ~study,
1,1,5,1,
2,1,7,3,
3,2,4,1,
4,2,6,4,
5,3,3,3,
6,3,5,5,
7,4,2,4,
8,4,4,6,
9,5,1,5,
10,5,3,7)

ggplot(simp, aes(x=study, y=test.score, group = group)) +
    geom_point() +   
    geom_smooth(method=lm,  
                se=FALSE) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-26-week-5-intenslive-longitudinal_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Also the relationship between a within person association and a between person association are likely correlated with one another. Someone who studies a lot on average may be more or less likely to study for a particular test.&lt;/p&gt;
&lt;p&gt;Take another example: affect. Those who feel happy have more friends. How would you answer this? Well it is actually two questions. First, a between person one where those that are happier in general tend to have more friends. Second, if I go and get more friends do I become happier? These sorts of questions are useful to ask when, for example, you are thinking about interventions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accounting-for-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Accounting for time&lt;/h1&gt;
&lt;p&gt;Usually time is not an important factor for studies that last for weeks. What happens to the time variable? Two options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Ignore it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use it to control, but mostly disregard.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Level 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}(Exercise_{ij}-\overline{Exercise_{j}}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}\overline{Exercise_{j}} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}\overline{Exercise_{j}} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20}  \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-importance-of-centering.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The importance of centering.&lt;/h1&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} =  [\gamma_{00} +   \gamma_{10}Time_{ij}   + \gamma_{20}(Exercise_{ij}-\overline{Exercise_{j}}) + \gamma_{30}Mood_{ij} +\gamma_{40}(Mood*(Exercise_{ij}-\overline{Exercise_{j}}))_{ij} ] + [ U_{0j}  + U_{1j}Time_{ij}+ \varepsilon_{ij}] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What happens if we want to add a level 2 predictor?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} =  [\gamma_{00} +   \gamma_{10}Time_{ij}   + \gamma_{20}(Exercise_{ij}-\overline{Exercise_{j}}) + \gamma_{30}Mood_{ij} +\gamma_{31}(Mood*\overline{Exercise_{j}})_{j}  +\gamma_{40}(Mood*(Exercise_{ij}-\overline{Exercise_{j}}))_{ij} ] + [ U_{0j}  + U_{1j}Time_{ij}+ \varepsilon_{ij}] \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lagged-associations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lagged associations&lt;/h1&gt;
&lt;p&gt;What if I want to predict something in the future? Such that my exercise today is associated with future health gains.&lt;/p&gt;
&lt;p&gt;Level 1
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}(Exercise_{(i-t)j}-\overline{Exercise_{j}}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}\overline{Exercise_{j}} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}\overline{Exercise_{j}} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20}  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Do I need to account for time also?&lt;/p&gt;
&lt;div id=&#34;lag-as-moderator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lag as moderator?&lt;/h2&gt;
&lt;p&gt;Is your X - &amp;gt; Y association dependent on time between assessments.&lt;/p&gt;
&lt;p&gt;Create a new lag variable that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Lag_{ij} + \beta_{2j}(Exercise_{(i-t)j}-\overline{Exercise_{j}}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}\overline{Exercise_{j}} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}\overline{Exercise_{j}} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20}  \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;within-person-mediation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Within person mediation&lt;/h1&gt;
&lt;p&gt;Pick your poison. Overall, Josh is very &lt;code&gt;blah&lt;/code&gt; about MLMM&lt;/p&gt;
&lt;p&gt;1-1-1
2-1-1
2-2-1?&lt;/p&gt;
&lt;p&gt;see:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://quantpsy.org/pubs/bauer_preacher_gil_2006.pdf&#34; class=&#34;uri&#34;&gt;http://quantpsy.org/pubs/bauer_preacher_gil_2006.pdf&lt;/a&gt; for a general overview&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://vuorre.netlify.com/pdf/2017-vuorre-bolger.pdf&#34; class=&#34;uri&#34;&gt;https://vuorre.netlify.com/pdf/2017-vuorre-bolger.pdf&lt;/a&gt; for a experimental plus Bayesian perspective&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statmodel.com/download/pzz_012610_for_web.pdf&#34; class=&#34;uri&#34;&gt;https://www.statmodel.com/download/pzz_012610_for_web.pdf&lt;/a&gt; for why MLM might not be best and using SEM along with MLM is preferable.&lt;/p&gt;
&lt;p&gt;We will talk more about longitudinal mediation models when we cover SEM approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-residual-correlation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling Residual Correlation&lt;/h1&gt;
&lt;p&gt;Typically we model error assuming that there is $ {}_{ij} (0, ^{2}) $ such that there is no correlation among residuals. This is likely met when we have small number of repeated measures, as a linear trajectory likely captures the data well, and all of the deviations from that trajectory are likely noise. However, when we have a lot of repeated measures (and where we not be modeling time systematically) there is more likelihood that we will have correlated residuals. This is problematic. We got rid of a similar concern of correlated errors by fitting MLMs in the first place (ie nesting observations within person), but this different.&lt;/p&gt;
&lt;p&gt;This is hard to do within ‘lme4’ but we can do it easier within Bayesian frameworks&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 4</title>
      <link>/lectures/week-4/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/week-4/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#review-from-last-time&#34;&gt;Review from last time&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation&#34;&gt;Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimation&#34;&gt;Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing-significance-adapted-from-ben-bolker&#34;&gt;Testing significance (adapted from Ben Bolker)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#quick-aside-p-values-are-not-included&#34;&gt;Quick aside: P values are not included&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likelihood-ratio-test&#34;&gt;Likelihood ratio test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likelihood-tests-for-random-effects&#34;&gt;Likelihood tests for random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aic-and-bic&#34;&gt;AIC and BIC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-of-determination-equivalents&#34;&gt;Coefficient of determination equivalents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#level-1-predictors-aka-time-varying-covariates-tvcs&#34;&gt;Level 1 predictors AKA Time-varying covariates (TVCs)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introducing-a-random-slope-for-a-tvc&#34;&gt;Introducing a random slope for a TVC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interactions-among-level-1-variables&#34;&gt;Interactions among level 1 variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#centering-redux&#34;&gt;Centering redux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#readings&#34;&gt;Readings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#flexible-time-metrics&#34;&gt;Flexible time metrics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#categorical-structured-vs-continuous-unstructured&#34;&gt;Categorical (structured) vs continuous (unstructured)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#balanced-vs-unbalanced&#34;&gt;Balanced vs unbalanced&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convergence-issues-or-other-warnings&#34;&gt;Convergence issues or other warnings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#polynomial-and-splines&#34;&gt;Polynomial and Splines&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#polynomial-example&#34;&gt;polynomial example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#importance-of-centering&#34;&gt;importance of centering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-terms&#34;&gt;random terms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#splines-aka-piecewise&#34;&gt;Splines aka piecewise&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#separate-curves&#34;&gt;separate curves&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#splines-polynomial-polynomial-piecewise&#34;&gt;splines + polynomial = polynomial piecewise&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;review-from-last-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Review from last time&lt;/h1&gt;
&lt;div id=&#34;interpretation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpretation&lt;/h2&gt;
&lt;p&gt;Looked at between person predictors&lt;/p&gt;
&lt;p&gt;Can you to interpret each fixed and random effect?&lt;/p&gt;
&lt;p&gt;What do these different models look like graphically?&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}Exercise_{j} +  \gamma_{02}Intervention_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}Intervention_{j} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation&lt;/h1&gt;
&lt;p&gt;Now that we are able to build and visualize models, how do we test the parameters of interest?&lt;/p&gt;
&lt;p&gt;Maximum likelihood estimation. Uses a likelihood function that describes the probability of observing the sample data as a function of the parameters. Attempts to maximize the function through an iterative process. Because it is iterative, it might fail.&lt;/p&gt;
&lt;p&gt;There are fixed effects as well as random effects we need to count for. Maximum likelihood takes our assumptions about the model (normally distributed residuals, etc) and creates probability densities for each parameters. For example, based on certain fixed effects and sd of random effects, how likely is it that person x has a slope of z? The algorithm looks at the full sample to see how likely different parameters are, spits back the most likely, and gives you a number to show how likely they are (compared to others). This is akin to saying you rolled 10 dice, 5 came up as 2s. How likely is this dice fair? But instead of fair vs not fair it gives a likelihood to certain possibilities (e.g., a 2 comes up at 25%, 50% 75% rates).&lt;/p&gt;
&lt;p&gt;Restricted maximum likelihood (REML) vs Full Maximum likelihood (ML). Will give you similar parameters, the differences are in the standard errors. REML is similar to dividing by N - 1 for SE whereas ML is similar to dividing by N.&lt;/p&gt;
&lt;p&gt;Differences account for the fact that fixed effects are being estimated simultaneously with the variance parameters in ML. Estimates of the variance parameters assume that the fixed effects estimates are known and thus does not account for uncertainty in these estimates.&lt;/p&gt;
&lt;p&gt;REML accounts for uncertainty in the fixed effects before estimating residual variance. REML attempts to maximize the likelihood of the residuals whereas ML maximizes the sample data. REML can be thought of as an unbiased estimate of the residual variance.&lt;/p&gt;
&lt;p&gt;REML is good for small sample size both N and group. However, if you use REML you should be careful in testing fixed effects against each other (more down below). Deviance tests for fixed effects should be done with ML, but only random effects with REML. ML can also look at random effects too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-significance-adapted-from-ben-bolker&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Testing significance (adapted from Ben Bolker)&lt;/h1&gt;
&lt;p&gt;4 Methods for testing single parameters
From worst to best:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Wald Z-tests.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Wald t-tests&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Easy to compute - test statistic over standard error However, they are asymptotic standard error approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal and that (2) the sampling distribution of the log-likelihood is (proportional to) χ2.&lt;/p&gt;
&lt;p&gt;The above two are okay to do for single parameter estimates of fixed effects. But beware that a) degrees of freedom calculations are not straightforward and b) the assumptions for random effects are be hard to meet.&lt;/p&gt;
&lt;div id=&#34;quick-aside-p-values-are-not-included&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quick aside: P values are not included&lt;/h2&gt;
&lt;p&gt;Authors of the package we will be using first lme4 are not convinced of the utility of the general approach of testing with reference to an approximate null distribution. In general, it is not clear that the null distribution of the computed ratio of sums of squares is really an F distribution, for any choice of denominator degrees of freedom. While this is true for special cases that correspond to classical experimental designs (nested, split-plot, randomized block, etc.), it is apparently not true for more complex designs (unbalanced, GLMMs, temporal or spatial correlation, etc.).&lt;/p&gt;
&lt;p&gt;tl;dr: it gets messy with more complex models.&lt;/p&gt;
&lt;p&gt;If you really want p values&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(lmerTest)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Likelihood ratio test (also called deviance test).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Markov chain Monte Carlo (MCMC) or parametric bootstrap confidence intervals ( we will get to this later)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-ratio-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood ratio test&lt;/h2&gt;
&lt;p&gt;Used for model comparisons (often multiparameter comparisons) and for tests of random effects. REML can only be used if model compared have the same fixed parts and only differ in random. Otherwise ML must be used.&lt;/p&gt;
&lt;p&gt;How much more likely the data is under a more complex model than under the simpler model (these models need to be nested to compare this).&lt;/p&gt;
&lt;p&gt;Log Likelihood (LL) is derived from ML estimation. Logs are used because they are computationally simpler; logs of multiplications are reduced to adding the logs together.&lt;/p&gt;
&lt;p&gt;Larger the LL the better the fit.&lt;/p&gt;
&lt;p&gt;Deviance compares two LLs. Current model and a saturated model (that fits data perfectly). Asks how much worse the current model is to the best possible model. Deviance = -2[LL current - LL saturated]&lt;/p&gt;
&lt;p&gt;LL saturated = 1 for MLMs (probability it will perfectly recapture data). log of 1 is 0. So this term drops out. Deviance = -2(LL current model). AKA -2logL or -2LL&lt;/p&gt;
&lt;p&gt;Can compare two models via subtraction, often referred to as a full and reduced model. Differences is distributed as a chi square with a df equal to how many “constraints” are included. Constraints can be thought of as forcing a parameter to be zero ie removing it.&lt;/p&gt;
&lt;p&gt;Comparing 2 models is called a likelihood ratio test. Need to have:
1. same data
2. nested models (think of constraining a parameter to zero)&lt;/p&gt;
&lt;p&gt;Why work with deviances and not just log likelihoods? Why -2? Why a ratio test when you subtract deviances? Maths. Working with deviances allows us to subtract two from one another, which is equivalent to taking the ratio of likelihoods.&lt;/p&gt;
&lt;p&gt;You can test in r using the same procedure we would to test different regression models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.2, mod.2r)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-tests-for-random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood tests for random effects&lt;/h2&gt;
&lt;p&gt;Not listed in the output because it is harder to do this with variances. Remember variances do not have values below zero and thus the distributions get a wonky quickly. Needs mixture distributions (Cannot be easily done with chi square, for example)&lt;/p&gt;
&lt;p&gt;Can technically do anova comparisons for random effects, though that falls to many similar problems as trying to do a Wald test.&lt;/p&gt;
&lt;p&gt;The sampling distribution of variance estimates is in general strongly asymmetric: the standard error may be a poor characterization of the uncertainty. Thus the best way to handle is to do bootstrapped estimates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aic-and-bic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;AIC and BIC&lt;/h2&gt;
&lt;p&gt;Used when you want to compare non-nested data. Need to have the same data, however.&lt;/p&gt;
&lt;p&gt;AIC (Akaike’s Information Criterion) and the BIC (Bayesian Information Criterion) where “smaller is better.” This is the opposite of LL. As with the other types, these may give you wonky findings depending on some factors as they are related to LLs.&lt;/p&gt;
&lt;p&gt;AIC = 2(number of parameters) + (−2LL)
BIC = ln(n)(number of parameters) + (−2LL)&lt;/p&gt;
&lt;p&gt;BIC penalizes models with more parameters more than AIC does.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-determination-equivalents&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coefficient of determination equivalents&lt;/h2&gt;
&lt;p&gt;You want to get a model fit estimate. BIC and AIC are good to compare nested models but they aren’t standardized and thus make comparison across non nested models difficult.&lt;/p&gt;
&lt;p&gt;With MLM models we cannot directly compute R2. Instead we will use pseudo R2. Pseudo R2 is similar to R2 in that it can be thought of as the correlation between your predicted and actual scores. For example, assume we have three waves of data. The intercept is 1, the slope is 2 and time is coded 0,1,2. The predicted scores are: 1, 3, 5. We would then correlate everyone’s first, second and third wave scores with these predicted scores. This correlation squared is pseudo R2, telling us how much variance time explains in our DV.&lt;/p&gt;
&lt;p&gt;Yes, we typically think of this as a measure of variance explained divided by total variance. This is where things get tricky: should you include or exclude variation of different random-effects terms? These are error, but they are modeled in the sense that they are not unexplained. Is the effect size wanted after you are “controlling for” or do you want to talk about total variation. There are similarities here with regards to Eta and Partial Eta squared.&lt;/p&gt;
&lt;p&gt;The general idea is to be upfront about what you are comparing and what is included. Typically this is done with comparing models, much like a hierarchical regression. Taking the difference in variance between model 1 and model 2 and dividing it by model 1 makes it explicit what you are looking at and what you are including or not including.&lt;/p&gt;
&lt;p&gt;E.g,. residual variance in varying intercept model subtracted from growth model divided by intercept only model. This can tell you how much unexplained variance is explained by time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sigma(mod.1) - sigma(mod.2)) / sigma(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;level-1-predictors-aka-time-varying-covariates-tvcs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Level 1 predictors AKA Time-varying covariates (TVCs)&lt;/h1&gt;
&lt;p&gt;Thus far we have been talking about level 2, between person predictors. But we can extend this to level 1, within person, repeated measures as predictors and covariates.&lt;/p&gt;
&lt;p&gt;These are predictors that are assessed at level 1, which repeat. Note that there are some variables that are inherently level 2 (e.g. handedness), some that make sense more as a level 1 (e.g., mood) and some that could be considered either depending on your research question and/or your data (e.g. income). The latter type could conceivably change across time (And thus be appropriate for a level 1 variable; tvc) but may not change at the rate of your construct or not be important.&lt;/p&gt;
&lt;p&gt;What do level 1 predictors look like in your dataset?&lt;/p&gt;
&lt;p&gt;Consider health across time predicted by a level 1 exercise variable (1 = yes, exercised). Note that we had a similar model presented at the end of last class, but exercise was a level 2 predictor. Be comfortable with how these differ.&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}Exercise_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Combined:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} =  [\gamma_{00} +   \gamma_{10}Time_{ij}   + \gamma_{20}Exercise_{ij}] + [ U_{0j}  + U_{1j}Time_{ij}+ \varepsilon_{ij}] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Two things to keep in mind:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;These can be treated as another predictor with the effect of “controlling” for some TVC. Thus the regression coefficients in the model are conditional on this covariate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$ _{10} $ is the average rate of change in health, controlling for exercise&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\gamma_{20}\)&lt;/span&gt; is the average difference in health when exercising and when not. Ie the difference in health trajectory.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; is the average health at Time = 0 for those that do not exercise. Ie when both predictors are at zero.&lt;/p&gt;
&lt;p&gt;How would you visualize the fixed effects for varying combinations of exercise?&lt;/p&gt;
&lt;div id=&#34;introducing-a-random-slope-for-a-tvc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introducing a random slope for a TVC&lt;/h2&gt;
&lt;p&gt;Person specific residuals make the interpretation of parameters a little more difficult as the model says that the gap between exercise and not exercise is the same for everyone. Should we allow it to be this way?&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}Exercise_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} +  U_{2j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2 variance-covariance matrix:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \\ {U}_{2j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,  &amp;amp;  \tau_{0}^{2} &amp;amp; \tau_{01}   &amp;amp; \tau_{02}   \\ 
  0, &amp;amp; \tau_{10} &amp;amp; \tau_{1}^{2} &amp;amp; \tau_{12}  \\
  0, &amp;amp; \tau_{20} &amp;amp; \tau_{21} &amp;amp; \tau_{2}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Residual variance at level 1&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Compared to time invariant (level 2) predictors, tvc/level 1 predictors are likely to explain variance level 1 and level 2 variance terms as they differ between and within. Typically level 2 predictors tend to only reduce level 2 variance. It is possible, however, that including a level 1 predictor will increase the variance in level 2 variance components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interactions-among-level-1-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactions among level 1 variables&lt;/h2&gt;
&lt;p&gt;Couldn’t exercise levels influence the slope of health? The previous models constrained the slopes to be the same, saying that people differ on level when exercising vs not but not on rate of change.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} =  [\gamma_{00} +   \gamma_{10}Time_{ij}   + \gamma_{20}Exercise_{ij} + \gamma_{30}TimeXExercise_{ij}]] + [ U_{0j}  + U_{1j}Time_{ij}+ \varepsilon_{ij}] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How could you visualize this model?&lt;/p&gt;
&lt;p&gt;How do you interpret each of the terms (knowing what you know about interactions)?&lt;/p&gt;
&lt;p&gt;How would all of this change if our level 1 variable was continuous?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;centering-redux&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Centering redux&lt;/h2&gt;
&lt;p&gt;Especially when you are working with level 1 interactions, centering is important to interpret your lower order terms. How would $ _{10}$ be interpreted in the above if exercise was centered vs not? Also, be clear about what you mean by centering. Is it the person average or the grand mean average. These will differ in interpretation. Do you want to model a person’s average exercise or the grand mean exercise?&lt;/p&gt;
&lt;p&gt;Typically for level 1 we will want to within person-mean center.&lt;/p&gt;
&lt;p&gt;However, this gets rid of all mean level information for a person. The question at hand is not whether you exercise more or less it is compared to your typical levels, what happens when you exercise more or less. This is a within-person question and may be quite important for your theoretical tests.&lt;/p&gt;
&lt;p&gt;However, if you are including a level 1 person centered variable in the model, note that 1) the average level of exercise is not controlled for and 2) the variation around the level will likely be related to the persons mean score. In other words, the within and between person variance of exercise is not neatly decomposed. To do so, we will have to create a new variable out of the existing level 1 variable, a person mean.&lt;/p&gt;
&lt;p&gt;Level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}(Exercise_{ij}-\overline{Exercise_{j}}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}\overline{Exercise_{j}} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20}  \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Readings&lt;/h2&gt;
&lt;p&gt;Check out this article for more information on how to model level 1 predictors.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3095386/&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3095386/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;flexible-time-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Flexible time metrics&lt;/h1&gt;
&lt;p&gt;Thus far we have been talking about time relatively naively, assuming that time is fixed on equal assessments for everyone ie wave. This treatment of time can be made more complex in two ways.&lt;/p&gt;
&lt;div id=&#34;categorical-structured-vs-continuous-unstructured&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categorical (structured) vs continuous (unstructured)&lt;/h2&gt;
&lt;p&gt;Our repeated assessments are often collected based on some sort of structure. You have enough funding for three waves of data, and you proceed to call participants. These three waves may be specified to occur every 6 months, for example. However, it rarely works out that nicely. People don’t show up, people reschedule, your team is in holiday. The resulting time in between assessments thus differs between and within a person. What to do?&lt;/p&gt;
&lt;p&gt;Well, we could ignore the timing differences. Do we think that a few weeks difference will make or break your general conclusions? Sticking with wave is seen as treating time as categorical.&lt;/p&gt;
&lt;p&gt;We could also treat it as continuous. This is usually preferred because why get rid of meaningful information? Within MLMs there is practically no downside to doing so.&lt;/p&gt;
&lt;p&gt;Treating time as categorical, however, is standard with SEM based longitudinal methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;balanced-vs-unbalanced&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Balanced vs unbalanced&lt;/h2&gt;
&lt;p&gt;Balanced for longitudinal models means that everyone has the same number of repeated assessments. As with ANOVA/experimental designs, balance makes the math easier. In terms of interpretation of the results after doing said maths, there is no difference. In longitudinal designs especially, it is important is where this unbalance comes from. Does the unbalance occur because of dumb luck or is it systematically related to some variable e.g., attrition via death/health.&lt;/p&gt;
&lt;p&gt;The downfalls from unbalanced designs come from difficulties in convergence and interpretation. This is especially true when time is categorical rather than continuous (as continuous time makes estimation of variance components easier as it is more likely to be separated from the fixed effects).&lt;/p&gt;
&lt;p&gt;If you have less than 2 repeated measures for a person, they still can be used. They will be used to estimate relevant fixed effects that can be estimated (as they are similar to standard regression coefficients), but likely not the variance estimates. The slopes for these people will be based on their observed values and the model based trajectory (ie uses partial pooling/shrinkage). However, a number of these individuals will lead to convergence issues.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-issues-or-other-warnings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Convergence issues or other warnings&lt;/h1&gt;
&lt;p&gt;If you have convergence issues it is likely because you have a) too few data points, b) too much imbalance in your repeated measures (ie missing data), c) too many parameters to estimate or d) a combination of all of the above.&lt;/p&gt;
&lt;p&gt;We will talk about fitting a “maximal model” – one that has as many variance components as possible. However, this may be asking too much of the data. Instead, we may have to get rid of some of these random terms to reduce model complexity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-and-splines&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomial and Splines&lt;/h1&gt;
&lt;p&gt;##Polynomials
level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}(Time_{ij} - \bar{X)} + \beta_{2j}(Time_{ij} - \bar{X)}^2 + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +   U_{0j}\]&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} +  U_{2j} \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;polynomial-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;polynomial example&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())

library(readr)
cdrs &amp;lt;- read_csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/cdrs.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   mapid = col_double(),
##   exclude = col_character(),
##   cdr = col_double(),
##   testdate = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality &amp;lt;- read_csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/Subject_personality.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   mapid = col_double(),
##   age = col_double(),
##   neodate = col_double(),
##   neuroticism = col_double(),
##   extraversion = col_double(),
##   openness = col_double(),
##   agreeablness = col_double(),
##   conscientiousness = col_double(),
##   gender = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2) 


gg1 &amp;lt;- ggplot(personality,
   aes(x = neodate, y = neuroticism, group = mapid)) + geom_line()  
gg1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ──────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ purrr   0.3.2          ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ──────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality&amp;lt;- personality %&amp;gt;% 
  group_by(mapid) %&amp;gt;%
  arrange(neodate) %&amp;gt;% 
  mutate(wave = seq_len(n())) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg2 &amp;lt;- ggplot(personality,
   aes(x = wave, y = neuroticism, group = mapid)) + geom_line()  
gg2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality$neodate &amp;lt;- as.Date(personality$neodate, origin = &amp;quot;1900-01-01&amp;quot;)

gg3 &amp;lt;- ggplot(personality,
   aes(x = neodate, y = neuroticism, group = mapid)) + geom_line()  
gg3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## convert to days from first assessment

personality.wide &amp;lt;- personality %&amp;gt;% 
  dplyr::select(mapid, wave, neodate) %&amp;gt;% 
  spread(wave, neodate) 

personality.wide$wave_1 &amp;lt;- personality.wide$&amp;#39;1&amp;#39;
personality.wide$wave_2 &amp;lt;- personality.wide$&amp;#39;2&amp;#39;
personality.wide$wave_3 &amp;lt;- personality.wide$&amp;#39;3&amp;#39;
personality.wide$wave_4 &amp;lt;- personality.wide$&amp;#39;4&amp;#39;
personality.wide$wave_5 &amp;lt;- personality.wide$&amp;#39;5&amp;#39;

personality.wide &amp;lt;- personality.wide %&amp;gt;% 
mutate (w_1 = (wave_1 - wave_1)/365,
          w_2 = (wave_2 - wave_1)/365,
          w_3 = (wave_3 - wave_1)/365,
          w_4 = (wave_4 - wave_1)/365,
        w_5 = (wave_5 - wave_1)/365)

personality.long &amp;lt;- personality.wide %&amp;gt;% 
  dplyr::select(mapid, w_1:w_5) %&amp;gt;% 
  gather(wave, year, -mapid) %&amp;gt;% 
  separate(wave, c(&amp;#39;weeks&amp;#39;, &amp;#39;wave&amp;#39; ), sep=&amp;quot;_&amp;quot;) %&amp;gt;% 
 dplyr::select(-weeks) 

personality.long$wave &amp;lt;-  as.numeric(personality.long$wave)


personality &amp;lt;- personality %&amp;gt;% 
   left_join(personality.long, by = c(&amp;#39;mapid&amp;#39;, &amp;#39;wave&amp;#39; )) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg4 &amp;lt;- ggplot(personality,
   aes(x = year, y = neuroticism, group = mapid)) + geom_line()  
gg4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Don&amp;#39;t know how to automatically pick scale for object of type difftime. Defaulting to continuous.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;Matrix&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- lmer(neuroticism ~ year + (1 | mapid), data=personality)
summary(p1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + (1 | mapid)
##    Data: personality
## 
## REML criterion at convergence: 13657.4
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7877 -0.4675 -0.0227  0.4289  3.3166 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  mapid    (Intercept) 42.16    6.493   
##  Residual             15.65    3.956   
## Number of obs: 2105, groups:  mapid, 1090
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 16.05632    0.22577  71.118
## year        -0.13204    0.03247  -4.067
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.247&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
personality.s &amp;lt;- personality %&amp;gt;% 
  group_by(mapid) %&amp;gt;% 
  tally() %&amp;gt;% 
   filter(n &amp;gt;=2) 

 personality &amp;lt;- personality %&amp;gt;% 
   filter(mapid %in% personality.s$mapid)

p2 &amp;lt;- lmer(neuroticism ~ year + (1 | mapid), data=personality)
summary(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + (1 | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10396.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7542 -0.5122 -0.0282  0.4698  3.3369 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  mapid    (Intercept) 40.92    6.397   
##  Residual             15.61    3.950   
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  15.3797     0.2915  52.761
## year         -0.1083     0.0331  -3.271
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.320&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;- lmer(neuroticism ~ year + (year | mapid), data=personality)
summary(p3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + (year | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10389.2
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7440 -0.4825 -0.0304  0.4443  3.3453 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  mapid    (Intercept) 41.6916  6.4569        
##           year         0.0983  0.3135   -0.10
##  Residual             14.2561  3.7757        
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 15.37237    0.29136  52.760
## year        -0.10271    0.03602  -2.851
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.317&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;importance-of-centering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;importance of centering&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality$year &amp;lt;- as.numeric(personality$year)
  
p4 &amp;lt;- lmer(neuroticism ~ year + I(year^2) + (year | mapid), data=personality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 0.00216391
## (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(p4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + I(year^2) + (year | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10395.8
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7664 -0.4836 -0.0251  0.4422  3.3259 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  mapid    (Intercept) 41.73017 6.4599        
##           year         0.09818 0.3133   -0.10
##  Residual             14.26174 3.7765        
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##              Estimate Std. Error t value
## (Intercept) 15.324317   0.297096  51.580
## year        -0.031791   0.092090  -0.345
## I(year^2)   -0.008789   0.010490  -0.838
## 
## Correlation of Fixed Effects:
##           (Intr) year  
## year      -0.300       
## I(year^2)  0.194 -0.920
## convergence code: 0
## Model failed to converge with max|grad| = 0.00216391 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# woah, how do I interpret this? WHy all of a sudden non-sig? 
# what would happen if I changed my time metric? &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;psych&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%, alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;describe(personality$year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    vars    n mean   sd median trimmed  mad min   max range skew kurtosis
## X1    1 1635  3.1 3.29   2.45    2.66 3.63   0 12.78 12.78  0.8    -0.41
##      se
## X1 0.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality$year.c &amp;lt;- personality$year - 3.1

p5 &amp;lt;- lmer(neuroticism ~ year.c + I(year.c^2) + (year.c | mapid), data=personality)
summary(p5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year.c + I(year.c^2) + (year.c | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10395.8
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7663 -0.4836 -0.0251  0.4422  3.3258 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  mapid    (Intercept) 41.42901 6.4365       
##           year.c       0.09812 0.3132   0.05
##  Residual             14.26278 3.7766       
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##              Estimate Std. Error t value
## (Intercept) 15.141297   0.296070  51.141
## year.c      -0.086286   0.041061  -2.101
## I(year.c^2) -0.008789   0.010490  -0.838
## 
## Correlation of Fixed Effects:
##             (Intr) year.c
## year.c       0.226       
## I(year.c^2) -0.353 -0.480&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;random-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;random terms&lt;/h3&gt;
&lt;p&gt;fitting a random slope plus a random quadratic leads to difficulties ie non-convergence. What does this model say?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p6 &amp;lt;- lmer(neuroticism ~ year + I(year^2) + ( I(year^2) | mapid), data=personality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 0.167875
## (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue
##  - Rescale variables?&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(p6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + I(year^2) + (I(year^2) | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10398.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7747 -0.4937 -0.0197  0.4525  3.3481 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev. Corr
##  mapid    (Intercept) 4.082e+01 6.38890      
##           I(year^2)   4.851e-04 0.02203  0.02
##  Residual             1.505e+01 3.87965      
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##              Estimate Std. Error t value
## (Intercept) 15.321498   0.296523  51.670
## year        -0.026955   0.093386  -0.289
## I(year^2)   -0.009488   0.010729  -0.884
## 
## Correlation of Fixed Effects:
##           (Intr) year  
## year      -0.300       
## I(year^2)  0.202 -0.928
## convergence code: 0
## Model failed to converge with max|grad| = 0.167875 (tol = 0.002, component 1)
## Model is nearly unidentifiable: very large eigenvalue
##  - Rescale variables?&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;splines-aka-piecewise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splines aka piecewise&lt;/h2&gt;
&lt;p&gt;Fit more than 1 trajectory. Best to use when we have a reason for a qualitative difference at some identified time point. For example, before your health event you may have a different trajectory than after it and thus you would want to model two separate trajectories. Splines allow you to do this in a single model. You can do this in simple regression and the logic follows for growth models.&lt;/p&gt;
&lt;p&gt;We simply replace time with dummy variables that represent different segments we wish to model. The point of separation is called a knot. You can have as many as you want and these can be pre-specified (usually for our case) or in more advanced treatments have the data specify it for you.&lt;/p&gt;
&lt;div id=&#34;separate-curves&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;separate curves&lt;/h3&gt;
&lt;p&gt;The most common is to create different trajectories that change across knots. The easiest example is to take your time variable and transform it into a Time1 and time2, that represent the different time periods. This is easiest to see if we choose our wave variable as our time metric, though you do not have to necessarily do it this way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t1 &amp;lt;- tribble(
  ~time, ~t0, ~t1,~t2,~t3,~t4,~t5,
  &amp;quot;time 1&amp;quot;, 0, 1,2,2,2,2,
  &amp;quot;time 2&amp;quot;, 0, 0,0,1,2,3
)
t1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   time      t0    t1    t2    t3    t4    t5
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 time 1     0     1     2     2     2     2
## 2 time 2     0     0     0     1     2     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The idea is that once you hit the knot your value stays the same. Same logic for the second knot, until you get to that knot you don’t have a trajectory.&lt;/p&gt;
&lt;p&gt;###incremental curves
This can be contrasted with a different type of coding, called incremental. Here the first trajectory keeps going, whereas the second trajectory starts at the position of the knot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t2 &amp;lt;- tribble(
  ~time, ~t0, ~t1,~t2,~t3,~t4,~t5,
  &amp;quot;time 1&amp;quot;, 0, 1,2,3,4,5,
  &amp;quot;time 2&amp;quot;, 0, 0,0,1,2,3
)
t2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   time      t0    t1    t2    t3    t4    t5
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 time 1     0     1     2     3     4     5
## 2 time 2     0     0     0     1     2     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The two coding schemes propose the same type of trajectory, the only thing that differs is the interpretation of the coefficients.&lt;/p&gt;
&lt;p&gt;In the first, the two slope coefficients represent the actual slope in the respective time period.&lt;/p&gt;
&lt;p&gt;In the second, the coefficient for time 2 represents the deviation from the slope in period 1. The positive of this second method is you can easily test whether these two slopes are different from one another.&lt;/p&gt;
&lt;p&gt;level 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time1_{ij} + \beta_{2j}Time2_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +  U_{0j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} +  U_{2j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;###splines example&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality$time1 &amp;lt;- recode(personality$wave, &amp;#39;1&amp;#39; = 0 , &amp;#39;2&amp;#39; = 1,  &amp;#39;3&amp;#39; = 1, &amp;#39;4&amp;#39; = 1,&amp;#39;5&amp;#39; = 1)      
personality$time2 &amp;lt;- recode(personality$wave, &amp;#39;1&amp;#39; = 0 , &amp;#39;2&amp;#39; = 0,  &amp;#39;3&amp;#39; = 1, &amp;#39;4&amp;#39; = 2,&amp;#39;5&amp;#39; = 3) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p7 &amp;lt;- lmer(conscientiousness ~ time1 + time2 + (time1   | mapid) , data=personality)
summary(p7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: conscientiousness ~ time1 + time2 + (time1 | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10003.8
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.2558 -0.4068  0.0272  0.4304  4.5854 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  mapid    (Intercept) 32.979   5.743         
##           time1        4.729   2.175    -0.13
##  Residual             10.702   3.271         
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  34.1871     0.2654 128.800
## time1        -0.5365     0.2018  -2.658
## time2         0.2184     0.1561   1.399
## 
## Correlation of Fixed Effects:
##       (Intr) time1 
## time1 -0.370       
## time2  0.000 -0.301&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg5 &amp;lt;- ggplot(personality, aes(x = wave, y = conscientiousness, group = mapid)) +  stat_smooth(method = &amp;#39;lm&amp;#39;, formula = y ~ poly(x,2, raw = TRUE),data = personality, aes(x = wave, y = conscientiousness, group=1)) + scale_y_continuous(limits = c(30, 40))
gg5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 609 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;splines-polynomial-polynomial-piecewise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;splines + polynomial = polynomial piecewise&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time1_{ij} +  \beta_{2j}Time1_{ij}^2 + \beta_{3j}Time2_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +  U_{0j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} +  U_{2j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{3j} = \gamma_{30} +  U_{3j}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 3</title>
      <link>/lectures/2019-08-14-lecture-3-test/</link>
      <pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/2019-08-14-lecture-3-test/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-models&#34;&gt;Conditional models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#level-2-predictors&#34;&gt;Level 2 predictors&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#group-predictors-of-intercept&#34;&gt;Group predictors of intercept&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-of-fixed-effects&#34;&gt;Interpretation of fixed effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-of-random-effects&#34;&gt;Interpretation of random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#seperatinng-these-into-intercept-and-slope&#34;&gt;Seperatinng these into intercept and slope&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slope-and-intercept-group-predictors&#34;&gt;Slope and Intercept Group Predictors&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cross-level-interactions&#34;&gt;Cross-level interactions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#equations-necessary-for-plotting&#34;&gt;Equations necessary for plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-predictors-of-intercept-and-slope&#34;&gt;Continuous predictors of intercept and slope&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#equations-necessary-for-plotting-1&#34;&gt;Equations necessary for plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-more-level-2-predictors&#34;&gt;Adding more level 2 predictors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#centering&#34;&gt;Centering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-centering&#34;&gt;Types of centering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#time-centering&#34;&gt;Time centering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#level-2-centering&#34;&gt;Level 2 centering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects-and-residual-standard-assumptions&#34;&gt;Random effects and residual (standard) assumptions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-generating-process-dgp&#34;&gt;Data generating process (DGP)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimation&#34;&gt;Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing-significance-adapted-from-ben-bolker&#34;&gt;Testing significance (adapted from Ben Bolker)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#quick-aside-p-values-are-not-included&#34;&gt;Quick aside: P values are not included&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likelihood-ratio-test&#34;&gt;Likelihood ratio test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likelihood-tests-for-random-effects&#34;&gt;Likelihood tests for random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aic-and-bic&#34;&gt;AIC and BIC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-of-determination-equivalents&#34;&gt;Coefficient of determination equivalents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#level-1-predictors-aka-time-varying-covariates-tvcs&#34;&gt;Level 1 predictors AKA Time-varying covariates (TVCs)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;conditional-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conditional models&lt;/h1&gt;
&lt;p&gt;We are now going to introduce predictors to our models. These predictors are similar to predictors in standard regression – dummy for nominal, interactions change lower order terms, etcetera. These predictors can occur at different levels. Just like in standard regression, there are different ways to interpret the resulting coefficients depending on the type and where the predictor is added.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;level-2-predictors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Level 2 predictors&lt;/h1&gt;
&lt;div id=&#34;group-predictors-of-intercept&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Group predictors of intercept&lt;/h2&gt;
&lt;p&gt;Starting with the basic, let’s add a group level variable to the model that is dummy coded. Note that group here only is measured once, it is a between person variable. Thus it can only be meaningfully added to level 2. Here we are asking the question, does group 1 differ from group 2 in their…?&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;interpretation-of-fixed-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation of fixed effects&lt;/h3&gt;
&lt;p&gt;Notice we have a new gamma term, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}\)&lt;/span&gt;. How do we interpret this new fixed effect, especially in the presense of other fixed effects? What is the slope and what is the effect of group on the slope? &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; is the intercept and can be considered the slope when G = 0 whereas the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}\)&lt;/span&gt; is the difference in slope between groups. What is then the slope for the group = 1? &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00} + \gamma_{01}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation-of-random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation of random effects&lt;/h3&gt;
&lt;p&gt;One thing to keep in mind is that we are now changing the meaning of the random effect. Previously, the random effect was interpretted as the deviation from the mean of the interept. These random effects are interpretted almost like residual terms where it is what is left over. Now that we have a predictor in the model, the &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; is the person specific deviation from the group predicted intercept, not the grand mean intercept. It is the difference from what would be expected given all the terms. In other words, it is conditional on all other predictors in the model.&lt;/p&gt;
&lt;p&gt;Combined
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (Time_{ij}) + U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is helpful to start looking at your equation in terms of what to expect in your model output. Here you have 3 fixed effects, two random effects, and one residual term.&lt;/p&gt;
&lt;p&gt;Level 2 covariance matrix
&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     &amp;amp; \tau_{00}^{2} &amp;amp; \tau_{01}\\ 
  0, &amp;amp; \tau_{01} &amp;amp; \tau_{10}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Same as before in terms of struture, but the calculations will be slightly different. Why?&lt;/p&gt;
&lt;p&gt;Level 1 residual variance
&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Same as before too. But, would you expect the residual to be smaller or larger compared to a model without a group predictor of the itnercept?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seperatinng-these-into-intercept-and-slope&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Seperatinng these into intercept and slope&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = [\gamma_{00} + \gamma_{01}G_{j}+ U_{0j}]  + [(\gamma_{10}  + U_{1j})(Time_{ij})] + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Understanding how to re-write the equation will help for calculating estimated scores for your predictors in addition to being able to interpret the coefficients. This is going to be helpful for predictions and graphing, come later.&lt;/p&gt;
&lt;p&gt;What would differ between the two equations if calculating predicted scores for group coded = 0 versus a group = 1?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;slope-and-intercept-group-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Slope and Intercept Group Predictors&lt;/h2&gt;
&lt;p&gt;Predicting the intrecept only can only answer static questions, not about change. To do that we need to introduce predictions for the slope variable, as that is our variable that indexes how people change.&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}G_{j} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Similar to before, the interpretation of &lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; changes. The term is now what is left over after accounting for group differences in the mean slope.&lt;/p&gt;
&lt;p&gt;Can you visualize what &lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; captures? Can you visualize how &lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; differs in this model and one that does not have the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{11}G_{j}\)&lt;/span&gt; term?&lt;/p&gt;
&lt;p&gt;Combined
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (Time_{ij}) + \gamma_{11}(G_{j}*Time_{ij}) +  U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;cross-level-interactions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cross-level interactions&lt;/h3&gt;
&lt;p&gt;Notice that when we combine Level 1 and Level 2, the slope effect predictor becomes an interaction with time. This is called “cross-level” interaction. Anytime you have a predictor of time that will be an interaction with time in that we are asking does group status (or what ever variable) differs in their trajectory across time. One of these is a level 2 predictor and one is a level 1 predictor, thus a “cross level” interaction. Even though we don’t explicitly model an interaction, it is there because you are inserting the level to prediction, within the level 1 model to get your combined model. As a result, you are replacing your $ {}&lt;em&gt;{1j} $ (that was originally multipled by your level 1 time variable), by $ &lt;/em&gt;{10} + &lt;em&gt;{11}G&lt;/em&gt;{j} + U_{1j} $. Each of these in turn must be multipled with time.&lt;/p&gt;
&lt;p&gt;Level 2 covariance matrix
&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     &amp;amp; \tau_{00}^{2} &amp;amp; \tau_{01}\\ 
  0, &amp;amp; \tau_{01} &amp;amp; \tau_{10}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How does your variance-covariance matrix change? What is the interpretation of &lt;span class=&#34;math inline&#34;&gt;\(\tau_{01}\)&lt;/span&gt;? It is the association between random effects after accounting for (controling) group differences in intercept and slope.&lt;/p&gt;
&lt;p&gt;Level 1 residual variance
&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How does your residual change relative to a model without group effects? Can you graph conceptually what this now captures?&lt;/p&gt;
&lt;p&gt;Alternative combined
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = [\gamma_{00} + U_{0j} +\gamma_{01}G_{j}] + [(\gamma_{10}  + \gamma_{11}G_{j}+  U_{1j})(Time_{ij})] + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is just rearranged so you can see that different groups have different intercepts and slopes – very much alike simple slopes analyses for interactions in standard regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;equations-necessary-for-plotting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Equations necessary for plotting&lt;/h3&gt;
&lt;p&gt;Note that the above equation can be simplified to get rid of the random effects to focus only on fixed effects portion. This is what you would use to get an estimated trajectory. This can be easily lifted from your output.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{ij} = [\gamma_{00} +\gamma_{01}G_{j}] + [(\gamma_{10}  + \gamma_{11}G_{j})(Time_{ij})] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice how when G = 0, the equation simplifies:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[     \hat{Y}_{ij} = \gamma_{00} + \gamma_{10} (Time_{ij}) \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-predictors-of-intercept-and-slope&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous predictors of intercept and slope&lt;/h2&gt;
&lt;p&gt;Introducing a continuous predictor is similar to the group predictors, and is similar to how continuous predictors are used in regression – remember, MLM, is just fancy regression. Here the continuous predictor is again only measured once. It is thought of as a between person variable, one that is not assessed multiple times. As a result, it must go into a level 2 equation.&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}C_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}C_{j} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Combined:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{01}C_{j}+  \gamma_{10} (Time_{ij}) + \gamma_{11}(C_{j}*Time_{ij}) +  U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As with nominal level 2 predictors, the interpretation of our intercept is now when all preditors are at zero ie time AND C.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}C_{j}\)&lt;/span&gt; is now the effect of time when the continous predictor is zero. Zero is meaningful when you code for dummy or effect variables, but is not always straightforward with continuous variables. It is thus recommended to &lt;em&gt;always&lt;/em&gt; center your predictors to aide in interpretation. More on what we mean by this below.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{11}\)&lt;/span&gt; coefficient is now the difference in slopes for one unit of our C variable.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; Is the random effect for intercept after accounting for C.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; Is the random effect for the slope after accounting for C.&lt;/p&gt;
&lt;p&gt;The covariance between them is now accounting for or controlling for this predictor.&lt;/p&gt;
&lt;div id=&#34;equations-necessary-for-plotting-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Equations necessary for plotting&lt;/h3&gt;
&lt;p&gt;The same logic for plotting models with nominal variables applies to continuous predictor variables. Remembering back to decomposing interactions in standard regression models, it is important to plot predicted lines at different levels of interest. Usually plus minus one SD, but if other levels are interesting then you can do that too.&lt;/p&gt;
&lt;p&gt;As an example, lets say we have the mean of C = 0 with a SD of 1. What would our equation look like to plot a predicted trajectory a SD above and below, as well as mean trajectory?&lt;/p&gt;
&lt;p&gt;-1sd
&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{ij} = [\gamma_{00} +(\gamma_{01}*-1)] + [(\gamma_{10}  + (\gamma_{11}*-1))(Time_{ij})] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Mean
&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{ij} = \gamma_{00} + \gamma_{10}  (Time_{ij}) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;+1sd
&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{ij} = [\gamma_{00} +\gamma_{01}] + [(\gamma_{10}  + \gamma_{11})(Time_{ij})] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What would individual level trajectories look like?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = [\gamma_{00} + \gamma_{01}C_{j}+  U_{0j}] + [(\gamma_{10}  + \gamma_{11} + U_{1j})   (Time_{ij})] + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice how these are just the level 2 equations, to specify intercept and slope.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-more-level-2-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding more level 2 predictors&lt;/h2&gt;
&lt;p&gt;These same principles apply to more complex models. As the semester progresses we can continue to add in more complex models, as well as the ability to compare models that differ in complexity.&lt;/p&gt;
&lt;p&gt;It is important to be able to interpret and visualize what these more complex models may look like. For example, can you think about the interpretation of each parameter as well as the plots you would want to do for a model such as looking at health across time, examing the effects of an intervention, while controlling for initial exercise status?:&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}Exercise_{j} +  \gamma_{02}Intervention_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}Intervention_{j} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;centering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Centering&lt;/h1&gt;
&lt;div id=&#34;types-of-centering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Types of centering&lt;/h2&gt;
&lt;p&gt;Changing the scale of your predictors changes the interpretation of your model. (Redux of how to interpret lower order terms in an interaction regression model.) We have more options for centering here compared to standard regression, however.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Original metric (no centering)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Group-mean centering (our group/nesting is person so this is also called person centering). This will be more appropriate when we talk about level 1 predictors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Grand-mean centering (this is taking the average across everyone)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Centering on a value of theoretical or applied interest&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Importantly, centering can both change the interpretation of the coefficients, as well as the fit of the model. The latter is especially true when 1) people differ on the number of assessment points (ie grand mean =/= average person mean) and 2) the intercept is far away from a group or grand mean. The latter will influence the random effect variances and their covariances. You can see this with time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;time-centering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Time centering&lt;/h2&gt;
&lt;p&gt;Our time variable is our only level 1 predictor that we have worked with up to this point. Thus far we have centered it at the beginning. We typically center time around each person’s initial time to make the intercept more interpretable. However, this can cause correlations between an intercept and a slope. If high, the correlation can be problematic in terms of estimation. Often we center time in the middle of the repeated assessments to minimize this association. Doing so is especially important if you want to use some variable to predict intercept and slope (or use interecept/slope to predict some variable).&lt;/p&gt;
&lt;p&gt;Can you visualize why a slope may be more or less correlated with an intercept depending on how we scale time?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     &amp;amp; \tau_{00}^{2} &amp;amp; \tau_{01}\\ 
  0, &amp;amp; \tau_{01} &amp;amp; \tau_{10}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is sometimes helpful to center time as the last time point. Why? So as to use a predictor in the model that is trying to longitudinally predict from the initial point, not change, but a timepoint far in the future&lt;/p&gt;
&lt;p&gt;We will talk more about centering later, but it is important to note that it can get tricky for longitudinal models when people don’t have the same number of assessment waves or the same timespan. Where do you center? One option, the most clean, is to center within each person’s own time, regardless of whether it lines up with others. This is #2 above. This is nice because it makes the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; interpretable as the average score across people.&lt;/p&gt;
&lt;p&gt;However, what is the average score? If you are looking at longitudinal data where people span in age from 20 to 80 and the time each person was in the study differed from 1 to 10 years. How do you interpret the average person intercept? Data wise it is consistent but interpretation wise it may not be. Thus you may want to center on an age ie #4 above. The &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; can now easily be interpreted as age 40, for example. Buuut, this results in wonky residual terms, perhaps leading to greater covariance between intercept and slope.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;level-2-centering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Level 2 centering&lt;/h2&gt;
&lt;p&gt;Because level 2 is involved with cross level interactions, it is always helpful to at least consider centering. For level 2, the centering options are much easier, as one can generally go with grand mean centering. As everyone has only 1 value to contribute to, the calculation and the interpretation is more straightforward.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects-and-residual-standard-assumptions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Random effects and residual (standard) assumptions&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Joint normal distribution of random effects&lt;/li&gt;
&lt;li&gt;Normally distributed residual&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Constant variance over time&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Random effects &lt;span class=&#34;math inline&#34;&gt;\(\pm U_{0j}\)&lt;/span&gt; and residual &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ij}\)&lt;/span&gt; are uncorrelated and have a mean of zero&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some of these we can relax, some of these are not too bad if we violate, some of these we cannot escape. A solution, to many of these standard assumptions is to change the model. The model that we are presenting is basic in that it is all defaults.&lt;/p&gt;
&lt;div id=&#34;data-generating-process-dgp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data generating process (DGP)&lt;/h2&gt;
&lt;p&gt;Our standard assumption is that the DV comes from a data generating process that results in normal distributions. This does not mean that it needs to result in an observed normal distribution. Instead, the default of assuming an Gaussian DGP is practical: it is robust against violations and the alternatives are sometimes harder to justify.&lt;/p&gt;
&lt;p&gt;If you think you have a non-Gaussian DGP (like a Poisson or a negative binomial if you are using some sort of count data) you will need to use a different estimation technique. You can do this somewhat with the package we will be working with primarily, lme4. However, the BRMS package – which uses Bayesian estimation – has many more possibilities: geometric, log normal, weibull, exponential, gamma, Beta, hurdle Poisson/gamma/negative binomial, zero inflated beta/Poisson/negative binomial, cumulative. We will fit some of these later in the semester. Currently, however, assume we are working with&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\({Y}_{ij} \sim \mathcal{N}(0, \sigma^{2})\)&lt;/span&gt;. Altering the assumed DGP will alter the assumptions we have.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation&lt;/h1&gt;
&lt;p&gt;Maximum likelihood estimation. Uses a likelihood function that describes the probability of observing the sample data as a function of the parameters. Attempts to maximize the function through an iterative process. Because it is iterative, it might fail.&lt;/p&gt;
&lt;p&gt;There are fixed effects as well as random effects we need to count for. Maximum likelihood takes our assumptions about the model (normally distributed residuals, etc) and creates probability densities for each parameters. For example, based on certain fixed effects and sd of random effects, how likely is it that person x has a slope of z? The algorithm looks at the full sample to see how likely different parameters are, spits back the most likely, and gives you a number to show how likely they are (compared to others). This is akin to saying you rolled 10 dice, 5 came up as 2s. How likely is this dice fair? But instead of fair vs not fair it gives a likelihood to certain possibilities (e.g., a 2 comes up at 25%, 50% 75% rates).&lt;/p&gt;
&lt;p&gt;Restricted maximum likelihood (REML) vs Full Maximum likelihood (ML). Will give you similar parameters, the differences are in the standard errors. REML is similar to dividing by N - 1 for SE whereas ML is similar to dividing by N.&lt;/p&gt;
&lt;p&gt;Differences account for the fact that fixed effects are being estimated simultaneously with the variance parameters in ML. Estimates of the variance parameters assume that the fixed effects estimates are known and thus does not account for uncertainty in these estimates.&lt;/p&gt;
&lt;p&gt;REML accounts for uncertainty in the fixed effects before estimating residual variance. REML attempts to maximize the likelihood of the residuals whereas ML maximizes the sample data. REML can be thought of as an unbiased estimate of the residual variance.&lt;/p&gt;
&lt;p&gt;REML is good for small sample size both N and group. However, if you use REML you should be careful in testing fixed effects against each other (more down below). Deviances tests for fixed effects should be done with ML, but only random effects with REML. ML can also look at random effects too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-significance-adapted-from-ben-bolker&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Testing significance (adapted from Ben Bolker)&lt;/h1&gt;
&lt;p&gt;4 Methods for testing single parameters
From worst to best:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Wald Z-tests.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Wald t-tests&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Easy to compute - test statistic over standard error However, they are asymptotic standard error approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal and that (2) the sampling distribution of the log-likelihood is (proportional to) χ2.&lt;/p&gt;
&lt;p&gt;The above two are okay to do for single parameter estimates of fixed effects. But beware that a) degrees of freedom calculations are not straightforward and b) the assumptions for random effects are be hard to meet.&lt;/p&gt;
&lt;div id=&#34;quick-aside-p-values-are-not-included&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quick aside: P values are not included&lt;/h2&gt;
&lt;p&gt;Authors of the package we will be using first lme4 are not convinced of the utility of the general approach of testing with reference to an approximate null distribution. In general, it is not clear that the null distribution of the computed ratio of sums of squares is really an F distribution, for any choice of denominator degrees of freedom. While this is true for special cases that correspond to classical experimental designs (nested, split-plot, randomized block, etc.), it is apparently not true for more complex designs (unbalanced, GLMMs, temporal or spatial correlation, etc.).&lt;/p&gt;
&lt;p&gt;tl;dr: it gets messy with more complex models.&lt;/p&gt;
&lt;p&gt;If you really want p values&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(lmerTest)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Likelihood ratio test (also called deviance test).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Markov chain Monte Carlo (MCMC) or parametric bootstrap confidence intervals ( we will get to this later)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-ratio-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood ratio test&lt;/h2&gt;
&lt;p&gt;Used for model comparisons (often multiparameter comparisons) and for tests of random effects. REML can only be used if model compared have the same fixed parts and only differ in random. Otherwise ML must be used.&lt;/p&gt;
&lt;p&gt;How much more likely the data is under a more complex model than under the simpler model (these models need to be nested to compare this).&lt;/p&gt;
&lt;p&gt;Log Likelihood (LL) is derived from ML estimation. Logs are used because they are computationally simpler; logs of multiplications are reduced to adding the logs together.&lt;/p&gt;
&lt;p&gt;Larger the LL the better the fit.&lt;/p&gt;
&lt;p&gt;Deviance compares two LLs. Current model and a saturated model (that fits data perfectly). Asks how much worse the current model is to the best possible model. Deviance = -2[LL current - LL saturated]&lt;/p&gt;
&lt;p&gt;LL saturated = 1 for MLMs (probability it will perfectly recapture data). log of 1 is 0. So this term drops out. Deviance = -2(LL current model). AKA -2logL or -2LL&lt;/p&gt;
&lt;p&gt;Can compare two models via subtraction, often referred to as a full and reduced model. Differences is distributed as a chi square with a df equal to how many “constraints” are included. Constraints can be thought of as forcing a parameter to be zero ie removing it.&lt;/p&gt;
&lt;p&gt;Comparing 2 models is called a likelihood ratio test. Need to have:
1. same data
2. nested models (think of constraining a parameter to zero)&lt;/p&gt;
&lt;p&gt;Why work with deviances and not just log likelihoods? Why -2? Why a ratio test when you subtract deviances? Maths. Working with deviances allows us to subtract two from one another, which is equivalent to taking the ratio of likelihoods.&lt;/p&gt;
&lt;p&gt;You can test in r using the same procedure we would to test different regression models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.2, mod.2r)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-tests-for-random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood tests for random effects&lt;/h2&gt;
&lt;p&gt;Not listed in the output because it is harder to do this with variances. Remember variances do not have values below zero and thus the distributions get a wonky quickly. Needs mixture distributions (Cannot be easily done with chi square, for example)&lt;/p&gt;
&lt;p&gt;Can technically do anova comparisons for random effects, though that falls to many similar problems as trying to do a Wald test.&lt;/p&gt;
&lt;p&gt;The sampling distribution of variance estimates is in general strongly asymmetric: the standard error may be a poor characterization of the uncertainty. Thus the best way to handle is to do bootstrapped estimates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aic-and-bic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;AIC and BIC&lt;/h2&gt;
&lt;p&gt;Used when you want to compare non-nested data. Need to have the same data, however.&lt;/p&gt;
&lt;p&gt;AIC (Akaike’s Information Criterion) and the BIC (Bayesian Information Criterion) where “smaller is better.” This is the opposite of LL. As with the other types, these may give you wonky findings depending on some factors as they are related to LLs.&lt;/p&gt;
&lt;p&gt;AIC = 2(number of parameters) + (−2LL)
BIC = ln(n)(number of parameters) + (−2LL)&lt;/p&gt;
&lt;p&gt;BIC penalizes models with more parameters more than AIC does.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-determination-equivalents&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Coefficient of determination equivalents&lt;/h1&gt;
&lt;p&gt;You want to get a model fit estimate. BIC and AIC are good to compare nested models but they aren’t standardized and thus make comparison across non nested models difficult.&lt;/p&gt;
&lt;p&gt;With MLM models we cannot directly compute R2. Instead we will use pseudo R2. Pseudo R2 is similar to R2 in that it can be thought of as the correlation between your predicted and actual scores. For example, assume we have three waves of data. The intercept is 1, the slope is 2 and time is coded 0,1,2. The predicted scores are: 1, 3, 5. We would then correlate everyone’s first, second and third wave scores with these predicted scores. This correlation squared is pseudo R2, telling us how much variance time explains in our DV.&lt;/p&gt;
&lt;p&gt;Yes, we typically think of this as a measure of variance explained divided by total variance. This is where things get tricky: should you include or exclude variation of different random-effects terms? These are error, but they are modeled in the sense that they are not unexplained. Is the effect size wanted after you are “controlling for” or do you want to talk about total variation. There are similarities here with regards to Eta and Partial Eta squared.&lt;/p&gt;
&lt;p&gt;The general idea is to be upfront about what you are comparing and what is included. Typically this is done with comparing models, much like a hierarchical regression. Taking the difference in variance between model 1 and model 2 and dividing it by model 1 makes it explicit what you are looking at and what you are including or not including.&lt;/p&gt;
&lt;p&gt;E.g,. residual variance in varying intercept model subtracted from growth model divided by intercept only model. This can tell you how much unexplained variance is explained by time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sigma(mod.1) - sigma(mod.2)) / sigma(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;level-1-predictors-aka-time-varying-covariates-tvcs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Level 1 predictors AKA Time-varying covariates (TVCs)&lt;/h1&gt;
&lt;p&gt;These are predictors that are assessed at level 1, which repeate. Note that there are some variables that are inherently level 2 (e.g. handedness), some that make sense more as a level 1 (e.g., mood) and some that could be considered either depending on your research question and/or your data (e.g. income). The latter type could concievably change across time (And thus be appropriate for a level 1 variable; tvc) but may not change at the rate of your construct or not be important.&lt;/p&gt;
&lt;p&gt;We will go into these in more depth in further weeks. The two points I want to discuss now are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;These can be treated as another predictor with the effect of “controlling” for some TVC. Thus the regression coefficents in the model are conditional on this covariate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, if you had group status (yes, no) as your TVC the fixed effect for this would indicate the difference in slope for the two conditions. The slope coefficient would be that average slope (depending on how the covariate is scaled)&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The level 1 and level 2 models are not that different from previous forms. Here is an example model with a TVC.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}Job_{ij} +\varepsilon_{ij} \]&lt;/span&gt;
Level 2:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +    U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + U_{1j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} \]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It is not necessary to specify a random effect for the TVC. Doing so would suggest that the differences in group membership within a person is not the same across people. For example, the effect of jobloss may effect some peoples development but not others.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key question is whether or not we think the variability across people in their TVC effects are systematic or not. If they are systematic, then maybe it is important to predict them by another variable. Can we go further and also fit a random effects term? This is a tricky issue in that this adds an additional parameter to the random effects and thus increases the number of covariances estimated. Often our data are not large enough to estimate the increased number of parameters and results in non-convergence.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The introduction of the TVC can reduce &lt;span class=&#34;math inline&#34;&gt;\(\tau^2_{U_{0j}}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau^2_{U_{1j}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ij}\)&lt;/span&gt;. Normal time-invariant covariates only reduce the between person variance in intercept and slope and cannot account for the within person variance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But, but, because you are adding a new variable that changes the interpretation of the gamma terms, you may actually get increases in your variance components. As a result, it is difficult to directly compare models that have TVCs and those that do not.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;You may need to seperate between person and within person effects for TVC. This is done through various centering techniques.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 2</title>
      <link>/lectures/03-growth-curves/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/03-growth-curves/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#growth-curves&#34;&gt;Growth curves&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#between-person-models-and-cross-sectional-data&#34;&gt;Between person models and cross sectional data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#within-person-models-e.g.-2-level-models&#34;&gt;Within person models e.g., 2-level models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#thinking-about-random-effects&#34;&gt;Thinking about random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects&#34;&gt;Random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#empty-model-equation&#34;&gt;Empty model equation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#putting-it-together&#34;&gt;Putting it together&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualize-what-you-are-doing&#34;&gt;Visualize what you are doing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#icc&#34;&gt;ICC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-time&#34;&gt;Adding time&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-does-this-look-like-graphically&#34;&gt;What does this look like graphically?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-a-random-slope&#34;&gt;Adding a random slope?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#individual-level-random-effects&#34;&gt;Individual level random effects&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#calculation-of-individual-level-random-effects&#34;&gt;Calculation of individual level random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-are-these-random-effects-calculated&#34;&gt;How are these random effects calculated?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effect-decomposition&#34;&gt;Random effect decomposition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;growth-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Growth curves&lt;/h1&gt;
&lt;div id=&#34;between-person-models-and-cross-sectional-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Between person models and cross sectional data&lt;/h2&gt;
&lt;p&gt;You already know this, but it gives us a chance to review regression&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... +\epsilon_{i} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Parameters are considered fixed where one regression value corresponds to everyone. I.e., that association between X1 and Y is the same for everyone.&lt;/p&gt;
&lt;p&gt;Each person has a Y, denoted by the subscript i, and each has a residual associated with them, also designated by i.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
example &amp;lt;- read_csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/ALDA/example copy.csv&amp;quot;)
example$ID &amp;lt;- as.factor(example$ID)
# you can find the data on my github at: https://github.com/josh-jackson/ALDA/example%20copy.csv&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets look at some data. These data examine older adults who came into a study up to six times over a six year period. Multiple cognitive, psychiatric and imaging assessments were done. Let’s look at functional connectivity network called SMN7.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ───────────────────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1           ✔ dplyr   0.8.3      
## ✔ tibble  2.1.3           ✔ stringr 1.4.0      
## ✔ tidyr   0.8.99.9000     ✔ forcats 0.4.0      
## ✔ purrr   0.3.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ───────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
gg1 &amp;lt;- ggplot(example,
   aes(x = week, y = SMN7)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;)   
print(gg1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/03-Growth-curves_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What happens if we run a regression?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regression &amp;lt;- lm(SMN7 ~ week, data = example)
summary(regression)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = SMN7 ~ week, data = example)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.099294 -0.039929 -0.005938  0.032715  0.169885 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 0.100161   0.005261  19.039   &amp;lt;2e-16 ***
## week        0.004087   0.002563   1.595    0.112    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.05562 on 214 degrees of freedom
##   (9 observations deleted due to missingness)
## Multiple R-squared:  0.01174,    Adjusted R-squared:  0.007124 
## F-statistic: 2.543 on 1 and 214 DF,  p-value: 0.1123&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;within-person-models-e.g.-2-level-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Within person models e.g., 2-level models&lt;/h2&gt;
&lt;p&gt;We saw this last time where we can think of everyone being run in a separate regression model. Here the lines connect the dots of the same people across time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
gg2 &amp;lt;- ggplot(example,
   aes(x = week, y = SMN7, group = ID)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)   

gg3 &amp;lt;- gg2 +  stat_smooth(data = example, aes(x = week, y = SMN7, group=1, colour=&amp;quot;#990000&amp;quot;), method = &amp;quot;lm&amp;quot;, size = 3, se=FALSE) + theme(legend.position = &amp;quot;none&amp;quot;)
print(gg3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/03-Growth-curves_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each person has multiple assessments, so we need to distinguish between people and their assessments. In normal regression we wouldn’t think about this as everyone datapoint is assumed to be independent. However, this is not the case here. Failing to distinguish would lead to violation of independence, an important assumption of the standard regression model.&lt;/p&gt;
&lt;p&gt;As seen in the graph above, what we have now is both individual level slopes as well as an average level slope. The average level slope is going to be the average of the individual level slopes, which will look like our average slope ignoring all dependencies. Same for the intercept.&lt;/p&gt;
&lt;p&gt;One way to do this is to run separate regressions for each person. Then we could just pool (or average) together where people start and how much they change to get the average intercept (starting value) and trajectory (how much people change). We will see later that this is a somewhat poor approach.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;ID&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regressions &amp;lt;- example2 %&amp;gt;% 
  group_by(ID) %&amp;gt;% 
  do(tidy(lm(SMN7 ~ week, data = .)))

head(regressions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
## # Groups:   ID [3]
##   ID    term        estimate std.error statistic  p.value
##   &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 67    (Intercept)  0.0921    0.0161       5.72   0.0292
## 2 67    week         0.00662   0.00657      1.01   0.420 
## 3 75    (Intercept)  0.126   NaN          NaN    NaN     
## 4 75    week         0.00771 NaN          NaN    NaN     
## 5 87    (Intercept)  0.0787  NaN          NaN    NaN     
## 6 87    week        -0.0227  NaN          NaN    NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to the average intercept and the average trajectory there is also the amount of variation around each of these estimates. Do people tend to change the same? Are there individual differences in the initial assessment?&lt;/p&gt;
&lt;p&gt;This type of meaningful variation is lost when we have a between subjects only model that ignores the individual level. This variation will be called Random Effects (or variance estimates in SEM).&lt;/p&gt;
&lt;p&gt;[Side note: note how some people do not have se estimates for their regression coefficients. The reason for this will impact our ability to fit longitudinal models later on.]&lt;/p&gt;
&lt;p&gt;There is another important source of variation different from standard regression models. The within-subjects error that can be seen in the below graph. If we did not take people into account and just collapsed across people to get a between subjects assessment of change, this error would be confounded with individual differences in change. We will discuss this error more in depth later, but one way to think about our goal is to utilize our repeated assessments to make better predictions. A way to do that is to create additional buckets of explained variance, resulting in a smaller bucket of unexplained variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example3 &amp;lt;- example2 %&amp;gt;% 
  filter(ID == &amp;quot;67&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg4 &amp;lt;-  ggplot(example3, aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;)

gg4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/03-Growth-curves_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;thinking-about-random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thinking about random effects&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random effects&lt;/h3&gt;
&lt;p&gt;Within subjects variability in either starting value or slope/trajectory is referenced in terms of random effects. How do we represent this in our equation? Easy, we just say that the typical regression parameters we have are not the same for everyone – that they are random (in contrast to fixed).&lt;/p&gt;
&lt;p&gt;In general, when would we want to use random effects? If there is some sort of selection (random or not) of many possible values of the predictor (e.g., stimuli are 3 random depression drugs, three semi random chosen levels of a drug). With longitudinal data this is (random) people.&lt;/p&gt;
&lt;p&gt;Side bar: Even in situations where these levels are not random (eg working with U.S. states) it is still useful to use MLM and we still call them random effects. To be consistent with language, random here can refer to as random from the population average, not randomly selected. When talking about “random effects” you can mean either of these definitions (and a few more). Luckily we can mostly ignore these semantic issues.&lt;/p&gt;
&lt;p&gt;What is necessary for modeling random effects? For longitudinal models, there needs to be multiple assessments per your grouping category (people, schools, neighborhoods, trials).&lt;/p&gt;
&lt;p&gt;We are assuming these random effects are sampled from some population and thus vary from group to group (or person to person). This means that your coefficients (like traditional regression coefficients) are estimates of some population parameter and thus have error associated with them. This error is not like like a standard residual, which represents error for your overall model. Nor is it like the standard error for a point estimate. Random effects can best be thought of as deviation of individual regression lines from the group regression line (though it technically is not this).&lt;/p&gt;
&lt;p&gt;To facilitate the multiple assessments per person we will now use both i and j subscripts. We will see that the random effects are part of the overall error term in the model. Counterintuitively, the main focus of these types of models will be the fixed effects, with less attention paid to the random effects. That said, the random effects are necessary to account for dependency in the data. One can think about these models as normal fixed effects regressions, with the random effects there to account for the longitudinal nature of the data. They are made up of a number of standard regression equations, each for a single individual. Doing so side steps the trouble of having correlated errors, and thus allows us to interpret our findings without concern.&lt;/p&gt;
&lt;p&gt;To facilitate adding random effects to our model it is helpful to think about two “levels” to our regression equation. We are going to put a regression equation within our regression equation. (Que Xzhibit joke). The first level will be the within-person model, in that it described how people differ across time. The second level will be the between person level. Note that these do not correspond to fixed or random effects. Instead they can be thought to model either within person differences or between person differences. Mastering thinking at these two levels will help make sense of these MLM models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empty-model-equation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Empty model equation&lt;/h3&gt;
&lt;p&gt;Let’s start with the most basic model and then expand from there.&lt;/p&gt;
&lt;p&gt;Level 1 - within person
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  +\varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that we have multiple responses per individual j, noted with an i to refer to specific times.&lt;/p&gt;
&lt;p&gt;Also note that the intercept has a subscript. In typical regression it does not. This suggests that not everyone has the same intercept.&lt;/p&gt;
&lt;p&gt;The residuals at this level are thought of as measurement error OR as something that can be explained by time varying predictors.&lt;/p&gt;
&lt;p&gt;Level 2 - between person
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + U_{0j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2 takes the intercept (or other parameter) at level 1 and breaks it down into an equation for each individual, j. An overall group average (the gamma) and a residual term specific to deviation around the intercept (see below).&lt;/p&gt;
&lt;p&gt;And two variance components:
1. a random effect of the intercept
&lt;span class=&#34;math display&#34;&gt;\[ {U}_{0j} \sim \mathcal{N}(0, \tau_{00}^{2})  \]&lt;/span&gt;
The subscript of the &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; refers to the number of the parameter where 0 is the intercept, 1 is the first regression coefficient, and so on. The second refers to the individual, j. So &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; refers to the intercept whereas &lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; would refer to the random effect of the first regression coefficient.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; random effect is said to be normally distributed with a mean of zero and a variance of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the residual error term
&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;
Much like in normal regression there is an error term for all of the variation we cannot account for. What is unique here is that we took that normal variation and split it into two components. One that is attributable to variation around the intercept &lt;span class=&#34;math inline&#34;&gt;\({U}_{0j}\)&lt;/span&gt; and a catch all residual.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Technically this is not a growth model, nor one that is inherently longitudinal. However, it does serve as a nice starting point to identify random effects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-together&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Putting it together&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-what-you-are-doing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualize what you are doing&lt;/h3&gt;
&lt;p&gt;Imagine the raw data plotted without knowing person j, how would &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i}\)&lt;/span&gt; be calculated?&lt;/p&gt;
&lt;p&gt;Now think about the data plotted again but with knowing each person has their own intercept. How would &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ij}\)&lt;/span&gt; be calculated?&lt;/p&gt;
&lt;p&gt;Finally, how is &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; calculated?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;icc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ICC&lt;/h3&gt;
&lt;p&gt;If the ICC is greater than zero, we are breaking standard regression assumptions.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{U_{0j}}{U_{0j}+ \varepsilon_{ij}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Is defined as % variation between over total variance.&lt;/p&gt;
&lt;p&gt;ICC can also be interpreted as the average (or expected) correlation within a nested group, in this case a person. On other words, the ICC is the correlation between any person’s repeated measures (technically residuals).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding time&lt;/h2&gt;
&lt;p&gt;Here is the basic growth model where our predictor is a time variable&lt;/p&gt;
&lt;p&gt;Level 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note how similar this looks like to a normal regression equation. Again, the differences are due to those pesky subscripts. Like before, think of this as a normal regression equation at the level of a person. Each person would have one of these equations with, in addition to a unique Y, X and residual, a unique &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;. Look above to those individual regressions we did at the start of this section.&lt;/p&gt;
&lt;p&gt;Level 2:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2 takes the parameters at level 1 and decomposes them into a fixed component that reflects that average and then the individual deviations around that fixed effect. &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; is not error in the traditional sense. It describes how much variation there is around that parameter. Do some people start higher while some start lower, for example.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The new level 2 term refers to the first predictor in the level 1 regression equation ie the slope. This slope is fixed in that the level 2 equation only has a gamma term and no U residual term.&lt;/p&gt;
&lt;p&gt;Putting it together:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{10} (X_{1j})+ U_{0j}  + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that in computing a single individuals Y, it depends on the two fixed effects, the Xj, and the random effect for the intercept.&lt;/p&gt;
&lt;div id=&#34;what-does-this-look-like-graphically&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What does this look like graphically?&lt;/h3&gt;
&lt;p&gt;And how does this differ from the random intercept model?&lt;/p&gt;
&lt;p&gt;Can you draw out the sources of error? The random effects for each participant? The fixed effects?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-a-random-slope&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding a random slope?&lt;/h3&gt;
&lt;p&gt;What happens when we add a random slope?
Level 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{1j} + \varepsilon_{ij} \]&lt;/span&gt;
Level 2:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Putting it together:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{10}(X_{ij})+ U_{0j} + U_{1j}(X_{ij}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Can think of a persons score divided up into a fixed component as well as the random component.&lt;/p&gt;
&lt;p&gt;These random effects are likely related to one another. For example, if someone starts high on a construct they are then less likely to increase across time. This negative correlation can be seen in the residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     &amp;amp; \tau_{00}^{2} &amp;amp; \tau_{01}\\ 
  0, &amp;amp; \tau_{01} &amp;amp; \tau_{10}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that it is possible to have a different error structures, one where there is no relationship between the intercept and the slope, for example. We will discuss this more later in the semester. Right now just know that the default is to have correlated random effects.&lt;/p&gt;
&lt;p&gt;We also have the within subject variance term that accounts for deviations that are not accounted for by time variable and other level 1 predictors.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that it is possible to model these level 1 residuals with different structures. This specification implies that there is no correlation across an individuals residuals, once you account for level 1 predictors (ie growth trajectories). Having a specific level 1 autoregressive or other type of pattern is common in other treatments of longitudinal models (panel models) but is not necessary with growth models (but possible).&lt;/p&gt;
&lt;p&gt;This is the basic format of the growth model. It will be expanded later on by adding variables to the level 1 model and to the level 2 model. Adding to the level 1 model is only possible with repeated variables.&lt;/p&gt;
&lt;p&gt;Level 1 regression coefficients are added to the level 2 model. These coefficients are decomposed into a fixed effect, a random effect (possibly), and between person predictors. As with any regression model, each of these only have a single error term.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;individual-level-random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Individual level random effects&lt;/h2&gt;
&lt;div id=&#34;calculation-of-individual-level-random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculation of individual level random effects&lt;/h3&gt;
&lt;p&gt;Random effects are often thought in terms of variance components. We can see this if we think of individual level regressions for each person where we then have a mean and a variance for both the intercept or the slope. The greater the variance around the intercept and the slope means that not everyone starts at the same position and not everyone changes at the same rate.&lt;/p&gt;
&lt;p&gt;If you want to look at a specific person’s random effect you can think of it as a deviation from the fixed effect where subject 6’s intercept can be thought of as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{06} = \gamma_{00} \pm U_{06}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;e.g 2.2 = 3 - .8&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-are-these-random-effects-calculated&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How are these random effects calculated?&lt;/h3&gt;
&lt;p&gt;It isn’t as straightforward as calculating a slope for each person and then using the difference between that slope and the average slope. Instead, the estimates are partially pooled towards the overall mean of the sample, the fixed effect. We do this to get a better estimate of the parameters, the same way that using regression to predict y-hat given an X is better than binning X and calculating y-hat. More information = better.&lt;/p&gt;
&lt;p&gt;Why not full pooling ie give everyone the same slope? Because it ignores individual differences in change. Often individual differences in (intraindividual) change is what we care about.&lt;/p&gt;
&lt;p&gt;The result is that the variance of the change trajectories (using MLM) will be smaller than the variance of the fitted linear models. Trajectories are “regressed” towards the average trajectory under the assumption that extreme scores are extreme because of (measurement) error, not that people are actually extreme.&lt;/p&gt;
&lt;p&gt;Can think about this in terms of creating an average for your intercept. Do you want the average to be the grand mean average, ignoring group? Do you want it to be the person average, ignoring that some people have more data points and thus are better assessed? No right answer, so maybe lets meet in the middle? This is sometimes called an empirical Bayes estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effect-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random effect decomposition&lt;/h3&gt;
&lt;p&gt;Think of the original total variance in a scatter plot of our DVs. Adding random effects takes that variance and trims it down.&lt;/p&gt;
&lt;p&gt;The intercept only MLM separates it into a level 1 variance (which at this stage is treated as error) and a level 2 random intercept variance.&lt;/p&gt;
&lt;p&gt;Creating a random slopes model takes the Level 1 residual variance and creates a new “pile” of explained or accounted for variance.&lt;/p&gt;
&lt;p&gt;We can then further explain the variance or reduce the pile by predictors at level 1 and level 2. Our goal isn’t necessarily to explain all of the variance but it is helpful to reduce the unexplained variance &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ij}\)&lt;/span&gt; to improve model fit.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 1</title>
      <link>/lectures/02-lda-basics/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/02-lda-basics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lda-basics&#34;&gt;LDA basics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#procedural&#34;&gt;Procedural&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#goals&#34;&gt;Goals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivation-terms-concepts&#34;&gt;Motivation, terms, concepts&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#why-longitudinal-what-can-we-ask&#34;&gt;Why longitudinal? What can we ask?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-change-most-common&#34;&gt;Types of change (most common)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#between-person-versus-within-person-variables&#34;&gt;Between person versus within person variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling-frameworks-mlm-sem&#34;&gt;Modeling frameworks: MLM &amp;amp; SEM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#meaningful-time-metric&#34;&gt;Meaningful time metric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thinking-through-longitudinal-data-example&#34;&gt;Thinking through longitudinal data example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#person-level&#34;&gt;Person level&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#doing-this-with-mlm-and-sem&#34;&gt;Doing this with MLM (and SEM)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#design-considerations&#34;&gt;Design considerations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#number-of-assessment-waves&#34;&gt;1. Number of assessment waves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#measurement&#34;&gt;2. Measurement&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#threats-to-validity&#34;&gt;Threats to validity&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#missing-data&#34;&gt;1. Missing data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attritionmortality&#34;&gt;2. Attrition/Mortality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#historycohort&#34;&gt;3. History/cohort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maturation&#34;&gt;4. Maturation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing&#34;&gt;5. Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selection&#34;&gt;6. Selection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-not-rm-anova&#34;&gt;Why not RM ANOVA?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;lda-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;LDA basics&lt;/h1&gt;
&lt;div id=&#34;procedural&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Procedural&lt;/h2&gt;
&lt;p&gt;By virtue of being on this page, you know the class website. You may access all of the code and datasets and everything that is used to create the lectures through my github: &lt;a href=&#34;https://github.com/josh-jackson/ALDA&#34; class=&#34;uri&#34;&gt;https://github.com/josh-jackson/ALDA&lt;/a&gt;. The provided code should be enough but if you want to search farther go for it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goals&lt;/h2&gt;
&lt;p&gt;This first class is set to orientate you to the world of longitudinal data and MLM models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation-terms-concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation, terms, concepts&lt;/h2&gt;
&lt;div id=&#34;why-longitudinal-what-can-we-ask&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why longitudinal? What can we ask?&lt;/h3&gt;
&lt;p&gt;At least 7 reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Identification of intraindividual change (and stability). Do people increase or decrease with time or age. Is this pattern monotonic? Should this best be conceptualized as a stable process or something that is more dynamic? On average how do people change? Ex: people decline in cognitive ability across time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Inter-individual differences in intraindividual change. Does everyone change the same? Do some people start higher but change less? Do some increase while some decrease? Ex: not all people people decline in cognitive ability across time, but some do.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Examine joint relationship among intraindividual change for two or more constructs. If variable X goes up does variable Y also go up across time? Does this always happen or only during certain times? Is this association due to a third variable or does it mean that change occurs for similar reasons? Ex: changes in cognitive ability are associated with changes in health across time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Determinants of intraindividual change. What are the repeated experiences that can push construct X around. Do these have similar effects at all times? Ex: people declinein cognitive ability across time. Ex: I have better memory compared to other times when I engage in cognitive activities vs times that I do not.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Determinants of interindividual differences in intraindividual change. Do events, background characteristics, interventions or other between person characteristic shape why certain people change while others don’t? Ex: people decline less in cognitive ability across time if tend to do cognitively engaging activities.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Inter-individual differences in intraindividual fluctuation and determinants of intraindividual fluctuation. Does everyone vary the same? Why are some more variable than others? Ex: Someone who is depressed fluctuates more in happiness than someone who is not depressed&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Are there different classes/populations/mixtures of intraindividual change? Ex: do people who decrease vs don’t in cognitive ability across time exist as different groups? (Vs construing differences as on a continuum).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;types-of-change-most-common&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Types of change (most common)&lt;/h3&gt;
&lt;p&gt;There are many ways to think of change and stability. We will only have time to go into a few of these types, but it is helpful to think about what type you are interested in when you plan a project or sit down to analyze data. “Change” can mean different things. The above questions you can ask mostly map onto #3 definition of change below.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Differential / rank order consistency/ rank order stability. Goes by many names but in the end it is just a correlation. This is a group/sample/population level variable and indexes the relative standing of a person with regard to the rest of the members in the sample. Does not take into account mean structure. Best used with heterotypic continuity where the construct may be the same but the measurement of the construct changes e.g., childhood IQ or acting out in school versus when you are an adult.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A specialized case of this is ipsative change, which looks at the rank order of constructs within a person. This is not done on a single variable (depression) but on a broad number of them (all PD symptoms). Often uses profiles.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Mean level/ absolute change. Takes into account mean structure and indexes absolute levels of a construct. A strong assumption is that the construct means (not a pun) the same thing across time. That is, my measure of depression is interpreted the same for a 40 year old and a 90 year old if I want to look at absolute differences between the two ages.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mean level change is not dependent at all on rank order consistency. Can have no mean level change and high rank order consistency and vice versa.&lt;/p&gt;
&lt;p&gt;Perfect rank order, mean level increase&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ────────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1          ✔ purrr   0.3.2     
## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ readr   1.3.1          ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simp&amp;lt;- tribble(
  ~ID,  ~Y, ~time,
1,5,1,
1,7,2,
2,4,1,
2,6,2,
3,3,1,
3,5,2,
4,2,1,
4,4,2,
5,1,1,
5,3,2)

ro.ml &amp;lt;- ggplot(simp, aes(x=time, y=Y)) +
    geom_point() +   
   stat_summary(fun.y = mean, geom=&amp;quot;line&amp;quot;, size = 4) +
    geom_smooth(aes(group = ID), method=lm,  
                se=FALSE)  

ggsave(&amp;quot;ro.ml.png&amp;quot;, ro.ml)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Saving 7 x 5 in image&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No rank order, mean level increase&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
simp&amp;lt;- tribble(
  ~ID,  ~Y, ~time,
1,1,1,
1,5,2,
2,1.5,1,
2,4.5,2,
3,2,1,
3,4,2,
4,2.5,1,
4,3.5,2,
5,3,1,
5,3,2)

noro.ml&amp;lt;- ggplot(simp, aes(x=time, y=Y)) +
    geom_point() +   
   stat_summary(fun.y = mean, geom=&amp;quot;line&amp;quot;, size = 4) +
    geom_smooth(aes(group = ID), method=lm,  
                se=FALSE)  
ggsave(&amp;quot;noro.ml.png&amp;quot;, noro.ml)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Saving 7 x 5 in image&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Individual differences in change. Rank order and mean level provide an index of change and or stability for the sample. Here this provides an assessment of change for an individual. For example, if it is typical to decline in cognitive ability do some people buck the trend and stay at their past level? Individual differences in change get at both mean level changes as well as the tendency of the sample to show stability. It is the type of change that we will focus on the most.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Structural. Does the construct (or measure) change across time? The assumption for mean level change assumes that the measurement properties stays the same. But maybe it is theoretically interesting to ask whether what you are measuring changes. Examples include practice effects, age effects (cog ability in kids vs adults), differences due to health and life events.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Variance. Does your experiment lead to an increas in variability in response? You may show no mean levels (which is what is looked at in typical t-tests and ANOVAs) but you could see people increase or decrease in their expected range of response.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So how do we refer to ‘change’? Usually it is easier to refer to pictorially or in terms of an equation. Putting a word onto it usually causes some confusion, which is why there are a lot of redundant terms in the literature. All of these might refer to the same thing when used within a model. However, the names of some models use these terms differently and thus can refer to different models or conditions that you are working with. In this class I will try to point out the important differences but you will be fine if you supplement your terms with graphs or equations. Math is the great equalizer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;between-person-versus-within-person-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Between person versus within person variables&lt;/h3&gt;
&lt;p&gt;Between-person versus within-person are the shortened version of interindividaul differences in change versus intraindividaul differences in change. Refers to across people versus within a particular person. Do you care about how people differ from their previous and future self or do you care about how people differ from other people? (See examples in the 7 types of questions we can ask)&lt;/p&gt;
&lt;p&gt;Often we are interested in modeling both between person and within person variables simultaneously. This is related to Level 1 and Level 2 (for those of you familiar with this terminology). It is helpful to start thinking about what variables you are working with and whether they are within or between person variables. For predictors, it is typically the case that between person effects are constant (between person) variables (e.g., gender) that do not change from assessment to assessment or are only assessed once. In contrast, within person questions are best understood by time varying predictors (within person variables e.g., daily mood) that are assessed more than once.&lt;/p&gt;
&lt;p&gt;We will incorporate both time invariant (between person) and time varying (within person) predictors into our eventual model. In addition to thinking about the types of questions you want to ask it is important to think about what “type” or variables you are working with. Your choice of questions you can ask depends on how often you assess variables or how you conceptualize them.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-frameworks-mlm-sem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modeling frameworks: MLM &amp;amp; SEM&lt;/h3&gt;
&lt;p&gt;In this class (and in the field) two primary techniques are used with longitudinal models: MLM and SEM. At some levels they are completely equivalent. At others, one is better than the other and vice versa.&lt;/p&gt;
&lt;p&gt;MLM/HLM is a simple extension of standard regression models. As a result it is easy to interpret and implement. In terms of longitudinal data it is best suited to run models when the time of measurement differs from person to person (compared to equal intervals). For this class we will use lme4 and brms as our MLM program but there are many others we could use e.g., nlme.&lt;/p&gt;
&lt;p&gt;SEM is related to regression in that regression is a subset of SEM techniques. In other words, an SEM program could run a simple regression analysis.&lt;/p&gt;
&lt;p&gt;The primary advantage of MLM is that you may have assessment waves that vary in length between participants. An assumption of SEM models is that everyone has the same amount of time between assessment waves (though this assumption can be relaxed). MLM is also better suited for complex error structures and complex nesting above and beyond assessments within person. It is also easier to model interactions. Currently, it is easier to do MLM within a Bayesian framework too.&lt;/p&gt;
&lt;p&gt;SEM primary advantage is the ability to account for measurement error via latent assessment of the repeated measures. Other advantages include the ability to model multiple DVs at once, and do so in a flexible manner to look at, for example, the associations between change in one construct and change in the another (though these are also possible with MLMs). Another major advantage is the ability to look at latent groups via latent class or mixture models.&lt;/p&gt;
&lt;p&gt;Bottom line: MLM is probably best suited for “basic” or “standard” growth models. More complex analyses of change with multiple variables would benefit from an SEM approach. This is also an oversimplification.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;meaningful-time-metric&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Meaningful time metric&lt;/h2&gt;
&lt;p&gt;Time is the most important part of a longitudinal analyses. Without some sort of explicit operationalization of time or thought into how you handle time in your analyses you are not conducting longitudinal analyses. The key to interpreting your output is to know how you handled your time variable. What units is it in? Does everyone have the same differences between assessments? Is time something you are explicitly interested in or merely there as a means to collect repeated measures? We will discuss more of these as the semester progresses. Right now however an important distinction is what should the scale of our x-axis variable, time, be in?&lt;/p&gt;
&lt;p&gt;At one level, the distinction is relevant to what is the process that is changing someone? Is it a naturally occurring developmental process? Then maybe age is the best metric. What about tracking child’s cognitive ability, something that might be influenced by level of schooling? Here grade may be more important than age. Another common metric is time in study. This may be useful if you are running an intervention or if you want to put everyone on the same starting metric and then control for nuisance variables like age or schooling level. Similarly, year of study as a prime time candidate may be useful if you are working from panel studies and interested in historical events and or cohort effects. A wave variable (ie study measurement occasion) may be good enough to use as a time metric (though this makes some assumptions about the regularity of assessments both within and across people).&lt;/p&gt;
&lt;p&gt;Depending on your choice of time metric you may see different rates of change and variability in change. For psychological applications the most common would be age and time in study (followed by grades for assessments of kids). Age is nice because it captures a number of developmental processes thought to drive change (maturation, history, time-in-study) but does not identify a single reason. Time in study is the opposite in that it does not index any other type of change but that simplicity aides in testing different reasons for change (e.g, age moderation). Thus choosing one type of time metric will naturally guide the types of questions you are able to address. E.g. if you use age as your time metric you won’t be able to control for age or examine the effects of age as simply as if you used time in study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;thinking-through-longitudinal-data-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thinking through longitudinal data example&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using some resting state imaging data, lets think about how we can model and think about this data using our current skills (ie standard regression and plotting)&lt;/p&gt;
&lt;p&gt;We defined time as year in study. How would this look if we used age?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg1 &amp;lt;- ggplot(example,
   aes(x = year, y = SMN7, group = ID)) + geom_point()  
print(gg1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above graph just plots datapoints. Do we have repeated assessments per person? Lets find out.&lt;/p&gt;
&lt;div id=&#34;person-level&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Person level&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg2 &amp;lt;- ggplot(example,
   aes(x = year, y = SMN7, group = ID)) + geom_line()  
gg2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
Note that some people start at different levels. Some people have more data in terms of assessment points and years. Note that the shape of change isn’t necessarily a straight line.&lt;/p&gt;
&lt;p&gt;We often want to look at this at a per person level to get more info.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg3 &amp;lt;- ggplot(example,
   aes(x = year, y = SMN7, group = ID)) + geom_line() +  geom_point() + facet_wrap( ~ ID)
gg3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
As part of our dataset we have different groups. A question we may have is if they change differently across time. Lets take a look at this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg4 &amp;lt;- ggplot(example,
   aes(x = year, y = SMN7, group = ID)) + geom_line() + facet_grid(. ~ group)
gg4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re not in Kansas anymore - look at the technocolor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg5 &amp;lt;-  gg2 + aes(colour = factor(ID)) + guides(colour=FALSE) 
gg5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Okay, beside the occular technique, we’re going to need to do something more to address our theoretical questions. Lets look at some random people in the sample and run some regressions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(11)
ex.random &amp;lt;- example %&amp;gt;% 
  dplyr::select(ID) %&amp;gt;% 
  distinct %&amp;gt;% 
  sample_n(10) 

example2 &amp;lt;-
  left_join(ex.random, example)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;ID&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg6 &amp;lt;- ggplot(example2,
   aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method=&amp;quot;lm&amp;quot;) + facet_wrap( ~ID)
gg6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lets look at individual level regressions&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(broom)

regressions &amp;lt;- example2 %&amp;gt;% 
  group_by(ID) %&amp;gt;% 
  do(tidy(lm(SMN7 ~ week, data=.)))

regressions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 6
## # Groups:   ID [10]
##       ID term        estimate std.error statistic  p.value
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1    67 (Intercept)  0.0921    0.0161      5.72    0.0292
##  2    67 week         0.00662   0.00657     1.01    0.420 
##  3    75 (Intercept)  0.126   NaN         NaN     NaN     
##  4    75 week         0.00771 NaN         NaN     NaN     
##  5    87 (Intercept)  0.0787  NaN         NaN     NaN     
##  6    87 week        -0.0227  NaN         NaN     NaN     
##  7    99 (Intercept)  0.111     0.0236      4.69    0.0426
##  8    99 week         0.00545   0.0122      0.446   0.699 
##  9   101 (Intercept)  0.111     0.0424      2.62    0.232 
## 10   101 week         0.0421    0.0217      1.94    0.303 
## 11   103 (Intercept)  0.0887  NaN         NaN     NaN     
## 12   103 week        -0.0168  NaN         NaN     NaN     
## 13   105 (Intercept)  0.0465    0.00658     7.06    0.0896
## 14   105 week         0.00122   0.00467     0.261   0.838 
## 15   142 (Intercept)  0.197   NaN         NaN     NaN     
## 16   142 week         0.0130  NaN         NaN     NaN     
## 17   149 (Intercept)  0.0801  NaN         NaN     NaN     
## 18   149 week         0.00497 NaN         NaN     NaN     
## 19   152 (Intercept)  0.0921  NaN         NaN     NaN     
## 20   152 week        -0.0172  NaN         NaN     NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What can we see? Estimates give us an intercept and regression coefficient for each person. Some people increase across time, some decrease. Some we cannot do statistical tests on – why?&lt;/p&gt;
&lt;p&gt;Well that is per person. Lets get the average starting value and change per week&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regressions %&amp;gt;% 
  group_by(term) %&amp;gt;% 
  summarise(avg.reg = mean(estimate))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   term        avg.reg
##   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;
## 1 (Intercept) 0.102  
## 2 week        0.00244&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets plot the average trend across everyone. Start with a best fit line not taking into account that people have repeated measures.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg7 &amp;lt;-  gg1 &amp;lt;- ggplot(example, aes(x = year, y = SMN7)) + geom_point() + stat_smooth() 
gg7&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That lowess line is a little strange. How about a linear estimate.&lt;/p&gt;
&lt;p&gt;Split up by group&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg8 &amp;lt;-  ggplot(example, aes(x = year, y = SMN7)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;) + facet_grid(. ~ group)
gg8&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But I also want to see the individual slopes, not just the average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg9 &amp;lt;- ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_point(alpha = 0.05) + stat_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)   

gg10 &amp;lt;- gg9 +  stat_smooth(data = example, aes(x = year, y = SMN7, group=1, color = &amp;quot;black&amp;quot;), method = &amp;quot;lm&amp;quot;, size = 2) + guides(fill=FALSE)


gg11 &amp;lt;- gg10 + facet_grid(.~ group) + theme(legend.position=&amp;quot;none&amp;quot;)

gg11&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing non-finite values (stat_smooth).

## Warning: Removed 9 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;doing-this-with-mlm-and-sem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Doing this with MLM (and SEM)&lt;/h3&gt;
&lt;p&gt;These regressions and plots are how you should begin to think about longitudinal data analysis. These growth models (the simplest form of longitudinal data analysis) are just a bunch of regressions for each person plus a little extra stuff. MLM is just a fancy regression equation. We want to create a line for everyone. Does someone go up? Does someone go down? On average, do people’s lines indciate that this construct increases or decreases? That is it. Seriously.&lt;/p&gt;
&lt;p&gt;What is different from normal regression? Extra error terms, mostly. For regression, we think of error as existing in one big bucket. For MLMs (and other longitudinal models) we will be breaking up unexplained variance (error) into multiple buckets.&lt;/p&gt;
&lt;p&gt;This is where fixed effects and random effects come into play. We will discuss this more next class, but the gist is that fixed effects are the regression coefficients you are used to. Fixed effects index group level change. Do people decline in memory across time, on average? Random effects vary among individuals (in the longitudinal models we are talking about) and index variation from the group. While most people decline in their memory, does everyone? In other words, an average trajectory will be termed a fixed effect and the random effect indexes how much variability there is around that group level effect. This extra variability we measure through random effects means that we are explaining more variance and thus there is less unexplained variance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;design-considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Design considerations&lt;/h2&gt;
&lt;div id=&#34;number-of-assessment-waves&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Number of assessment waves&lt;/h3&gt;
&lt;p&gt;Remember high school algebra: two points define a line. But, that assumes we can measure constructs without error. Three assessment points will better define changes in psychological variables. As a default, you need three waves of data to use MLM models. However, some simplifications can be made with MLM. Two wave assessments are mostly better with SEM approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Measurement&lt;/h3&gt;
&lt;div id=&#34;scale-of-measurement&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Scale of measurement&lt;/h4&gt;
&lt;p&gt;Measurement is always the basis for good quantiative analysis. Without good measurement you are just spitting into the wind. Standard measurement concerns remain (reliability, dimensionality) but extra concerns exist with longitudinal data.&lt;/p&gt;
&lt;p&gt;What does it mean for categorical variables to change over time? Can you imagine a trajectory for what this is measuring? How would dichotomous responses impact ability to measure change?&lt;/p&gt;
&lt;p&gt;What about ranks, such as in preference for school subjects? What if the class composition changes – what is this assessing? Given that ranks are related such that if I increase someone has to decrease, how does that impact change assessments?&lt;/p&gt;
&lt;p&gt;Can I analyze childhood and adult variables simultaneously if assess the same construct, even though they may be measured differently? How can you measure change in the same construct but with different measures? To assess math ability in 5 year olds you can ask them about addition, can you do that in a sample of 20 year olds? Does that measure continue to assess math ability?&lt;/p&gt;
&lt;p&gt;Often the answer to these is to use a different form of the longitudinal model. In general, the better measured the construct (continuous, not dependent on others, using the same scale) the more complex/sophisticated analysis you can run. Worse measurment leads to simplification both in terms of the models and the types of conclusions you can make.&lt;/p&gt;
&lt;p&gt;As with all of science, everything rests on measurement. Poor measurement results in poor conclusions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standardizing&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Standardizing&lt;/h4&gt;
&lt;p&gt;It is standard practice to z-score to get standardized responses. However, it is not straight forward to do so when using longitudinal data. Why would z-scoring your variables be problematic?&lt;/p&gt;
&lt;p&gt;First, if you scale for age, for example, this takes out a potential explanatory variable.&lt;/p&gt;
&lt;p&gt;Second, more worriesome, it also can add error if not everyone is standardized consistently (say if standardization is across age groups and someone just misses a cut). Or if the sample changes due to attrition.&lt;/p&gt;
&lt;p&gt;Third, is that you take away the mean for each assessment such that the expected change across time is zero. We will talk more about solutions to this problem as the semester progresses but the short answer is to avoid or to use SEM.&lt;/p&gt;
&lt;p&gt;While helpful for cross sectional analyses, z-scoring adds in layers of computational and interpretational problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reliability&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Reliability&lt;/h4&gt;
&lt;p&gt;The goal of longitudinal analyses is to understand why some construct changes or stays the same across time. A major difficulty in addressing this goal is whether you are able to accurately assess the construct of interest. One of the key characteristics (but not the only characteristic) is whether or not your assessment would be consistent if you gave an alternative measure or if you retook it immediately after your first assessment. This is known as reliability of measurement. To the extent that your measure is reliable it assesses true score variance as opposed to error variance. The amount of error score variance assessed is important given that error variance will masquerade as change across time given that error can correlate with anything else. The more error in your measurement the more change you will find. Of course this is unreliable change – change that is not true change, just stochastic noise.&lt;/p&gt;
&lt;p&gt;We can think of reliability two ways. First, reliability of the change estimate. This depends on how much error there is in the assessment and the number of waves. These two components are similar to inter item correlation and number of items being the two main components that effect reliability in cross sectional analyses. Increase the number of items (waves) you increase your alpha. Increase the average correlation among items, you increase your alpha. The parallel to average correlation among items is our ability to accurately assess the construct. When comparing a construct across time we examine this with measurement invariance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-invariance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Measurement invariance&lt;/h4&gt;
&lt;p&gt;The second way to think of reliability is in terms of how consistently the measure is assessed across time. Or, do you assess the same construct at each time? What would happen if we looked at change in IQ from 1st grade to 12 grade and used the first grade IQ test at each time? The construct that you assessed at the first wave is likely not the same assessed later.
To test this formally is called measurement invariance and is typically done through SEM. We will talk more about this later in the semester. Until we get there we make a large assumption that what we are measuring now is the same at each wave of assessment.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;threats-to-validity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Threats to validity&lt;/h2&gt;
&lt;div id=&#34;missing-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Missing data&lt;/h3&gt;
&lt;div id=&#34;types-of-missing-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Types of missing data&lt;/h4&gt;
&lt;p&gt;On a scale from 1 to you’re completely screwed, how confident are you that the missingness is not related to your study variables?&lt;/p&gt;
&lt;p&gt;Missing completely at random (MCAR) means that the missingness pattern is due entirely to randomness&lt;/p&gt;
&lt;p&gt;Missing at random (MAR) means that there is conditional randomness. Missingness may be due to other variables in the dataset. Pretty standard for longitudinal data.&lt;/p&gt;
&lt;p&gt;Not missing at random (NMAR) means that the missingness is systematic based on the missing values and not associated with measured variables. For example, in a study of reading ability, kids with low reading ability drop out, due to not liking to take tests on reading ability. However, if reading ability is associated with other variables in the model, then this missingness becomes closer in kind to MAR, and thus somewhat less problematic.&lt;/p&gt;
&lt;p&gt;Typically, we make the assumption we are working under MAR and thus we will have unbiased estimates when predictors of missingness are incorporated into the model.&lt;/p&gt;
&lt;p&gt;There are tests to distinguish MAR from NMAR but you cannot distinguish MCAR from MAR because it is based entirely on knowing something that you dont have.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-handle-missing-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How to handle missing data&lt;/h4&gt;
&lt;p&gt;Listwise? Nah&lt;/p&gt;
&lt;p&gt;Full information maximum likelihood and other ML approaches? Sure.
Multiple imputation? Cannot hurt. More on these approaches later.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;attritionmortality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Attrition/Mortality&lt;/h3&gt;
&lt;p&gt;Major contributor to missing data&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;historycohort&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. History/cohort&lt;/h3&gt;
&lt;p&gt;Know that the processes driving change can be due to a specific event or cohort.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maturation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. Maturation&lt;/h3&gt;
&lt;p&gt;Change may occur because of natural processes. Thus if you just follow someone across time they will likely change irregardless of say, if they are in the control group.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5. Testing&lt;/h3&gt;
&lt;p&gt;Having people take the same survey, test or interview multiple times may lead them to respond differently. Does that change result from development or does it result from them being familiar with the test?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;selection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;6. Selection&lt;/h3&gt;
&lt;p&gt;If you are looking at life events, know that life events are not distributed randomly. Moreover, people who stay in studies and even sign up for studies are different from those that do not. As a result, it is often hard to make internally valid inferences with longitudinal data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-not-rm-anova&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why not RM ANOVA?&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Cannot handle missing data&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assumes rate of change is the same for all individuals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Time is usually done with orthogonal polynomials, making it difficult to interpret or to model non-linear. In other words, you have flexibility on how you want to model time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Accounting for correlation across time uses up many parameters, MLM is more efficient.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can accommodate differences in time between assessment waves across participants&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Handles various types of predictors - continuous vs nominal &amp;amp; static vs dynamic&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bottom line: this is an old way of doing these analyses with no upside. Don’t do them.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
