<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Applied Longitudinal Data Analysis</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Applied Longitudinal Data Analysis</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 07 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Applied Longitudinal Data Analysis</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Homework 4-6 posted</title>
      <link>/post/homework-4-6-posted/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/homework-4-6-posted/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Week 11</title>
      <link>/lectures/week-11/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/week-11/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#suggested-readings&#34;&gt;Suggested readings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-wave-assessments&#34;&gt;Two wave assessments&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lords-paradox&#34;&gt;Lords Paradox&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#common-two-wave-models-in-path-analysis-form&#34;&gt;common two wave models in path analysis form&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#residualized-change-model&#34;&gt;Residualized change model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-change-score&#34;&gt;Latent change score&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#residualized-latent-change-score&#34;&gt;Residualized latent change score&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-models&#34;&gt;Conditional Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-group-models&#34;&gt;Multiple group models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bi-variate-latent-change-model&#34;&gt;Bi-variate latent change model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-differencechange-score-model&#34;&gt;Latent difference/change score model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-wave-latent-difference-score-model-lds&#34;&gt;Multiple wave Latent Difference Score model (LDS)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#predicting-change&#34;&gt;predicting change&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-intercept-and-growth-factors-to-lds-models&#34;&gt;adding intercept and growth factors to LDS models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#steyer-models&#34;&gt;“steyer” models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-time-varying-covariates&#34;&gt;adding time varying covariates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-duel-change-score-model-like-a-residualized-latent-change-score&#34;&gt;Latent Duel change score model (like a residualized latent change score)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bivariate-multi-wave-latent-change-model&#34;&gt;Bivariate multi wave latent change model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#alt-and-alt-sr&#34;&gt;ALT and ALT-SR&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;suggested-readings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Suggested readings&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.annualreviews.org/doi/abs/10.1146/annurev.psych.60.110707.163612&#34; class=&#34;uri&#34;&gt;https://www.annualreviews.org/doi/abs/10.1146/annurev.psych.60.110707.163612&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S187892931730021X#sec0125&#34; class=&#34;uri&#34;&gt;https://www.sciencedirect.com/science/article/pii/S187892931730021X#sec0125&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-wave-assessments&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Two wave assessments&lt;/h1&gt;
&lt;p&gt;There are a lot of pre-post designs. In fact, the recent Nobel prize winners have a famous paper looking at difference in difference designs, basically looking at two group pre-post tests. Collecting longitudinal data is hard, so it makes sense that a lot of this type of data will be laying around. How do we analyze it? The first thing to notice is that we jumped right into MLM this semester, bypassing simple discussions. As a result, we can only really look at change with 3 or more waves.&lt;/p&gt;
&lt;p&gt;How to measure change, or should we? &lt;a href=&#34;https://www.gwern.net/docs/dnb/1970-cronbach.pdf&#34; class=&#34;uri&#34;&gt;https://www.gwern.net/docs/dnb/1970-cronbach.pdf&lt;/a&gt; This paper lays out some of the problems that occur with standard treatments of two wave assessments.&lt;/p&gt;
&lt;p&gt;The most basic two wave form of change is a difference score. However, many have said these are problematic.
The issues are:
1. hard to separate measurement error from true change
2. unreliable estimate of change
3. initial level (or last level) may be driving change. How to account for?&lt;/p&gt;
&lt;p&gt;The second alternative is a standard residual gain/change score where you regress time 2 onto time 1. This overcomes some of the issues raised about because we are being conservative about the error by “regressing to the mean” such that people with larger changes than average will have their change scores “shrunken” to the average, must like we do with MLMs. This also helps with accounting for starting values that may be responsible for the changes, as this is literally controlling for the initial level.&lt;/p&gt;
&lt;p&gt;The issues with this however are:
1. it isn’t true change, as you are implying people change similarly
2. it does not account for unreliability of change in a principled way
3. error, which should be random, is considered as change (ie residual is re-characterized) and it is likely associated with T1.&lt;/p&gt;
&lt;div id=&#34;lords-paradox&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lords Paradox&lt;/h2&gt;
&lt;p&gt;This has lead to what is known as Lord’s paradox. Take the two approaches above, simplified to:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;lm(t2-t1 ~ group)&lt;/code&gt;
&lt;code&gt;lm(t2 ~ t1 + group)&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(df)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id group          T1          T2        change
## 1  1    Tx -0.30176644 -0.07218389  0.2295825445
## 2  2    Tx  0.06935731  0.09741980  0.0280624915
## 3  3    Tx  0.27111029  0.12699551 -0.1441147849
## 4  4    Tx -0.58642443 -0.16449642  0.4219280069
## 5  5    Tx  0.10728117  0.07408057 -0.0332006005
## 6  6    Tx  0.12651397  0.12665183  0.0001378524&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;change score/gain score model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(change ~ group, df)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = change ~ group, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.5174 -0.1133  0.0208  0.1310  0.4612 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)  
## (Intercept)  0.038975   0.017300   2.253   0.0254 *
## groupControl 0.004028   0.024466   0.165   0.8694  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.173 on 198 degrees of freedom
## Multiple R-squared:  0.0001369,  Adjusted R-squared:  -0.004913 
## F-statistic: 0.02711 on 1 and 198 DF,  p-value: 0.8694&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;residualized change score model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(T2 ~ group + T1, df))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = T2 ~ group + T1, data = df)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.315819 -0.066341  0.005835  0.060977  0.268154 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   0.01724    0.01008    1.71   0.0888 .  
## groupControl  0.44744    0.02648   16.90   &amp;lt;2e-16 ***
## T1            0.44538    0.02797   15.92   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1002 on 197 degrees of freedom
## Multiple R-squared:  0.9463, Adjusted R-squared:  0.9457 
## F-statistic:  1734 on 2 and 197 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is going on? We are asking different questions by not accounting for T1 in the former model. The change score model is accounting for the total effect (in mediation language) whereas the residualized change score model is only interested in the direct effect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is lavaan 0.6-4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan is BETA software! Please report any bugs.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(semPlot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 methods overwritten by &amp;#39;huge&amp;#39;:
##   method    from   
##   plot.sim  BDgraph
##   print.sim BDgraph&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod &amp;lt;- &amp;#39;
  T1 ~ a*group
  T2 ~ b*group + c*T1
  
  # total effect
  TE := (a*-1) + (a*c*1) + (b*1)  
&amp;#39;
lord &amp;lt;- sem(mod, data=df)
summary(lord)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 28 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          5
## 
##   Number of observations                           200
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.000
##   Degrees of freedom                                 0
##   Minimum Function Value               0.0000000000000
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   T1 ~                                                
##     group      (a)    0.800    0.036   22.317    0.000
##   T2 ~                                                
##     group      (b)    0.447    0.026   17.028    0.000
##     T1         (c)    0.445    0.028   16.043    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .T1                0.064    0.006   10.000    0.000
##    .T2                0.010    0.001   10.000    0.000
## 
## Defined Parameters:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     TE                0.004    0.024    0.165    0.869&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(lord)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-11-07-week-11_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What is not immediately obvious is that the change score can be conceptualized as a series of regressions. Starting with the residualized change score model&lt;/p&gt;
&lt;p&gt;&lt;code&gt;T2 = b*T1 + e&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If we assume that the relationship (b) between T1 and T2 is 1. We can re-write as:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;T2 = 1*T1 + e&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Then we can subtract T1 fro each side of the model, leaving:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;T2 - T1 = e&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;In other words, a change score is equivalent to assuming a perfect regression association (correlation) between timepoints.&lt;/p&gt;
&lt;p&gt;Here, the residual will be equal to the average change and the variance of that will be the variance in the change. This can be thought of as akin to the mean and variance of our latent slope variable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;common-two-wave-models-in-path-analysis-form&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;common two wave models in path analysis form&lt;/h1&gt;
&lt;p&gt;Lets visualize each of these models via path models&lt;/p&gt;
&lt;div id=&#34;residualized-change-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Residualized change model&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;res.change.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Our latent residual can be conceptualized as what is left over from T2 after accounting for T1 (based on the average association between T1 and T2). We now have a measure of error/change that is not correlated to T1.&lt;/p&gt;
&lt;p&gt;If we wanted to, because this is SEM, we could test this model against a no change model. What would this look like? Well we would fix &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;⁩ to zero and compare the models&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res.change &amp;lt;- &amp;#39;
  T2 ~ T1
&amp;#39;
res.change &amp;lt;- sem(res.change, data=df)
summary(res.change)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 13 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          2
## 
##   Number of observations                           200
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.000
##   Degrees of freedom                                 0
##   Minimum Function Value               0.0000000000000
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   T2 ~                                                
##     T1                0.845    0.023   36.317    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .T2                0.024    0.002   10.000    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-change-score&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Latent change score&lt;/h2&gt;
&lt;p&gt;Looping back to concerns about difference scores and residualized change scores, we can address these using SEM. The problems raised above go away when: 1) measuring change latently, and thus error free. 2.) separate initial levels from change. Both of these are accomplished above. However, what is not accomplished is getting terms similar to the slope component ie a mean and a variance of a slope.&lt;/p&gt;
&lt;p&gt;Knowing what we know about recreating difference scores via constraints, we can also make a latent change score by modifying this path model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;latent.change.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can interpret the residual as change, as it is explicitly what is left over from T2 after accoutering for T1. This is starting to look like what we have been doing recently. We have:
1. Mean and variance of the slope(change), akin to our random and fixed effects in MLM
2. Covariance between intercept and slope.&lt;/p&gt;
&lt;p&gt;To test whether or not our slope is significant we can compare that with a model where slope is constrained to be zero. Same for testing the slope variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)
latent.change &amp;lt;- &amp;#39;
  #define difference score
  T2 ~ 1*T1
  
  # define the latent change variable
  change =~ 1*T2
  
  #estimate means
  change ~ 1
  T1 ~ 1
  
  #Constrains mean of T2 to 0
  T2 ~0*1

  #estimate variance of change
  change ~~ change

  #estimate variance of T1 intercept
  T1 ~~ T1
  
  #constrain variance of T2 to 0
  T2 ~~ 0*T2

  #intercept slope covariance
  change ~~ T1
&amp;#39;

latent.change &amp;lt;- sem(latent.change, data=df)
summary(latent.change)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 18 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          5
## 
##   Number of observations                           200
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.000
##   Degrees of freedom                                 0
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change =~                                           
##     T2                1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   T2 ~                                                
##     T1                1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change ~~                                           
##     T1               -0.035    0.006   -5.553    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change            0.041    0.012    3.367    0.001
##     T1                0.361    0.033   10.774    0.000
##    .T2                0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change            0.030    0.003   10.000    0.000
##     T1                0.224    0.022   10.000    0.000
##    .T2                0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;residualized-latent-change-score&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Residualized latent change score&lt;/h2&gt;
&lt;p&gt;Note that we haven’t yet removed the variance from the T1. This may or may not be something you want to do. It is mostly helpful if change has occurred prior to T1 and you are looking at the impact of some variable on change. If you are doing an intervention that takes place after T1 then maybe stick to latent change model. If you are measuring a developmental process across time and want to make sure that initial levels aren’t influencing change then you may want to do this. If you are doing that but think that initial levels are related to the change process then maybe you would be over controlling, wiping away what may be important. ¯_(ツ)_/¯&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;res.lat.change.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;res.latent.change &amp;lt;- &amp;#39;
  #define difference score
  T2 ~ 1*T1
  
  # define the latent change variable
  change =~ 1*T2
  
  #estimate means
  change ~ 1
  T1 ~ 1
  
  #Constrains mean of T2 to 0
  T2 ~0*1

  #estimate variance of change
  change ~~ change

  #estimate variance of T1 intercept
  T1 ~~ T1
  
  #constrain variance of T2 to 0
  T2 ~~ 0*T2

  #this is the only difference
  #intercept slope regression
  change ~ T1
&amp;#39;

res.lat.change &amp;lt;- sem(res.latent.change , data=df)
summary(res.lat.change)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 18 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          5
## 
##   Number of observations                           200
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.000
##   Degrees of freedom                                 0
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change =~                                           
##     T2                1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   T2 ~                                                
##     T1                1.000                           
##   change ~                                            
##     T1               -0.155    0.023   -6.678    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .change            0.097    0.014    7.009    0.000
##     T1                0.361    0.033   10.774    0.000
##    .T2                0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .change            0.024    0.002   10.000    0.000
##     T1                0.224    0.022   10.000    0.000
##    .T2                0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conditional Models&lt;/h2&gt;
&lt;p&gt;What if we want to predict initial status (or control for covariate) as well as see if change is predicted by some variable?&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cond.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)
cond &amp;lt;- &amp;#39;
  #define difference score
  T2 ~ 1*T1
  
  # define the latent change variable
  change =~ 1*T2
  
  #estimate means
  change ~ 1
  T1 ~ 1
  
  #Constrains mean of T2 to 0
  T2 ~0*1

  #estimate variance of change
  change ~~ change

  #estimate variance of T1 intercept
  T1 ~~ T1
  
  #constrain variance of T2 to 0
  T2 ~~ 0*T2

  #intercept slope covariance
  change ~~ T1
  
  # predictor predicting initial status and change
  T1 ~ group
  change ~ group
  
&amp;#39;

cond &amp;lt;- sem(cond, data=df)
summary(cond)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 37 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          7
## 
##   Number of observations                           200
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.000
##   Degrees of freedom                                 0
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change =~                                           
##     T2                1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   T2 ~                                                
##     T1                1.000                           
##   T1 ~                                                
##     group             0.800    0.036   22.317    0.000
##   change ~                                            
##     group             0.004    0.024    0.165    0.869
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##  .change ~~                                           
##    .T1               -0.036    0.004   -8.942    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .change            0.035    0.038    0.908    0.364
##    .T1               -0.839    0.057  -14.806    0.000
##    .T2                0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .change            0.030    0.003   10.000    0.000
##    .T1                0.064    0.006   10.000    0.000
##    .T2                0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does that change regressed on group look familiar?&lt;/p&gt;
&lt;p&gt;This approach works the case with categorical as well as continuous predictors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-group-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple group models&lt;/h2&gt;
&lt;p&gt;As we have seen before, simple predictors does not allow the full range of tests on how two groups may differ. Just like before, we can specify multiple group models with this data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;multi.group.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)
group &amp;lt;- &amp;#39;
  #define difference score
  T2 ~ 1*T1
  
  # define the latent change variable
  change =~ 1*T2
  
  #estimate means
  change ~ 1
  T1 ~ 1
  
  #Constrains mean of T2 to 0
  T2 ~0*1

  #estimate variance of change
  change ~~ change

  #estimate variance of T1 intercept
  T1 ~~ T1
  
  #constrain variance of T2 to 0
  T2 ~~ 0*T2

  #intercept slope covariance
  change ~~ T1
  
&amp;#39;

group  &amp;lt;- sem(group, group = &amp;quot;group&amp;quot;, data=df)
summary(group)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 59 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         10
## 
##   Number of observations per group         
##   Tx                                               100
##   Control                                          100
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.000
##   Degrees of freedom                                 0
## 
## Chi-square for each group:
## 
##   Tx                                             0.000
##   Control                                        0.000
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## 
## Group 1 [Tx]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change =~                                           
##     T2                1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   T2 ~                                                
##     T1                1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change ~~                                           
##     T1               -0.036    0.006   -6.390    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change            0.039    0.017    2.279    0.023
##     T1               -0.039    0.025   -1.569    0.117
##    .T2                0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change            0.029    0.004    7.071    0.000
##     T1                0.062    0.009    7.071    0.000
##    .T2                0.000                           
## 
## 
## Group 2 [Control]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change =~                                           
##     T2                1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   T2 ~                                                
##     T1                1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change ~~                                           
##     T1               -0.036    0.006   -6.258    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change            0.043    0.017    2.482    0.013
##     T1                0.760    0.026   29.612    0.000
##    .T2                0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change            0.030    0.004    7.071    0.000
##     T1                0.066    0.009    7.071    0.000
##    .T2                0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare that with a model where we constrain the slopes to be the same&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group2 &amp;lt;- &amp;#39;
  #define difference score
  T2 ~ 1*T1
  
  # define the latent change variable
  change =~ 1*T2
  
  #estimate means (here is where we constrain)
  change ~ c(s,s)*1
  T1 ~ 1
  
  #Constrains mean of T2 to 0
  T2 ~0*1

  #estimate variance of change
  change ~~ change

  #estimate variance of T1 intercept
  T1 ~~ T1
  
  #constrain variance of T2 to 0
  T2 ~~ 0*T2

  #intercept slope covariance
  change ~~ T1
  
&amp;#39;

group2  &amp;lt;- sem(group2, group = &amp;quot;group&amp;quot;, data=df)
summary(group2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 59 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         10
##   Number of equality constraints                     1
##   Row rank of the constraints matrix                 1
## 
##   Number of observations per group         
##   Tx                                               100
##   Control                                          100
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.027
##   Degrees of freedom                                 1
##   P-value (Chi-square)                           0.869
## 
## Chi-square for each group:
## 
##   Tx                                             0.014
##   Control                                        0.014
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## 
## Group 1 [Tx]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change =~                                           
##     T2                1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   T2 ~                                                
##     T1                1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change ~~                                           
##     T1               -0.036    0.006   -6.390    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change     (s)    0.041    0.012    3.365    0.001
##     T1               -0.042    0.020   -2.051    0.040
##    .T2                0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change            0.029    0.004    7.071    0.000
##     T1                0.062    0.009    7.071    0.000
##    .T2                0.000                           
## 
## 
## Group 2 [Control]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change =~                                           
##     T2                1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   T2 ~                                                
##     T1                1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change ~~                                           
##     T1               -0.036    0.006   -6.258    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change     (s)    0.041    0.012    3.365    0.001
##     T1                0.763    0.021   36.181    0.000
##    .T2                0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change            0.030    0.004    7.071    0.000
##     T1                0.066    0.009    7.071    0.000
##    .T2                0.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(group, group2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Chi Square Difference Test
## 
##        Df     AIC     BIC  Chisq Chisq diff Df diff Pr(&amp;gt;Chisq)
## group   0 -318.30 -285.31 0.0000                              
## group2  1 -320.27 -290.58 0.0274   0.027381       1     0.8686&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that p value from before?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bi-variate-latent-change-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bi-variate latent change model&lt;/h2&gt;
&lt;p&gt;Similar to the bivariate latent grown model, this is helpful if we want to ask whether or not two variables change in tandem.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long &amp;lt;- read.csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/SEM_workshop/longitudinal.csv&amp;quot;)

bv.lcm &amp;lt;- &amp;#39;
  #define difference score
  PosAFF12 ~ 1*PosAFF11
  
  # define the latent change variable
  change.pa =~ 1*PosAFF12
  
  #estimate means
  change.pa ~ 1
  PosAFF11 ~ 1
  
  #Constrains mean of T2 to 0
  PosAFF12 ~0*1

  #estimate variance of change
  change.pa ~~ change.pa

  #estimate variance of T1 intercept
  PosAFF11 ~~ PosAFF11
  
  #constrain variance of T2 to 0
  PosAFF12 ~~ 0*PosAFF12

  #intercept slope covariance
  change.pa ~~ PosAFF11

  ## second process
  
   #define difference score
  NegAFF12 ~ 1*NegAFF11
  
  # define the latent change variable
  change.na =~ 1*NegAFF12
  
  #estimate means
  change.na ~ 1
  NegAFF11 ~ 1
  
  #Constrains mean of T2 to 0
  NegAFF12 ~0*1

  #estimate variance of change
  change.na ~~ change.na

  #estimate variance of T1 intercept
  NegAFF11 ~~ NegAFF11
  
  #constrain variance of T2 to 0
  NegAFF12 ~~ 0*NegAFF12

  #intercept slope covariance
  change.na ~~ NegAFF11
  
  
  ## look at the covariance between change and intercept parameters
  
  change.na ~~ change.pa

  NegAFF11 ~~ PosAFF11
  
&amp;#39;

bv.lcm &amp;lt;- sem(bv.lcm, data=long)
summary(bv.lcm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 29 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         12
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      17.880
##   Degrees of freedom                                 2
##   P-value (Chi-square)                           0.000
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change.pa =~                                        
##     PosAFF12          1.000                           
##   change.na =~                                        
##     NegAFF12          1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   PosAFF12 ~                                          
##     PosAFF11          1.000                           
##   NegAFF12 ~                                          
##     NegAFF11          1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   change.pa ~~                                        
##     PosAFF11         -0.338    0.033  -10.247    0.000
##   change.na ~~                                        
##     NegAFF11         -0.297    0.026  -11.565    0.000
##   change.pa ~~                                        
##     change.na        -0.070    0.015   -4.682    0.000
##   PosAFF11 ~~                                         
##     NegAFF11         -0.056    0.015   -3.848    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change.pa         0.041    0.040    1.033    0.302
##     PosAFF11          3.212    0.037   86.264    0.000
##    .PosAFF12          0.000                           
##     change.na        -0.110    0.032   -3.428    0.001
##     NegAFF11          1.522    0.034   45.326    0.000
##    .NegAFF12          0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     change.pa         0.575    0.042   13.721    0.000
##     PosAFF11          0.510    0.037   13.668    0.000
##    .PosAFF12          0.000                           
##     change.na         0.382    0.028   13.791    0.000
##     NegAFF11          0.415    0.030   13.713    0.000
##    .NegAFF12          0.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(bv.lcm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-11-07-week-11_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-differencechange-score-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Latent difference/change score model&lt;/h1&gt;
&lt;p&gt;What if we had only two time points but had latent variables? This is where the model really starts to shine as we are better able to overcome some of the negatives of difference scores – namely that they are error filled. Here we will be measuring the variables latently to reduce error and constraining the measurement so that the assumption of measurement invariance holds.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##     T1X1     T1X2     T1X3     T2X1     T2X2     T2X3 
## 4.857927 5.415410 4.218875 5.451574 6.204090 4.819225&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ──────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1          ✔ purrr   0.3.2     
## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ readr   1.3.1          ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ──────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;change &amp;lt;-

#setup is similar to standard latent difference except that we will not use latent variables as our repeated measures. 
  
&amp;#39;C_T1=~1*T1X1 + L2*T1X2  + L3*T1X3    
C_T2=~1*T2X1 + L2*T2X2 + L3*T2X3   

C_T2 ~ 1*C_T1     # Fixed regression of C_T2 on C_T1
change_C =~ 1*C_T2     # Fixed regression of dC on C_T2
C_T2 ~ 0*1        # This line constrains the intercept of C_T2 to 0
C_T2 ~~ 0*C_T2    # This fixes the variance of the C_T2 to 0 

change_C ~ 1             # This estimates the intercept of the change score 
C_T1 ~  1           # This estimates the intercept of C_T1 
change_C ~~  change_C       # This estimates the variance of the change scores 
C_T1 ~~ C_T1     # This estimates the variance of the C_T1 
change_C ~~ C_T1 # This estimates the self-feedback parameter, which for now is just the covaration with change and T1

## new compared to no latent repeated measures
T1X1~~T2X1   # This allows residual covariance on indicator X1 across T1 and T2
T1X2~~T2X2   # This allows residual covariance on indicator X2 across T1 and T2
T1X3~~T2X3   # This allows residual covariance on indicator X3 across T1 and T2

T1X1~~T1X1   # This allows residual variance on indicator X1 
T1X2~~T1X2   # This allows residual variance on indicator X2
T1X3~~T1X3   # This allows residual variance on indicator X3

T2X1~~T2X1  # This allows residual variance on indicator X1 at T2 
T2X2~~T2X2  # This allows residual variance on indicator X2 at T2 
T2X3~~T2X3  # This allows residual variance on indicator X3 at T2


#scales the items
T1X1~0*1    # This constrains the intercept of X1 to 0 at T1
T1X2~M2*1   # This estimates the intercept of X2 at T1
T1X3~M3*1   # This estimates the intercept of X3 at T1
T2X1~0*1    # This constrains the intercept of X1 to 0 at T2
T2X2~M2*1   # This estimates the intercept of X2 at T2
T2X3~M3*1   # This estimates the intercept of X3 at T2

&amp;#39;

change.fit &amp;lt;- sem(change, data=simdat, missing=&amp;quot;ML&amp;quot;)
summary(change.fit, fit.measures=TRUE, standardized=TRUE, rsquare=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 49 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         22
##   Number of equality constraints                     4
##   Row rank of the constraints matrix                 4
## 
##   Number of observations                           500
##   Number of missing patterns                         1
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       3.171
##   Degrees of freedom                                 9
##   P-value (Chi-square)                           0.957
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             1005.424
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    1.000
##   Tucker-Lewis Index (TLI)                       1.010
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -4825.052
##   Loglikelihood unrestricted model (H1)      -4823.467
## 
##   Number of free parameters                         18
##   Akaike (AIC)                                9686.104
##   Bayesian (BIC)                              9761.967
##   Sample-size adjusted Bayesian (BIC)         9704.834
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.000
##   90 Percent Confidence Interval          0.000  0.000
##   P-value RMSEA &amp;lt;= 0.05                          0.999
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.012
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   C_T1 =~                                                               
##     T1X1              1.000                               0.943    0.675
##     T1X2      (L2)    1.129    0.055   20.690    0.000    1.065    0.714
##     T1X3      (L3)    0.958    0.048   20.003    0.000    0.903    0.690
##   C_T2 =~                                                               
##     T2X1              1.000                               1.047    0.721
##     T2X2      (L2)    1.129    0.055   20.690    0.000    1.182    0.789
##     T2X3      (L3)    0.958    0.048   20.003    0.000    1.003    0.702
##   change_C =~                                                           
##     C_T2              1.000                               0.513    0.513
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   C_T2 ~                                                                
##     C_T1              1.000                               0.901    0.901
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   C_T1 ~~                                                               
##     change_C         -0.041    0.055   -0.747    0.455   -0.081   -0.081
##  .T1X1 ~~                                                               
##    .T2X1             -0.010    0.059   -0.174    0.862   -0.010   -0.010
##  .T1X2 ~~                                                               
##    .T2X2             -0.161    0.063   -2.571    0.010   -0.161   -0.168
##  .T1X3 ~~                                                               
##    .T2X3             -0.102    0.055   -1.868    0.062   -0.102   -0.106
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .C_T2              0.000                               0.000    0.000
##     change_C          0.644    0.048   13.548    0.000    1.199    1.199
##     C_T1              4.832    0.057   84.764    0.000    5.125    5.125
##    .T1X1              0.000                               0.000    0.000
##    .T1X2      (M2)   -0.007    0.286   -0.026    0.980   -0.007   -0.005
##    .T1X3      (M3)   -0.417    0.250   -1.669    0.095   -0.417   -0.319
##    .T2X1              0.000                               0.000    0.000
##    .T2X2      (M2)   -0.007    0.286   -0.026    0.980   -0.007   -0.005
##    .T2X3      (M3)   -0.417    0.250   -1.669    0.095   -0.417   -0.292
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .C_T2              0.000                               0.000    0.000
##     change_C          0.288    0.070    4.092    0.000    1.000    1.000
##     C_T1              0.889    0.095    9.322    0.000    1.000    1.000
##    .T1X1              1.065    0.084   12.748    0.000    1.065    0.545
##    .T1X2              1.091    0.093   11.669    0.000    1.091    0.490
##    .T1X3              0.896    0.072   12.411    0.000    0.896    0.523
##    .T2X1              1.014    0.082   12.329    0.000    1.014    0.481
##    .T2X2              0.846    0.084   10.135    0.000    0.846    0.377
##    .T2X3              1.037    0.082   12.684    0.000    1.037    0.508
## 
## R-Square:
##                    Estimate
##     C_T2              1.000
##     T1X1              0.455
##     T1X2              0.510
##     T1X3              0.477
##     T2X1              0.519
##     T2X2              0.623
##     T2X3              0.492&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(change.fit )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-11-07-week-11_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Same as before, we can run multiple groups analyses, introduce covariates/predictors, regress change onto time 1 and look at bi-variate processes simultanously.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-wave-latent-difference-score-model-lds&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple wave Latent Difference Score model (LDS)&lt;/h1&gt;
&lt;p&gt;What is especially nice is that we can extend the model into more than 2 waves. This is a rethinking of how to assess 3+ waves of data. As opposed to thinking of a standard growth process that is unchanging, this instead reconceptualizing the change as a set of difference scores. You can increase, then decrease, then increase, for example. There is flexibility in this approach. First, you can get more precise assessments of when change is occuring. If you are measuring grades 5-12 with some event predictor maybe the change only occured during grades 6-7. How would you test that in a standard growth model? Second, you can model more non-linear associations with parameters that are easier to interpret than quadratics and higher order terms.&lt;/p&gt;
&lt;p&gt;However there is also downsides. First, there are more parameters to estimate. More parameters mean a more difficult time in estimating the model. So it is possible that you have not as accurate (mean est) and not as precise (larger SEs) compared to a more simplistic model like a growth model. Second, there are increased analytic choices which could lead to confusing, p-hacking and all around disillusion on what is the correct model. For example, do you model all change paramters to have similar covariances across time? Do you constrain variance to be the same and allow the means to differ – or vice versa? Do you impose MI?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y_t_all &amp;lt;- read.csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/ALDA/combined_models.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mw.model &amp;lt;-
&amp;#39;e7 =~ 1*TPER7_01 + cat6*TPER7_04 + cat2*TPER7_15 + cat3*TPER7_19R + cat4*TPER7_22R
e8 =~ 1*TPER8_01 + cat6*TPER8_04 + cat2*TPER8_15 + cat3*TPER8_19R + cat4*TPER8_22R
e9 =~ 1*TPER9_01 + cat6*TPER9_04 + cat2*TPER9_15 + cat3*TPER9_19R + cat4*TPER9_22R
e10 =~ 1*TPER10_01 + cat6*TPER10_04 + cat2*TPER10_15 + cat3*TPER10_19R + cat4*TPER10_22R


# settting the fixed regressions to 1

e8 ~ 1*e7     
e9 ~ 1*e8  
e10 ~ 1*e9  

# creating the change scores 

c_7.8 =~ 1*e8     
c_8.9 =~ 1*e9 
c_9.10 =~ 1*e10 


# constraint intercepts
e8 ~ 0*1        
e9 ~ 0*1   
e10 ~ 0*1   

# This fixes the variance to 0

e8 ~~ 0*e8     
e9 ~~ 0*e9  
e10 ~~ 0*e10  

# Estimate the intercept of the change score
c_7.8 ~ 1    
c_8.9 ~ 1 
c_9.10 ~ 1             

# This estimates the intercept and variance of initial time
e7 ~  1           
e7 ~~  e7 

#  Estimates the variance of the change scores
c_7.8 ~~ c_7.8    
c_8.9 ~~ c_8.9 
c_9.10 ~~ c_9.10       


# Estimates the covaration with change scores and T1
e7  ~~ c_7.8 + c_8.9 + c_9.10 
c_7.8 ~~ c_8.9 + c_9.10
c_8.9 ~~ c_9.10

## Allow indicators to covary
TPER7_01 ~~ TPER8_01 + TPER9_01 + TPER10_01
TPER8_01 ~~ TPER9_01 + TPER10_01
TPER9_01 ~~ TPER10_01

TPER7_04 ~~ TPER8_04 + TPER9_04 + TPER10_04
TPER8_04 ~~ TPER9_04 + TPER10_04
TPER9_04 ~~ TPER10_04

TPER7_15 ~~ TPER8_15 + TPER9_15 + TPER10_15
TPER8_15 ~~ TPER9_15 + TPER10_15
TPER9_15 ~~ TPER10_15

TPER7_19R ~~ TPER8_19R + TPER9_19R + TPER10_19R
TPER8_19R ~~ TPER9_19R + TPER10_19R
TPER9_19R ~~ TPER10_19R

TPER7_22R ~~ TPER8_22R + TPER9_22R + TPER10_22R
TPER8_22R ~~ TPER9_22R + TPER10_22R
TPER9_22R ~~ TPER10_22R



#scales the items and add measurement invariance

TPER7_01 ~ 0*1
TPER7_04 ~ (nu1)*1
TPER7_15 ~ (nu2)*1
TPER7_19R ~ (nu3)*1
TPER7_22R ~ (nu4)*1

TPER8_01~ 0*1
TPER8_04 ~ (nu1)*1
TPER8_15~ (nu2)*1
TPER8_19R ~ (nu3)*1
TPER8_22R~ (nu4)*1

TPER9_01~ 0*1
TPER9_04 ~ (nu1)*1
TPER9_15~ (nu2)*1
TPER9_19R ~ (nu3)*1
TPER9_22R~ (nu4)*1

TPER10_01 ~ 0*1
TPER10_04 ~ (nu1)*1
TPER10_15 ~ (nu2)*1
TPER10_19R ~ (nu3)*1
TPER10_22R ~ (nu4)*1

&amp;#39;

mw.model &amp;lt;- sem(mw.model, data=y_t_all , missing=&amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: some cases are empty and will be ignored:
##   14 24 32 33 34 36 37 39 48 49 51 57 76 78 80 83 84 86 90 91 98 109 119 129 130 131 140 144 145 157 165 167 170 171 177 180 182 183 190 206 222 227 229 234 247 250 260 287 292 295 301 302 307 327 339 342 345 349 354 356 359 360 369 370 377 383 384 386 393 406 411 414 419 421 422 425 426 429 440 442 446 448 449 450 456 460 462 470 473 476 485 491 494 496 503 504 507 508 516 532 535 539 546 547 548 552 554 555 556 559 561 563 566 571 584 586 587 591 596 599 601 607 620 630 634 635 642 645 650 651 658 666 673 678 687 716 718 732 735 737 738 740 745 751 761 766 776 785 786 788 790 795 796 802 804 823 827 831 842 847 851 853 860 887 892 901 920 923 926 927 936 938 940 947 949 952 953 955 956 957 969 972 977 979 981 985 993 999 1000 1003 1016 1029 1032 1035 1038 1040 1044 1046 1048 1050 1055 1059 1066&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, :
## lavaan WARNING: due to missing values, some pairwise combinations have less
## than 10% coverage&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mw.model, fit.measures=TRUE, standardized=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 73 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         96
##   Number of equality constraints                    24
##   Row rank of the constraints matrix                24
## 
##                                                   Used       Total
##   Number of observations                           864        1067
##   Number of missing patterns                        29
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     833.294
##   Degrees of freedom                               158
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             6120.154
##   Degrees of freedom                               190
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.886
##   Tucker-Lewis Index (TLI)                       0.863
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -11678.547
##   Loglikelihood unrestricted model (H1)     -11261.900
## 
##   Number of free parameters                         72
##   Akaike (AIC)                               23501.094
##   Bayesian (BIC)                             23843.927
##   Sample-size adjusted Bayesian (BIC)        23615.274
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.070
##   90 Percent Confidence Interval          0.066  0.075
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.064
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   e7 =~                                                                 
##     TPER7_0           1.000                               1.007    0.872
##     TPER7_0 (cat6)    0.789    0.024   32.827    0.000    0.794    0.726
##     TPER7_1 (cat2)    0.987    0.022   44.223    0.000    0.994    0.854
##     TPER7_1 (cat3)    1.020    0.029   35.076    0.000    1.027    0.768
##     TPER7_2 (cat4)    1.022    0.030   34.287    0.000    1.030    0.755
##   e8 =~                                                                 
##     TPER8_0           1.000                               1.009    0.871
##     TPER8_0 (cat6)    0.789    0.024   32.827    0.000    0.796    0.703
##     TPER8_1 (cat2)    0.987    0.022   44.223    0.000    0.996    0.855
##     TPER8_1 (cat3)    1.020    0.029   35.076    0.000    1.029    0.758
##     TPER8_2 (cat4)    1.022    0.030   34.287    0.000    1.032    0.746
##   e9 =~                                                                 
##     TPER9_0           1.000                               1.032    0.856
##     TPER9_0 (cat6)    0.789    0.024   32.827    0.000    0.814    0.694
##     TPER9_1 (cat2)    0.987    0.022   44.223    0.000    1.018    0.855
##     TPER9_1 (cat3)    1.020    0.029   35.076    0.000    1.052    0.781
##     TPER9_2 (cat4)    1.022    0.030   34.287    0.000    1.055    0.779
##   e10 =~                                                                
##     TPER10_           1.000                               0.971    0.855
##     TPER10_ (cat6)    0.789    0.024   32.827    0.000    0.766    0.699
##     TPER10_ (cat2)    0.987    0.022   44.223    0.000    0.958    0.815
##     TPER10_ (cat3)    1.020    0.029   35.076    0.000    0.990    0.784
##     TPER10_ (cat4)    1.022    0.030   34.287    0.000    0.993    0.778
##   c_7.8 =~                                                              
##     e8                1.000                               1.024    1.024
##   c_8.9 =~                                                              
##     e9                1.000                               0.949    0.949
##   c_9.10 =~                                                             
##     e10               1.000                               1.017    1.017
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   e8 ~                                                                  
##     e7                1.000                               0.998    0.998
##   e9 ~                                                                  
##     e8                1.000                               0.978    0.978
##   e10 ~                                                                 
##     e9                1.000                               1.063    1.063
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   e7 ~~                                                                 
##     c_7.8            -0.532    0.073   -7.291    0.000   -0.511   -0.511
##     c_8.9             0.115    0.073    1.568    0.117    0.117    0.117
##     c_9.10           -0.085    0.117   -0.722    0.470   -0.085   -0.085
##   c_7.8 ~~                                                              
##     c_8.9            -0.571    0.086   -6.606    0.000   -0.565   -0.565
##     c_9.10            0.018    0.134    0.131    0.896    0.017    0.017
##   c_8.9 ~~                                                              
##     c_9.10           -0.482    0.089   -5.437    0.000   -0.498   -0.498
##  .TPER7_01 ~~                                                           
##    .TPER8_01          0.049    0.030    1.634    0.102    0.049    0.151
##    .TPER9_01          0.087    0.035    2.457    0.014    0.087    0.246
##    .TPER10_01        -0.205    0.052   -3.936    0.000   -0.205   -0.612
##  .TPER8_01 ~~                                                           
##    .TPER9_01          0.079    0.028    2.814    0.005    0.079    0.222
##    .TPER10_01         0.061    0.035    1.749    0.080    0.061    0.183
##  .TPER9_01 ~~                                                           
##    .TPER10_01         0.021    0.034    0.616    0.538    0.021    0.056
##  .TPER7_04 ~~                                                           
##    .TPER8_04          0.163    0.039    4.133    0.000    0.163    0.269
##    .TPER9_04          0.141    0.051    2.769    0.006    0.141    0.222
##    .TPER10_04         0.078    0.094    0.829    0.407    0.078    0.132
##  .TPER8_04 ~~                                                           
##    .TPER9_04          0.225    0.043    5.289    0.000    0.225    0.330
##    .TPER10_04         0.086    0.053    1.604    0.109    0.086    0.136
##  .TPER9_04 ~~                                                           
##    .TPER10_04         0.201    0.047    4.234    0.000    0.201    0.304
##  .TPER7_15 ~~                                                           
##    .TPER8_15          0.022    0.030    0.725    0.468    0.022    0.059
##    .TPER9_15          0.008    0.034    0.241    0.810    0.008    0.022
##    .TPER10_15        -0.137    0.070   -1.958    0.050   -0.137   -0.332
##  .TPER8_15 ~~                                                           
##    .TPER9_15          0.027    0.028    0.948    0.343    0.027    0.071
##    .TPER10_15         0.013    0.042    0.322    0.747    0.013    0.033
##  .TPER9_15 ~~                                                           
##    .TPER10_15         0.008    0.033    0.240    0.810    0.008    0.019
##  .TPER7_19R ~~                                                          
##    .TPER8_19R         0.104    0.055    1.890    0.059    0.104    0.137
##    .TPER9_19R        -0.074    0.060   -1.233    0.218   -0.074   -0.103
##    .TPER10_19R        0.189    0.092    2.059    0.040    0.189    0.282
##  .TPER8_19R ~~                                                          
##    .TPER9_19R         0.110    0.053    2.078    0.038    0.110    0.148
##    .TPER10_19R        0.053    0.069    0.775    0.438    0.053    0.077
##  .TPER9_19R ~~                                                          
##    .TPER10_19R       -0.036    0.052   -0.690    0.490   -0.036   -0.054
##  .TPER7_22R ~~                                                          
##    .TPER8_22R         0.143    0.060    2.406    0.016    0.143    0.174
##    .TPER9_22R         0.019    0.061    0.304    0.761    0.019    0.024
##    .TPER10_22R        0.160    0.108    1.487    0.137    0.160    0.223
##  .TPER8_22R ~~                                                          
##    .TPER9_22R         0.049    0.052    0.941    0.347    0.049    0.063
##    .TPER10_22R       -0.014    0.062   -0.227    0.820   -0.014   -0.019
##  .TPER9_22R ~~                                                          
##    .TPER10_22R        0.025    0.054    0.467    0.640    0.025    0.037
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .e8                0.000                               0.000    0.000
##    .e9                0.000                               0.000    0.000
##    .e10               0.000                               0.000    0.000
##     c_7.8            -0.015    0.056   -0.259    0.795   -0.014   -0.014
##     c_8.9            -0.012    0.052   -0.225    0.822   -0.012   -0.012
##     c_9.10            0.119    0.058    2.054    0.040    0.121    0.121
##     e7                3.681    0.051   72.878    0.000    3.655    3.655
##    .TPER7_01          0.000                               0.000    0.000
##    .TPER7_04 (nu1)    0.579    0.092    6.266    0.000    0.579    0.529
##    .TPER7_15 (nu2)   -0.059    0.086   -0.689    0.491   -0.059   -0.051
##    .TPER7_19 (nu3)   -0.461    0.111   -4.161    0.000   -0.461   -0.345
##    .TPER7_22 (nu4)   -0.581    0.114   -5.109    0.000   -0.581   -0.426
##    .TPER8_01          0.000                               0.000    0.000
##    .TPER8_04 (nu1)    0.579    0.092    6.266    0.000    0.579    0.511
##    .TPER8_15 (nu2)   -0.059    0.086   -0.689    0.491   -0.059   -0.051
##    .TPER8_19 (nu3)   -0.461    0.111   -4.161    0.000   -0.461   -0.340
##    .TPER8_22 (nu4)   -0.581    0.114   -5.109    0.000   -0.581   -0.420
##    .TPER9_01          0.000                               0.000    0.000
##    .TPER9_04 (nu1)    0.579    0.092    6.266    0.000    0.579    0.494
##    .TPER9_15 (nu2)   -0.059    0.086   -0.689    0.491   -0.059   -0.049
##    .TPER9_19 (nu3)   -0.461    0.111   -4.161    0.000   -0.461   -0.343
##    .TPER9_22 (nu4)   -0.581    0.114   -5.109    0.000   -0.581   -0.428
##    .TPER10_0          0.000                               0.000    0.000
##    .TPER10_0 (nu1)    0.579    0.092    6.266    0.000    0.579    0.528
##    .TPER10_1 (nu2)   -0.059    0.086   -0.689    0.491   -0.059   -0.050
##    .TPER10_1 (nu3)   -0.461    0.111   -4.161    0.000   -0.461   -0.366
##    .TPER10_2 (nu4)   -0.581    0.114   -5.109    0.000   -0.581   -0.455
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .e8                0.000                               0.000    0.000
##    .e9                0.000                               0.000    0.000
##    .e10               0.000                               0.000    0.000
##     e7                1.014    0.083   12.153    0.000    1.000    1.000
##     c_7.8             1.067    0.108    9.831    0.000    1.000    1.000
##     c_8.9             0.960    0.099    9.655    0.000    1.000    1.000
##     c_9.10            0.975    0.104    9.412    0.000    1.000    1.000
##    .TPER7_01          0.321    0.035    9.069    0.000    0.321    0.240
##    .TPER7_04          0.566    0.046   12.253    0.000    0.566    0.473
##    .TPER7_15          0.368    0.035   10.408    0.000    0.368    0.271
##    .TPER7_19R         0.733    0.063   11.624    0.000    0.733    0.410
##    .TPER7_22R         0.800    0.067   11.858    0.000    0.800    0.430
##    .TPER8_01          0.325    0.031   10.530    0.000    0.325    0.242
##    .TPER8_04          0.650    0.046   14.201    0.000    0.650    0.506
##    .TPER8_15          0.366    0.031   11.624    0.000    0.366    0.270
##    .TPER8_19R         0.786    0.059   13.328    0.000    0.786    0.426
##    .TPER8_22R         0.850    0.063   13.482    0.000    0.850    0.444
##    .TPER9_01          0.388    0.034   11.345    0.000    0.388    0.267
##    .TPER9_04          0.713    0.049   14.525    0.000    0.713    0.518
##    .TPER9_15          0.380    0.033   11.634    0.000    0.380    0.268
##    .TPER9_19R         0.706    0.055   12.837    0.000    0.706    0.389
##    .TPER9_22R         0.724    0.056   12.918    0.000    0.724    0.394
##    .TPER10_01         0.348    0.039    8.953    0.000    0.348    0.270
##    .TPER10_04         0.615    0.053   11.579    0.000    0.615    0.512
##    .TPER10_15         0.465    0.045   10.239    0.000    0.465    0.336
##    .TPER10_19R        0.613    0.059   10.412    0.000    0.613    0.385
##    .TPER10_22R        0.642    0.062   10.427    0.000    0.642    0.394&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(mw.model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-11-07-week-11_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;
Gross. We have to draw this by hand to get a good representation.&lt;/p&gt;
&lt;div id=&#34;predicting-change&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;predicting change&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;c.mw.model &amp;lt;-
&amp;#39;e7 =~ 1*TPER7_01 + cat6*TPER7_04 + cat2*TPER7_15 + cat3*TPER7_19R + cat4*TPER7_22R
e8 =~ 1*TPER8_01 + cat6*TPER8_04 + cat2*TPER8_15 + cat3*TPER8_19R + cat4*TPER8_22R
e9 =~ 1*TPER9_01 + cat6*TPER9_04 + cat2*TPER9_15 + cat3*TPER9_19R + cat4*TPER9_22R
e10 =~ 1*TPER10_01 + cat6*TPER10_04 + cat2*TPER10_15 + cat3*TPER10_19R + cat4*TPER10_22R


# settting the fixed regressions to 1

e8 ~ 1*e7     
e9 ~ 1*e8  
e10 ~ 1*e9  

# creating the change scores 

c_7.8 =~ 1*e8     
c_8.9 =~ 1*e9 
c_9.10 =~ 1*e10 


# constraint intercepts
e8 ~ 0*1        
e9 ~ 0*1   
e10 ~ 0*1   

# This fixes the variance to 0

e8 ~~ 0*e8     
e9 ~~ 0*e9  
e10 ~~ 0*e10  

# Estimate the intercept of the change score
c_7.8 ~ 1    
c_8.9 ~ 1 
c_9.10 ~ 1             

# This estimates the intercept and variance of initial time
e7 ~  1           
e7 ~~  e7 

#  Estimates the variance of the change scores
c_7.8 ~~ c_7.8    
c_8.9 ~~ c_8.9 
c_9.10 ~~ c_9.10       


# Estimates the covaration with change scores and T1
e7  ~~ c_7.8 + c_8.9 + c_9.10 
c_7.8 ~~ c_8.9 + c_9.10
c_8.9 ~~ c_9.10

## Allow indicators to covary
TPER7_01 ~~ TPER8_01 + TPER9_01 + TPER10_01
TPER8_01 ~~ TPER9_01 + TPER10_01
TPER9_01 ~~ TPER10_01

TPER7_04 ~~ TPER8_04 + TPER9_04 + TPER10_04
TPER8_04 ~~ TPER9_04 + TPER10_04
TPER9_04 ~~ TPER10_04

TPER7_15 ~~ TPER8_15 + TPER9_15 + TPER10_15
TPER8_15 ~~ TPER9_15 + TPER10_15
TPER9_15 ~~ TPER10_15

TPER7_19R ~~ TPER8_19R + TPER9_19R + TPER10_19R
TPER8_19R ~~ TPER9_19R + TPER10_19R
TPER9_19R ~~ TPER10_19R

TPER7_22R ~~ TPER8_22R + TPER9_22R + TPER10_22R
TPER8_22R ~~ TPER9_22R + TPER10_22R
TPER9_22R ~~ TPER10_22R



#scales the items and add measurement invariance

TPER7_01 ~ 0*1
TPER7_04 ~ (nu1)*1
TPER7_15 ~ (nu2)*1
TPER7_19R ~ (nu3)*1
TPER7_22R ~ (nu4)*1

TPER8_01~ 0*1
TPER8_04 ~ (nu1)*1
TPER8_15~ (nu2)*1
TPER8_19R ~ (nu3)*1
TPER8_22R~ (nu4)*1

TPER9_01~ 0*1
TPER9_04 ~ (nu1)*1
TPER9_15~ (nu2)*1
TPER9_19R ~ (nu3)*1
TPER9_22R~ (nu4)*1

TPER10_01 ~ 0*1
TPER10_04 ~ (nu1)*1
TPER10_15 ~ (nu2)*1
TPER10_19R ~ (nu3)*1
TPER10_22R ~ (nu4)*1

# predicting change

c_7.8 ~ SEX1    
c_8.9 ~ SEX1 
c_9.10 ~ SEX1 


&amp;#39;

c.mw.model &amp;lt;- sem(c.mw.model, data=y_t_all , missing=&amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: 68 cases were deleted due to missing values in 
##        exogenous variable(s), while fixed.x = TRUE.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, :
## lavaan WARNING: due to missing values, some pairwise combinations have less
## than 10% coverage&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(c.mw.model, fit.measures=TRUE, standardized=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 79 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         99
##   Number of equality constraints                    24
##   Row rank of the constraints matrix                24
## 
##                                                   Used       Total
##   Number of observations                           999        1067
##   Number of missing patterns                        30
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     871.333
##   Degrees of freedom                               175
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             5976.297
##   Degrees of freedom                               210
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.879
##   Tucker-Lewis Index (TLI)                       0.855
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -11131.232
##   Loglikelihood unrestricted model (H1)     -10695.565
## 
##   Number of free parameters                         75
##   Akaike (AIC)                               22412.463
##   Bayesian (BIC)                             22780.470
##   Sample-size adjusted Bayesian (BIC)        22542.266
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.063
##   90 Percent Confidence Interval          0.059  0.067
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.065
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   e7 =~                                                                 
##     TPER7_0           1.000                               1.009    0.872
##     TPER7_0 (cat6)    0.783    0.024   32.145    0.000    0.790    0.724
##     TPER7_1 (cat2)    0.987    0.023   43.539    0.000    0.996    0.854
##     TPER7_1 (cat3)    1.011    0.030   34.231    0.000    1.020    0.766
##     TPER7_2 (cat4)    1.015    0.030   33.472    0.000    1.024    0.754
##   e8 =~                                                                 
##     TPER8_0           1.000                               1.010    0.875
##     TPER8_0 (cat6)    0.783    0.024   32.145    0.000    0.791    0.701
##     TPER8_1 (cat2)    0.987    0.023   43.539    0.000    0.997    0.855
##     TPER8_1 (cat3)    1.011    0.030   34.231    0.000    1.022    0.753
##     TPER8_2 (cat4)    1.015    0.030   33.472    0.000    1.026    0.741
##   e9 =~                                                                 
##     TPER9_0           1.000                               1.037    0.857
##     TPER9_0 (cat6)    0.783    0.024   32.145    0.000    0.813    0.704
##     TPER9_1 (cat2)    0.987    0.023   43.539    0.000    1.024    0.859
##     TPER9_1 (cat3)    1.011    0.030   34.231    0.000    1.049    0.779
##     TPER9_2 (cat4)    1.015    0.030   33.472    0.000    1.053    0.779
##   e10 =~                                                                
##     TPER10_           1.000                               0.971    0.859
##     TPER10_ (cat6)    0.783    0.024   32.145    0.000    0.760    0.685
##     TPER10_ (cat2)    0.987    0.023   43.539    0.000    0.958    0.820
##     TPER10_ (cat3)    1.011    0.030   34.231    0.000    0.981    0.777
##     TPER10_ (cat4)    1.015    0.030   33.472    0.000    0.985    0.775
##   c_7.8 =~                                                              
##     e8                1.000                               1.032    1.032
##   c_8.9 =~                                                              
##     e9                1.000                               0.940    0.940
##   c_9.10 =~                                                             
##     e10               1.000                               1.017    1.017
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   e8 ~                                                                  
##     e7                1.000                               0.998    0.998
##   e9 ~                                                                  
##     e8                1.000                               0.974    0.974
##   e10 ~                                                                 
##     e9                1.000                               1.069    1.069
##   c_7.8 ~                                                               
##     SEX1              0.206    0.086    2.387    0.017    0.197    0.099
##   c_8.9 ~                                                               
##     SEX1              0.067    0.106    0.632    0.527    0.069    0.034
##   c_9.10 ~                                                              
##     SEX1              0.075    0.120    0.626    0.532    0.076    0.038
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   e7 ~~                                                                 
##    .c_7.8            -0.542    0.074   -7.359    0.000   -0.518   -0.518
##    .c_8.9             0.110    0.074    1.494    0.135    0.112    0.112
##    .c_9.10           -0.081    0.117   -0.689    0.491   -0.081   -0.081
##  .c_7.8 ~~                                                              
##    .c_8.9            -0.562    0.086   -6.504    0.000   -0.555   -0.555
##    .c_9.10            0.004    0.132    0.033    0.974    0.004    0.004
##  .c_8.9 ~~                                                              
##    .c_9.10           -0.483    0.088   -5.474    0.000   -0.502   -0.502
##  .TPER7_01 ~~                                                           
##    .TPER8_01          0.047    0.029    1.582    0.114    0.047    0.147
##    .TPER9_01          0.084    0.036    2.370    0.018    0.084    0.239
##    .TPER10_01        -0.197    0.052   -3.762    0.000   -0.197   -0.604
##  .TPER8_01 ~~                                                           
##    .TPER9_01          0.069    0.028    2.443    0.015    0.069    0.198
##    .TPER10_01         0.059    0.034    1.747    0.081    0.059    0.183
##  .TPER9_01 ~~                                                           
##    .TPER10_01         0.028    0.035    0.815    0.415    0.028    0.078
##  .TPER7_04 ~~                                                           
##    .TPER8_04          0.162    0.039    4.110    0.000    0.162    0.267
##    .TPER9_04          0.135    0.049    2.782    0.005    0.135    0.219
##    .TPER10_04         0.090    0.098    0.920    0.357    0.090    0.148
##  .TPER8_04 ~~                                                           
##    .TPER9_04          0.213    0.041    5.158    0.000    0.213    0.323
##    .TPER10_04         0.092    0.056    1.630    0.103    0.092    0.141
##  .TPER9_04 ~~                                                           
##    .TPER10_04         0.224    0.050    4.459    0.000    0.224    0.338
##  .TPER7_15 ~~                                                           
##    .TPER8_15          0.022    0.030    0.733    0.464    0.022    0.060
##    .TPER9_15          0.005    0.034    0.137    0.891    0.005    0.012
##    .TPER10_15        -0.130    0.069   -1.886    0.059   -0.130   -0.321
##  .TPER8_15 ~~                                                           
##    .TPER9_15          0.026    0.028    0.918    0.359    0.026    0.069
##    .TPER10_15         0.015    0.040    0.366    0.714    0.015    0.037
##  .TPER9_15 ~~                                                           
##    .TPER10_15         0.030    0.034    0.863    0.388    0.030    0.072
##  .TPER7_19R ~~                                                          
##    .TPER8_19R         0.108    0.055    1.948    0.051    0.108    0.141
##    .TPER9_19R        -0.073    0.060   -1.205    0.228   -0.073   -0.101
##    .TPER10_19R        0.193    0.093    2.071    0.038    0.193    0.284
##  .TPER8_19R ~~                                                          
##    .TPER9_19R         0.111    0.054    2.046    0.041    0.111    0.146
##    .TPER10_19R        0.053    0.070    0.763    0.445    0.053    0.075
##  .TPER9_19R ~~                                                          
##    .TPER10_19R       -0.054    0.056   -0.970    0.332   -0.054   -0.081
##  .TPER7_22R ~~                                                          
##    .TPER8_22R         0.145    0.060    2.423    0.015    0.145    0.175
##    .TPER9_22R         0.021    0.060    0.348    0.728    0.021    0.028
##    .TPER10_22R        0.165    0.107    1.538    0.124    0.165    0.230
##  .TPER8_22R ~~                                                          
##    .TPER9_22R         0.049    0.052    0.944    0.345    0.049    0.062
##    .TPER10_22R       -0.011    0.062   -0.182    0.856   -0.011   -0.015
##  .TPER9_22R ~~                                                          
##    .TPER10_22R        0.050    0.057    0.874    0.382    0.050    0.073
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .e8                0.000                               0.000    0.000
##    .e9                0.000                               0.000    0.000
##    .e10               0.000                               0.000    0.000
##    .c_7.8            -0.325    0.142   -2.284    0.022   -0.312   -0.312
##    .c_8.9            -0.104    0.166   -0.628    0.530   -0.107   -0.107
##    .c_9.10            0.008    0.190    0.041    0.968    0.008    0.008
##     e7                3.689    0.051   72.452    0.000    3.657    3.657
##    .TPER7_01          0.000                               0.000    0.000
##    .TPER7_04 (nu1)    0.613    0.094    6.526    0.000    0.613    0.562
##    .TPER7_15 (nu2)   -0.061    0.087   -0.703    0.482   -0.061   -0.053
##    .TPER7_19 (nu3)   -0.433    0.113   -3.837    0.000   -0.433   -0.325
##    .TPER7_22 (nu4)   -0.547    0.116   -4.719    0.000   -0.547   -0.402
##    .TPER8_01          0.000                               0.000    0.000
##    .TPER8_04 (nu1)    0.613    0.094    6.526    0.000    0.613    0.543
##    .TPER8_15 (nu2)   -0.061    0.087   -0.703    0.482   -0.061   -0.053
##    .TPER8_19 (nu3)   -0.433    0.113   -3.837    0.000   -0.433   -0.319
##    .TPER8_22 (nu4)   -0.547    0.116   -4.719    0.000   -0.547   -0.395
##    .TPER9_01          0.000                               0.000    0.000
##    .TPER9_04 (nu1)    0.613    0.094    6.526    0.000    0.613    0.531
##    .TPER9_15 (nu2)   -0.061    0.087   -0.703    0.482   -0.061   -0.051
##    .TPER9_19 (nu3)   -0.433    0.113   -3.837    0.000   -0.433   -0.322
##    .TPER9_22 (nu4)   -0.547    0.116   -4.719    0.000   -0.547   -0.404
##    .TPER10_0          0.000                               0.000    0.000
##    .TPER10_0 (nu1)    0.613    0.094    6.526    0.000    0.613    0.552
##    .TPER10_1 (nu2)   -0.061    0.087   -0.703    0.482   -0.061   -0.052
##    .TPER10_1 (nu3)   -0.433    0.113   -3.837    0.000   -0.433   -0.343
##    .TPER10_2 (nu4)   -0.547    0.116   -4.719    0.000   -0.547   -0.430
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .e8                0.000                               0.000    0.000
##    .e9                0.000                               0.000    0.000
##    .e10               0.000                               0.000    0.000
##     e7                1.018    0.084   12.144    0.000    1.000    1.000
##    .c_7.8             1.078    0.110    9.814    0.000    0.990    0.990
##    .c_8.9             0.951    0.099    9.632    0.000    0.999    0.999
##    .c_9.10            0.973    0.108    9.011    0.000    0.999    0.999
##    .TPER7_01          0.320    0.035    9.025    0.000    0.320    0.239
##    .TPER7_04          0.568    0.046   12.254    0.000    0.568    0.476
##    .TPER7_15          0.368    0.035   10.392    0.000    0.368    0.271
##    .TPER7_19R         0.732    0.063   11.631    0.000    0.732    0.413
##    .TPER7_22R         0.798    0.067   11.857    0.000    0.798    0.432
##    .TPER8_01          0.313    0.030   10.313    0.000    0.313    0.235
##    .TPER8_04          0.648    0.046   14.180    0.000    0.648    0.508
##    .TPER8_15          0.366    0.032   11.610    0.000    0.366    0.269
##    .TPER8_19R         0.798    0.060   13.376    0.000    0.798    0.433
##    .TPER8_22R         0.863    0.064   13.515    0.000    0.863    0.451
##    .TPER9_01          0.388    0.035   10.934    0.000    0.388    0.265
##    .TPER9_04          0.672    0.048   13.878    0.000    0.672    0.505
##    .TPER9_15          0.374    0.034   11.144    0.000    0.374    0.263
##    .TPER9_19R         0.714    0.057   12.438    0.000    0.714    0.394
##    .TPER9_22R         0.720    0.058   12.462    0.000    0.720    0.394
##    .TPER10_01         0.334    0.040    8.302    0.000    0.334    0.262
##    .TPER10_04         0.655    0.060   10.998    0.000    0.655    0.531
##    .TPER10_15         0.447    0.047    9.528    0.000    0.447    0.327
##    .TPER10_19R        0.630    0.064    9.904    0.000    0.630    0.396
##    .TPER10_22R        0.644    0.065    9.844    0.000    0.644    0.399&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-intercept-and-growth-factors-to-lds-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;adding intercept and growth factors to LDS models&lt;/h2&gt;
&lt;p&gt;One top of this type of model you can fit a growth curve. Why? Can capture both general trends and more fine-grained temporal dynamics. This can be especially useful when trying to separate a known, more stable change occurring during development (e.g.natural aging process) from more specific fluctuations due to external factors (e.g., life events).&lt;/p&gt;
&lt;p&gt;However, there are some differences from our standard latent growth model. First, if the difference scores are equal across time the mean of the change factor will be 0. This is because the latent change variable are using difference scores as indicators, not standard indicators. As such, this can be interpreted as a constant rate of change (or not).&lt;/p&gt;
&lt;p&gt;If you want to get at a standard interpretation of a growth curve, seperate from change at a specific time point, you can define the change score with loadings of 1 for all difference score variables as opposed to increasing linearly eg 1,2,3,4,5…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lg.mw.model &amp;lt;-
&amp;#39;e7 =~ 1*TPER7_01 + cat6*TPER7_04 + cat2*TPER7_15 + cat3*TPER7_19R + cat4*TPER7_22R
e8 =~ 1*TPER8_01 + cat6*TPER8_04 + cat2*TPER8_15 + cat3*TPER8_19R + cat4*TPER8_22R
e9 =~ 1*TPER9_01 + cat6*TPER9_04 + cat2*TPER9_15 + cat3*TPER9_19R + cat4*TPER9_22R
e10 =~ 1*TPER10_01 + cat6*TPER10_04 + cat2*TPER10_15 + cat3*TPER10_19R + cat4*TPER10_22R


# settting the fixed regressions to 1

e8 ~ 1*e7     
e9 ~ 1*e8  
e10 ~ 1*e9  

# creating the change scores 

c_7.8 =~ 1*e8     
c_8.9 =~ 1*e9 
c_9.10 =~ 1*e10 


# constraint intercepts
e8 ~ 0*1        
e9 ~ 0*1   
e10 ~ 0*1   

# This fixes the variance to 0

e8 ~~ 0*e8     
e9 ~~ 0*e9  
e10 ~~ 0*e10  

# # a change from the previous models is we now fix the mean of change scores to zero. Estimate the intercept of the change score
c_7.8 ~ 0*1    
c_8.9 ~ 0*1 
c_9.10 ~ 0*1             

# This estimates the intercept and variance of initial time
e7 ~  1           
e7 ~~  e7 

#  Estimates the variance of the change scores
c_7.8 ~~ c_7.8    
c_8.9 ~~ c_8.9 
c_9.10 ~~ c_9.10       


# # A change from the previous model is we no longer estimate the covariance between change scores a) with eachother and b) with the intercept. We will only covary intercept with a growth component, defined below. We must set some of these covariances to zero to make sure they are not estimated.  
e7  ~~ 0*c_7.8 + 0*c_8.9 + 0*c_9.10
c_7.8 ~~ 0*c_8.9 + 0*c_9.10
c_8.9 ~~ 0*c_9.10


## Allow indicators to covary
TPER7_01 ~~ TPER8_01 + TPER9_01 + TPER10_01
TPER8_01 ~~ TPER9_01 + TPER10_01
TPER9_01 ~~ TPER10_01

TPER7_04 ~~ TPER8_04 + TPER9_04 + TPER10_04
TPER8_04 ~~ TPER9_04 + TPER10_04
TPER9_04 ~~ TPER10_04

TPER7_15 ~~ TPER8_15 + TPER9_15 + TPER10_15
TPER8_15 ~~ TPER9_15 + TPER10_15
TPER9_15 ~~ TPER10_15

TPER7_19R ~~ TPER8_19R + TPER9_19R + TPER10_19R
TPER8_19R ~~ TPER9_19R + TPER10_19R
TPER9_19R ~~ TPER10_19R

TPER7_22R ~~ TPER8_22R + TPER9_22R + TPER10_22R
TPER8_22R ~~ TPER9_22R + TPER10_22R
TPER9_22R ~~ TPER10_22R



#scales the items and add measurement invariance

TPER7_01 ~ 0*1
TPER7_04 ~ (nu1)*1
TPER7_15 ~ (nu2)*1
TPER7_19R ~ (nu3)*1
TPER7_22R ~ (nu4)*1

TPER8_01~ 0*1
TPER8_04 ~ (nu1)*1
TPER8_15~ (nu2)*1
TPER8_19R ~ (nu3)*1
TPER8_22R~ (nu4)*1

TPER9_01~ 0*1
TPER9_04 ~ (nu1)*1
TPER9_15~ (nu2)*1
TPER9_19R ~ (nu3)*1
TPER9_22R~ (nu4)*1

TPER10_01 ~ 0*1
TPER10_04 ~ (nu1)*1
TPER10_15 ~ (nu2)*1
TPER10_19R ~ (nu3)*1
TPER10_22R ~ (nu4)*1

# create a latent intercept growth component
E.change  =~ 1*e8 + 2*e9 + 3*e10
E.change ~ 1

# Covary the intecept and new overall change component and make sure change does not covary with difference scores
E.change ~~ e7
E.change ~~ 0*c_7.8 + 0*c_8.9 + 0*c_9.10

&amp;#39;

lg.mw.model &amp;lt;- sem(lg.mw.model, data=y_t_all , missing=&amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: some cases are empty and will be ignored:
##   14 24 32 33 34 36 37 39 48 49 51 57 76 78 80 83 84 86 90 91 98 109 119 129 130 131 140 144 145 157 165 167 170 171 177 180 182 183 190 206 222 227 229 234 247 250 260 287 292 295 301 302 307 327 339 342 345 349 354 356 359 360 369 370 377 383 384 386 393 406 411 414 419 421 422 425 426 429 440 442 446 448 449 450 456 460 462 470 473 476 485 491 494 496 503 504 507 508 516 532 535 539 546 547 548 552 554 555 556 559 561 563 566 571 584 586 587 591 596 599 601 607 620 630 634 635 642 645 650 651 658 666 673 678 687 716 718 732 735 737 738 740 745 751 761 766 776 785 786 788 790 795 796 802 804 823 827 831 842 847 851 853 860 887 892 901 920 923 926 927 936 938 940 947 949 952 953 955 956 957 969 972 977 979 981 985 993 999 1000 1003 1016 1029 1032 1035 1038 1040 1044 1046 1048 1050 1055 1059 1066&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, :
## lavaan WARNING: due to missing values, some pairwise combinations have less
## than 10% coverage&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_object_post_check(object): lavaan WARNING: some estimated lv
## variances are negative&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lg.mw.model, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 63 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         90
##   Number of equality constraints                    24
##   Row rank of the constraints matrix                24
## 
##                                                   Used       Total
##   Number of observations                           864        1067
##   Number of missing patterns                        29
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     949.774
##   Degrees of freedom                               164
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             6120.154
##   Degrees of freedom                               190
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.867
##   Tucker-Lewis Index (TLI)                       0.846
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -11736.787
##   Loglikelihood unrestricted model (H1)     -11261.900
## 
##   Number of free parameters                         66
##   Akaike (AIC)                               23605.574
##   Bayesian (BIC)                             23919.837
##   Sample-size adjusted Bayesian (BIC)        23710.238
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.074
##   90 Percent Confidence Interval          0.070  0.079
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.156
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   e7 =~                                               
##     TPER7_0           1.000                           
##     TPER7_0 (cat6)    0.790    0.024   32.432    0.000
##     TPER7_1 (cat2)    0.986    0.022   43.857    0.000
##     TPER7_1 (cat3)    1.024    0.029   35.060    0.000
##     TPER7_2 (cat4)    1.024    0.030   34.275    0.000
##   e8 =~                                               
##     TPER8_0           1.000                           
##     TPER8_0 (cat6)    0.790    0.024   32.432    0.000
##     TPER8_1 (cat2)    0.986    0.022   43.857    0.000
##     TPER8_1 (cat3)    1.024    0.029   35.060    0.000
##     TPER8_2 (cat4)    1.024    0.030   34.275    0.000
##   e9 =~                                               
##     TPER9_0           1.000                           
##     TPER9_0 (cat6)    0.790    0.024   32.432    0.000
##     TPER9_1 (cat2)    0.986    0.022   43.857    0.000
##     TPER9_1 (cat3)    1.024    0.029   35.060    0.000
##     TPER9_2 (cat4)    1.024    0.030   34.275    0.000
##   e10 =~                                              
##     TPER10_           1.000                           
##     TPER10_ (cat6)    0.790    0.024   32.432    0.000
##     TPER10_ (cat2)    0.986    0.022   43.857    0.000
##     TPER10_ (cat3)    1.024    0.029   35.060    0.000
##     TPER10_ (cat4)    1.024    0.030   34.275    0.000
##   c_7.8 =~                                            
##     e8                1.000                           
##   c_8.9 =~                                            
##     e9                1.000                           
##   c_9.10 =~                                           
##     e10               1.000                           
##   E.change =~                                         
##     e8                1.000                           
##     e9                2.000                           
##     e10               3.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   e8 ~                                                
##     e7                1.000                           
##   e9 ~                                                
##     e8                1.000                           
##   e10 ~                                               
##     e9                1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   e7 ~~                                               
##     c_7.8             0.000                           
##     c_8.9             0.000                           
##     c_9.10            0.000                           
##   c_7.8 ~~                                            
##     c_8.9             0.000                           
##     c_9.10            0.000                           
##   c_8.9 ~~                                            
##     c_9.10            0.000                           
##  .TPER7_01 ~~                                         
##    .TPER8_01          0.018    0.030    0.586    0.558
##    .TPER9_01          0.090    0.036    2.505    0.012
##    .TPER10_01        -0.204    0.053   -3.819    0.000
##  .TPER8_01 ~~                                         
##    .TPER9_01          0.072    0.028    2.569    0.010
##    .TPER10_01         0.067    0.036    1.868    0.062
##  .TPER9_01 ~~                                         
##    .TPER10_01         0.028    0.034    0.838    0.402
##  .TPER7_04 ~~                                         
##    .TPER8_04          0.149    0.040    3.766    0.000
##    .TPER9_04          0.150    0.051    2.936    0.003
##    .TPER10_04         0.109    0.096    1.134    0.257
##  .TPER8_04 ~~                                         
##    .TPER9_04          0.222    0.043    5.209    0.000
##    .TPER10_04         0.085    0.054    1.585    0.113
##  .TPER9_04 ~~                                         
##    .TPER10_04         0.203    0.048    4.251    0.000
##  .TPER7_15 ~~                                         
##    .TPER8_15          0.007    0.030    0.234    0.815
##    .TPER9_15          0.009    0.034    0.270    0.787
##    .TPER10_15        -0.137    0.072   -1.904    0.057
##  .TPER8_15 ~~                                         
##    .TPER9_15          0.025    0.028    0.898    0.369
##    .TPER10_15         0.016    0.042    0.388    0.698
##  .TPER9_15 ~~                                         
##    .TPER10_15         0.010    0.033    0.294    0.769
##  .TPER7_19R ~~                                        
##    .TPER8_19R         0.111    0.054    2.034    0.042
##    .TPER9_19R        -0.062    0.060   -1.032    0.302
##    .TPER10_19R        0.179    0.093    1.925    0.054
##  .TPER8_19R ~~                                        
##    .TPER9_19R         0.104    0.053    1.972    0.049
##    .TPER10_19R        0.050    0.069    0.728    0.467
##  .TPER9_19R ~~                                        
##    .TPER10_19R       -0.037    0.052   -0.714    0.475
##  .TPER7_22R ~~                                        
##    .TPER8_22R         0.144    0.059    2.443    0.015
##    .TPER9_22R         0.026    0.061    0.427    0.670
##    .TPER10_22R        0.141    0.109    1.293    0.196
##  .TPER8_22R ~~                                        
##    .TPER9_22R         0.041    0.052    0.784    0.433
##    .TPER10_22R       -0.010    0.062   -0.167    0.867
##  .TPER9_22R ~~                                        
##    .TPER10_22R        0.020    0.054    0.375    0.707
##   e7 ~~                                               
##     E.change         -0.092    0.016   -5.901    0.000
##   c_7.8 ~~                                            
##     E.change          0.000                           
##   c_8.9 ~~                                            
##     E.change          0.000                           
##   c_9.10 ~~                                           
##     E.change          0.000                           
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .e8                0.000                           
##    .e9                0.000                           
##    .e10               0.000                           
##     c_7.8             0.000                           
##     c_8.9             0.000                           
##     c_9.10            0.000                           
##     e7                3.668    0.043   84.664    0.000
##    .TPER7_01          0.000                           
##    .TPER7_04 (nu1)    0.572    0.094    6.114    0.000
##    .TPER7_15 (nu2)   -0.058    0.086   -0.676    0.499
##    .TPER7_19 (nu3)   -0.477    0.111   -4.287    0.000
##    .TPER7_22 (nu4)   -0.587    0.114   -5.159    0.000
##    .TPER8_01          0.000                           
##    .TPER8_04 (nu1)    0.572    0.094    6.114    0.000
##    .TPER8_15 (nu2)   -0.058    0.086   -0.676    0.499
##    .TPER8_19 (nu3)   -0.477    0.111   -4.287    0.000
##    .TPER8_22 (nu4)   -0.587    0.114   -5.159    0.000
##    .TPER9_01          0.000                           
##    .TPER9_04 (nu1)    0.572    0.094    6.114    0.000
##    .TPER9_15 (nu2)   -0.058    0.086   -0.676    0.499
##    .TPER9_19 (nu3)   -0.477    0.111   -4.287    0.000
##    .TPER9_22 (nu4)   -0.587    0.114   -5.159    0.000
##    .TPER10_0          0.000                           
##    .TPER10_0 (nu1)    0.572    0.094    6.114    0.000
##    .TPER10_1 (nu2)   -0.058    0.086   -0.676    0.499
##    .TPER10_1 (nu3)   -0.477    0.111   -4.287    0.000
##    .TPER10_2 (nu4)   -0.587    0.114   -5.159    0.000
##     E.change          0.015    0.010    1.448    0.148
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .e8                0.000                           
##    .e9                0.000                           
##    .e10               0.000                           
##     e7                0.869    0.066   13.069    0.000
##     c_7.8             0.712    0.059   12.107    0.000
##     c_8.9             1.094    0.096   11.395    0.000
##     c_9.10            1.997    0.206    9.674    0.000
##    .TPER7_01          0.338    0.036    9.411    0.000
##    .TPER7_04          0.575    0.047   12.274    0.000
##    .TPER7_15          0.378    0.036   10.549    0.000
##    .TPER7_19R         0.742    0.063   11.779    0.000
##    .TPER7_22R         0.807    0.067   12.024    0.000
##    .TPER8_01          0.338    0.031   10.782    0.000
##    .TPER8_04          0.654    0.046   14.228    0.000
##    .TPER8_15          0.366    0.031   11.648    0.000
##    .TPER8_19R         0.778    0.058   13.327    0.000
##    .TPER8_22R         0.853    0.063   13.538    0.000
##    .TPER9_01          0.384    0.034   11.214    0.000
##    .TPER9_04          0.713    0.049   14.497    0.000
##    .TPER9_15          0.383    0.033   11.611    0.000
##    .TPER9_19R         0.701    0.055   12.752    0.000
##    .TPER9_22R         0.720    0.056   12.846    0.000
##    .TPER10_01         0.347    0.039    8.916    0.000
##    .TPER10_04         0.616    0.053   11.551    0.000
##    .TPER10_15         0.464    0.045   10.233    0.000
##    .TPER10_19R        0.610    0.059   10.367    0.000
##    .TPER10_22R        0.639    0.061   10.398    0.000
##     E.change         -0.074    0.010   -7.106    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now lets try this same model but with equal loadings on our general change factor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;e.lg.mw.model &amp;lt;-
&amp;#39;e7 =~ 1*TPER7_01 + cat6*TPER7_04 + cat2*TPER7_15 + cat3*TPER7_19R + cat4*TPER7_22R
e8 =~ 1*TPER8_01 + cat6*TPER8_04 + cat2*TPER8_15 + cat3*TPER8_19R + cat4*TPER8_22R
e9 =~ 1*TPER9_01 + cat6*TPER9_04 + cat2*TPER9_15 + cat3*TPER9_19R + cat4*TPER9_22R
e10 =~ 1*TPER10_01 + cat6*TPER10_04 + cat2*TPER10_15 + cat3*TPER10_19R + cat4*TPER10_22R


# settting the fixed regressions to 1

e8 ~ 1*e7     
e9 ~ 1*e8  
e10 ~ 1*e9  

# creating the change scores 

c_7.8 =~ 1*e8     
c_8.9 =~ 1*e9 
c_9.10 =~ 1*e10 


# constraint intercepts
e8 ~ 0*1        
e9 ~ 0*1   
e10 ~ 0*1   

# This fixes the variance to 0

e8 ~~ 0*e8     
e9 ~~ 0*e9  
e10 ~~ 0*e10  

# Estimate the intercept of the change scores
c_7.8 ~ 0*1    
c_8.9 ~ 0*1 
c_9.10 ~ 0*1             

# This estimates the intercept and variance of initial time
e7 ~  1           
e7 ~~  e7 

#  Estimates the variance of the change scores
c_7.8 ~~ c_7.8    
c_8.9 ~~ c_8.9 
c_9.10 ~~ c_9.10       


# # A change from the previous model is we no longer estimate the covariance between change scores a) with eachother and b) with the intercept. We will only covary intercept with a growth component, defined below. We must set some of these covariances to zero to make sure they are not estimated.  
e7  ~~ 0*c_7.8 + 0*c_8.9 + 0*c_9.10
c_7.8 ~~ 0*c_8.9 + 0*c_9.10
c_8.9 ~~ 0*c_9.10


## Allow indicators to covary
TPER7_01 ~~ TPER8_01 + TPER9_01 + TPER10_01
TPER8_01 ~~ TPER9_01 + TPER10_01
TPER9_01 ~~ TPER10_01

TPER7_04 ~~ TPER8_04 + TPER9_04 + TPER10_04
TPER8_04 ~~ TPER9_04 + TPER10_04
TPER9_04 ~~ TPER10_04

TPER7_15 ~~ TPER8_15 + TPER9_15 + TPER10_15
TPER8_15 ~~ TPER9_15 + TPER10_15
TPER9_15 ~~ TPER10_15

TPER7_19R ~~ TPER8_19R + TPER9_19R + TPER10_19R
TPER8_19R ~~ TPER9_19R + TPER10_19R
TPER9_19R ~~ TPER10_19R

TPER7_22R ~~ TPER8_22R + TPER9_22R + TPER10_22R
TPER8_22R ~~ TPER9_22R + TPER10_22R
TPER9_22R ~~ TPER10_22R



#scales the items and add measurement invariance

TPER7_01 ~ 0*1
TPER7_04 ~ (nu1)*1
TPER7_15 ~ (nu2)*1
TPER7_19R ~ (nu3)*1
TPER7_22R ~ (nu4)*1

TPER8_01~ 0*1
TPER8_04 ~ (nu1)*1
TPER8_15~ (nu2)*1
TPER8_19R ~ (nu3)*1
TPER8_22R~ (nu4)*1

TPER9_01~ 0*1
TPER9_04 ~ (nu1)*1
TPER9_15~ (nu2)*1
TPER9_19R ~ (nu3)*1
TPER9_22R~ (nu4)*1

TPER10_01 ~ 0*1
TPER10_04 ~ (nu1)*1
TPER10_15 ~ (nu2)*1
TPER10_19R ~ (nu3)*1
TPER10_22R ~ (nu4)*1

# create a latent intercept growth component
E.change  =~ 1*e8 + 1*e9 + 1*e10
E.change ~ 1

# Covary the intecept and new overall change component and make sure change does not covary with difference scores
E.change ~~ e7
E.change ~~ 0*c_7.8 + 0*c_8.9 + 0*c_9.10

&amp;#39;

e.lg.mw.model &amp;lt;- sem(e.lg.mw.model, data=y_t_all , missing=&amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: some cases are empty and will be ignored:
##   14 24 32 33 34 36 37 39 48 49 51 57 76 78 80 83 84 86 90 91 98 109 119 129 130 131 140 144 145 157 165 167 170 171 177 180 182 183 190 206 222 227 229 234 247 250 260 287 292 295 301 302 307 327 339 342 345 349 354 356 359 360 369 370 377 383 384 386 393 406 411 414 419 421 422 425 426 429 440 442 446 448 449 450 456 460 462 470 473 476 485 491 494 496 503 504 507 508 516 532 535 539 546 547 548 552 554 555 556 559 561 563 566 571 584 586 587 591 596 599 601 607 620 630 634 635 642 645 650 651 658 666 673 678 687 716 718 732 735 737 738 740 745 751 761 766 776 785 786 788 790 795 796 802 804 823 827 831 842 847 851 853 860 887 892 901 920 923 926 927 936 938 940 947 949 952 953 955 956 957 969 972 977 979 981 985 993 999 1000 1003 1016 1029 1032 1035 1038 1040 1044 1046 1048 1050 1055 1059 1066&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, :
## lavaan WARNING: due to missing values, some pairwise combinations have less
## than 10% coverage&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_object_post_check(object): lavaan WARNING: some estimated lv
## variances are negative&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(e.lg.mw.model, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 59 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         90
##   Number of equality constraints                    24
##   Row rank of the constraints matrix                24
## 
##                                                   Used       Total
##   Number of observations                           864        1067
##   Number of missing patterns                        29
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     897.471
##   Degrees of freedom                               164
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             6120.154
##   Degrees of freedom                               190
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.876
##   Tucker-Lewis Index (TLI)                       0.857
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -11710.635
##   Loglikelihood unrestricted model (H1)     -11261.900
## 
##   Number of free parameters                         66
##   Akaike (AIC)                               23553.270
##   Bayesian (BIC)                             23867.534
##   Sample-size adjusted Bayesian (BIC)        23657.935
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.072
##   90 Percent Confidence Interval          0.067  0.077
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.124
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   e7 =~                                               
##     TPER7_0           1.000                           
##     TPER7_0 (cat6)    0.791    0.024   32.656    0.000
##     TPER7_1 (cat2)    0.987    0.022   44.101    0.000
##     TPER7_1 (cat3)    1.023    0.029   35.086    0.000
##     TPER7_2 (cat4)    1.024    0.030   34.249    0.000
##   e8 =~                                               
##     TPER8_0           1.000                           
##     TPER8_0 (cat6)    0.791    0.024   32.656    0.000
##     TPER8_1 (cat2)    0.987    0.022   44.101    0.000
##     TPER8_1 (cat3)    1.023    0.029   35.086    0.000
##     TPER8_2 (cat4)    1.024    0.030   34.249    0.000
##   e9 =~                                               
##     TPER9_0           1.000                           
##     TPER9_0 (cat6)    0.791    0.024   32.656    0.000
##     TPER9_1 (cat2)    0.987    0.022   44.101    0.000
##     TPER9_1 (cat3)    1.023    0.029   35.086    0.000
##     TPER9_2 (cat4)    1.024    0.030   34.249    0.000
##   e10 =~                                              
##     TPER10_           1.000                           
##     TPER10_ (cat6)    0.791    0.024   32.656    0.000
##     TPER10_ (cat2)    0.987    0.022   44.101    0.000
##     TPER10_ (cat3)    1.023    0.029   35.086    0.000
##     TPER10_ (cat4)    1.024    0.030   34.249    0.000
##   c_7.8 =~                                            
##     e8                1.000                           
##   c_8.9 =~                                            
##     e9                1.000                           
##   c_9.10 =~                                           
##     e10               1.000                           
##   E.change =~                                         
##     e8                1.000                           
##     e9                1.000                           
##     e10               1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   e8 ~                                                
##     e7                1.000                           
##   e9 ~                                                
##     e8                1.000                           
##   e10 ~                                               
##     e9                1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   e7 ~~                                               
##     c_7.8             0.000                           
##     c_8.9             0.000                           
##     c_9.10            0.000                           
##   c_7.8 ~~                                            
##     c_8.9             0.000                           
##     c_9.10            0.000                           
##   c_8.9 ~~                                            
##     c_9.10            0.000                           
##  .TPER7_01 ~~                                         
##    .TPER8_01          0.029    0.030    0.970    0.332
##    .TPER9_01          0.090    0.036    2.509    0.012
##    .TPER10_01        -0.201    0.053   -3.763    0.000
##  .TPER8_01 ~~                                         
##    .TPER9_01          0.073    0.028    2.590    0.010
##    .TPER10_01         0.064    0.036    1.809    0.071
##  .TPER9_01 ~~                                         
##    .TPER10_01         0.024    0.034    0.719    0.472
##  .TPER7_04 ~~                                         
##    .TPER8_04          0.153    0.039    3.895    0.000
##    .TPER9_04          0.147    0.051    2.897    0.004
##    .TPER10_04         0.103    0.096    1.073    0.283
##  .TPER8_04 ~~                                         
##    .TPER9_04          0.222    0.043    5.227    0.000
##    .TPER10_04         0.084    0.054    1.575    0.115
##  .TPER9_04 ~~                                         
##    .TPER10_04         0.204    0.048    4.276    0.000
##  .TPER7_15 ~~                                         
##    .TPER8_15          0.012    0.030    0.418    0.676
##    .TPER9_15          0.008    0.034    0.249    0.804
##    .TPER10_15        -0.135    0.071   -1.891    0.059
##  .TPER8_15 ~~                                         
##    .TPER9_15          0.026    0.028    0.939    0.348
##    .TPER10_15         0.019    0.042    0.460    0.645
##  .TPER9_15 ~~                                         
##    .TPER10_15         0.011    0.033    0.322    0.747
##  .TPER7_19R ~~                                        
##    .TPER8_19R         0.113    0.055    2.052    0.040
##    .TPER9_19R        -0.066    0.060   -1.087    0.277
##    .TPER10_19R        0.177    0.093    1.890    0.059
##  .TPER8_19R ~~                                        
##    .TPER9_19R         0.107    0.053    2.030    0.042
##    .TPER10_19R        0.058    0.069    0.835    0.404
##  .TPER9_19R ~~                                        
##    .TPER10_19R       -0.038    0.052   -0.734    0.463
##  .TPER7_22R ~~                                        
##    .TPER8_22R         0.148    0.059    2.502    0.012
##    .TPER9_22R         0.025    0.061    0.401    0.688
##    .TPER10_22R        0.138    0.109    1.268    0.205
##  .TPER8_22R ~~                                        
##    .TPER9_22R         0.044    0.052    0.842    0.400
##    .TPER10_22R       -0.006    0.062   -0.093    0.926
##  .TPER9_22R ~~                                        
##    .TPER10_22R        0.018    0.054    0.343    0.732
##   e7 ~~                                               
##     E.change         -0.202    0.031   -6.451    0.000
##   c_7.8 ~~                                            
##     E.change          0.000                           
##   c_8.9 ~~                                            
##     E.change          0.000                           
##   c_9.10 ~~                                           
##     E.change          0.000                           
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .e8                0.000                           
##    .e9                0.000                           
##    .e10               0.000                           
##     c_7.8             0.000                           
##     c_8.9             0.000                           
##     c_9.10            0.000                           
##     e7                3.671    0.047   78.914    0.000
##    .TPER7_01          0.000                           
##    .TPER7_04 (nu1)    0.571    0.093    6.133    0.000
##    .TPER7_15 (nu2)   -0.062    0.086   -0.720    0.472
##    .TPER7_19 (nu3)   -0.475    0.111   -4.276    0.000
##    .TPER7_22 (nu4)   -0.588    0.114   -5.159    0.000
##    .TPER8_01          0.000                           
##    .TPER8_04 (nu1)    0.571    0.093    6.133    0.000
##    .TPER8_15 (nu2)   -0.062    0.086   -0.720    0.472
##    .TPER8_19 (nu3)   -0.475    0.111   -4.276    0.000
##    .TPER8_22 (nu4)   -0.588    0.114   -5.159    0.000
##    .TPER9_01          0.000                           
##    .TPER9_04 (nu1)    0.571    0.093    6.133    0.000
##    .TPER9_15 (nu2)   -0.062    0.086   -0.720    0.472
##    .TPER9_19 (nu3)   -0.475    0.111   -4.276    0.000
##    .TPER9_22 (nu4)   -0.588    0.114   -5.159    0.000
##    .TPER10_0          0.000                           
##    .TPER10_0 (nu1)    0.571    0.093    6.133    0.000
##    .TPER10_1 (nu2)   -0.062    0.086   -0.720    0.472
##    .TPER10_1 (nu3)   -0.475    0.111   -4.276    0.000
##    .TPER10_2 (nu4)   -0.588    0.114   -5.159    0.000
##     E.change          0.019    0.021    0.917    0.359
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .e8                0.000                           
##    .e9                0.000                           
##    .e10               0.000                           
##     e7                0.939    0.074   12.627    0.000
##     c_7.8             1.003    0.081   12.389    0.000
##     c_8.9             1.100    0.091   12.048    0.000
##     c_9.10            1.412    0.132   10.707    0.000
##    .TPER7_01          0.328    0.035    9.235    0.000
##    .TPER7_04          0.568    0.046   12.251    0.000
##    .TPER7_15          0.372    0.035   10.475    0.000
##    .TPER7_19R         0.741    0.063   11.726    0.000
##    .TPER7_22R         0.806    0.067   11.967    0.000
##    .TPER8_01          0.332    0.031   10.658    0.000
##    .TPER8_04          0.652    0.046   14.206    0.000
##    .TPER8_15          0.363    0.031   11.584    0.000
##    .TPER8_19R         0.779    0.059   13.299    0.000
##    .TPER8_22R         0.852    0.063   13.497    0.000
##    .TPER9_01          0.386    0.034   11.298    0.000
##    .TPER9_04          0.713    0.049   14.512    0.000
##    .TPER9_15          0.383    0.033   11.646    0.000
##    .TPER9_19R         0.703    0.055   12.800    0.000
##    .TPER9_22R         0.722    0.056   12.889    0.000
##    .TPER10_01         0.348    0.039    8.929    0.000
##    .TPER10_04         0.616    0.053   11.551    0.000
##    .TPER10_15         0.463    0.045   10.225    0.000
##    .TPER10_19R        0.609    0.059   10.360    0.000
##    .TPER10_22R        0.640    0.062   10.397    0.000
##     E.change         -0.262    0.035   -7.462    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does the negative variance here mean?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;steyer-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;“steyer” models&lt;/h2&gt;
&lt;p&gt;Similar to the above models but are addative in their interpretation. That is, the difference score between waves can be thought of adding or subtracting cumulatively from the previous wave. This is a closer hybrid between the growth model and the latent difference score model as the final difference score estimate is interpretted as across the entire time period.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model &amp;lt;-
&amp;#39;extra7 =~ 1*TPER7_01 + cat6*TPER7_04 + cat2*TPER7_15 + cat3*TPER7_19R + cat4*TPER7_22R
extra8 =~ 1*TPER8_01 + cat6*TPER8_04 + cat2*TPER8_15 + cat3*TPER8_19R + cat4*TPER8_22R
extra9 =~ 1*TPER9_01 + cat6*TPER9_04 + cat2*TPER9_15 + cat3*TPER9_19R + cat4*TPER9_22R
extra10 =~ 1*TPER10_01 + cat6*TPER10_04 + cat2*TPER10_15 + cat3*TPER10_19R + cat4*TPER10_22R

TPER7_01 ~ 0*1
TPER7_04 ~ (nu1)*1
TPER7_15 ~ (nu2)*1
TPER7_19R ~ (nu3)*1
TPER7_22R ~ (nu4)*1

TPER8_01~ 0*1
TPER8_04 ~ (nu1)*1
TPER8_15~ (nu2)*1
TPER8_19R ~ (nu3)*1
TPER8_22R~ (nu4)*1

TPER9_01~ 0*1
TPER9_04 ~ (nu1)*1
TPER9_15~ (nu2)*1
TPER9_19R ~ (nu3)*1
TPER9_22R~ (nu4)*1

TPER10_01 ~ 0*1
TPER10_04 ~ (nu1)*1
TPER10_15 ~ (nu2)*1
TPER10_19R ~ (nu3)*1
TPER10_22R ~ (nu4)*1

TPER7_01 ~~ TPER8_01 + TPER9_01 + TPER10_01
TPER8_01 ~~ TPER9_01 + TPER10_01
TPER9_01 ~~ TPER10_01

TPER7_04 ~~ TPER8_04 + TPER9_04 + TPER10_04
TPER8_04 ~~ TPER9_04 + TPER10_04
TPER9_04 ~~ TPER10_04

TPER7_15 ~~ TPER8_15 + TPER9_15 + TPER10_15
TPER8_15 ~~ TPER9_15 + TPER10_15
TPER9_15 ~~ TPER10_15

TPER7_19R ~~ TPER8_19R + TPER9_19R + TPER10_19R
TPER8_19R ~~ TPER9_19R + TPER10_19R
TPER9_19R ~~ TPER10_19R

TPER7_22R ~~ TPER8_22R + TPER9_22R + TPER10_22R
TPER8_22R ~~ TPER9_22R + TPER10_22R
TPER9_22R ~~ TPER10_22R

extra7 ~ 0*1
extra8 ~ 0*1
extra9 ~ 0*1
extra10 ~ 0*1

extra7 ~~ 0*extra7
extra8 ~~ 0*extra8
extra9 ~~ 0*extra9
extra10 ~~ 0*extra10

t7 =~ 1*extra7 + 1*extra8 + 1*extra9 + 1*extra10
t7 ~ 1
t7 ~~ t7

d8 =~ 1*extra8
d8 ~~ d8
d8 ~ 1

d9 =~ 1*extra9
d9 ~~ d9
d9 ~ 1

d10 =~ 1*extra10
d10 ~~ d10
d10 ~ 1

t7 ~~ d8 + d9 + d10
d8 ~~ d9 + d10
d9 ~~ d10&amp;#39;

fitE_t &amp;lt;- sem(model, missing=&amp;quot;ML&amp;quot;, data = y_t_all)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: some cases are empty and will be ignored:
##   14 24 32 33 34 36 37 39 48 49 51 57 76 78 80 83 84 86 90 91 98 109 119 129 130 131 140 144 145 157 165 167 170 171 177 180 182 183 190 206 222 227 229 234 247 250 260 287 292 295 301 302 307 327 339 342 345 349 354 356 359 360 369 370 377 383 384 386 393 406 411 414 419 421 422 425 426 429 440 442 446 448 449 450 456 460 462 470 473 476 485 491 494 496 503 504 507 508 516 532 535 539 546 547 548 552 554 555 556 559 561 563 566 571 584 586 587 591 596 599 601 607 620 630 634 635 642 645 650 651 658 666 673 678 687 716 718 732 735 737 738 740 745 751 761 766 776 785 786 788 790 795 796 802 804 823 827 831 842 847 851 853 860 887 892 901 920 923 926 927 936 938 940 947 949 952 953 955 956 957 969 972 977 979 981 985 993 999 1000 1003 1016 1029 1032 1035 1038 1040 1044 1046 1048 1050 1055 1059 1066&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, :
## lavaan WARNING: due to missing values, some pairwise combinations have less
## than 10% coverage&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fitE_t)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 74 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         96
##   Number of equality constraints                    24
##   Row rank of the constraints matrix                24
## 
##                                                   Used       Total
##   Number of observations                           864        1067
##   Number of missing patterns                        29
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     833.294
##   Degrees of freedom                               158
##   P-value (Chi-square)                           0.000
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   extra7 =~                                           
##     TPER7_0           1.000                           
##     TPER7_0 (cat6)    0.789    0.024   32.827    0.000
##     TPER7_1 (cat2)    0.987    0.022   44.223    0.000
##     TPER7_1 (cat3)    1.020    0.029   35.076    0.000
##     TPER7_2 (cat4)    1.022    0.030   34.287    0.000
##   extra8 =~                                           
##     TPER8_0           1.000                           
##     TPER8_0 (cat6)    0.789    0.024   32.827    0.000
##     TPER8_1 (cat2)    0.987    0.022   44.223    0.000
##     TPER8_1 (cat3)    1.020    0.029   35.076    0.000
##     TPER8_2 (cat4)    1.022    0.030   34.287    0.000
##   extra9 =~                                           
##     TPER9_0           1.000                           
##     TPER9_0 (cat6)    0.789    0.024   32.827    0.000
##     TPER9_1 (cat2)    0.987    0.022   44.223    0.000
##     TPER9_1 (cat3)    1.020    0.029   35.076    0.000
##     TPER9_2 (cat4)    1.022    0.030   34.287    0.000
##   extra10 =~                                          
##     TPER10_           1.000                           
##     TPER10_ (cat6)    0.789    0.024   32.827    0.000
##     TPER10_ (cat2)    0.987    0.022   44.223    0.000
##     TPER10_ (cat3)    1.020    0.029   35.076    0.000
##     TPER10_ (cat4)    1.022    0.030   34.287    0.000
##   t7 =~                                               
##     extra7            1.000                           
##     extra8            1.000                           
##     extra9            1.000                           
##     extra10           1.000                           
##   d8 =~                                               
##     extra8            1.000                           
##   d9 =~                                               
##     extra9            1.000                           
##   d10 =~                                              
##     extra10           1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##  .TPER7_01 ~~                                         
##    .TPER8_01          0.049    0.030    1.634    0.102
##    .TPER9_01          0.087    0.035    2.457    0.014
##    .TPER10_01        -0.205    0.052   -3.937    0.000
##  .TPER8_01 ~~                                         
##    .TPER9_01          0.079    0.028    2.814    0.005
##    .TPER10_01         0.061    0.035    1.749    0.080
##  .TPER9_01 ~~                                         
##    .TPER10_01         0.021    0.034    0.616    0.538
##  .TPER7_04 ~~                                         
##    .TPER8_04          0.163    0.039    4.133    0.000
##    .TPER9_04          0.141    0.051    2.769    0.006
##    .TPER10_04         0.078    0.094    0.829    0.407
##  .TPER8_04 ~~                                         
##    .TPER9_04          0.225    0.043    5.289    0.000
##    .TPER10_04         0.086    0.053    1.604    0.109
##  .TPER9_04 ~~                                         
##    .TPER10_04         0.201    0.047    4.234    0.000
##  .TPER7_15 ~~                                         
##    .TPER8_15          0.022    0.030    0.725    0.468
##    .TPER9_15          0.008    0.034    0.241    0.810
##    .TPER10_15        -0.137    0.070   -1.958    0.050
##  .TPER8_15 ~~                                         
##    .TPER9_15          0.027    0.028    0.948    0.343
##    .TPER10_15         0.013    0.042    0.322    0.747
##  .TPER9_15 ~~                                         
##    .TPER10_15         0.008    0.033    0.240    0.810
##  .TPER7_19R ~~                                        
##    .TPER8_19R         0.104    0.055    1.890    0.059
##    .TPER9_19R        -0.074    0.060   -1.233    0.218
##    .TPER10_19R        0.189    0.092    2.059    0.039
##  .TPER8_19R ~~                                        
##    .TPER9_19R         0.110    0.053    2.078    0.038
##    .TPER10_19R        0.053    0.069    0.775    0.438
##  .TPER9_19R ~~                                        
##    .TPER10_19R       -0.036    0.052   -0.690    0.490
##  .TPER7_22R ~~                                        
##    .TPER8_22R         0.143    0.060    2.406    0.016
##    .TPER9_22R         0.019    0.061    0.304    0.761
##    .TPER10_22R        0.160    0.108    1.487    0.137
##  .TPER8_22R ~~                                        
##    .TPER9_22R         0.049    0.052    0.941    0.347
##    .TPER10_22R       -0.014    0.062   -0.227    0.820
##  .TPER9_22R ~~                                        
##    .TPER10_22R        0.025    0.054    0.467    0.640
##   t7 ~~                                               
##     d8               -0.532    0.073   -7.291    0.000
##     d9               -0.417    0.073   -5.693    0.000
##     d10              -0.501    0.118   -4.250    0.000
##   d8 ~~                                               
##     d9                0.495    0.090    5.533    0.000
##     d10               0.513    0.130    3.931    0.000
##   d9 ~~                                               
##     d10               0.420    0.121    3.472    0.001
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .TPER7_01          0.000                           
##    .TPER7_04 (nu1)    0.579    0.092    6.266    0.000
##    .TPER7_15 (nu2)   -0.059    0.086   -0.689    0.491
##    .TPER7_19 (nu3)   -0.461    0.111   -4.161    0.000
##    .TPER7_22 (nu4)   -0.581    0.114   -5.109    0.000
##    .TPER8_01          0.000                           
##    .TPER8_04 (nu1)    0.579    0.092    6.266    0.000
##    .TPER8_15 (nu2)   -0.059    0.086   -0.689    0.491
##    .TPER8_19 (nu3)   -0.461    0.111   -4.161    0.000
##    .TPER8_22 (nu4)   -0.581    0.114   -5.109    0.000
##    .TPER9_01          0.000                           
##    .TPER9_04 (nu1)    0.579    0.092    6.266    0.000
##    .TPER9_15 (nu2)   -0.059    0.086   -0.689    0.491
##    .TPER9_19 (nu3)   -0.461    0.111   -4.161    0.000
##    .TPER9_22 (nu4)   -0.581    0.114   -5.109    0.000
##    .TPER10_0          0.000                           
##    .TPER10_0 (nu1)    0.579    0.092    6.266    0.000
##    .TPER10_1 (nu2)   -0.059    0.086   -0.689    0.491
##    .TPER10_1 (nu3)   -0.461    0.111   -4.161    0.000
##    .TPER10_2 (nu4)   -0.581    0.114   -5.109    0.000
##    .extra7            0.000                           
##    .extra8            0.000                           
##    .extra9            0.000                           
##    .extra10           0.000                           
##     t7                3.681    0.051   72.878    0.000
##     d8               -0.015    0.056   -0.259    0.795
##     d9               -0.026    0.056   -0.466    0.641
##     d10               0.093    0.066    1.406    0.160
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .extra7            0.000                           
##    .extra8            0.000                           
##    .extra9            0.000                           
##    .extra10           0.000                           
##     t7                1.014    0.083   12.153    0.000
##     d8                1.067    0.108    9.831    0.000
##     d9                0.884    0.111    7.971    0.000
##     d10               0.931    0.203    4.589    0.000
##    .TPER7_01          0.321    0.035    9.069    0.000
##    .TPER7_04          0.566    0.046   12.253    0.000
##    .TPER7_15          0.368    0.035   10.408    0.000
##    .TPER7_19R         0.733    0.063   11.624    0.000
##    .TPER7_22R         0.800    0.067   11.858    0.000
##    .TPER8_01          0.325    0.031   10.530    0.000
##    .TPER8_04          0.650    0.046   14.201    0.000
##    .TPER8_15          0.366    0.031   11.624    0.000
##    .TPER8_19R         0.786    0.059   13.328    0.000
##    .TPER8_22R         0.850    0.063   13.482    0.000
##    .TPER9_01          0.388    0.034   11.345    0.000
##    .TPER9_04          0.713    0.049   14.525    0.000
##    .TPER9_15          0.380    0.033   11.634    0.000
##    .TPER9_19R         0.706    0.055   12.837    0.000
##    .TPER9_22R         0.724    0.056   12.918    0.000
##    .TPER10_01         0.348    0.039    8.953    0.000
##    .TPER10_04         0.615    0.053   11.579    0.000
##    .TPER10_15         0.465    0.045   10.239    0.000
##    .TPER10_19R        0.613    0.059   10.412    0.000
##    .TPER10_22R        0.642    0.062   10.427    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare the means here with the means of the original latent difference score model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-time-varying-covariates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;adding time varying covariates&lt;/h2&gt;
&lt;p&gt;The tricky tricky part is interpretation. It is also possible to constraint the effect of the TVC to be the same across time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tvc.mw.model &amp;lt;-
&amp;#39;e7 =~ 1*TPER7_01 + cat6*TPER7_04 + cat2*TPER7_15 + cat3*TPER7_19R + cat4*TPER7_22R
e8 =~ 1*TPER8_01 + cat6*TPER8_04 + cat2*TPER8_15 + cat3*TPER8_19R + cat4*TPER8_22R
e9 =~ 1*TPER9_01 + cat6*TPER9_04 + cat2*TPER9_15 + cat3*TPER9_19R + cat4*TPER9_22R
e10 =~ 1*TPER10_01 + cat6*TPER10_04 + cat2*TPER10_15 + cat3*TPER10_19R + cat4*TPER10_22R


# settting the fixed regressions to 1

e8 ~ 1*e7     
e9 ~ 1*e8  
e10 ~ 1*e9  

# creating the change scores 

c_7.8 =~ 1*e8     
c_8.9 =~ 1*e9 
c_9.10 =~ 1*e10 


# constraint intercepts
e8 ~ 0*1        
e9 ~ 0*1   
e10 ~ 0*1   

# This fixes the variance to 0

e8 ~~ 0*e8     
e9 ~~ 0*e9  
e10 ~~ 0*e10  

# Estimate the intercept of the change score
c_7.8 ~ 1    
c_8.9 ~ 1 
c_9.10 ~ 1             

# This estimates the intercept and variance of initial time
e7 ~  1           
e7 ~~  e7 

#  Estimates the variance of the change scores
c_7.8 ~~ c_7.8    
c_8.9 ~~ c_8.9 
c_9.10 ~~ c_9.10       


# Estimates the covaration with change scores and T1
e7  ~~ c_7.8 + c_8.9 + c_9.10 
c_7.8 ~~ c_8.9 + c_9.10
c_8.9 ~~ c_9.10

## Allow indicators to covary
TPER7_01 ~~ TPER8_01 + TPER9_01 + TPER10_01
TPER8_01 ~~ TPER9_01 + TPER10_01
TPER9_01 ~~ TPER10_01

TPER7_04 ~~ TPER8_04 + TPER9_04 + TPER10_04
TPER8_04 ~~ TPER9_04 + TPER10_04
TPER9_04 ~~ TPER10_04

TPER7_15 ~~ TPER8_15 + TPER9_15 + TPER10_15
TPER8_15 ~~ TPER9_15 + TPER10_15
TPER9_15 ~~ TPER10_15

TPER7_19R ~~ TPER8_19R + TPER9_19R + TPER10_19R
TPER8_19R ~~ TPER9_19R + TPER10_19R
TPER9_19R ~~ TPER10_19R

TPER7_22R ~~ TPER8_22R + TPER9_22R + TPER10_22R
TPER8_22R ~~ TPER9_22R + TPER10_22R
TPER9_22R ~~ TPER10_22R



#scales the items and add measurement invariance

TPER7_01 ~ 0*1
TPER7_04 ~ (nu1)*1
TPER7_15 ~ (nu2)*1
TPER7_19R ~ (nu3)*1
TPER7_22R ~ (nu4)*1

TPER8_01~ 0*1
TPER8_04 ~ (nu1)*1
TPER8_15~ (nu2)*1
TPER8_19R ~ (nu3)*1
TPER8_22R~ (nu4)*1

TPER9_01~ 0*1
TPER9_04 ~ (nu1)*1
TPER9_15~ (nu2)*1
TPER9_19R ~ (nu3)*1
TPER9_22R~ (nu4)*1

TPER10_01 ~ 0*1
TPER10_04 ~ (nu1)*1
TPER10_15 ~ (nu2)*1
TPER10_19R ~ (nu3)*1
TPER10_22R ~ (nu4)*1

# predicting change
# Note: Technically the covariate model above is set up the same way.  

c_7.8 ~  ALC8_01   
c_8.9 ~  ALC9_01
c_9.10 ~ ALC10_01


&amp;#39;

tvc.mw.model &amp;lt;- sem(tvc.mw.model, data=y_t_all , missing=&amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: 250 cases were deleted due to missing values in 
##        exogenous variable(s), while fixed.x = TRUE.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, :
## lavaan WARNING: due to missing values, some pairwise combinations have less
## than 10% coverage&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(tvc.mw.model, fit.measures=TRUE, standardized=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 80 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         99
##   Number of equality constraints                    24
##   Row rank of the constraints matrix                24
## 
##                                                   Used       Total
##   Number of observations                           817        1067
##   Number of missing patterns                        30
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     884.163
##   Degrees of freedom                               215
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             5667.448
##   Degrees of freedom                               250
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.876
##   Tucker-Lewis Index (TLI)                       0.856
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -10485.293
##   Loglikelihood unrestricted model (H1)     -10043.212
## 
##   Number of free parameters                         75
##   Akaike (AIC)                               21120.586
##   Bayesian (BIC)                             21473.509
##   Sample-size adjusted Bayesian (BIC)        21235.338
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.062
##   90 Percent Confidence Interval          0.058  0.066
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.065
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   e7 =~                                                                 
##     TPER7_0           1.000                               1.012    0.872
##     TPER7_0 (cat6)    0.794    0.025   31.293    0.000    0.804    0.730
##     TPER7_1 (cat2)    1.003    0.024   41.765    0.000    1.015    0.860
##     TPER7_1 (cat3)    1.025    0.031   32.905    0.000    1.038    0.769
##     TPER7_2 (cat4)    1.029    0.032   32.169    0.000    1.042    0.758
##   e8 =~                                                                 
##     TPER8_0           1.000                               0.977    0.868
##     TPER8_0 (cat6)    0.794    0.025   31.293    0.000    0.776    0.697
##     TPER8_1 (cat2)    1.003    0.024   41.765    0.000    0.980    0.851
##     TPER8_1 (cat3)    1.025    0.031   32.905    0.000    1.002    0.744
##     TPER8_2 (cat4)    1.029    0.032   32.169    0.000    1.006    0.733
##   e9 =~                                                                 
##     TPER9_0           1.000                               1.027    0.855
##     TPER9_0 (cat6)    0.794    0.025   31.293    0.000    0.816    0.721
##     TPER9_1 (cat2)    1.003    0.024   41.765    0.000    1.030    0.857
##     TPER9_1 (cat3)    1.025    0.031   32.905    0.000    1.053    0.778
##     TPER9_2 (cat4)    1.029    0.032   32.169    0.000    1.057    0.779
##   e10 =~                                                                
##     TPER10_           1.000                               0.952    0.850
##     TPER10_ (cat6)    0.794    0.025   31.293    0.000    0.756    0.680
##     TPER10_ (cat2)    1.003    0.024   41.765    0.000    0.954    0.822
##     TPER10_ (cat3)    1.025    0.031   32.905    0.000    0.976    0.779
##     TPER10_ (cat4)    1.029    0.032   32.169    0.000    0.980    0.775
##   c_7.8 =~                                                              
##     e8                1.000                               1.044    1.044
##   c_8.9 =~                                                              
##     e9                1.000                               0.954    0.954
##   c_9.10 =~                                                             
##     e10               1.000                               1.033    1.033
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   e8 ~                                                                  
##     e7                1.000                               1.036    1.036
##   e9 ~                                                                  
##     e8                1.000                               0.951    0.951
##   e10 ~                                                                 
##     e9                1.000                               1.079    1.079
##   c_7.8 ~                                                               
##     ALC8_01          -0.114    0.082   -1.395    0.163   -0.112   -0.051
##   c_8.9 ~                                                               
##     ALC9_01          -0.113    0.101   -1.122    0.262   -0.116   -0.049
##   c_9.10 ~                                                              
##     ALC10_01         -0.130    0.131   -0.995    0.320   -0.133   -0.053
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   e7 ~~                                                                 
##    .c_7.8            -0.555    0.076   -7.293    0.000   -0.538   -0.538
##    .c_8.9             0.122    0.075    1.614    0.107    0.123    0.123
##    .c_9.10           -0.093    0.118   -0.791    0.429   -0.094   -0.094
##  .c_7.8 ~~                                                              
##    .c_8.9            -0.553    0.087   -6.330    0.000   -0.555   -0.555
##    .c_9.10            0.022    0.131    0.168    0.866    0.022    0.022
##  .c_8.9 ~~                                                              
##    .c_9.10           -0.490    0.088   -5.539    0.000   -0.510   -0.510
##  .TPER7_01 ~~                                                           
##    .TPER8_01          0.048    0.030    1.589    0.112    0.048    0.153
##    .TPER9_01          0.078    0.036    2.149    0.032    0.078    0.220
##    .TPER10_01        -0.208    0.053   -3.911    0.000   -0.208   -0.622
##  .TPER8_01 ~~                                                           
##    .TPER9_01          0.066    0.028    2.355    0.019    0.066    0.190
##    .TPER10_01         0.067    0.034    1.941    0.052    0.067    0.202
##  .TPER9_01 ~~                                                           
##    .TPER10_01         0.025    0.035    0.699    0.485    0.025    0.067
##  .TPER7_04 ~~                                                           
##    .TPER8_04          0.168    0.041    4.098    0.000    0.168    0.280
##    .TPER9_04          0.123    0.048    2.588    0.010    0.123    0.209
##    .TPER10_04         0.123    0.100    1.229    0.219    0.123    0.201
##  .TPER8_04 ~~                                                           
##    .TPER9_04          0.201    0.040    5.037    0.000    0.201    0.320
##    .TPER10_04         0.096    0.056    1.707    0.088    0.096    0.148
##  .TPER9_04 ~~                                                           
##    .TPER10_04         0.219    0.048    4.600    0.000    0.219    0.343
##  .TPER7_15 ~~                                                           
##    .TPER8_15          0.028    0.031    0.889    0.374    0.028    0.076
##    .TPER9_15          0.000    0.035    0.010    0.992    0.000    0.001
##    .TPER10_15        -0.093    0.079   -1.171    0.242   -0.093   -0.233
##  .TPER8_15 ~~                                                           
##    .TPER9_15          0.017    0.029    0.578    0.563    0.017    0.045
##    .TPER10_15         0.019    0.041    0.452    0.651    0.019    0.046
##  .TPER9_15 ~~                                                           
##    .TPER10_15         0.042    0.035    1.195    0.232    0.042    0.103
##  .TPER7_19R ~~                                                          
##    .TPER8_19R         0.119    0.058    2.045    0.041    0.119    0.153
##    .TPER9_19R        -0.076    0.063   -1.203    0.229   -0.076   -0.104
##    .TPER10_19R        0.171    0.095    1.791    0.073    0.171    0.252
##  .TPER8_19R ~~                                                          
##    .TPER9_19R         0.123    0.055    2.223    0.026    0.123    0.161
##    .TPER10_19R        0.050    0.071    0.715    0.475    0.050    0.071
##  .TPER9_19R ~~                                                          
##    .TPER10_19R       -0.066    0.057   -1.155    0.248   -0.066   -0.098
##  .TPER7_22R ~~                                                          
##    .TPER8_22R         0.149    0.062    2.381    0.017    0.149    0.177
##    .TPER9_22R         0.024    0.061    0.399    0.690    0.024    0.032
##    .TPER10_22R        0.140    0.110    1.275    0.202    0.140    0.195
##  .TPER8_22R ~~                                                          
##    .TPER9_22R         0.063    0.053    1.184    0.237    0.063    0.080
##    .TPER10_22R       -0.016    0.063   -0.253    0.800   -0.016   -0.021
##  .TPER9_22R ~~                                                          
##    .TPER10_22R        0.034    0.058    0.590    0.555    0.034    0.050
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .e8                0.000                               0.000    0.000
##    .e9                0.000                               0.000    0.000
##    .e10               0.000                               0.000    0.000
##    .c_7.8             0.160    0.118    1.350    0.177    0.157    0.157
##    .c_8.9             0.117    0.138    0.849    0.396    0.120    0.120
##    .c_9.10            0.266    0.172    1.543    0.123    0.271    0.271
##     e7                3.703    0.053   69.590    0.000    3.658    3.658
##    .TPER7_01          0.000                               0.000    0.000
##    .TPER7_04 (nu1)    0.593    0.098    6.036    0.000    0.593    0.539
##    .TPER7_15 (nu2)   -0.126    0.093   -1.362    0.173   -0.126   -0.107
##    .TPER7_19 (nu3)   -0.485    0.120   -4.060    0.000   -0.485   -0.360
##    .TPER7_22 (nu4)   -0.603    0.123   -4.917    0.000   -0.603   -0.439
##    .TPER8_01          0.000                               0.000    0.000
##    .TPER8_04 (nu1)    0.593    0.098    6.036    0.000    0.593    0.532
##    .TPER8_15 (nu2)   -0.126    0.093   -1.362    0.173   -0.126   -0.110
##    .TPER8_19 (nu3)   -0.485    0.120   -4.060    0.000   -0.485   -0.361
##    .TPER8_22 (nu4)   -0.603    0.123   -4.917    0.000   -0.603   -0.439
##    .TPER9_01          0.000                               0.000    0.000
##    .TPER9_04 (nu1)    0.593    0.098    6.036    0.000    0.593    0.524
##    .TPER9_15 (nu2)   -0.126    0.093   -1.362    0.173   -0.126   -0.105
##    .TPER9_19 (nu3)   -0.485    0.120   -4.060    0.000   -0.485   -0.359
##    .TPER9_22 (nu4)   -0.603    0.123   -4.917    0.000   -0.603   -0.445
##    .TPER10_0          0.000                               0.000    0.000
##    .TPER10_0 (nu1)    0.593    0.098    6.036    0.000    0.593    0.533
##    .TPER10_1 (nu2)   -0.126    0.093   -1.362    0.173   -0.126   -0.109
##    .TPER10_1 (nu3)   -0.485    0.120   -4.060    0.000   -0.485   -0.387
##    .TPER10_2 (nu4)   -0.603    0.123   -4.917    0.000   -0.603   -0.477
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .e8                0.000                               0.000    0.000
##    .e9                0.000                               0.000    0.000
##    .e10               0.000                               0.000    0.000
##     e7                1.025    0.089   11.575    0.000    1.000    1.000
##    .c_7.8             1.038    0.110    9.443    0.000    0.997    0.997
##    .c_8.9             0.958    0.101    9.506    0.000    0.998    0.998
##    .c_9.10            0.964    0.108    8.946    0.000    0.997    0.997
##    .TPER7_01          0.322    0.038    8.547    0.000    0.322    0.239
##    .TPER7_04          0.565    0.049   11.563    0.000    0.565    0.466
##    .TPER7_15          0.364    0.037    9.799    0.000    0.364    0.261
##    .TPER7_19R         0.743    0.067   11.029    0.000    0.743    0.408
##    .TPER7_22R         0.806    0.072   11.252    0.000    0.806    0.426
##    .TPER8_01          0.313    0.031   10.128    0.000    0.313    0.247
##    .TPER8_04          0.639    0.047   13.714    0.000    0.639    0.515
##    .TPER8_15          0.367    0.033   11.197    0.000    0.367    0.276
##    .TPER8_19R         0.807    0.062   12.973    0.000    0.807    0.446
##    .TPER8_22R         0.873    0.067   13.103    0.000    0.873    0.463
##    .TPER9_01          0.389    0.036   10.749    0.000    0.389    0.269
##    .TPER9_04          0.615    0.046   13.322    0.000    0.615    0.480
##    .TPER9_15          0.382    0.035   10.897    0.000    0.382    0.265
##    .TPER9_19R         0.721    0.059   12.121    0.000    0.721    0.394
##    .TPER9_22R         0.723    0.060   12.140    0.000    0.723    0.393
##    .TPER10_01         0.348    0.041    8.406    0.000    0.348    0.278
##    .TPER10_04         0.664    0.061   10.949    0.000    0.664    0.538
##    .TPER10_15         0.438    0.047    9.358    0.000    0.438    0.325
##    .TPER10_19R        0.618    0.063    9.746    0.000    0.618    0.394
##    .TPER10_22R        0.639    0.066    9.736    0.000    0.639    0.400&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y_t_all$alc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## NULL&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-duel-change-score-model-like-a-residualized-latent-change-score&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Latent Duel change score model (like a residualized latent change score)&lt;/h1&gt;
&lt;p&gt;The regression from t-1 to change at t can also be introduced into these models. It will have a new name: self-feedback. It is akin to our standard intercept-change correlation where it may be those who start higher change more (or less). This parameter will become important when we start to examine bivariate models. It is called duel because there are two components: the rate of change (slope) and the self-feedback.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;duel.model &amp;lt;-
&amp;#39;e7 =~ 1*TPER7_01 + cat6*TPER7_04 + cat2*TPER7_15 + cat3*TPER7_19R + cat4*TPER7_22R
e8 =~ 1*TPER8_01 + cat6*TPER8_04 + cat2*TPER8_15 + cat3*TPER8_19R + cat4*TPER8_22R
e9 =~ 1*TPER9_01 + cat6*TPER9_04 + cat2*TPER9_15 + cat3*TPER9_19R + cat4*TPER9_22R
e10 =~ 1*TPER10_01 + cat6*TPER10_04 + cat2*TPER10_15 + cat3*TPER10_19R + cat4*TPER10_22R


# settting the fixed regressions to 1

e8 ~ 1*e7     
e9 ~ 1*e8  
e10 ~ 1*e9  

# creating the change scores 

c_7.8 =~ 1*e8     
c_8.9 =~ 1*e9 
c_9.10 =~ 1*e10 


# constraint intercepts
e8 ~ 0*1        
e9 ~ 0*1   
e10 ~ 0*1   

# This fixes the variance to 0

e8 ~~ 0*e8     
e9 ~~ 0*e9  
e10 ~~ 0*e10  

# Estimate the intercept of the change score
c_7.8 ~ 1    
c_8.9 ~ 1 
c_9.10 ~ 1             

# This estimates the intercept and variance of initial time
e7 ~  1           
e7 ~~  e7 

#  Estimates the variance of the change scores
c_7.8 ~~ c_7.8    
c_8.9 ~~ c_8.9 
c_9.10 ~~ c_9.10       

## This is the novel component of the DUEL Latent Difference Scores
# Instead of estimating the covariance, we are going to regress change onto the previous timepoint. 
c_7.8 ~ e7 
c_8.9 ~ e8
c_9.10 ~ e9

## Allow indicators to covary
TPER7_01 ~~ TPER8_01 + TPER9_01 + TPER10_01
TPER8_01 ~~ TPER9_01 + TPER10_01
TPER9_01 ~~ TPER10_01

TPER7_04 ~~ TPER8_04 + TPER9_04 + TPER10_04
TPER8_04 ~~ TPER9_04 + TPER10_04
TPER9_04 ~~ TPER10_04

TPER7_15 ~~ TPER8_15 + TPER9_15 + TPER10_15
TPER8_15 ~~ TPER9_15 + TPER10_15
TPER9_15 ~~ TPER10_15

TPER7_19R ~~ TPER8_19R + TPER9_19R + TPER10_19R
TPER8_19R ~~ TPER9_19R + TPER10_19R
TPER9_19R ~~ TPER10_19R

TPER7_22R ~~ TPER8_22R + TPER9_22R + TPER10_22R
TPER8_22R ~~ TPER9_22R + TPER10_22R
TPER9_22R ~~ TPER10_22R



#scales the items and add measurement invariance

TPER7_01 ~ 0*1
TPER7_04 ~ (nu1)*1
TPER7_15 ~ (nu2)*1
TPER7_19R ~ (nu3)*1
TPER7_22R ~ (nu4)*1

TPER8_01~ 0*1
TPER8_04 ~ (nu1)*1
TPER8_15~ (nu2)*1
TPER8_19R ~ (nu3)*1
TPER8_22R~ (nu4)*1

TPER9_01~ 0*1
TPER9_04 ~ (nu1)*1
TPER9_15~ (nu2)*1
TPER9_19R ~ (nu3)*1
TPER9_22R~ (nu4)*1

TPER10_01 ~ 0*1
TPER10_04 ~ (nu1)*1
TPER10_15 ~ (nu2)*1
TPER10_19R ~ (nu3)*1
TPER10_22R ~ (nu4)*1

&amp;#39;

duel.model &amp;lt;- sem(duel.model, data=y_t_all , missing=&amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan WARNING: some cases are empty and will be ignored:
##   14 24 32 33 34 36 37 39 48 49 51 57 76 78 80 83 84 86 90 91 98 109 119 129 130 131 140 144 145 157 165 167 170 171 177 180 182 183 190 206 222 227 229 234 247 250 260 287 292 295 301 302 307 327 339 342 345 349 354 356 359 360 369 370 377 383 384 386 393 406 411 414 419 421 422 425 426 429 440 442 446 448 449 450 456 460 462 470 473 476 485 491 494 496 503 504 507 508 516 532 535 539 546 547 548 552 554 555 556 559 561 563 566 571 584 586 587 591 596 599 601 607 620 630 634 635 642 645 650 651 658 666 673 678 687 716 718 732 735 737 738 740 745 751 761 766 776 785 786 788 790 795 796 802 804 823 827 831 842 847 851 853 860 887 892 901 920 923 926 927 936 938 940 947 949 952 953 955 956 957 969 972 977 979 981 985 993 999 1000 1003 1016 1029 1032 1035 1038 1040 1044 1046 1048 1050 1055 1059 1066&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_data_full(data = data, group = group, cluster = cluster, :
## lavaan WARNING: due to missing values, some pairwise combinations have less
## than 10% coverage&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(duel.model, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 87 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         96
##   Number of equality constraints                    24
##   Row rank of the constraints matrix                24
## 
##                                                   Used       Total
##   Number of observations                           864        1067
##   Number of missing patterns                        29
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     833.294
##   Degrees of freedom                               158
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             6120.154
##   Degrees of freedom                               190
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.886
##   Tucker-Lewis Index (TLI)                       0.863
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -11678.547
##   Loglikelihood unrestricted model (H1)     -11261.900
## 
##   Number of free parameters                         72
##   Akaike (AIC)                               23501.094
##   Bayesian (BIC)                             23843.927
##   Sample-size adjusted Bayesian (BIC)        23615.274
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.070
##   90 Percent Confidence Interval          0.066  0.075
##   P-value RMSEA &amp;lt;= 0.05                          0.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.064
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   e7 =~                                               
##     TPER7_0           1.000                           
##     TPER7_0 (cat6)    0.789    0.024   32.827    0.000
##     TPER7_1 (cat2)    0.987    0.022   44.223    0.000
##     TPER7_1 (cat3)    1.020    0.029   35.076    0.000
##     TPER7_2 (cat4)    1.022    0.030   34.288    0.000
##   e8 =~                                               
##     TPER8_0           1.000                           
##     TPER8_0 (cat6)    0.789    0.024   32.827    0.000
##     TPER8_1 (cat2)    0.987    0.022   44.223    0.000
##     TPER8_1 (cat3)    1.020    0.029   35.076    0.000
##     TPER8_2 (cat4)    1.022    0.030   34.288    0.000
##   e9 =~                                               
##     TPER9_0           1.000                           
##     TPER9_0 (cat6)    0.789    0.024   32.827    0.000
##     TPER9_1 (cat2)    0.987    0.022   44.223    0.000
##     TPER9_1 (cat3)    1.020    0.029   35.076    0.000
##     TPER9_2 (cat4)    1.022    0.030   34.288    0.000
##   e10 =~                                              
##     TPER10_           1.000                           
##     TPER10_ (cat6)    0.789    0.024   32.827    0.000
##     TPER10_ (cat2)    0.987    0.022   44.223    0.000
##     TPER10_ (cat3)    1.020    0.029   35.076    0.000
##     TPER10_ (cat4)    1.022    0.030   34.288    0.000
##   c_7.8 =~                                            
##     e8                1.000                           
##   c_8.9 =~                                            
##     e9                1.000                           
##   c_9.10 =~                                           
##     e10               1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   e8 ~                                                
##     e7                1.000                           
##   e9 ~                                                
##     e8                1.000                           
##   e10 ~                                               
##     e9                1.000                           
##   c_7.8 ~                                             
##     e7               -0.524    0.054   -9.628    0.000
##   c_8.9 ~                                             
##     e8                0.238    0.168    1.414    0.157
##   c_9.10 ~                                            
##     e9               -0.142    0.189   -0.752    0.452
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##  .TPER7_01 ~~                                         
##    .TPER8_01          0.049    0.030    1.634    0.102
##    .TPER9_01          0.087    0.035    2.457    0.014
##    .TPER10_01        -0.205    0.052   -3.936    0.000
##  .TPER8_01 ~~                                         
##    .TPER9_01          0.079    0.028    2.814    0.005
##    .TPER10_01         0.061    0.035    1.749    0.080
##  .TPER9_01 ~~                                         
##    .TPER10_01         0.021    0.034    0.616    0.538
##  .TPER7_04 ~~                                         
##    .TPER8_04          0.163    0.039    4.133    0.000
##    .TPER9_04          0.141    0.051    2.770    0.006
##    .TPER10_04         0.078    0.094    0.829    0.407
##  .TPER8_04 ~~                                         
##    .TPER9_04          0.225    0.043    5.289    0.000
##    .TPER10_04         0.086    0.053    1.605    0.109
##  .TPER9_04 ~~                                         
##    .TPER10_04         0.201    0.047    4.234    0.000
##  .TPER7_15 ~~                                         
##    .TPER8_15          0.022    0.030    0.725    0.468
##    .TPER9_15          0.008    0.034    0.241    0.810
##    .TPER10_15        -0.137    0.070   -1.958    0.050
##  .TPER8_15 ~~                                         
##    .TPER9_15          0.027    0.028    0.948    0.343
##    .TPER10_15         0.013    0.042    0.322    0.747
##  .TPER9_15 ~~                                         
##    .TPER10_15         0.008    0.033    0.240    0.810
##  .TPER7_19R ~~                                        
##    .TPER8_19R         0.104    0.055    1.890    0.059
##    .TPER9_19R        -0.074    0.060   -1.232    0.218
##    .TPER10_19R        0.189    0.092    2.059    0.039
##  .TPER8_19R ~~                                        
##    .TPER9_19R         0.110    0.053    2.078    0.038
##    .TPER10_19R        0.053    0.069    0.775    0.438
##  .TPER9_19R ~~                                        
##    .TPER10_19R       -0.036    0.052   -0.690    0.490
##  .TPER7_22R ~~                                        
##    .TPER8_22R         0.143    0.060    2.406    0.016
##    .TPER9_22R         0.019    0.061    0.304    0.761
##    .TPER10_22R        0.160    0.108    1.487    0.137
##  .TPER8_22R ~~                                        
##    .TPER9_22R         0.049    0.052    0.941    0.347
##    .TPER10_22R       -0.014    0.062   -0.227    0.820
##  .TPER9_22R ~~                                        
##    .TPER10_22R        0.025    0.054    0.467    0.640
##  .c_7.8 ~~                                            
##    .c_8.9            -0.699    0.169   -4.141    0.000
##    .c_9.10            0.012    0.123    0.100    0.920
##  .c_8.9 ~~                                            
##    .c_9.10           -0.413    0.117   -3.520    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .e8                0.000                           
##    .e9                0.000                           
##    .e10               0.000                           
##    .c_7.8             1.914    0.206    9.291    0.000
##    .c_8.9            -0.885    0.622   -1.423    0.155
##    .c_9.10            0.638    0.698    0.913    0.361
##     e7                3.681    0.051   72.878    0.000
##    .TPER7_01          0.000                           
##    .TPER7_04 (nu1)    0.579    0.092    6.266    0.000
##    .TPER7_15 (nu2)   -0.059    0.086   -0.689    0.491
##    .TPER7_19 (nu3)   -0.461    0.111   -4.161    0.000
##    .TPER7_22 (nu4)   -0.581    0.114   -5.109    0.000
##    .TPER8_01          0.000                           
##    .TPER8_04 (nu1)    0.579    0.092    6.266    0.000
##    .TPER8_15 (nu2)   -0.059    0.086   -0.689    0.491
##    .TPER8_19 (nu3)   -0.461    0.111   -4.161    0.000
##    .TPER8_22 (nu4)   -0.581    0.114   -5.109    0.000
##    .TPER9_01          0.000                           
##    .TPER9_04 (nu1)    0.579    0.092    6.266    0.000
##    .TPER9_15 (nu2)   -0.059    0.086   -0.689    0.491
##    .TPER9_19 (nu3)   -0.461    0.111   -4.161    0.000
##    .TPER9_22 (nu4)   -0.581    0.114   -5.109    0.000
##    .TPER10_0          0.000                           
##    .TPER10_0 (nu1)    0.579    0.092    6.266    0.000
##    .TPER10_1 (nu2)   -0.059    0.086   -0.689    0.491
##    .TPER10_1 (nu3)   -0.461    0.111   -4.161    0.000
##    .TPER10_2 (nu4)   -0.581    0.114   -5.109    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .e8                0.000                           
##    .e9                0.000                           
##    .e10               0.000                           
##     e7                1.014    0.083   12.153    0.000
##    .c_7.8             0.788    0.071   11.163    0.000
##    .c_8.9             1.235    0.265    4.664    0.000
##    .c_9.10            0.841    0.175    4.818    0.000
##    .TPER7_01          0.321    0.035    9.069    0.000
##    .TPER7_04          0.566    0.046   12.253    0.000
##    .TPER7_15          0.368    0.035   10.408    0.000
##    .TPER7_19R         0.733    0.063   11.624    0.000
##    .TPER7_22R         0.800    0.067   11.858    0.000
##    .TPER8_01          0.325    0.031   10.530    0.000
##    .TPER8_04          0.650    0.046   14.201    0.000
##    .TPER8_15          0.366    0.031   11.624    0.000
##    .TPER8_19R         0.786    0.059   13.328    0.000
##    .TPER8_22R         0.850    0.063   13.482    0.000
##    .TPER9_01          0.388    0.034   11.345    0.000
##    .TPER9_04          0.713    0.049   14.525    0.000
##    .TPER9_15          0.380    0.033   11.634    0.000
##    .TPER9_19R         0.706    0.055   12.837    0.000
##    .TPER9_22R         0.724    0.056   12.918    0.000
##    .TPER10_01         0.348    0.039    8.953    0.000
##    .TPER10_04         0.615    0.053   11.579    0.000
##    .TPER10_15         0.465    0.045   10.239    0.000
##    .TPER10_19R        0.613    0.059   10.412    0.000
##    .TPER10_22R        0.642    0.062   10.427    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice the large changes within this model in terms of mean level change. This suggests that a lot of the change was due to intercept differences or that there is regression towards the mean.&lt;/p&gt;
&lt;p&gt;One could attempt to constrain each of the self-feedback parameters to be the same to see if this fit the model better or worse.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bivariate-multi-wave-latent-change-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Bivariate multi wave latent change model&lt;/h1&gt;
&lt;p&gt;The real benefit of this type of framework is building on this type of change model to incorporate multiple change models simultanously. This allows one to look at multiple change processes in a single model, opening the door to analyzing more dynamic processes.&lt;/p&gt;
&lt;p&gt;One of the main parameters of interest is what is called a coupling parameter. Coupling parameter is like a self-feedback, but instead of feeding into change in the same variable, it is feeding into change in a different variable (not unlike the idea of cross-lags).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BDCS&amp;lt;-&amp;#39;

COGlv_T1=~1*COG_T1        # Defining the COG latent variables
COGlv_T2=~1*COG_T2        # Defining the COG latent variables
COGlv_T3=~1*COG_T3        # Defining the COG latent variables
COGlv_T4=~1*COG_T4        # Defining the COG latent variables

NEUlv_T1=~1*NEU_T1        # Defining the NEU latent variables
NEUlv_T2=~1*NEU_T2        # Defining the NEU latent variables
NEUlv_T3=~1*NEU_T3        # Defining the NEU latent variables
NEUlv_T4=~1*NEU_T4        # Defining the NEU latent variables


COGlv_T2 ~ 1*COGlv_T1     # This parameter regresses COG_T2 perfectly on COG_T1
COGlv_T3 ~ 1*COGlv_T2     # This parameter regresses COG_T3 perfectly on COG_T2
COGlv_T4 ~ 1*COGlv_T3     # This parameter regresses COG_T4 perfectly on COG_T3

NEUlv_T2 ~ 1*NEUlv_T1     # This parameter regresses NEU_T2 perfectly on NEU_T1
NEUlv_T3 ~ 1*NEUlv_T2     # This parameter regresses NEU_T3 perfectly on NEU_T2
NEUlv_T4 ~ 1*NEUlv_T3     # This parameter regresses NEU_T4 perfectly on NEU_T3

dCOG1 =~ 1*COGlv_T2       # This defines the change score as measured perfectly by scores on COG_T2
dCOG2 =~ 1*COGlv_T3       # This defines the change score as measured perfectly by scores on COG_T3
dCOG3 =~ 1*COGlv_T4       # This defines the change score as measured perfectly by scores on COG_T4

dNEU1 =~ 1*NEUlv_T2       # This defines the change score as measured perfectly by scores on NEU_T2
dNEU2 =~ 1*NEUlv_T3       # This defines the change score as measured perfectly by scores on NEU_T3
dNEU3 =~ 1*NEUlv_T4       # This defines the change score as measured perfectly by scores on NEU_T4

COG_T1~~COG_T1          # This estimates the COG residual variances 
COG_T2~~COG_T2          # This estimates the COG residual variances 
COG_T3~~COG_T3          # This estimates the COG residual variances 
COG_T4~~COG_T4          # This estimates the COG residual variances 

NEU_T1~~NEU_T1          # This estimates the NEU residual variances 
NEU_T2~~NEU_T2          # This estimates the NEU residual variances 
NEU_T3~~NEU_T3          # This estimates the NEU residual variances 
NEU_T4~~NEU_T4          # This estimates the NEU residual variances 

#Dynamics

dNEU1~B1*NEUlv_T1       # This estimates the NEU self-feedback parameter 
dNEU2~B1*NEUlv_T2       # This estimates the NEU self-feedback parameter 
dNEU3~B1*NEUlv_T3       # This estimates the NEU self-feedback parameter 

dCOG1~B2*COGlv_T1       # This estimates the COG self-feedback parameter 
dCOG2~B2*COGlv_T2       # This estimates the COG self-feedback parameter 
dCOG3~B2*COGlv_T3       # This estimates the COG self-feedback parameter 

dNEU1~C1*COGlv_T1         # This estimates the COG to NEU coupling parameter 
dNEU2~C1*COGlv_T2         # This estimates the COG to NEU coupling parameter 
dNEU3~C1*COGlv_T3         # This estimates the COG to NEU coupling parameter 

dCOG1~C2*NEUlv_T1        # This estimates the NEU to COG coupling parameter 
dCOG2~C2*NEUlv_T2        # This estimates the NEU to COG coupling parameter 
dCOG3~C2*NEUlv_T3        # This estimates the NEU to COG coupling parameter 


iCOG=~1*COGlv_T1                   # This defines the COG intercept measurement model
sCOG=~1*dCOG1+1*dCOG2+1*dCOG3      # This defines the COG slope measurement model
iCOG~1                             # This estimates the COG intercept intercept (mean)
iCOG~~iCOG                         # This estimates the COG intercept variance
sCOG~1                             # This estimates the COG slope intercept
sCOG~~sCOG                         # This estimates the COG slope variance

iNEU=~1*NEUlv_T1                   # This defines the NEU slope measurement model
sNEU=~1*dNEU1+1*dNEU2+1*dNEU3      # This defines the NEU slope measurement model
iNEU~1                             # This estimates the NEU intercept intercept (mean)
iNEU~~iNEU                         # This estimates the NEU intercept variance
sNEU~1                             # This estimates the NEU slope intercept
sNEU~~sNEU                         # This estimates the NEU slope variance

iNEU~~sNEU                      # This estimates the iNEU sNEU covariance
iNEU~~sCOG                      # This estimates the iNEU sCOG covariance
iNEU~~iCOG                      # This estimates the iNEU iCOG covariance
iCOG~~sCOG                      # This estimates the iCOG sCOG covariance        
iCOG~~sNEU                      # This estimates the iCOG sNEU covariance
sCOG~~sNEU                      # This estimates the sCOG sNEU covariance  

&amp;#39;

fitBDCS &amp;lt;- lavaan(BDCS, data=simdatBD, missing=&amp;quot;ML&amp;quot;)
summary(fitBDCS, fit.measures=TRUE, standardized=TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 149 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         34
##   Number of equality constraints                     8
##   Row rank of the constraints matrix                 8
## 
##   Number of observations                           500
##   Number of missing patterns                         1
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      13.389
##   Degrees of freedom                                18
##   P-value (Chi-square)                           0.768
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             3140.634
##   Degrees of freedom                                28
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    1.000
##   Tucker-Lewis Index (TLI)                       1.002
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -10661.038
##   Loglikelihood unrestricted model (H1)     -10654.344
## 
##   Number of free parameters                         26
##   Akaike (AIC)                               21374.077
##   Bayesian (BIC)                             21483.657
##   Sample-size adjusted Bayesian (BIC)        21401.131
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.000
##   90 Percent Confidence Interval          0.000  0.028
##   P-value RMSEA &amp;lt;= 0.05                          0.999
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.021
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   COGlv_T1 =~                                                           
##     COG_T1            1.000                               1.606    0.575
##   COGlv_T2 =~                                                           
##     COG_T2            1.000                               3.447    0.842
##   COGlv_T3 =~                                                           
##     COG_T3            1.000                               6.131    0.929
##   COGlv_T4 =~                                                           
##     COG_T4            1.000                               9.893    0.965
##   NEUlv_T1 =~                                                           
##     NEU_T1            1.000                               1.742    0.605
##   NEUlv_T2 =~                                                           
##     NEU_T2            1.000                               3.175    0.792
##   NEUlv_T3 =~                                                           
##     NEU_T3            1.000                               5.492    0.926
##   NEUlv_T4 =~                                                           
##     NEU_T4            1.000                               8.717    0.954
##   dCOG1 =~                                                              
##     COGlv_T2          1.000                               0.732    0.732
##   dCOG2 =~                                                              
##     COGlv_T3          1.000                               0.467    0.467
##   dCOG3 =~                                                              
##     COGlv_T4          1.000                               0.399    0.399
##   dNEU1 =~                                                              
##     NEUlv_T2          1.000                               0.673    0.673
##   dNEU2 =~                                                              
##     NEUlv_T3          1.000                               0.448    0.448
##   dNEU3 =~                                                              
##     NEUlv_T4          1.000                               0.381    0.381
##   iCOG =~                                                               
##     COGlv_T1          1.000                               1.000    1.000
##   sCOG =~                                                               
##     dCOG1             1.000                               0.851    0.851
##     dCOG2             1.000                               0.750    0.750
##     dCOG3             1.000                               0.544    0.544
##   iNEU =~                                                               
##     NEUlv_T1          1.000                               1.000    1.000
##   sNEU =~                                                               
##     dNEU1             1.000                               0.910    0.910
##     dNEU2             1.000                               0.791    0.791
##     dNEU3             1.000                               0.586    0.586
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   COGlv_T2 ~                                                            
##     COGlv_T1          1.000                               0.466    0.466
##   COGlv_T3 ~                                                            
##     COGlv_T2          1.000                               0.562    0.562
##   COGlv_T4 ~                                                            
##     COGlv_T3          1.000                               0.620    0.620
##   NEUlv_T2 ~                                                            
##     NEUlv_T1          1.000                               0.549    0.549
##   NEUlv_T3 ~                                                            
##     NEUlv_T2          1.000                               0.578    0.578
##   NEUlv_T4 ~                                                            
##     NEUlv_T3          1.000                               0.630    0.630
##   dNEU1 ~                                                               
##     NEUlv_T1  (B1)   -0.030    0.078   -0.388    0.698   -0.025   -0.025
##   dNEU2 ~                                                               
##     NEUlv_T2  (B1)   -0.030    0.078   -0.388    0.698   -0.039   -0.039
##   dNEU3 ~                                                               
##     NEUlv_T3  (B1)   -0.030    0.078   -0.388    0.698   -0.050   -0.050
##   dCOG1 ~                                                               
##     COGlv_T1  (B2)   -0.122    0.068   -1.778    0.075   -0.077   -0.077
##   dCOG2 ~                                                               
##     COGlv_T2  (B2)   -0.122    0.068   -1.778    0.075   -0.146   -0.146
##   dCOG3 ~                                                               
##     COGlv_T3  (B2)   -0.122    0.068   -1.778    0.075   -0.189   -0.189
##   dNEU1 ~                                                               
##     COGlv_T1  (C1)    0.357    0.069    5.151    0.000    0.268    0.268
##   dNEU2 ~                                                               
##     COGlv_T2  (C1)    0.357    0.069    5.151    0.000    0.500    0.500
##   dNEU3 ~                                                               
##     COGlv_T3  (C1)    0.357    0.069    5.151    0.000    0.659    0.659
##   dCOG1 ~                                                               
##     NEUlv_T1  (C2)    0.627    0.076    8.296    0.000    0.433    0.433
##   dCOG2 ~                                                               
##     NEUlv_T2  (C2)    0.627    0.076    8.296    0.000    0.696    0.696
##   dCOG3 ~                                                               
##     NEUlv_T3  (C2)    0.627    0.076    8.296    0.000    0.873    0.873
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   iNEU ~~                                                               
##     sNEU              0.948    0.318    2.983    0.003    0.280    0.280
##   sCOG ~~                                                               
##     iNEU              0.766    0.360    2.129    0.033    0.205    0.205
##   iCOG ~~                                                               
##     iNEU              1.068    0.325    3.283    0.001    0.382    0.382
##     sCOG              1.113    0.340    3.272    0.001    0.323    0.323
##     sNEU              0.754    0.300    2.511    0.012    0.241    0.241
##   sCOG ~~                                                               
##     sNEU              0.399    0.393    1.015    0.310    0.096    0.096
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     iCOG              2.187    0.121   18.079    0.000    1.362    1.362
##     sCOG              1.818    0.182    9.999    0.000    0.847    0.847
##     iNEU              1.929    0.124   15.575    0.000    1.107    1.107
##     sNEU              2.012    0.172   11.718    0.000    1.035    1.035
##    .COG_T1            0.000                               0.000    0.000
##    .COG_T2            0.000                               0.000    0.000
##    .COG_T3            0.000                               0.000    0.000
##    .COG_T4            0.000                               0.000    0.000
##    .NEU_T1            0.000                               0.000    0.000
##    .NEU_T2            0.000                               0.000    0.000
##    .NEU_T3            0.000                               0.000    0.000
##    .NEU_T4            0.000                               0.000    0.000
##    .COGlv_T1          0.000                               0.000    0.000
##    .COGlv_T2          0.000                               0.000    0.000
##    .COGlv_T3          0.000                               0.000    0.000
##    .COGlv_T4          0.000                               0.000    0.000
##    .NEUlv_T1          0.000                               0.000    0.000
##    .NEUlv_T2          0.000                               0.000    0.000
##    .NEUlv_T3          0.000                               0.000    0.000
##    .NEUlv_T4          0.000                               0.000    0.000
##    .dCOG1             0.000                               0.000    0.000
##    .dCOG2             0.000                               0.000    0.000
##    .dCOG3             0.000                               0.000    0.000
##    .dNEU1             0.000                               0.000    0.000
##    .dNEU2             0.000                               0.000    0.000
##    .dNEU3             0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .COG_T1            5.229    0.628    8.321    0.000    5.229    0.670
##    .COG_T2            4.875    0.404   12.070    0.000    4.875    0.291
##    .COG_T3            5.994    0.587   10.210    0.000    5.994    0.138
##    .COG_T4            7.294    1.053    6.925    0.000    7.294    0.069
##    .NEU_T1            5.261    0.605    8.691    0.000    5.261    0.634
##    .NEU_T2            5.989    0.456   13.120    0.000    5.989    0.373
##    .NEU_T3            4.986    0.518    9.622    0.000    4.986    0.142
##    .NEU_T4            7.503    1.033    7.260    0.000    7.503    0.090
##     iCOG              2.579    0.584    4.414    0.000    1.000    1.000
##     sCOG              4.605    0.663    6.949    0.000    1.000    1.000
##     iNEU              3.036    0.563    5.389    0.000    1.000    1.000
##     sNEU              3.783    0.587    6.446    0.000    1.000    1.000
##    .COGlv_T1          0.000                               0.000    0.000
##    .COGlv_T2          0.000                               0.000    0.000
##    .COGlv_T3          0.000                               0.000    0.000
##    .COGlv_T4          0.000                               0.000    0.000
##    .NEUlv_T1          0.000                               0.000    0.000
##    .NEUlv_T2          0.000                               0.000    0.000
##    .NEUlv_T3          0.000                               0.000    0.000
##    .NEUlv_T4          0.000                               0.000    0.000
##    .dCOG1             0.000                               0.000    0.000
##    .dCOG2             0.000                               0.000    0.000
##    .dCOG3             0.000                               0.000    0.000
##    .dNEU1             0.000                               0.000    0.000
##    .dNEU2             0.000                               0.000    0.000
##    .dNEU3             0.000                               0.000    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;alt-and-alt-sr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ALT and ALT-SR&lt;/h2&gt;
&lt;p&gt;Another way to build both a growth model and look at more specific time associations are through auto-regressive latent trajectory models (ALT) and ALT models with structured residuals (ALT-SR). The latter is a newer (and better) implimentation of the ALT idea.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>homeworks 4-6</title>
      <link>/homeworks/homeworks-4-6/</link>
      <pubDate>Thu, 07 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/homeworks/homeworks-4-6/</guid>
      <description>


&lt;p&gt;Please email your homework files to: &lt;a href=&#34;mailto:HW4_6.wfwewdvlhfgict5w@u.box.com&#34; class=&#34;email&#34;&gt;HW4_6.wfwewdvlhfgict5w@u.box.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Use the dataHW4-6 to answer the questions below. Note that there is also a codebook to help interpret the variables (there are more variables in the codebook than in the file, fyi).&lt;/p&gt;
&lt;p&gt;Files are available in github. See here: &lt;a href=&#34;https://raw.githubusercontent.com/josh-jackson/ALDA/master/dataHW4-6.csv&#34; class=&#34;uri&#34;&gt;https://raw.githubusercontent.com/josh-jackson/ALDA/master/dataHW4-6.csv&lt;/a&gt;
&lt;a href=&#34;https://raw.githubusercontent.com/josh-jackson/ALDA/master/codebookHW4-6.csv&#34; class=&#34;uri&#34;&gt;https://raw.githubusercontent.com/josh-jackson/ALDA/master/codebookHW4-6.csv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For the main growth modeling questions please choose one of the big five as your main variable for analysis. Note that for some of the initial questions you will need to create a composite variable AND take the dataset from wide into long format. Starting with q#6 you will use the item variables.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Run a growth model in 1) MLM using me4, 2) Bayesian MLM using brms and 3) SEM using lavaan. For each of these models used the created composites, not the individual items. What are the similarities and differences?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using the latent growth model in MLM, produce a spaghetti plot showing average and individual slopes within ggplot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Try two different time coding schemes. First, use a different time coding that defines the intercept as the final time point. Second, try a latent basis coding where you allow the data to inform how time should be coded. For each, describe how this differs from the initial latent growth curve model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduce a predictor to the lgc model (such as current health). How does this impact the model compared to a) no predictor b) a centered predictor?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create another graph of the model, this time showing the effect of the predictor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using the items, create three latent variables for each wave. Check for measurement invariance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a second order latent growth model, imposing measurement invariance. Compare the fit and statistics from this model to the lgc model that used composites for the repeated measures.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create another graph of this model, showing the individual slopes and the overall mean level trend.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduce the same predictor to the model as you did in #4. Did you find any differences or are the results mostly the same?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Code time differently in this model as you did for #3 above. Did you find any differences or are the results mostly the same?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run a multivariate second order latent growth model where you are examining two growth processes simultaneously. What parameters are you most interested in? How do the results of the growth process modeled above (mean, variances, etc) change when you model two processes simultaneously?&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Week 9 &amp; 10</title>
      <link>/lectures/week-9/</link>
      <pubDate>Thu, 24 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/week-9/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-longitudinal-models-other-than-growth-models&#34;&gt;Types of longitudinal models other than growth models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#longitudinal-cfa&#34;&gt;1. Longitudinal CFA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#measurement-invariance-mi&#34;&gt;Measurement Invariance (MI)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-mi&#34;&gt;types of MI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing-mi&#34;&gt;Testing MI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-the-models&#34;&gt;Comparing the models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#panelcross-laglongitudinal-path-modelmediation&#34;&gt;2. Panel/cross lag/longitudinal path model/mediation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#longitudinal-cross-lagged-panel-model-clpm&#34;&gt;Longitudinal Cross lagged panel model (clpm)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#longitudinal-mediation-model&#34;&gt;Longitudinal mediation model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#including-covariates&#34;&gt;Including covariates&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#growth-models&#34;&gt;3. Growth models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#coding-time&#34;&gt;Coding time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-do-we-do-that-in-lavaan&#34;&gt;How do we do that in lavaan?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#compare-with-mlm&#34;&gt;Compare with MLM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#we-can-rescale-our-time-variable-to-be-whatever-we-want&#34;&gt;we can rescale our time variable to be whatever we want&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-of-variances&#34;&gt;Interpretation of variances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#constraining-slope-to-be-fixed-only&#34;&gt;constraining slope to be fixed only&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introducing-covariatespredictors&#34;&gt;introducing covariates/predictors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#centered-predictors&#34;&gt;centered predictors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#introducing-time-varying-covariates&#34;&gt;introducing time varying covariates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#latent-basis-model&#34;&gt;latent basis model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quadratic-model&#34;&gt;quadratic model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#b-second-order-growth-model&#34;&gt;3.b Second order growth model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#c-multivariate-models&#34;&gt;3.c Multivariate models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#d-factor-of-curves-model&#34;&gt;3.d Factor of curves model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-groups&#34;&gt;4. Multiple groups&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#measurement-invariance-revisited&#34;&gt;Measurement invariance revisited&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#when-to-use&#34;&gt;When to use&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#planned-missing-data&#34;&gt;Planned missing data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#missing-data&#34;&gt;Missing data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#power&#34;&gt;Power&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;types-of-longitudinal-models-other-than-growth-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Types of longitudinal models other than growth models&lt;/h1&gt;
&lt;p&gt;With SEM there are many different types of longitudinal models you can run. The basic ones are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Longitudinal CFA&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Panel/cross lag/longitudinal path model/mediation&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Growth models&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Latent change/differene score models&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mixture or class based longitudinal models&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Within these you can do multiple groups or change how the repeated measures are assessed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is lavaan 0.6-4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan is BETA software! Please report any bugs.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long &amp;lt;- read.csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/SEM_workshop/longitudinal.csv&amp;quot;)

summary(long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     PosAFF11        PosAFF21         PosAFF31        NegAFF11      
##  Min.   :1.365   Min.   :0.4152   Min.   :1.140   Min.   :-0.8584  
##  1st Qu.:2.739   1st Qu.:2.6343   1st Qu.:2.797   1st Qu.: 1.1035  
##  Median :3.209   Median :3.1143   Median :3.204   Median : 1.5075  
##  Mean   :3.212   Mean   :3.1050   Mean   :3.248   Mean   : 1.5220  
##  3rd Qu.:3.688   3rd Qu.:3.6216   3rd Qu.:3.775   3rd Qu.: 1.9815  
##  Max.   :5.804   Max.   :6.1970   Max.   :6.048   Max.   : 3.2403  
##     NegAFF21          NegAFF31          PosAFF12        PosAFF22     
##  Min.   :-0.3991   Min.   :-0.5606   Min.   :1.528   Min.   :0.6575  
##  1st Qu.: 1.0229   1st Qu.: 1.0100   1st Qu.:2.852   1st Qu.:2.6571  
##  Median : 1.3718   Median : 1.4335   Median :3.215   Median :3.1206  
##  Mean   : 1.3971   Mean   : 1.3981   Mean   :3.253   Mean   :3.1256  
##  3rd Qu.: 1.7566   3rd Qu.: 1.8101   3rd Qu.:3.637   3rd Qu.:3.5467  
##  Max.   : 2.9844   Max.   : 2.7674   Max.   :5.413   Max.   :5.4420  
##     PosAFF32         NegAFF12         NegAFF22         NegAFF32       
##  Min.   :0.7369   Min.   :0.1797   Min.   :0.1784   Min.   :-0.03494  
##  1st Qu.:2.8484   1st Qu.:1.1464   1st Qu.:0.9963   1st Qu.: 1.02027  
##  Median :3.2692   Median :1.3818   Median :1.3172   Median : 1.31692  
##  Mean   :3.2737   Mean   :1.4115   Mean   :1.3237   Mean   : 1.30002  
##  3rd Qu.:3.7170   3rd Qu.:1.7251   3rd Qu.:1.6382   3rd Qu.: 1.56441  
##  Max.   :5.9676   Max.   :2.5033   Max.   :2.5587   Max.   : 2.44236  
##     PosAFF13        PosAFF23         PosAFF33        NegAFF13       
##  Min.   :1.307   Min.   :0.8057   Min.   :1.629   Min.   :-0.01837  
##  1st Qu.:2.979   1st Qu.:2.7147   1st Qu.:2.858   1st Qu.: 1.15739  
##  Median :3.299   Median :3.0832   Median :3.325   Median : 1.43937  
##  Mean   :3.302   Mean   :3.0945   Mean   :3.280   Mean   : 1.43015  
##  3rd Qu.:3.683   3rd Qu.:3.5296   3rd Qu.:3.698   3rd Qu.: 1.73650  
##  Max.   :4.712   Max.   :4.8007   Max.   :5.014   Max.   : 2.75085  
##     NegAFF23        NegAFF33     
##  Min.   :0.147   Min.   :0.3145  
##  1st Qu.:1.009   1st Qu.:1.0261  
##  Median :1.294   Median :1.3154  
##  Mean   :1.281   Mean   :1.2974  
##  3rd Qu.:1.560   3rd Qu.:1.5583  
##  Max.   :2.447   Max.   :2.6385&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Remember, we need wide data for SEM models. 
head(long)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   PosAFF11 PosAFF21 PosAFF31  NegAFF11  NegAFF21 NegAFF31 PosAFF12
## 1 3.407196 3.161366 2.691232 2.4207020 2.3531140 2.338767 3.608672
## 2 2.594691 2.682974 2.676173 1.6652403 0.8228416 1.083938 3.103346
## 3 3.168698 2.854090 3.277873 1.5646086 1.6909398 1.502032 2.327999
## 4 4.600160 4.698997 4.751315 0.9352668 0.7944903 0.205178 4.254750
## 5 1.911294 2.184561 1.661057 1.6076395 1.5046016 1.209850 3.971308
## 6 3.472354 4.016982 4.021543 1.9754108 1.1348223 1.729405 3.845904
##   PosAFF22 PosAFF32 NegAFF12  NegAFF22  NegAFF32 PosAFF13 PosAFF23
## 1 2.385396 3.029975 2.283468 2.3173734 2.3932485 3.561401 3.506353
## 2 2.666753 3.251786 1.651941 1.7019345 1.3462416 2.893546 2.804488
## 3 3.061529 3.272173 1.749022 1.6237575 1.5763265 2.705572 2.310391
## 4 3.792440 3.934392 1.279030 0.8741891 1.1822674 3.212826 2.775791
## 5 4.674580 4.093508 1.019751 1.0376248 0.7776716 3.487932 2.673107
## 6 4.186825 4.316398 1.189548 1.1866801 1.3158520 3.236481 3.256453
##   PosAFF33 NegAFF13  NegAFF23  NegAFF33
## 1 3.862975 2.090484 1.3276333 1.6680488
## 2 2.637762 1.257376 1.6585273 1.4298172
## 3 2.841122 1.699849 1.7373582 1.8542867
## 4 3.239004 1.824778 1.3016450 1.5356285
## 5 3.093565 1.316975 0.7120284 0.7083461
## 6 4.023383 1.053895 1.4449087 0.9600753&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Anyone have some comments on naming conventions for this dataset?&lt;/p&gt;
&lt;div id=&#34;longitudinal-cfa&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Longitudinal CFA&lt;/h2&gt;
&lt;p&gt;Starting point in longitudinal analysis. Can be simply thought of as does this construct relate to itself across time? And, to the extent that it does not, is that due to changes in how the construct is measured over time?&lt;/p&gt;
&lt;p&gt;key questions:
1. Should the correlations be the same across time?
2. Should the error variances be correlated?
3. Are the loadings the same across time?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long.cfa &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ PosAFF11 + PosAFF21 + PosAFF31
Pos2 =~ PosAFF12 + PosAFF22 + PosAFF32
Pos3 =~ PosAFF13 + PosAFF23 + PosAFF33
Neg1 =~ NegAFF11 + NegAFF21 + NegAFF31
Neg2 =~ NegAFF12 + NegAFF22 + NegAFF32
Neg3 =~ NegAFF13 + NegAFF23 + NegAFF33

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33

&amp;#39;

fit.long.cfa &amp;lt;- cfa(long.cfa, data=long, std.lv=TRUE)

summary(fit.long.cfa, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 128 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         69
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     119.443
##   Degrees of freedom                               102
##   P-value (Chi-square)                           0.114
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             5253.085
##   Degrees of freedom                               153
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.997
##   Tucker-Lewis Index (TLI)                       0.995
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3060.353
##   Loglikelihood unrestricted model (H1)      -3000.632
## 
##   Number of free parameters                         69
##   Akaike (AIC)                                6258.707
##   Bayesian (BIC)                              6528.365
##   Sample-size adjusted Bayesian (BIC)         6309.453
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.022
##   90 Percent Confidence Interval          0.000  0.036
##   P-value RMSEA &amp;lt;= 0.05                          1.000
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.028
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11          0.654    0.030   21.936    0.000    0.654    0.903
##     PosAFF21          0.651    0.031   20.864    0.000    0.651    0.875
##     PosAFF31          0.685    0.031   22.361    0.000    0.685    0.912
##   Pos2 =~                                                               
##     PosAFF12          0.556    0.026   21.256    0.000    0.556    0.883
##     PosAFF22          0.638    0.030   21.448    0.000    0.638    0.887
##     PosAFF32          0.644    0.027   23.567    0.000    0.644    0.940
##   Pos3 =~                                                               
##     PosAFF13          0.508    0.024   21.028    0.000    0.508    0.887
##     PosAFF23          0.545    0.027   20.347    0.000    0.545    0.867
##     PosAFF33          0.538    0.026   20.827    0.000    0.538    0.879
##   Neg1 =~                                                               
##     NegAFF11          0.563    0.028   20.465    0.000    0.563    0.868
##     NegAFF21          0.479    0.024   19.856    0.000    0.479    0.847
##     NegAFF31          0.555    0.025   22.373    0.000    0.555    0.920
##   Neg2 =~                                                               
##     NegAFF12          0.365    0.019   18.989    0.000    0.365    0.826
##     NegAFF22          0.375    0.017   21.452    0.000    0.375    0.889
##     NegAFF32          0.368    0.017   21.383    0.000    0.368    0.896
##   Neg3 =~                                                               
##     NegAFF13          0.363    0.021   17.128    0.000    0.363    0.782
##     NegAFF23          0.341    0.017   19.493    0.000    0.341    0.855
##     NegAFF33          0.344    0.017   19.700    0.000    0.344    0.869
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.004    0.007    0.578    0.563    0.004    0.043
##    .PosAFF13          0.000    0.007    0.037    0.971    0.000    0.003
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.004    0.006    0.674    0.500    0.004    0.050
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.008    0.008    1.020    0.308    0.008    0.071
##    .PosAFF23          0.008    0.008    0.991    0.322    0.008    0.070
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.011    0.007    1.470    0.142    0.011    0.104
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.004    0.007    0.616    0.538    0.004    0.057
##    .PosAFF33          0.016    0.007    2.182    0.029    0.016    0.177
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.004    0.006    0.690    0.490    0.004    0.061
##  .NegAFF11 ~~                                                           
##    .NegAFF12          0.005    0.005    0.966    0.334    0.005    0.065
##    .NegAFF13          0.006    0.006    1.036    0.300    0.006    0.070
##  .NegAFF12 ~~                                                           
##    .NegAFF13          0.007    0.005    1.528    0.126    0.007    0.099
##  .NegAFF21 ~~                                                           
##    .NegAFF22          0.015    0.004    3.605    0.000    0.015    0.267
##    .NegAFF23          0.011    0.005    2.387    0.017    0.011    0.173
##  .NegAFF22 ~~                                                           
##    .NegAFF23          0.010    0.003    3.145    0.002    0.010    0.253
##  .NegAFF31 ~~                                                           
##    .NegAFF32         -0.006    0.004   -1.607    0.108   -0.006   -0.147
##    .NegAFF33         -0.008    0.004   -1.778    0.075   -0.008   -0.163
##  .NegAFF32 ~~                                                           
##    .NegAFF33         -0.001    0.003   -0.481    0.630   -0.001   -0.041
##   Pos1 ~~                                                               
##     Pos2              0.473    0.044   10.663    0.000    0.473    0.473
##     Pos3              0.399    0.048    8.228    0.000    0.399    0.399
##     Neg1             -0.436    0.047   -9.358    0.000   -0.436   -0.436
##     Neg2             -0.297    0.052   -5.706    0.000   -0.297   -0.297
##     Neg3             -0.169    0.056   -3.003    0.003   -0.169   -0.169
##   Pos2 ~~                                                               
##     Pos3              0.449    0.046    9.777    0.000    0.449    0.449
##     Neg1             -0.179    0.054   -3.279    0.001   -0.179   -0.179
##     Neg2             -0.543    0.041  -13.203    0.000   -0.543   -0.543
##     Neg3             -0.198    0.055   -3.578    0.000   -0.198   -0.198
##   Pos3 ~~                                                               
##     Neg1             -0.074    0.057   -1.304    0.192   -0.074   -0.074
##     Neg2             -0.167    0.056   -2.989    0.003   -0.167   -0.167
##     Neg3             -0.292    0.054   -5.442    0.000   -0.292   -0.292
##   Neg1 ~~                                                               
##     Neg2              0.526    0.043   12.317    0.000    0.526    0.526
##     Neg3              0.351    0.052    6.778    0.000    0.351    0.351
##   Neg2 ~~                                                               
##     Neg3              0.435    0.048    9.006    0.000    0.435    0.435
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .PosAFF11          0.096    0.011    8.497    0.000    0.096    0.184
##    .PosAFF21          0.130    0.013    9.956    0.000    0.130    0.235
##    .PosAFF31          0.095    0.012    7.944    0.000    0.095    0.168
##    .PosAFF12          0.087    0.009   10.044    0.000    0.087    0.220
##    .PosAFF22          0.110    0.011    9.883    0.000    0.110    0.213
##    .PosAFF32          0.055    0.009    6.438    0.000    0.055    0.117
##    .PosAFF13          0.070    0.008    8.319    0.000    0.070    0.214
##    .PosAFF23          0.098    0.011    9.317    0.000    0.098    0.249
##    .PosAFF33          0.085    0.010    8.716    0.000    0.085    0.227
##    .NegAFF11          0.104    0.011    9.546    0.000    0.104    0.246
##    .NegAFF21          0.091    0.009   10.363    0.000    0.091    0.283
##    .NegAFF31          0.056    0.009    6.475    0.000    0.056    0.153
##    .NegAFF12          0.062    0.006   10.835    0.000    0.062    0.317
##    .NegAFF22          0.037    0.004    8.445    0.000    0.037    0.209
##    .NegAFF32          0.033    0.004    7.917    0.000    0.033    0.198
##    .NegAFF13          0.084    0.008   10.660    0.000    0.084    0.389
##    .NegAFF23          0.043    0.005    8.170    0.000    0.043    0.270
##    .NegAFF33          0.038    0.005    7.372    0.000    0.038    0.245
##     Pos1              1.000                               1.000    1.000
##     Pos2              1.000                               1.000    1.000
##     Pos3              1.000                               1.000    1.000
##     Neg1              1.000                               1.000    1.000
##     Neg2              1.000                               1.000    1.000
##     Neg3              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(semPlot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 methods overwritten by &amp;#39;huge&amp;#39;:
##   method    from   
##   plot.sim  BDgraph
##   print.sim BDgraph&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.long.cfa)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-24-week-9_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-invariance-mi&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measurement Invariance (MI)&lt;/h2&gt;
&lt;p&gt;To meaningfully look at means, we need to have the means mean the same thing. In other words, without using the word mean, we need to make sure that the measurement of the construct is consistent across time. If it isn’t, then what we may see as change actually reflect people responding to the indicators differently. For example, a common item on an extraversion scale is “Do you like to go to parties?” This is likely interpreted differently by a 20 year old compared to a 70 year old. This is due to what is normative, what parties look like that a typical 20 and 70 year old go to, etcetera. Another way to look at this is the item “2 x 3 = X, solve for X”. The reasons that a 8 year old and a 18 year old get the item incorrect is likely for different reasons (ie knowledge vs not being careful).&lt;/p&gt;
&lt;p&gt;Maturation is the easiest way to see differences, but it also happens when you want to compare groups ie some anova design. This assumption is typically never critically examined.&lt;/p&gt;
&lt;div id=&#34;types-of-mi&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;types of MI&lt;/h3&gt;
&lt;p&gt;Configural (pattern). Typically always true with a decent measure of your construct. Can be tested through test statistics and eye-balling. Serves as default.&lt;/p&gt;
&lt;p&gt;Weak (metric/loading). Can be easily met. Not meeting this shows big problems, unless you are working with a really large dataset (where there is large power to find differences).&lt;/p&gt;
&lt;p&gt;Strong (Scalar/intercept). Need to meet this designation to run longitudinal models and look at means across time.&lt;/p&gt;
&lt;p&gt;Strict (residual/error variance). Not necessarily better than Strong, and does not need to be satisfied to use longitudinal models. Why might this not hold even if you are assessing the same construct? Hint: think of what residual variance is made up of.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-mi&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing MI&lt;/h3&gt;
&lt;div id=&#34;configural-baseline&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;configural (baseline)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;config &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ PosAFF11 + PosAFF21 + PosAFF31
Pos2 =~ PosAFF12 + PosAFF22 + PosAFF32
Pos3 =~ PosAFF13 + PosAFF23 + PosAFF33


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33


&amp;#39;

config &amp;lt;- cfa(config, data=long, meanstructure=TRUE, std.lv=TRUE)

summary(config, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 76 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         39
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       9.266
##   Degrees of freedom                                15
##   P-value (Chi-square)                           0.863
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             2688.088
##   Degrees of freedom                                36
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    1.000
##   Tucker-Lewis Index (TLI)                       1.005
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -2040.957
##   Loglikelihood unrestricted model (H1)      -2036.324
## 
##   Number of free parameters                         39
##   Akaike (AIC)                                4159.914
##   Bayesian (BIC)                              4312.329
##   Sample-size adjusted Bayesian (BIC)         4188.596
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.000
##   90 Percent Confidence Interval          0.000  0.026
##   P-value RMSEA &amp;lt;= 0.05                          0.997
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.011
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11          0.656    0.030   22.030    0.000    0.656    0.906
##     PosAFF21          0.652    0.031   20.908    0.000    0.652    0.877
##     PosAFF31          0.682    0.031   22.157    0.000    0.682    0.908
##   Pos2 =~                                                               
##     PosAFF12          0.555    0.026   21.138    0.000    0.555    0.881
##     PosAFF22          0.643    0.030   21.651    0.000    0.643    0.894
##     PosAFF32          0.642    0.027   23.348    0.000    0.642    0.936
##   Pos3 =~                                                               
##     PosAFF13          0.509    0.024   21.036    0.000    0.509    0.888
##     PosAFF23          0.545    0.027   20.347    0.000    0.545    0.867
##     PosAFF33          0.537    0.026   20.763    0.000    0.537    0.878
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.002    0.007    0.325    0.745    0.002    0.024
##    .PosAFF13          0.001    0.007    0.207    0.836    0.001    0.017
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.003    0.006    0.517    0.605    0.003    0.039
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.006    0.008    0.784    0.433    0.006    0.056
##    .PosAFF23          0.006    0.008    0.773    0.439    0.006    0.055
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.011    0.007    1.525    0.127    0.011    0.111
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.008    0.007    1.097    0.273    0.008    0.100
##    .PosAFF33          0.016    0.007    2.215    0.027    0.016    0.177
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.005    0.006    0.773    0.440    0.005    0.068
##   Pos1 ~~                                                               
##     Pos2              0.471    0.044   10.609    0.000    0.471    0.471
##     Pos3              0.399    0.048    8.226    0.000    0.399    0.399
##   Pos2 ~~                                                               
##     Pos3              0.450    0.046    9.806    0.000    0.450    0.450
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .PosAFF11          3.212    0.038   85.122    0.000    3.212    4.437
##    .PosAFF21          3.105    0.039   80.035    0.000    3.105    4.172
##    .PosAFF31          3.248    0.039   82.962    0.000    3.248    4.325
##    .PosAFF12          3.253    0.033   99.116    0.000    3.253    5.167
##    .PosAFF22          3.126    0.037   83.356    0.000    3.126    4.345
##    .PosAFF32          3.274    0.036   91.602    0.000    3.274    4.775
##    .PosAFF13          3.302    0.030  110.504    0.000    3.302    5.760
##    .PosAFF23          3.094    0.033   94.403    0.000    3.094    4.921
##    .PosAFF33          3.280    0.032  102.942    0.000    3.280    5.366
##     Pos1              0.000                               0.000    0.000
##     Pos2              0.000                               0.000    0.000
##     Pos3              0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .PosAFF11          0.093    0.011    8.155    0.000    0.093    0.178
##    .PosAFF21          0.128    0.013    9.770    0.000    0.128    0.231
##    .PosAFF31          0.099    0.012    8.092    0.000    0.099    0.176
##    .PosAFF12          0.089    0.009    9.942    0.000    0.089    0.224
##    .PosAFF22          0.104    0.011    9.363    0.000    0.104    0.202
##    .PosAFF32          0.058    0.009    6.467    0.000    0.058    0.124
##    .PosAFF13          0.070    0.008    8.211    0.000    0.070    0.212
##    .PosAFF23          0.098    0.011    9.234    0.000    0.098    0.248
##    .PosAFF33          0.086    0.010    8.719    0.000    0.086    0.229
##     Pos1              1.000                               1.000    1.000
##     Pos2              1.000                               1.000    1.000
##     Pos3              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;weak-constrain-loadings&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Weak (constrain loadings)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;weak &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33


## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33


&amp;#39;

weak &amp;lt;- cfa(weak, data=long, meanstructure=TRUE, std.lv=TRUE)

summary(weak, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 81 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         41
##   Number of equality constraints                     6
##   Row rank of the constraints matrix                 6
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      17.684
##   Degrees of freedom                                19
##   P-value (Chi-square)                           0.544
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             2688.088
##   Degrees of freedom                                36
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    1.000
##   Tucker-Lewis Index (TLI)                       1.001
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -2045.166
##   Loglikelihood unrestricted model (H1)      -2036.324
## 
##   Number of free parameters                         35
##   Akaike (AIC)                                4160.332
##   Bayesian (BIC)                              4297.114
##   Sample-size adjusted Bayesian (BIC)         4186.072
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.000
##   90 Percent Confidence Interval          0.000  0.042
##   P-value RMSEA &amp;lt;= 0.05                          0.983
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.021
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11  (L1)    0.631    0.027   23.651    0.000    0.631    0.894
##     PosAFF21  (L2)    0.677    0.029   23.460    0.000    0.677    0.887
##     PosAFF31  (L3)    0.686    0.029   23.837    0.000    0.686    0.910
##   Pos2 =~                                                               
##     PosAFF12  (L1)    0.631    0.027   23.651    0.000    0.582    0.894
##     PosAFF22  (L2)    0.677    0.029   23.460    0.000    0.625    0.886
##     PosAFF32  (L3)    0.686    0.029   23.837    0.000    0.633    0.931
##   Pos3 =~                                                               
##     PosAFF13  (L1)    0.631    0.027   23.651    0.000    0.503    0.883
##     PosAFF23  (L2)    0.677    0.029   23.460    0.000    0.540    0.864
##     PosAFF33  (L3)    0.686    0.029   23.837    0.000    0.547    0.885
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.003    0.007    0.395    0.693    0.003    0.029
##    .PosAFF13          0.002    0.007    0.231    0.817    0.002    0.018
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.002    0.006    0.404    0.686    0.002    0.031
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.006    0.008    0.676    0.499    0.006    0.049
##    .PosAFF23          0.005    0.008    0.646    0.518    0.005    0.047
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.012    0.007    1.598    0.110    0.012    0.114
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.008    0.007    1.185    0.236    0.008    0.107
##    .PosAFF33          0.016    0.007    2.214    0.027    0.016    0.182
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.005    0.006    0.766    0.444    0.005    0.067
##   Pos1 ~~                                                               
##     Pos2              0.437    0.046    9.404    0.000    0.474    0.474
##     Pos3              0.319    0.042    7.578    0.000    0.400    0.400
##   Pos2 ~~                                                               
##     Pos3              0.332    0.047    7.030    0.000    0.450    0.450
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .PosAFF11          3.212    0.037   87.342    0.000    3.212    4.553
##    .PosAFF21          3.105    0.040   78.061    0.000    3.105    4.069
##    .PosAFF31          3.248    0.039   82.691    0.000    3.248    4.311
##    .PosAFF12          3.253    0.034   95.798    0.000    3.253    4.994
##    .PosAFF22          3.126    0.037   85.004    0.000    3.126    4.431
##    .PosAFF32          3.274    0.035   92.400    0.000    3.274    4.817
##    .PosAFF13          3.302    0.030  111.238    0.000    3.302    5.799
##    .PosAFF23          3.094    0.033   94.971    0.000    3.094    4.951
##    .PosAFF33          3.280    0.032  101.745    0.000    3.280    5.304
##     Pos1              0.000                               0.000    0.000
##     Pos2              0.000                               0.000    0.000
##     Pos3              0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     Pos2              0.853    0.086    9.942    0.000    1.000    1.000
##     Pos3              0.636    0.067    9.479    0.000    1.000    1.000
##    .PosAFF11          0.100    0.011    9.159    0.000    0.100    0.201
##    .PosAFF21          0.124    0.013    9.501    0.000    0.124    0.213
##    .PosAFF31          0.098    0.012    8.261    0.000    0.098    0.172
##    .PosAFF12          0.085    0.009    9.548    0.000    0.085    0.201
##    .PosAFF22          0.107    0.011    9.929    0.000    0.107    0.215
##    .PosAFF32          0.061    0.009    7.123    0.000    0.061    0.132
##    .PosAFF13          0.071    0.008    8.812    0.000    0.071    0.219
##    .PosAFF23          0.099    0.010    9.680    0.000    0.099    0.254
##    .PosAFF33          0.083    0.009    8.776    0.000    0.083    0.218
##     Pos1              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;strong-constrain-loadings-and-intercepts&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Strong (constrain loadings and intercepts)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;strong &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33


## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33


## constrain intercepts across time
PosAFF11 ~ t1*1
PosAFF21 ~ t2*1
PosAFF31 ~ t3*1


PosAFF12 ~ t1*1
PosAFF22 ~ t2*1
PosAFF32 ~ t3*1


PosAFF13 ~ t1*1
PosAFF23 ~ t2*1
PosAFF33 ~ t3*1


## free latent means at later times (only set the scale once)
Pos2 ~ NA*1
Pos3 ~ NA*1&amp;#39;

strong &amp;lt;- cfa(strong, data=long, meanstructure=TRUE, std.lv=TRUE)

summary(strong, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 85 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         43
##   Number of equality constraints                    12
##   Row rank of the constraints matrix                12
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      30.144
##   Degrees of freedom                                23
##   P-value (Chi-square)                           0.145
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             2688.088
##   Degrees of freedom                                36
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.997
##   Tucker-Lewis Index (TLI)                       0.996
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -2051.396
##   Loglikelihood unrestricted model (H1)      -2036.324
## 
##   Number of free parameters                         31
##   Akaike (AIC)                                4164.792
##   Bayesian (BIC)                              4285.943
##   Sample-size adjusted Bayesian (BIC)         4187.591
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.029
##   90 Percent Confidence Interval          0.000  0.055
##   P-value RMSEA &amp;lt;= 0.05                          0.901
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.024
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11  (L1)    0.631    0.027   23.635    0.000    0.631    0.893
##     PosAFF21  (L2)    0.676    0.029   23.441    0.000    0.676    0.886
##     PosAFF31  (L3)    0.686    0.029   23.838    0.000    0.686    0.910
##   Pos2 =~                                                               
##     PosAFF12  (L1)    0.631    0.027   23.635    0.000    0.582    0.894
##     PosAFF22  (L2)    0.676    0.029   23.441    0.000    0.624    0.886
##     PosAFF32  (L3)    0.686    0.029   23.838    0.000    0.633    0.932
##   Pos3 =~                                                               
##     PosAFF13  (L1)    0.631    0.027   23.635    0.000    0.503    0.882
##     PosAFF23  (L2)    0.676    0.029   23.441    0.000    0.539    0.862
##     PosAFF33  (L3)    0.686    0.029   23.838    0.000    0.547    0.885
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.003    0.007    0.471    0.638    0.003    0.035
##    .PosAFF13          0.000    0.007    0.030    0.976    0.000    0.002
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.002    0.006    0.299    0.765    0.002    0.023
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.006    0.008    0.718    0.473    0.006    0.051
##    .PosAFF23          0.004    0.008    0.521    0.602    0.004    0.038
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.011    0.007    1.530    0.126    0.011    0.109
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.008    0.007    1.160    0.246    0.008    0.105
##    .PosAFF33          0.017    0.007    2.261    0.024    0.017    0.187
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.005    0.006    0.795    0.427    0.005    0.070
##   Pos1 ~~                                                               
##     Pos2              0.438    0.047    9.409    0.000    0.474    0.474
##     Pos3              0.320    0.042    7.590    0.000    0.401    0.401
##   Pos2 ~~                                                               
##     Pos3              0.332    0.047    7.027    0.000    0.450    0.450
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .PosAFF11  (t1)    3.237    0.035   92.743    0.000    3.237    4.583
##    .PosAFF21  (t2)    3.084    0.038   81.945    0.000    3.084    4.043
##    .PosAFF31  (t3)    3.244    0.038   85.408    0.000    3.244    4.304
##    .PosAFF12  (t1)    3.237    0.035   92.743    0.000    3.237    4.967
##    .PosAFF22  (t2)    3.084    0.038   81.945    0.000    3.084    4.375
##    .PosAFF32  (t3)    3.244    0.038   85.408    0.000    3.244    4.771
##    .PosAFF13  (t1)    3.237    0.035   92.743    0.000    3.237    5.674
##    .PosAFF23  (t2)    3.084    0.038   81.945    0.000    3.084    4.931
##    .PosAFF33  (t3)    3.244    0.038   85.408    0.000    3.244    5.249
##     Pos2              0.044    0.055    0.798    0.425    0.047    0.047
##     Pos3              0.058    0.055    1.043    0.297    0.072    0.072
##     Pos1              0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     Pos2              0.853    0.086    9.942    0.000    1.000    1.000
##     Pos3              0.636    0.067    9.475    0.000    1.000    1.000
##    .PosAFF11          0.101    0.011    9.188    0.000    0.101    0.203
##    .PosAFF21          0.125    0.013    9.527    0.000    0.125    0.215
##    .PosAFF31          0.097    0.012    8.216    0.000    0.097    0.172
##    .PosAFF12          0.086    0.009    9.551    0.000    0.086    0.201
##    .PosAFF22          0.107    0.011    9.947    0.000    0.107    0.216
##    .PosAFF32          0.061    0.009    7.094    0.000    0.061    0.132
##    .PosAFF13          0.073    0.008    8.865    0.000    0.073    0.223
##    .PosAFF23          0.101    0.010    9.725    0.000    0.101    0.257
##    .PosAFF33          0.083    0.009    8.706    0.000    0.083    0.216
##     Pos1              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;strict-loadings-intercept-residual-variances&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Strict (loadings, intercept, residual variances)&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;strict &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33


## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

## equality of residuals 
PosAFF11 ~~ r*PosAFF11 
PosAFF12 ~~ r*PosAFF12
PosAFF13 ~~ r*PosAFF13

PosAFF21 ~~ r*PosAFF21 
PosAFF22 ~~ r*PosAFF22
PosAFF23 ~~ r*PosAFF23

PosAFF31 ~~ r*PosAFF31 
PosAFF32 ~~ r*PosAFF32
PosAFF33 ~~ r*PosAFF33


## constrain intercepts across time
PosAFF11 ~ t1*1
PosAFF21 ~ t2*1
PosAFF31 ~ t3*1


PosAFF12 ~ t1*1
PosAFF22 ~ t2*1
PosAFF32 ~ t3*1


PosAFF13 ~ t1*1
PosAFF23 ~ t2*1
PosAFF33 ~ t3*1


## free latent means at later times (only set the scale once)
Pos2 ~ NA*1
Pos3 ~ NA*1&amp;#39;

strict &amp;lt;- cfa(strict, data=long, meanstructure=TRUE, std.lv=TRUE)

summary(strict, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 82 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         43
##   Number of equality constraints                    20
##   Row rank of the constraints matrix                20
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      60.491
##   Degrees of freedom                                31
##   P-value (Chi-square)                           0.001
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             2688.088
##   Degrees of freedom                                36
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.989
##   Tucker-Lewis Index (TLI)                       0.987
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -2066.569
##   Loglikelihood unrestricted model (H1)      -2036.324
## 
##   Number of free parameters                         23
##   Akaike (AIC)                                4179.139
##   Bayesian (BIC)                              4269.025
##   Sample-size adjusted Bayesian (BIC)         4196.054
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.051
##   90 Percent Confidence Interval          0.031  0.070
##   P-value RMSEA &amp;lt;= 0.05                          0.445
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.026
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11  (L1)    0.631    0.026   23.891    0.000    0.631    0.901
##     PosAFF21  (L2)    0.689    0.028   24.210    0.000    0.689    0.915
##     PosAFF31  (L3)    0.680    0.028   24.061    0.000    0.680    0.913
##   Pos2 =~                                                               
##     PosAFF12  (L1)    0.631    0.026   23.891    0.000    0.579    0.885
##     PosAFF22  (L2)    0.689    0.028   24.210    0.000    0.632    0.901
##     PosAFF32  (L3)    0.680    0.028   24.061    0.000    0.623    0.899
##   Pos3 =~                                                               
##     PosAFF13  (L1)    0.631    0.026   23.891    0.000    0.500    0.854
##     PosAFF23  (L2)    0.689    0.028   24.210    0.000    0.545    0.873
##     PosAFF33  (L3)    0.680    0.028   24.061    0.000    0.538    0.871
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.004    0.007    0.617    0.537    0.004    0.045
##    .PosAFF13          0.002    0.007    0.280    0.780    0.002    0.020
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.003    0.007    0.394    0.693    0.003    0.029
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.003    0.007    0.346    0.730    0.003    0.027
##    .PosAFF23          0.002    0.007    0.228    0.820    0.002    0.018
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.009    0.007    1.259    0.208    0.009    0.098
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.011    0.007    1.508    0.132    0.011    0.117
##    .PosAFF33          0.016    0.007    2.310    0.021    0.016    0.177
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.008    0.007    1.186    0.236    0.008    0.091
##   Pos1 ~~                                                               
##     Pos2              0.433    0.046    9.372    0.000    0.472    0.472
##     Pos3              0.318    0.042    7.613    0.000    0.402    0.402
##   Pos2 ~~                                                               
##     Pos3              0.330    0.047    7.031    0.000    0.454    0.454
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .PosAFF11  (t1)    3.236    0.035   92.686    0.000    3.236    4.617
##    .PosAFF21  (t2)    3.087    0.038   81.529    0.000    3.087    4.100
##    .PosAFF31  (t3)    3.245    0.038   86.233    0.000    3.245    4.359
##    .PosAFF12  (t1)    3.236    0.035   92.686    0.000    3.236    4.947
##    .PosAFF22  (t2)    3.087    0.038   81.529    0.000    3.087    4.402
##    .PosAFF32  (t3)    3.245    0.038   86.233    0.000    3.245    4.679
##    .PosAFF13  (t1)    3.236    0.035   92.686    0.000    3.236    5.530
##    .PosAFF23  (t2)    3.087    0.038   81.529    0.000    3.087    4.944
##    .PosAFF33  (t3)    3.245    0.038   86.233    0.000    3.245    5.251
##     Pos2              0.044    0.055    0.799    0.424    0.048    0.048
##     Pos3              0.052    0.055    0.945    0.345    0.066    0.066
##     Pos1              0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     Pos2              0.841    0.085    9.950    0.000    1.000    1.000
##     Pos3              0.627    0.066    9.494    0.000    1.000    1.000
##    .PosAFF11   (r)    0.093    0.003   33.038    0.000    0.093    0.188
##    .PosAFF12   (r)    0.093    0.003   33.038    0.000    0.093    0.216
##    .PosAFF13   (r)    0.093    0.003   33.038    0.000    0.093    0.270
##    .PosAFF21   (r)    0.093    0.003   33.038    0.000    0.093    0.163
##    .PosAFF22   (r)    0.093    0.003   33.038    0.000    0.093    0.188
##    .PosAFF23   (r)    0.093    0.003   33.038    0.000    0.093    0.237
##    .PosAFF31   (r)    0.093    0.003   33.038    0.000    0.093    0.167
##    .PosAFF32   (r)    0.093    0.003   33.038    0.000    0.093    0.192
##    .PosAFF33   (r)    0.093    0.003   33.038    0.000    0.093    0.242
##     Pos1              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that there are other types of MI that we could investigate, depending on what we are interested in. We could look at equality of latent means and variances, as well as regressions, if they were in the model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-the-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing the models&lt;/h3&gt;
&lt;p&gt;Usually done through chi-square difference test. But this is a very sensitive test, especially with larger samples. Better to look at changes in CFI. If delta is .01 or greater than maybe it shows misfit.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##Compare configural and weak model
anova(config, weak)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Chi Square Difference Test
## 
##        Df    AIC    BIC   Chisq Chisq diff Df diff Pr(&amp;gt;Chisq)  
## config 15 4159.9 4312.3  9.2658                                
## weak   19 4160.3 4297.1 17.6836     8.4179       4    0.07742 .
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;##Compare weak and strong model
anova(weak, strong)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Chi Square Difference Test
## 
##        Df    AIC    BIC  Chisq Chisq diff Df diff Pr(&amp;gt;Chisq)  
## weak   19 4160.3 4297.1 17.684                                
## strong 23 4164.8 4285.9 30.144     12.461       4    0.01424 *
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitmeasures(weak)[&amp;#39;cfi&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## cfi 
##   1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitmeasures(strong)[&amp;#39;cfi&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       cfi 
## 0.9973062&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fitmeasures(strict)[&amp;#39;cfi&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       cfi 
## 0.9888801&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;panelcross-laglongitudinal-path-modelmediation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Panel/cross lag/longitudinal path model/mediation&lt;/h2&gt;
&lt;p&gt;Instead of covariances, the key component of this is to examine structural relationships between constructs over time. This can extend to cross-construct associations such as mediational models.&lt;/p&gt;
&lt;p&gt;key concerns:
1. Should the regressions be the same across time?
2. Should the error variances be correlated?
3. Are the loadings the same across time?&lt;/p&gt;
&lt;p&gt;Falling under this designation is the cross-lagged panel model (clpm) – the type of two wave model that josh went off on last time. The concerns of clpms are assuaged when you have more than two assessment points, you measure your variables latently, and/or you account for mean structure.&lt;/p&gt;
&lt;p&gt;Below is an example of two constructs modeled over time with three assessment points. Not yet a clpm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long.path &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33
Neg1 =~ L4*NegAFF11 + L5*NegAFF21 + L6*NegAFF31
Neg2 =~ L4*NegAFF12 + L5*NegAFF22 + L6*NegAFF32
Neg3 =~ L4*NegAFF13 + L5*NegAFF23 + L6*NegAFF33

## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3
Neg2 ~~ NA*Neg2
Neg3 ~~ NA*Neg3
Pos1 ~~ Neg1
Pos2 ~~ Neg2
Pos3 ~~ Neg3

## directional regression paths
Pos2 ~ Pos1
Pos3 ~ Pos2
Neg2 ~ Neg1
Neg3 ~ Neg2

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33
&amp;#39;

fit.long.path &amp;lt;- sem(long.path, data=long, std.lv=TRUE)

summary(fit.long.path, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 133 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         65
##   Number of equality constraints                    12
##   Row rank of the constraints matrix                12
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     170.843
##   Degrees of freedom                               118
##   P-value (Chi-square)                           0.001
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             5253.085
##   Degrees of freedom                               153
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.990
##   Tucker-Lewis Index (TLI)                       0.987
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3086.053
##   Loglikelihood unrestricted model (H1)      -3000.632
## 
##   Number of free parameters                         53
##   Akaike (AIC)                                6278.107
##   Bayesian (BIC)                              6485.235
##   Sample-size adjusted Bayesian (BIC)         6317.086
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.035
##   90 Percent Confidence Interval          0.023  0.046
##   P-value RMSEA &amp;lt;= 0.05                          0.989
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.055
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11  (L1)    0.630    0.027   23.609    0.000    0.630    0.892
##     PosAFF21  (L2)    0.673    0.029   23.387    0.000    0.673    0.884
##     PosAFF31  (L3)    0.686    0.029   23.966    0.000    0.686    0.913
##   Pos2 =~                                                               
##     PosAFF12  (L1)    0.630    0.027   23.609    0.000    0.575    0.893
##     PosAFF22  (L2)    0.673    0.029   23.387    0.000    0.614    0.878
##     PosAFF32  (L3)    0.686    0.029   23.966    0.000    0.626    0.932
##   Pos3 =~                                                               
##     PosAFF13  (L1)    0.630    0.027   23.609    0.000    0.504    0.884
##     PosAFF23  (L2)    0.673    0.029   23.387    0.000    0.539    0.861
##     PosAFF33  (L3)    0.686    0.029   23.966    0.000    0.549    0.887
##   Neg1 =~                                                               
##     NegAFF11  (L4)    0.546    0.024   22.398    0.000    0.546    0.859
##     NegAFF21  (L5)    0.510    0.023   22.505    0.000    0.510    0.868
##     NegAFF31  (L6)    0.537    0.023   23.717    0.000    0.537    0.908
##   Neg2 =~                                                               
##     NegAFF12  (L4)    0.546    0.024   22.398    0.000    0.384    0.841
##     NegAFF22  (L5)    0.510    0.023   22.505    0.000    0.358    0.871
##     NegAFF32  (L6)    0.537    0.023   23.717    0.000    0.377    0.904
##   Neg3 =~                                                               
##     NegAFF13  (L4)    0.546    0.024   22.398    0.000    0.362    0.780
##     NegAFF23  (L5)    0.510    0.023   22.505    0.000    0.338    0.847
##     NegAFF33  (L6)    0.537    0.023   23.717    0.000    0.356    0.883
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos2 ~                                                                
##     Pos1              0.416    0.042   10.020    0.000    0.456    0.456
##   Pos3 ~                                                                
##     Pos2              0.404    0.044    9.207    0.000    0.460    0.460
##   Neg2 ~                                                                
##     Neg1              0.382    0.031   12.203    0.000    0.544    0.544
##   Neg3 ~                                                                
##     Neg2              0.432    0.049    8.867    0.000    0.457    0.457
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 ~~                                                               
##     Neg1             -0.441    0.046   -9.502    0.000   -0.441   -0.441
##  .Pos2 ~~                                                               
##    .Neg2             -0.269    0.036   -7.544    0.000   -0.561   -0.561
##  .Pos3 ~~                                                               
##    .Neg3             -0.125    0.027   -4.596    0.000   -0.298   -0.298
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.004    0.007    0.536    0.592    0.004    0.039
##    .PosAFF13          0.003    0.007    0.386    0.699    0.003    0.030
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.003    0.006    0.585    0.559    0.003    0.044
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.007    0.008    0.865    0.387    0.007    0.061
##    .PosAFF23          0.008    0.008    1.030    0.303    0.008    0.074
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.011    0.007    1.516    0.130    0.011    0.106
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.005    0.007    0.705    0.481    0.005    0.064
##    .PosAFF33          0.016    0.007    2.173    0.030    0.016    0.180
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.004    0.006    0.580    0.562    0.004    0.051
##  .NegAFF11 ~~                                                           
##    .NegAFF12          0.005    0.005    0.947    0.344    0.005    0.064
##    .NegAFF13          0.007    0.006    1.107    0.268    0.007    0.073
##  .NegAFF12 ~~                                                           
##    .NegAFF13          0.007    0.005    1.539    0.124    0.007    0.100
##  .NegAFF21 ~~                                                           
##    .NegAFF22          0.015    0.004    3.399    0.001    0.015    0.249
##    .NegAFF23          0.010    0.005    2.217    0.027    0.010    0.163
##  .NegAFF22 ~~                                                           
##    .NegAFF23          0.011    0.003    3.430    0.001    0.011    0.259
##  .NegAFF31 ~~                                                           
##    .NegAFF32         -0.007    0.004   -1.724    0.085   -0.007   -0.155
##    .NegAFF33         -0.007    0.004   -1.587    0.113   -0.007   -0.143
##  .NegAFF32 ~~                                                           
##    .NegAFF33         -0.002    0.003   -0.734    0.463   -0.002   -0.066
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .Pos2              0.660    0.075    8.760    0.000    0.792    0.792
##    .Pos3              0.504    0.058    8.628    0.000    0.788    0.788
##    .Neg2              0.347    0.041    8.458    0.000    0.704    0.704
##    .Neg3              0.347    0.041    8.409    0.000    0.791    0.791
##    .PosAFF11          0.102    0.011    9.329    0.000    0.102    0.204
##    .PosAFF21          0.126    0.013    9.699    0.000    0.126    0.218
##    .PosAFF31          0.094    0.012    8.132    0.000    0.094    0.166
##    .PosAFF12          0.084    0.009    9.650    0.000    0.084    0.202
##    .PosAFF22          0.112    0.011   10.307    0.000    0.112    0.229
##    .PosAFF32          0.059    0.008    7.215    0.000    0.059    0.131
##    .PosAFF13          0.071    0.008    8.813    0.000    0.071    0.218
##    .PosAFF23          0.101    0.010    9.833    0.000    0.101    0.259
##    .PosAFF33          0.082    0.009    8.703    0.000    0.082    0.214
##    .NegAFF11          0.106    0.010   10.098    0.000    0.106    0.262
##    .NegAFF21          0.085    0.009    9.768    0.000    0.085    0.247
##    .NegAFF31          0.062    0.008    7.633    0.000    0.062    0.176
##    .NegAFF12          0.061    0.006   10.625    0.000    0.061    0.292
##    .NegAFF22          0.041    0.004    9.619    0.000    0.041    0.242
##    .NegAFF32          0.032    0.004    7.781    0.000    0.032    0.182
##    .NegAFF13          0.084    0.008   11.031    0.000    0.084    0.392
##    .NegAFF23          0.045    0.005    9.102    0.000    0.045    0.282
##    .NegAFF33          0.036    0.005    7.466    0.000    0.036    0.221
##     Pos1              1.000                               1.000    1.000
##     Neg1              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.long.path, layout = &amp;quot;tree3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-24-week-9_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## layout can also be done manually to get publications worthy plots&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;longitudinal-cross-lagged-panel-model-clpm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Longitudinal Cross lagged panel model (clpm)&lt;/h3&gt;
&lt;p&gt;key concerns:
1. Should the regressions (both cross lagged and autoregressive) be the same across time?
2. Should the indicator error variances be correlated (within time or within construct)?
3. Are the loadings the same across time? (more on this later)
4. Are the latent error variances the same or different?
5. Are the latent error variances correlated the same or different across time?
6. Are there more lagged effects?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long.cross &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33
Neg1 =~ L4*NegAFF11 + L5*NegAFF21 + L6*NegAFF31
Neg2 =~ L4*NegAFF12 + L5*NegAFF22 + L6*NegAFF32
Neg3 =~ L4*NegAFF13 + L5*NegAFF23 + L6*NegAFF33

## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3
Neg2 ~~ NA*Neg2
Neg3 ~~ NA*Neg3

Pos1 ~~ Neg1
Pos2 ~~ Neg2
Pos3 ~~ Neg3

## directional regression paths
Pos2 ~ Pos1 + Neg1
Neg2 ~ Pos1 + Neg1
Pos3 ~ Pos2 + Neg2
Neg3 ~ Pos2 + Neg2

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33
&amp;#39;

fit.long.cross &amp;lt;- sem(long.cross,data=long, std.lv=TRUE)

summary(fit.long.cross, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 137 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         69
##   Number of equality constraints                    12
##   Row rank of the constraints matrix                12
## 
##   Number of observations                           368
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     163.406
##   Degrees of freedom                               114
##   P-value (Chi-square)                           0.002
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             5253.085
##   Degrees of freedom                               153
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.990
##   Tucker-Lewis Index (TLI)                       0.987
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3082.335
##   Loglikelihood unrestricted model (H1)      -3000.632
## 
##   Number of free parameters                         57
##   Akaike (AIC)                                6278.669
##   Bayesian (BIC)                              6501.430
##   Sample-size adjusted Bayesian (BIC)         6320.590
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.034
##   90 Percent Confidence Interval          0.022  0.046
##   P-value RMSEA &amp;lt;= 0.05                          0.990
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.051
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 =~                                                               
##     PosAFF11  (L1)    0.630    0.027   23.619    0.000    0.630    0.892
##     PosAFF21  (L2)    0.673    0.029   23.393    0.000    0.673    0.884
##     PosAFF31  (L3)    0.686    0.029   23.990    0.000    0.686    0.914
##   Pos2 =~                                                               
##     PosAFF12  (L1)    0.630    0.027   23.619    0.000    0.582    0.896
##     PosAFF22  (L2)    0.673    0.029   23.393    0.000    0.622    0.880
##     PosAFF32  (L3)    0.686    0.029   23.990    0.000    0.634    0.933
##   Pos3 =~                                                               
##     PosAFF13  (L1)    0.630    0.027   23.619    0.000    0.503    0.884
##     PosAFF23  (L2)    0.673    0.029   23.393    0.000    0.537    0.861
##     PosAFF33  (L3)    0.686    0.029   23.990    0.000    0.547    0.886
##   Neg1 =~                                                               
##     NegAFF11  (L4)    0.547    0.024   22.394    0.000    0.547    0.860
##     NegAFF21  (L5)    0.510    0.023   22.488    0.000    0.510    0.868
##     NegAFF31  (L6)    0.538    0.023   23.710    0.000    0.538    0.908
##   Neg2 =~                                                               
##     NegAFF12  (L4)    0.547    0.024   22.394    0.000    0.382    0.840
##     NegAFF22  (L5)    0.510    0.023   22.488    0.000    0.356    0.869
##     NegAFF32  (L6)    0.538    0.023   23.710    0.000    0.375    0.903
##   Neg3 =~                                                               
##     NegAFF13  (L4)    0.547    0.024   22.394    0.000    0.358    0.777
##     NegAFF23  (L5)    0.510    0.023   22.488    0.000    0.334    0.844
##     NegAFF33  (L6)    0.538    0.023   23.710    0.000    0.352    0.881
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos2 ~                                                                
##     Pos1              0.463    0.052    8.829    0.000    0.501    0.501
##     Neg1              0.039    0.053    0.746    0.456    0.042    0.042
##   Neg2 ~                                                                
##     Pos1             -0.057    0.039   -1.454    0.146   -0.081   -0.081
##     Neg1              0.347    0.039    8.812    0.000    0.498    0.498
##   Pos3 ~                                                                
##     Pos2              0.451    0.054    8.307    0.000    0.522    0.522
##     Neg2              0.134    0.073    1.843    0.065    0.117    0.117
##   Neg3 ~                                                                
##     Pos2              0.046    0.046    1.000    0.317    0.065    0.065
##     Neg2              0.446    0.062    7.239    0.000    0.475    0.475
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Pos1 ~~                                                               
##     Neg1             -0.437    0.047   -9.375    0.000   -0.437   -0.437
##  .Pos2 ~~                                                               
##    .Neg2             -0.269    0.036   -7.567    0.000   -0.566   -0.566
##  .Pos3 ~~                                                               
##    .Neg3             -0.127    0.027   -4.710    0.000   -0.308   -0.308
##  .PosAFF11 ~~                                                           
##    .PosAFF12          0.004    0.007    0.529    0.597    0.004    0.039
##    .PosAFF13          0.002    0.007    0.371    0.711    0.002    0.028
##  .PosAFF12 ~~                                                           
##    .PosAFF13          0.003    0.006    0.569    0.569    0.003    0.043
##  .PosAFF21 ~~                                                           
##    .PosAFF22          0.007    0.008    0.869    0.385    0.007    0.061
##    .PosAFF23          0.008    0.008    0.964    0.335    0.008    0.069
##  .PosAFF22 ~~                                                           
##    .PosAFF23          0.011    0.007    1.416    0.157    0.011    0.099
##  .PosAFF31 ~~                                                           
##    .PosAFF32          0.004    0.007    0.649    0.516    0.004    0.059
##    .PosAFF33          0.016    0.007    2.257    0.024    0.016    0.187
##  .PosAFF32 ~~                                                           
##    .PosAFF33          0.004    0.006    0.580    0.562    0.004    0.050
##  .NegAFF11 ~~                                                           
##    .NegAFF12          0.005    0.005    0.986    0.324    0.005    0.067
##    .NegAFF13          0.007    0.006    1.088    0.277    0.007    0.072
##  .NegAFF12 ~~                                                           
##    .NegAFF13          0.007    0.005    1.537    0.124    0.007    0.100
##  .NegAFF21 ~~                                                           
##    .NegAFF22          0.015    0.004    3.440    0.001    0.015    0.252
##    .NegAFF23          0.010    0.005    2.238    0.025    0.010    0.165
##  .NegAFF22 ~~                                                           
##    .NegAFF23          0.011    0.003    3.397    0.001    0.011    0.256
##  .NegAFF31 ~~                                                           
##    .NegAFF32         -0.007    0.004   -1.748    0.080   -0.007   -0.157
##    .NegAFF33         -0.007    0.004   -1.602    0.109   -0.007   -0.145
##  .NegAFF32 ~~                                                           
##    .NegAFF33         -0.002    0.003   -0.751    0.453   -0.002   -0.068
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .Pos2              0.653    0.075    8.735    0.000    0.765    0.765
##    .Pos3              0.496    0.058    8.594    0.000    0.780    0.780
##    .Neg2              0.346    0.041    8.475    0.000    0.710    0.710
##    .Neg3              0.345    0.041    8.396    0.000    0.804    0.804
##    .PosAFF11          0.102    0.011    9.359    0.000    0.102    0.205
##    .PosAFF21          0.127    0.013    9.731    0.000    0.127    0.219
##    .PosAFF31          0.093    0.012    8.100    0.000    0.093    0.165
##    .PosAFF12          0.083    0.009    9.656    0.000    0.083    0.198
##    .PosAFF22          0.113    0.011   10.344    0.000    0.113    0.226
##    .PosAFF32          0.060    0.008    7.270    0.000    0.060    0.129
##    .PosAFF13          0.071    0.008    8.814    0.000    0.071    0.219
##    .PosAFF23          0.101    0.010    9.817    0.000    0.101    0.259
##    .PosAFF33          0.082    0.009    8.730    0.000    0.082    0.216
##    .NegAFF11          0.105    0.010   10.063    0.000    0.105    0.260
##    .NegAFF21          0.085    0.009    9.775    0.000    0.085    0.247
##    .NegAFF31          0.062    0.008    7.599    0.000    0.062    0.176
##    .NegAFF12          0.061    0.006   10.647    0.000    0.061    0.295
##    .NegAFF22          0.041    0.004    9.668    0.000    0.041    0.245
##    .NegAFF32          0.032    0.004    7.810    0.000    0.032    0.184
##    .NegAFF13          0.084    0.008   11.023    0.000    0.084    0.396
##    .NegAFF23          0.045    0.005    9.106    0.000    0.045    0.287
##    .NegAFF33          0.036    0.005    7.452    0.000    0.036    0.224
##     Pos1              1.000                               1.000    1.000
##     Neg1              1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.long.cross)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-24-week-9_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.long.cross, layout = &amp;quot;tree3&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-24-week-9_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;longitudinal-mediation-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Longitudinal mediation model&lt;/h3&gt;
&lt;p&gt;An extension of this model is to introduce indirect pathways. Indirect pathways are equivalent with mediational pathways. The calculation of indirect pathways are done through what is known as the tracing rules. The coefficients along each path are multiplied together; those products are them summed across different tracings or paths. In short, one finds the sum of all compound paths between two variables.&lt;/p&gt;
&lt;p&gt;Maxwell, S. E., Cole, D. A., &amp;amp; Mitchell, M. A. (2011). Bias in cross-sectional analyses of longitudinal mediation: Partial and complete mediation under an autoregressive model. Multivariate Behavioral Research, 46(5), 816-841.&lt;/p&gt;
&lt;p&gt;Selig, J. P., &amp;amp; Preacher, K. J. (2009). Mediation models for longitudinal data in developmental research. Research in Human Development, 6(2-3), 144-164.&lt;/p&gt;
&lt;p&gt;Example below from a paper looking at whether extraverted people are happier because they connect and interact with more people. Extraversion and connection were measured latently. Extraversion was measured once, connection thrice, and subjective well being 4 times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Do Self-Reported Social Experiences Mediate the Effect of Extraversion on Life Satisfaction and Happiness?
#number close friends
library(readr)
TSS_sub &amp;lt;- read_csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/TSS_sub.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Missing column names filled in: &amp;#39;X1&amp;#39; [1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   .default = col_double(),
##   f1acode = col_character(),
##   h1acode = col_character(),
##   h1lbf01 = col_character(),
##   h1dtf01 = col_character(),
##   h1aef01 = col_character(),
##   j1free = col_character(),
##   k1major = col_character(),
##   k1attpar = col_character(),
##   m1major = col_character(),
##   m1attpar = col_character(),
##   i1knwots = col_character(),
##   i2knwots = col_character(),
##   i3knwots = col_character(),
##   i4knwots = col_logical(),
##   n1knwots = col_character(),
##   n2knwots = col_character(),
##   n3knwots = col_character(),
##   n4knwots = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See spec(...) for full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;scon.model6&amp;lt;-&amp;#39;
# define extraversion
bfie =~ a1bfi01 + a1bfi06r + a1bfi11 + a1bfi16 + a1bfi21r + a1bfi26 + a1bfi31r + a1bfi36

# correlated residuals
a1bfi11 ~~  a1bfi16
a1bfi06r ~~ a1bfi21r + a1bfi31r
a1bfi21r ~~ a1bfi31r + a1bfi01

#define social connection at 4 waves
hconnect=~h1clrel + h1satfr + h1sosat + h1ced05
jconnect=~j1clrel + j1satfr + j1sosat + j1ced05
kconnect=~k1clrel + k1satfr + k1sosat + k1ced05
mconnect=~m1clrel + m1satfr + m1sosat + m1ced05

#correlate residuals
h1clrel ~~ j1clrel + k1clrel + m1clrel
j1clrel ~~ k1clrel + m1clrel
k1clrel ~~ m1clrel
h1satfr ~~ j1satfr + k1satfr + m1satfr
j1satfr ~~ k1satfr + m1satfr
k1satfr ~~ m1satfr 
h1sosat ~~ j1sosat + k1sosat + m1sosat 
j1sosat ~~ k1sosat + m1sosat 
k1sosat ~~ m1sosat
h1ced05 ~~ j1ced05 + k1ced05 + m1ced05
j1ced05 ~~ k1ced05 + m1ced05
k1ced05 ~~ m1ced05

# same time covariances between extraversion, connection, satisfaction
bfie~~a1swls
hconnect ~~ h1swls
jconnect ~~ j1swls
kconnect ~~ k1swls

#regressions to calculate indiret effects
hconnect ~ a1*bfie + d1*a1swls 
jconnect ~ a2*bfie + d2*h1swls + m1*hconnect
kconnect ~ a3*bfie + d3*j1swls + m2*jconnect
mconnect ~ a4*bfie + d4*k1swls + m3*kconnect
h1swls ~ y1*a1swls + c1*bfie 
j1swls ~ y2*h1swls + c2*bfie + b1*hconnect
k1swls ~ y3*j1swls + c3*bfie + b2*jconnect 
m1swls ~ y4*k1swls + c4*bfie + b3*kconnect

#effects 
# extraversion -&amp;gt; connect (a)
# connect -&amp;gt;  swb (b)
# extraversion -&amp;gt; swb (c)
# auto-regressive connection (m)
# auto-regressive swb (y)

ind:= a1*b1*y3*y4 + a1*m1*b2*y4 + a1*m1*m2*b3 + a2*b2*y4 + a2*m2*b3 + a3*b3
total:= ind + c4 + c3*y4 + c2*y3*y4 + c1*y2*y3*y4
&amp;#39;
scon62 &amp;lt;- sem(scon.model6, data=TSS_sub, missing = &amp;quot;ml&amp;quot;, fixed.x = FALSE)
summary(scon62, standardized=T, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 113 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                        138
## 
##   Number of observations                           393
##   Number of missing patterns                        30
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     600.051
##   Degrees of freedom                               326
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             4447.119
##   Degrees of freedom                               406
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.932
##   Tucker-Lewis Index (TLI)                       0.916
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)             -13558.593
##   Loglikelihood unrestricted model (H1)     -13258.568
## 
##   Number of free parameters                        138
##   Akaike (AIC)                               27393.187
##   Bayesian (BIC)                             27941.573
##   Sample-size adjusted Bayesian (BIC)        27503.702
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.046
##   90 Percent Confidence Interval          0.040  0.052
##   P-value RMSEA &amp;lt;= 0.05                          0.854
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.068
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   bfie =~                                                               
##     a1bfi01           1.000                               0.905    0.728
##     a1bfi06r          0.813    0.072   11.336    0.000    0.735    0.601
##     a1bfi11           0.605    0.056   10.808    0.000    0.547    0.592
##     a1bfi16           0.603    0.054   11.084    0.000    0.545    0.604
##     a1bfi21r          0.951    0.063   15.013    0.000    0.860    0.700
##     a1bfi26           0.806    0.067   12.066    0.000    0.729    0.648
##     a1bfi31r          0.823    0.072   11.471    0.000    0.744    0.618
##     a1bfi36           1.064    0.068   15.561    0.000    0.962    0.871
##   hconnect =~                                                           
##     h1clrel           1.000                               1.006    0.681
##     h1satfr           1.014    0.119    8.552    0.000    1.020    0.589
##     h1sosat           1.086    0.116    9.389    0.000    1.093    0.747
##     h1ced05          -0.562    0.066   -8.470    0.000   -0.565   -0.644
##   jconnect =~                                                           
##     j1clrel           1.000                               0.876    0.661
##     j1satfr           1.226    0.126    9.754    0.000    1.074    0.645
##     j1sosat           1.145    0.114   10.055    0.000    1.003    0.754
##     j1ced05          -0.567    0.066   -8.548    0.000   -0.497   -0.597
##   kconnect =~                                                           
##     k1clrel           1.000                               0.830    0.635
##     k1satfr           1.221    0.144    8.485    0.000    1.014    0.611
##     k1sosat           1.097    0.137    7.984    0.000    0.911    0.607
##     k1ced05          -0.612    0.076   -8.028    0.000   -0.508   -0.574
##   mconnect =~                                                           
##     m1clrel           1.000                               0.755    0.662
##     m1satfr           1.172    0.109   10.797    0.000    0.885    0.622
##     m1sosat           1.261    0.120   10.492    0.000    0.952    0.693
##     m1ced05          -0.656    0.068   -9.580    0.000   -0.495   -0.598
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   hconnect ~                                                            
##     bfie      (a1)    0.224    0.082    2.728    0.006    0.202    0.202
##     a1swls    (d1)    0.372    0.062    5.963    0.000    0.370    0.427
##   jconnect ~                                                            
##     bfie      (a2)    0.099    0.071    1.403    0.161    0.102    0.102
##     h1swls    (d2)    0.034    0.065    0.528    0.597    0.039    0.054
##     hconnect  (m1)    0.385    0.113    3.417    0.001    0.443    0.443
##   kconnect ~                                                            
##     bfie      (a3)    0.153    0.062    2.455    0.014    0.167    0.167
##     j1swls    (d3)    0.142    0.060    2.380    0.017    0.172    0.209
##     jconnect  (m2)    0.391    0.101    3.858    0.000    0.412    0.412
##   mconnect ~                                                            
##     bfie      (a4)    0.170    0.051    3.302    0.001    0.204    0.204
##     k1swls    (d4)   -0.070    0.052   -1.352    0.177   -0.093   -0.120
##     kconnect  (m3)    0.671    0.110    6.085    0.000    0.738    0.738
##   h1swls ~                                                              
##     a1swls    (y1)    0.564    0.068    8.289    0.000    0.564    0.468
##     bfie      (c1)    0.070    0.093    0.754    0.451    0.063    0.046
##   j1swls ~                                                              
##     h1swls    (y2)    0.364    0.075    4.839    0.000    0.364    0.415
##     bfie      (c2)    0.079    0.082    0.958    0.338    0.071    0.059
##     hconnect  (b1)    0.097    0.129    0.746    0.455    0.097    0.080
##   k1swls ~                                                              
##     j1swls    (y3)    0.538    0.076    7.091    0.000    0.538    0.507
##     bfie      (c3)    0.271    0.079    3.446    0.001    0.245    0.190
##     jconnect  (b2)    0.009    0.126    0.072    0.943    0.008    0.006
##   m1swls ~                                                              
##     k1swls    (y4)    0.338    0.068    4.944    0.000    0.338    0.362
##     bfie      (c4)    0.005    0.066    0.076    0.940    0.004    0.004
##     kconnect  (b3)    0.562    0.137    4.112    0.000    0.466    0.387
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##  .a1bfi11 ~~                                                            
##    .a1bfi16           0.181    0.032    5.659    0.000    0.181    0.337
##  .a1bfi06r ~~                                                           
##    .a1bfi21r          0.355    0.052    6.900    0.000    0.355    0.414
##    .a1bfi31r          0.325    0.056    5.786    0.000    0.325    0.351
##  .a1bfi21r ~~                                                           
##    .a1bfi31r          0.342    0.050    6.837    0.000    0.342    0.411
##  .a1bfi01 ~~                                                            
##    .a1bfi21r          0.178    0.040    4.518    0.000    0.178    0.238
##  .h1clrel ~~                                                            
##    .j1clrel           0.225    0.090    2.494    0.013    0.225    0.210
##    .k1clrel           0.234    0.090    2.603    0.009    0.234    0.215
##    .m1clrel           0.358    0.073    4.883    0.000    0.358    0.386
##  .j1clrel ~~                                                            
##    .k1clrel           0.334    0.082    4.100    0.000    0.334    0.333
##    .m1clrel           0.257    0.061    4.189    0.000    0.257    0.302
##  .k1clrel ~~                                                            
##    .m1clrel           0.277    0.065    4.259    0.000    0.277    0.321
##  .h1satfr ~~                                                            
##    .j1satfr           0.238    0.136    1.753    0.080    0.238    0.134
##    .k1satfr           0.273    0.145    1.886    0.059    0.273    0.149
##    .m1satfr           0.274    0.108    2.539    0.011    0.274    0.176
##  .j1satfr ~~                                                            
##    .k1satfr           0.471    0.127    3.700    0.000    0.471    0.282
##    .m1satfr           0.417    0.099    4.231    0.000    0.417    0.294
##  .k1satfr ~~                                                            
##    .m1satfr           0.662    0.107    6.207    0.000    0.662    0.452
##  .h1sosat ~~                                                            
##    .j1sosat           0.010    0.078    0.129    0.897    0.010    0.012
##    .k1sosat           0.021    0.107    0.198    0.843    0.021    0.018
##    .m1sosat           0.003    0.077    0.039    0.969    0.003    0.003
##  .j1sosat ~~                                                            
##    .k1sosat           0.135    0.088    1.546    0.122    0.135    0.130
##    .m1sosat           0.070    0.069    1.005    0.315    0.070    0.080
##  .k1sosat ~~                                                            
##    .m1sosat           0.301    0.092    3.256    0.001    0.301    0.254
##  .h1ced05 ~~                                                            
##    .j1ced05           0.122    0.034    3.624    0.000    0.122    0.272
##    .k1ced05           0.094    0.037    2.562    0.010    0.094    0.194
##    .m1ced05           0.069    0.032    2.128    0.033    0.069    0.154
##  .j1ced05 ~~                                                            
##    .k1ced05           0.101    0.034    2.968    0.003    0.101    0.209
##    .m1ced05           0.101    0.029    3.466    0.001    0.101    0.229
##  .k1ced05 ~~                                                            
##    .m1ced05           0.171    0.035    4.898    0.000    0.171    0.355
##   bfie ~~                                                               
##     a1swls            0.379    0.062    6.098    0.000    0.418    0.363
##  .hconnect ~~                                                           
##    .h1swls            0.569    0.091    6.242    0.000    0.668    0.551
##  .jconnect ~~                                                           
##    .j1swls            0.452    0.070    6.470    0.000    0.605    0.571
##  .kconnect ~~                                                           
##    .k1swls            0.378    0.063    6.020    0.000    0.587    0.558
##  .mconnect ~~                                                           
##    .m1swls            0.203    0.041    5.001    0.000    0.406    0.463
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .a1bfi01           3.594    0.063   57.181    0.000    3.594    2.891
##    .a1bfi06r          2.762    0.062   44.674    0.000    2.762    2.259
##    .a1bfi11           3.944    0.047   84.305    0.000    3.944    4.263
##    .a1bfi16           3.706    0.046   81.133    0.000    3.706    4.103
##    .a1bfi21r          3.016    0.062   48.523    0.000    3.016    2.453
##    .a1bfi26           3.604    0.057   63.320    0.000    3.604    3.202
##    .a1bfi31r          2.594    0.061   42.551    0.000    2.594    2.152
##    .a1bfi36           3.663    0.056   65.581    0.000    3.663    3.316
##    .h1clrel           3.756    0.344   10.908    0.000    3.756    2.542
##    .h1satfr           3.575    0.383    9.342    0.000    3.575    2.065
##    .h1sosat           2.697    0.372    7.243    0.000    2.697    1.844
##    .h1ced05           2.987    0.204   14.615    0.000    2.987    3.403
##    .j1clrel           4.785    0.267   17.946    0.000    4.785    3.609
##    .j1satfr           4.346    0.330   13.187    0.000    4.346    2.609
##    .j1sosat           3.799    0.300   12.661    0.000    3.799    2.855
##    .j1ced05           2.587    0.153   16.899    0.000    2.587    3.108
##    .k1clrel           4.744    0.280   16.970    0.000    4.744    3.631
##    .k1satfr           4.190    0.342   12.258    0.000    4.190    2.524
##    .k1sosat           3.367    0.311   10.835    0.000    3.367    2.241
##    .k1ced05           2.733    0.178   15.350    0.000    2.733    3.086
##    .m1clrel           5.734    0.247   23.230    0.000    5.734    5.023
##    .m1satfr           5.387    0.291   18.491    0.000    5.387    3.786
##    .m1sosat           4.568    0.314   14.554    0.000    4.568    3.323
##    .m1ced05           2.252    0.165   13.670    0.000    2.252    2.719
##    .h1swls            2.189    0.371    5.895    0.000    2.189    1.576
##    .j1swls            3.088    0.264   11.692    0.000    3.088    2.541
##    .k1swls            2.369    0.326    7.269    0.000    2.369    1.838
##    .m1swls            2.952    0.272   10.840    0.000    2.952    2.448
##     a1swls            5.341    0.058   91.684    0.000    5.341    4.635
##     bfie              0.000                               0.000    0.000
##    .hconnect          0.000                               0.000    0.000
##    .jconnect          0.000                               0.000    0.000
##    .kconnect          0.000                               0.000    0.000
##    .mconnect          0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .a1bfi01           0.727    0.063   11.517    0.000    0.727    0.470
##    .a1bfi06r          0.955    0.075   12.655    0.000    0.955    0.638
##    .a1bfi11           0.556    0.043   12.809    0.000    0.556    0.650
##    .a1bfi16           0.519    0.041   12.766    0.000    0.519    0.636
##    .a1bfi21r          0.771    0.061   12.625    0.000    0.771    0.510
##    .a1bfi26           0.735    0.059   12.378    0.000    0.735    0.580
##    .a1bfi31r          0.899    0.071   12.650    0.000    0.899    0.619
##    .a1bfi36           0.295    0.040    7.450    0.000    0.295    0.242
##    .h1clrel           1.170    0.138    8.506    0.000    1.170    0.536
##    .h1satfr           1.955    0.203    9.647    0.000    1.955    0.653
##    .h1sosat           0.946    0.119    7.956    0.000    0.946    0.442
##    .h1ced05           0.451    0.047    9.522    0.000    0.451    0.585
##    .j1clrel           0.990    0.102    9.736    0.000    0.990    0.563
##    .j1satfr           1.621    0.164    9.870    0.000    1.621    0.584
##    .j1sosat           0.765    0.094    8.152    0.000    0.765    0.432
##    .j1ced05           0.446    0.042   10.670    0.000    0.446    0.644
##    .k1clrel           1.018    0.108    9.423    0.000    1.018    0.596
##    .k1satfr           1.727    0.174    9.912    0.000    1.727    0.627
##    .k1sosat           1.426    0.148    9.668    0.000    1.426    0.632
##    .k1ced05           0.526    0.052   10.109    0.000    0.526    0.671
##    .m1clrel           0.733    0.068   10.788    0.000    0.733    0.562
##    .m1satfr           1.241    0.109   11.352    0.000    1.241    0.613
##    .m1sosat           0.983    0.095   10.399    0.000    0.983    0.520
##    .m1ced05           0.440    0.037   11.974    0.000    0.440    0.642
##    .h1swls            1.471    0.128   11.501    0.000    1.471    0.763
##    .j1swls            1.123    0.095   11.824    0.000    1.123    0.760
##    .k1swls            1.109    0.095   11.633    0.000    1.109    0.667
##    .m1swls            0.771    0.068   11.279    0.000    0.771    0.530
##     a1swls            1.328    0.095   13.990    0.000    1.328    1.000
##     bfie              0.818    0.104    7.882    0.000    1.000    1.000
##    .hconnect          0.724    0.136    5.331    0.000    0.715    0.715
##    .jconnect          0.556    0.101    5.518    0.000    0.725    0.725
##    .kconnect          0.414    0.085    4.865    0.000    0.600    0.600
##    .mconnect          0.249    0.047    5.346    0.000    0.437    0.437
## 
## Defined Parameters:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     ind               0.131    0.046    2.864    0.004    0.119    0.098
##     total             0.247    0.071    3.479    0.001    0.223    0.185&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# use se = &amp;quot;bootstrap&amp;quot; in the fit function to get bootstrapped se&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will come back to extensions of these panel models in later weeks, incorporating mean levels as well as seperating between and within person sources.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;including-covariates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Including covariates&lt;/h3&gt;
&lt;p&gt;Many times we would like to control for the influence of a variable. If we wanted to, how would we go about doing this with our panel model?&lt;/p&gt;
&lt;p&gt;First, you have to ask yourself is it a time varying covariate (tvc), akin to a level 1 predictor variable ie one that varies from wave to wave. Or is it a time invariate covariate that does not differ across time?&lt;/p&gt;
&lt;p&gt;Second, you have to ask where in the model you want to introduce covariates? The DV is tradiational in standard regression. But we could also control for with the IVs. But there are multiple IVs, do we control for them all?&lt;/p&gt;
&lt;p&gt;Third, do you want to control at the latent level or at the level of the indicators?&lt;/p&gt;
&lt;p&gt;It is potential defensible to: 1. only control for the DV or focal association. 2. only control for initial time. 3. control at all time points 4 control for only downstream timepoints ie those predicted by another variable. Each of these have different interpretation for how you think the covariates could influence the model.&lt;/p&gt;
&lt;p&gt;I would suggest 1. try to use covariates as closely as possible to your theoretical model and 2. not to over control. Remember that statistical models can be both too liberal (you get an asterisk, everyone gets an asterisk!) and too conservative (this one of many reasons why Josh dislikes correcting for type 1 error). The goal is to be accurate. Try them multiple ways.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;growth-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Growth models&lt;/h2&gt;
&lt;p&gt;The above models are well suited to address between subjects questions, but does not get at a within subjects questions at all. There are some that can separate these effects, which we will turn to in a few weeks. However, we will start with the most basic model, the model you are familiar with, the growth model.&lt;/p&gt;
&lt;p&gt;The implementation of growth models in an SEM framework is very similar to the HLM framework. The major differences is how time is treated. Here, time variables must be the same for everyone in that each assessment point must have a particular variable name associated with it. That is, time is considered categorical in SEM, whereas in MLM it could be treated continuously. This requirement also makes a differences in how our data need to be structured. Whereas previously we had a time variable, now we indirectly include time into our model by specifying when variables were assessed. This has the consequence of necessitating a wide format, as opposed to the long format of MLM.&lt;/p&gt;
&lt;p&gt;Other than time, the idea behind the growth model is exactly the same.&lt;/p&gt;
&lt;div id=&#34;coding-time&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Coding time&lt;/h3&gt;
&lt;p&gt;One key these models is how you code time. Because we are working with qualitative time rather than continuous everyone has to have the same time structure.&lt;/p&gt;
&lt;p&gt;Let’s use the long dataset from the previous Bayesian workshop, taken from chapter 4 of Singer and Willet. It is a three wave longitudinal study of adolescents. We are looking at alcohol use during the previous year, measured from 0 - 7. COA is a variable indicating the child’s parents are alcoholics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alcohol1_pp &amp;lt;- read_csv(&amp;quot;alcohol1_pp.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Missing column names filled in: &amp;#39;X1&amp;#39; [1]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   X1 = col_double(),
##   id = col_double(),
##   age = col_double(),
##   coa = col_double(),
##   male = col_double(),
##   age_14 = col_double(),
##   alcuse = col_double(),
##   peer = col_double(),
##   cpeer = col_double(),
##   ccoa = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(alcohol1_pp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 10
##      X1    id   age   coa  male age_14 alcuse  peer  cpeer  ccoa
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     1    14     1     0      0   1.73 1.26   0.247 0.549
## 2     2     1    15     1     0      1   2    1.26   0.247 0.549
## 3     3     1    16     1     0      2   2    1.26   0.247 0.549
## 4     4     2    14     1     1      0   0    0.894 -0.124 0.549
## 5     5     2    15     1     1      1   0    0.894 -0.124 0.549
## 6     6     2    16     1     1      2   1    0.894 -0.124 0.549&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1          ✔ dplyr   0.8.3     
## ✔ tibble  2.1.3          ✔ stringr 1.4.0     
## ✔ tidyr   1.0.0.9000     ✔ forcats 0.4.0     
## ✔ purrr   0.3.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alcohol.wide &amp;lt;- alcohol1_pp %&amp;gt;% 
  select(-X1, -age_14, -ccoa) %&amp;gt;% 
  pivot_wider(names_from = &amp;quot;age&amp;quot;, 
              names_prefix = &amp;quot;alcuse_&amp;quot;,
              values_from  = alcuse) 
head(alcohol.wide)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##      id   coa  male  peer  cpeer alcuse_14 alcuse_15 alcuse_16
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     1     1     0 1.26   0.247      1.73         2      2   
## 2     2     1     1 0.894 -0.124      0            0      1   
## 3     3     1     1 0.894 -0.124      1            2      3.32
## 4     4     1     1 1.79   0.771      0            2      1.73
## 5     5     1     0 0.894 -0.124      0            0      0   
## 6     6     1     1 1.55   0.531      3            3      3.16&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 22 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          8
## 
##   Number of observations                            82
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.636
##   Degrees of freedom                                 1
##   P-value (Chi-square)                           0.425
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     alcuse_14         1.000                           
##     alcuse_15         1.000                           
##     alcuse_16         1.000                           
##   s =~                                                
##     alcuse_14         0.000                           
##     alcuse_15         1.000                           
##     alcuse_16         2.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                -0.187    0.102   -1.841    0.066
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.000                           
##    .alcuse_15         0.000                           
##    .alcuse_16         0.000                           
##     i                 0.634    0.103    6.163    0.000
##     s                 0.277    0.062    4.481    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.064    0.147    0.436    0.663
##    .alcuse_15         0.420    0.094    4.463    0.000
##    .alcuse_16         0.280    0.180    1.556    0.120
##     i                 0.807    0.193    4.177    0.000
##     s                 0.234    0.083    2.803    0.005&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To fit a growth model within SEM we are going to create two latent variables: an intercept and a slope/trajectory. This is the same we did prior when we fit an MLM growth model with time as a predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-24-week-9_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;How do we define these two latent factors? They are going to be made up of our repeated measures. We then add constraints on the loadings to force the latent variables to be interpreted how we want them to be interpreted.&lt;/p&gt;
&lt;p&gt;As before, the simplest way to think of the intercept is the initial value. While the slope indexes change. To obtain that, we will constrain the loadings to the intercept to be 1 for all repeated measures. This will obtain what is &lt;em&gt;constant&lt;/em&gt; across time.&lt;/p&gt;
&lt;p&gt;But where is that constant located? Previously we could center or change our time variable to change the interpretation of the intercept. We do the same thing within SEM whereby we change the interpretation of intercept by changing how we define the slope parameter. As always, the intercept is defined as when X (or time in this case) equals zero. That is, the intercept is the mean of the DV when the predictor is 0, where we have time as the predictor.&lt;/p&gt;
&lt;p&gt;The slope loadings typically include 0, so as to make the intercept interpretable. However, it is up to you to define where zero goes and what the rest of the loadings are. A few rules: First, how you code the loadings represent the pattern of change you expect. 0,1,2 suggests a straight line that does not increase in speed. 0,1,5, suggests something different. Usually it is better to stick with linear change to start out with.&lt;/p&gt;
&lt;p&gt;Second, remember that this is just fancy regression. So the coefficients are interpreted as in regression whereas for a one unit change in the slope (time) corresponds to a coef change in your DV. Thus, you will change the magnitude of your slope parameter by choosing 0, .5, 1 versus 0,1,2 versus 0, 10, 20. As always, choose what makes sense to interpret. Changing the loadings (if they are just a multiple) will not change the substantive interpretation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-do-we-do-that-in-lavaan&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How do we do that in lavaan?&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.1 &amp;lt;- &amp;#39;  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16&amp;#39;
fit.1 &amp;lt;- growth(model.1, data=alcohol.wide)
summary(fit.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 22 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          8
## 
##   Number of observations                            82
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.636
##   Degrees of freedom                                 1
##   P-value (Chi-square)                           0.425
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     alcuse_14         1.000                           
##     alcuse_15         1.000                           
##     alcuse_16         1.000                           
##   s =~                                                
##     alcuse_14         0.000                           
##     alcuse_15         1.000                           
##     alcuse_16         2.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                -0.187    0.102   -1.841    0.066
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.000                           
##    .alcuse_15         0.000                           
##    .alcuse_16         0.000                           
##     i                 0.634    0.103    6.163    0.000
##     s                 0.277    0.062    4.481    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.064    0.147    0.436    0.663
##    .alcuse_15         0.420    0.094    4.463    0.000
##    .alcuse_16         0.280    0.180    1.556    0.120
##     i                 0.807    0.193    4.177    0.000
##     s                 0.234    0.083    2.803    0.005&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compare-with-mlm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Compare with MLM&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;Matrix&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1.mlm &amp;lt;- lmer(alcuse ~  age + (age | id), data = alcohol1_pp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 0.145863
## (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit1.mlm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: alcuse ~ age + (age | id)
##    Data: alcohol1_pp
## 
## REML criterion at convergence: 643.2
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.49036 -0.37763 -0.08226  0.38749  2.48769 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  id       (Intercept) 33.6864  5.8040        
##           age          0.1580  0.3975   -0.99
##  Residual              0.3356  0.5793        
## Number of obs: 246, groups:  id, 82
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) -3.13782    0.93414  -3.359
## age          0.27065    0.06303   4.294
## 
## Correlation of Fixed Effects:
##     (Intr)
## age -0.995
## convergence code: 0
## Model failed to converge with max|grad| = 0.145863 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;centered MLM&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit1.mlm.c &amp;lt;- lmer(alcuse ~  age_14 + (age_14 | id), data = alcohol1_pp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 0.00986507
## (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit1.mlm.c)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: alcuse ~ age_14 + (age_14 | id)
##    Data: alcohol1_pp
## 
## REML criterion at convergence: 643.2
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.48441 -0.37859 -0.07889  0.38922  2.49339 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  id       (Intercept) 0.6361   0.7976        
##           age_14      0.1553   0.3941   -0.23
##  Residual             0.3369   0.5805        
## Number of obs: 246, groups:  id, 82
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  0.65130    0.10575   6.159
## age_14       0.27065    0.06284   4.307
## 
## Correlation of Fixed Effects:
##        (Intr)
## age_14 -0.440
## convergence code: 0
## Model failed to converge with max|grad| = 0.00986507 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.1, &amp;#39;est&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-24-week-9_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;we-can-rescale-our-time-variable-to-be-whatever-we-want&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;we can rescale our time variable to be whatever we want&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.1 &amp;lt;- &amp;#39;  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16&amp;#39;
fit.1 &amp;lt;- growth(model.1, data=alcohol.wide)
summary(fit.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 26 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          8
## 
##   Number of observations                            82
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.636
##   Degrees of freedom                                 1
##   P-value (Chi-square)                           0.425
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     alcuse_14         1.000                           
##     alcuse_15         1.000                           
##     alcuse_16         1.000                           
##   s =~                                                
##     alcuse_14         0.000                           
##     alcuse_15         0.500                           
##     alcuse_16         1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                -0.374    0.203   -1.841    0.066
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.000                           
##    .alcuse_15         0.000                           
##    .alcuse_16         0.000                           
##     i                 0.634    0.103    6.163    0.000
##     s                 0.555    0.124    4.481    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.064    0.147    0.436    0.663
##    .alcuse_15         0.420    0.094    4.463    0.000
##    .alcuse_16         0.280    0.180    1.556    0.120
##     i                 0.807    0.193    4.177    0.000
##     s                 0.936    0.334    2.803    0.005&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This does not change our conclusions but it does change our numnbers.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation-of-variances&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation of variances&lt;/h3&gt;
&lt;p&gt;Intercept will reflect our fixed effects whereas our random effects are indexed by variances of the latent variables. These are interpreted the same as before. More variance means that more people differ from the fixed effect.&lt;/p&gt;
&lt;p&gt;In MLM we assumed that the residual variances were equal. Here we can better model if that is the case. Right now we are able to fit separate variances for each repeated measure. These residual variances are interpreted as what is left over or what is unique at each time point that cannot be explained by the trajectory.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;constraining-slope-to-be-fixed-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;constraining slope to be fixed only&lt;/h3&gt;
&lt;p&gt;As with MLM we have options to handle the inclusion of random effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.2 &amp;lt;- &amp;#39;  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16
               s ~~0*s&amp;#39;

fit.2 &amp;lt;- growth(model.2, data=alcohol.wide)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_object_post_check(object): lavaan WARNING: covariance matrix of latent variables
##                 is not positive definite;
##                 use lavInspect(fit, &amp;quot;cov.lv&amp;quot;) to investigate.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 21 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          7
## 
##   Number of observations                            82
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       7.508
##   Degrees of freedom                                 2
##   P-value (Chi-square)                           0.023
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     alcuse_14         1.000                           
##     alcuse_15         1.000                           
##     alcuse_16         1.000                           
##   s =~                                                
##     alcuse_14         0.000                           
##     alcuse_15         0.500                           
##     alcuse_16         1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                 0.083    0.114    0.731    0.465
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.000                           
##    .alcuse_15         0.000                           
##    .alcuse_16         0.000                           
##     i                 0.652    0.104    6.273    0.000
##     s                 0.556    0.115    4.849    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     s                 0.000                           
##    .alcuse_14         0.416    0.100    4.172    0.000
##    .alcuse_15         0.331    0.084    3.923    0.000
##    .alcuse_16         0.692    0.138    5.003    0.000
##     i                 0.541    0.147    3.676    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;introducing-covariatespredictors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;introducing covariates/predictors&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a linear growth model with a time invariatnt covariate

model.3 &amp;lt;- &amp;#39;  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16
               
# regressions
    
    i ~ male + coa
    s ~ male + coa
&amp;#39;
fit.3 &amp;lt;- growth(model.3, data = alcohol.wide)
summary(fit.3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 32 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         12
## 
##   Number of observations                            82
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       4.224
##   Degrees of freedom                                 3
##   P-value (Chi-square)                           0.238
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     alcuse_14         1.000                           
##     alcuse_15         1.000                           
##     alcuse_16         1.000                           
##   s =~                                                
##     alcuse_14         0.000                           
##     alcuse_15         0.500                           
##     alcuse_16         1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~                                                 
##     male             -0.147    0.188   -0.779    0.436
##     coa               0.759    0.189    4.016    0.000
##   s ~                                                 
##     male              0.550    0.240    2.289    0.022
##     coa              -0.132    0.242   -0.545    0.586
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##  .i ~~                                                
##    .s                -0.328    0.179   -1.827    0.068
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.000                           
##    .alcuse_15         0.000                           
##    .alcuse_16         0.000                           
##    .i                 0.368    0.157    2.346    0.019
##    .s                 0.327    0.200    1.635    0.102
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.074    0.134    0.548    0.584
##    .alcuse_15         0.435    0.092    4.747    0.000
##    .alcuse_16         0.210    0.167    1.257    0.209
##    .i                 0.653    0.168    3.884    0.000
##    .s                 0.907    0.307    2.951    0.003&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
fit3.mlm &amp;lt;- lmer(alcuse ~  age_14 + male*age_14 + age_14*coa + (age_14 | id), data = alcohol1_pp)
summary(fit3.mlm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: alcuse ~ age_14 + male * age_14 + age_14 * coa + (age_14 | id)
##    Data: alcohol1_pp
## 
## REML criterion at convergence: 630.3
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.55550 -0.50567 -0.09429  0.36820  2.26306 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  id       (Intercept) 0.5039   0.7099        
##           age_14      0.1402   0.3744   -0.19
##  Residual             0.3373   0.5808        
## Number of obs: 246, groups:  id, 82
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  0.42487    0.16318   2.604
## age_14       0.14877    0.10235   1.454
## male        -0.22278    0.19600  -1.137
## coa          0.75472    0.19688   3.833
## age_14:male  0.29492    0.12294   2.399
## age_14:coa  -0.06466    0.12350  -0.524
## 
## Correlation of Fixed Effects:
##             (Intr) age_14 male   coa    ag_14:m
## age_14      -0.446                             
## male        -0.587  0.262                      
## coa         -0.513  0.229 -0.051               
## age_14:male  0.262 -0.587 -0.446  0.023        
## age_14:coa   0.229 -0.513  0.023 -0.446 -0.051&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;centered-predictors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;centered predictors&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alcohol.wide$coa.c &amp;lt;- scale(alcohol.wide$coa, center=TRUE, scale = FALSE)
alcohol.wide$male.c &amp;lt;- scale(alcohol.wide$male, center=TRUE, scale = FALSE)


model.4 &amp;lt;- &amp;#39;  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16
               
# regressions
    
    i ~ male.c + coa.c
    s ~ male.c + coa.c
&amp;#39;
fit.4 &amp;lt;- growth(model.4, data = alcohol.wide)
summary(fit.4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 31 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         12
## 
##   Number of observations                            82
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       4.224
##   Degrees of freedom                                 3
##   P-value (Chi-square)                           0.238
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     alcuse_14         1.000                           
##     alcuse_15         1.000                           
##     alcuse_16         1.000                           
##   s =~                                                
##     alcuse_14         0.000                           
##     alcuse_15         0.500                           
##     alcuse_16         1.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~                                                 
##     male.c           -0.147    0.188   -0.779    0.436
##     coa.c             0.759    0.189    4.016    0.000
##   s ~                                                 
##     male.c            0.550    0.240    2.289    0.022
##     coa.c            -0.132    0.242   -0.545    0.586
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##  .i ~~                                                
##    .s                -0.328    0.179   -1.827    0.068
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.000                           
##    .alcuse_15         0.000                           
##    .alcuse_16         0.000                           
##    .i                 0.635    0.094    6.760    0.000
##    .s                 0.550    0.120    4.580    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.074    0.134    0.548    0.584
##    .alcuse_15         0.435    0.092    4.747    0.000
##    .alcuse_16         0.210    0.167    1.257    0.209
##    .i                 0.653    0.168    3.884    0.000
##    .s                 0.907    0.307    2.951    0.003&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is different what is the same? Notice that the main difference is the intercepts ie fixed effects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;introducing-time-varying-covariates&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;introducing time varying covariates&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# a linear growth model with a time-varying covariate

model.5 &amp;lt;- &amp;#39;  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16
               
# regressions
    
    i ~ male.c + coa.c
    s ~ male.c + coa.c
    
# time-varying covariates

    alcuse_14 ~ c1
    alcuse_15 ~ c2
    alcuse_16 ~ c3

&amp;#39;
fit.5 &amp;lt;- growth(model.5, data = alcohol.wide)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we interpet tvcs?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-basis-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;latent basis model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.6 &amp;lt;- &amp;#39;  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + alcuse_15 + 1*alcuse_16
              
&amp;#39;

fit.6 &amp;lt;- growth(model.6, data = alcohol.wide)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_object_post_check(object): lavaan WARNING: some estimated ov
## variances are negative&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 30 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                          9
## 
##   Number of observations                            82
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.000
##   Degrees of freedom                                 0
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     alcuse_14         1.000                           
##     alcuse_15         1.000                           
##     alcuse_16         1.000                           
##   s =~                                                
##     alcuse_14         0.000                           
##     alcuse_15         0.615    0.150    4.091    0.000
##     alcuse_16         1.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                -0.488    0.326   -1.497    0.134
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14         0.000                           
##    .alcuse_15         0.000                           
##    .alcuse_16         0.000                           
##     i                 0.630    0.103    6.118    0.000
##     s                 0.541    0.125    4.334    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .alcuse_14        -0.049    0.275   -0.179    0.858
##    .alcuse_15         0.406    0.097    4.187    0.000
##    .alcuse_16         0.383    0.191    2.005    0.045
##     i                 0.920    0.310    2.968    0.003
##     s                 0.945    0.361    2.618    0.009&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does not change the fit of the model nor the implied means, but it can change your parameters by changing the time scaling.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quadratic-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;quadratic model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.6 &amp;lt;- &amp;#39;  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16
            q =~ 0*alcuse_14 + 1*alcuse_15 + 4*alcuse_16  
&amp;#39;

fit.6 &amp;lt;- growth(model.6, data = alcohol.wide)
summary(fit.6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;b-second-order-growth-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.b Second order growth model&lt;/h2&gt;
&lt;p&gt;Repeated measures are latent. Why would we want to do this? At least two reasons. 1. We can take advantage of the benefits of latent variables ie no measurement error. 2. we can impose constraints for MI across time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sec.order &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ NA*PosAFF11 + L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ NA*PosAFF12 + L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ NA*PosAFF13 + L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33

## intercepts
PosAFF11 ~ t1*1
PosAFF21 ~ t2*1
PosAFF31 ~ t3*1

PosAFF12 ~ t1*1
PosAFF22 ~ t2*1
PosAFF32 ~ t3*1

PosAFF13 ~ t1*1
PosAFF23 ~ t2*1
PosAFF33 ~ t3*1


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33


## latent variable intercepts
Pos1 ~ 0*1
Pos2  ~ 0*1
Pos3  ~ 0*1

#model constraints for effect coding
## loadings must average to 1
L1 == 3 - L2 - L3
## means must average to 0
t1 == 0 - t2 - t3

i =~ 1*Pos1 + 1*Pos2 + 1*Pos3 
s =~ 0*Pos1 + 1*Pos2 + 2*Pos3 &amp;#39;


fit.sec.order &amp;lt;- growth(sec.order, data=long, missing = &amp;quot;ML&amp;quot;)
summary(fit.sec.order, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 99 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         44
##   Number of equality constraints                    14
##   Row rank of the constraints matrix                14
## 
##   Number of observations                           368
##   Number of missing patterns                         1
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      30.254
##   Degrees of freedom                                24
##   P-value (Chi-square)                           0.176
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             2688.088
##   Degrees of freedom                                36
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.998
##   Tucker-Lewis Index (TLI)                       0.996
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -2051.451
##   Loglikelihood unrestricted model (H1)      -2036.324
## 
##   Number of free parameters                         30
##   Akaike (AIC)                                4162.902
##   Bayesian (BIC)                              4280.145
##   Sample-size adjusted Bayesian (BIC)         4184.966
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.027
##   90 Percent Confidence Interval          0.000  0.053
##   P-value RMSEA &amp;lt;= 0.05                          0.926
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.025
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   Pos1 =~                                             
##     PosAFF11  (L1)    0.950    0.014   68.551    0.000
##     PosAFF21  (L2)    1.018    0.015   69.038    0.000
##     PosAFF31  (L3)    1.033    0.014   73.342    0.000
##   Pos2 =~                                             
##     PosAFF12  (L1)    0.950    0.014   68.551    0.000
##     PosAFF22  (L2)    1.018    0.015   69.038    0.000
##     PosAFF32  (L3)    1.033    0.014   73.342    0.000
##   Pos3 =~                                             
##     PosAFF13  (L1)    0.950    0.014   68.551    0.000
##     PosAFF23  (L2)    1.018    0.015   69.038    0.000
##     PosAFF33  (L3)    1.033    0.014   73.342    0.000
##   i =~                                                
##     Pos1              1.000                           
##     Pos2              1.000                           
##     Pos3              1.000                           
##   s =~                                                
##     Pos1              0.000                           
##     Pos2              1.000                           
##     Pos3              2.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##  .PosAFF11 ~~                                         
##    .PosAFF12          0.003    0.007    0.465    0.642
##    .PosAFF13          0.000    0.007    0.029    0.977
##  .PosAFF12 ~~                                         
##    .PosAFF13          0.002    0.006    0.306    0.759
##  .PosAFF21 ~~                                         
##    .PosAFF22          0.006    0.008    0.716    0.474
##    .PosAFF23          0.004    0.008    0.520    0.603
##  .PosAFF22 ~~                                         
##    .PosAFF23          0.011    0.007    1.518    0.129
##  .PosAFF31 ~~                                         
##    .PosAFF32          0.008    0.007    1.159    0.247
##    .PosAFF33          0.017    0.007    2.253    0.024
##  .PosAFF32 ~~                                         
##    .PosAFF33          0.005    0.006    0.791    0.429
##   i ~~                                                
##     s                -0.052    0.023   -2.216    0.027
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .PosAFF11  (t1)    0.210    0.045    4.645    0.000
##    .PosAFF21  (t2)   -0.161    0.048   -3.344    0.001
##    .PosAFF31  (t3)   -0.049    0.046   -1.067    0.286
##    .PosAFF12  (t1)    0.210    0.045    4.645    0.000
##    .PosAFF22  (t2)   -0.161    0.048   -3.344    0.001
##    .PosAFF32  (t3)   -0.049    0.046   -1.067    0.286
##    .PosAFF13  (t1)    0.210    0.045    4.645    0.000
##    .PosAFF23  (t2)   -0.161    0.048   -3.344    0.001
##    .PosAFF33  (t3)   -0.049    0.046   -1.067    0.286
##    .Pos1              0.000                           
##    .Pos2              0.000                           
##    .Pos3              0.000                           
##     i                 3.192    0.034   92.584    0.000
##     s                 0.019    0.018    1.016    0.310
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .PosAFF11          0.101    0.011    9.101    0.000
##    .PosAFF21          0.125    0.013    9.479    0.000
##    .PosAFF31          0.097    0.012    8.170    0.000
##    .PosAFF12          0.086    0.009    9.536    0.000
##    .PosAFF22          0.107    0.011    9.917    0.000
##    .PosAFF32          0.061    0.009    7.052    0.000
##    .PosAFF13          0.072    0.008    8.770    0.000
##    .PosAFF23          0.101    0.010    9.655    0.000
##    .PosAFF33          0.083    0.009    8.716    0.000
##    .Pos1              0.196    0.043    4.553    0.000
##    .Pos2              0.207    0.023    9.132    0.000
##    .Pos3              0.129    0.034    3.767    0.000
##     i                 0.245    0.044    5.627    0.000
##     s                 0.029    0.019    1.542    0.123
## 
## Constraints:
##                                                |Slack|
##     L1 - (3-L2-L3)                               0.000
##     t1 - (0-t2-t3)                               0.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.sec.order)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-24-week-9_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c-multivariate-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.c Multivariate models&lt;/h2&gt;
&lt;p&gt;For all of the models this semester we have focused only on a single outcome.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mv.sec.order &amp;lt;- &amp;#39;
## define latent variables
Pos1 =~ NA*PosAFF11 + L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ NA*PosAFF12 + L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ NA*PosAFF13 + L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33

Neg1 =~ NA*NegAFF11 + L4*NegAFF11 + L5*NegAFF21 + L6*NegAFF31
Neg2 =~ NA*NegAFF12 + L4*NegAFF12 + L5*NegAFF22 + L6*NegAFF32
Neg3 =~ NA*NegAFF13 + L4*NegAFF13 + L5*NegAFF23 + L6*NegAFF33

## intercepts
PosAFF11 ~ t1*1
PosAFF21 ~ t2*1
PosAFF31 ~ t3*1

PosAFF12 ~ t1*1
PosAFF22 ~ t2*1
PosAFF32 ~ t3*1

PosAFF13 ~ t1*1
PosAFF23 ~ t2*1
PosAFF33 ~ t3*1

NegAFF11 ~ tt1*1
NegAFF21 ~ tt2*1
NegAFF31 ~ tt3*1

NegAFF12 ~ tt1*1
NegAFF22 ~ tt2*1
NegAFF32 ~ tt3*1

NegAFF13 ~ tt1*1
NegAFF23 ~ tt2*1
NegAFF33 ~ tt3*1


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33

## latent variable intercepts
Pos1 ~ 0*1
Pos2  ~ 0*1
Pos3  ~ 0*1

Neg1 ~ 0*1
Neg2  ~ 0*1
Neg3  ~ 0*1

#model constraints for effect coding
## loadings must average to 1
L1 == 3 - L2 - L3
L4 == 3 - L5 - L6
## means must average to 0
t1 == 0 - t2 - t3
tt1 == 0 - tt2 - tt3

i.p =~ 1*Pos1 + 1*Pos2 + 1*Pos3 
s.p =~ 0*Pos1 + 1*Pos2 + 2*Pos3

i.n =~ 1*Neg1 + 1*Neg2 + 1*Neg3 
s.n =~ 0*Neg1 + 1*Neg2 + 2*Neg3&amp;#39;


mv.secondorder &amp;lt;- growth(mv.sec.order, data=long, missing = &amp;quot;ML&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_object_post_check(object): lavaan WARNING: covariance matrix of latent variables
##                 is not positive definite;
##                 use lavInspect(fit, &amp;quot;cov.lv&amp;quot;) to investigate.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mv.secondorder, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 186 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         92
##   Number of equality constraints                    28
##   Row rank of the constraints matrix                28
## 
##   Number of observations                           368
##   Number of missing patterns                         1
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                     248.244
##   Degrees of freedom                               125
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             5253.085
##   Degrees of freedom                               153
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.976
##   Tucker-Lewis Index (TLI)                       0.970
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3124.754
##   Loglikelihood unrestricted model (H1)      -3000.632
## 
##   Number of free parameters                         64
##   Akaike (AIC)                                6377.508
##   Bayesian (BIC)                              6627.625
##   Sample-size adjusted Bayesian (BIC)         6424.576
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.052
##   90 Percent Confidence Interval          0.042  0.061
##   P-value RMSEA &amp;lt;= 0.05                          0.367
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.060
## 
## Parameter Estimates:
## 
##   Information                                 Observed
##   Observed information based on                Hessian
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   Pos1 =~                                             
##     PosAFF11  (L1)    0.948    0.014   68.484    0.000
##     PosAFF21  (L2)    1.015    0.015   68.707    0.000
##     PosAFF31  (L3)    1.037    0.014   73.770    0.000
##   Pos2 =~                                             
##     PosAFF12  (L1)    0.948    0.014   68.484    0.000
##     PosAFF22  (L2)    1.015    0.015   68.707    0.000
##     PosAFF32  (L3)    1.037    0.014   73.770    0.000
##   Pos3 =~                                             
##     PosAFF13  (L1)    0.948    0.014   68.484    0.000
##     PosAFF23  (L2)    1.015    0.015   68.707    0.000
##     PosAFF33  (L3)    1.037    0.014   73.770    0.000
##   Neg1 =~                                             
##     NegAFF11  (L4)    1.029    0.018   58.212    0.000
##     NegAFF21  (L5)    0.959    0.017   55.123    0.000
##     NegAFF31  (L6)    1.011    0.016   62.776    0.000
##   Neg2 =~                                             
##     NegAFF12  (L4)    1.029    0.018   58.212    0.000
##     NegAFF22  (L5)    0.959    0.017   55.123    0.000
##     NegAFF32  (L6)    1.011    0.016   62.776    0.000
##   Neg3 =~                                             
##     NegAFF13  (L4)    1.029    0.018   58.212    0.000
##     NegAFF23  (L5)    0.959    0.017   55.123    0.000
##     NegAFF33  (L6)    1.011    0.016   62.776    0.000
##   i.p =~                                              
##     Pos1              1.000                           
##     Pos2              1.000                           
##     Pos3              1.000                           
##   s.p =~                                              
##     Pos1              0.000                           
##     Pos2              1.000                           
##     Pos3              2.000                           
##   i.n =~                                              
##     Neg1              1.000                           
##     Neg2              1.000                           
##     Neg3              1.000                           
##   s.n =~                                              
##     Neg1              0.000                           
##     Neg2              1.000                           
##     Neg3              2.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##  .PosAFF11 ~~                                         
##    .PosAFF12          0.004    0.007    0.581    0.561
##    .PosAFF13         -0.001    0.007   -0.091    0.927
##  .PosAFF12 ~~                                         
##    .PosAFF13          0.002    0.006    0.324    0.746
##  .PosAFF21 ~~                                         
##    .PosAFF22          0.008    0.008    0.969    0.333
##    .PosAFF23          0.006    0.008    0.772    0.440
##  .PosAFF22 ~~                                         
##    .PosAFF23          0.012    0.007    1.608    0.108
##  .PosAFF31 ~~                                         
##    .PosAFF32          0.005    0.007    0.768    0.443
##    .PosAFF33          0.016    0.007    2.126    0.034
##  .PosAFF32 ~~                                         
##    .PosAFF33          0.004    0.006    0.695    0.487
##  .NegAFF11 ~~                                         
##    .NegAFF12          0.006    0.006    1.048    0.294
##    .NegAFF13          0.006    0.006    0.998    0.318
##  .NegAFF12 ~~                                         
##    .NegAFF13          0.007    0.005    1.492    0.136
##  .NegAFF21 ~~                                         
##    .NegAFF22          0.015    0.004    3.263    0.001
##    .NegAFF23          0.010    0.005    2.168    0.030
##  .NegAFF22 ~~                                         
##    .NegAFF23          0.011    0.003    3.235    0.001
##  .NegAFF31 ~~                                         
##    .NegAFF32         -0.006    0.004   -1.577    0.115
##    .NegAFF33         -0.008    0.004   -1.793    0.073
##  .NegAFF32 ~~                                         
##    .NegAFF33         -0.002    0.003   -0.674    0.500
##   i.p ~~                                              
##     s.p              -0.036    0.022   -1.581    0.114
##     i.n              -0.151    0.019   -7.794    0.000
##     s.n               0.054    0.010    5.454    0.000
##   s.p ~~                                              
##     i.n               0.063    0.010    6.314    0.000
##     s.n              -0.036    0.005   -6.625    0.000
##   i.n ~~                                              
##     s.n              -0.035    0.012   -2.880    0.004
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .PosAFF11  (t1)    0.216    0.045    4.791    0.000
##    .PosAFF21  (t2)   -0.154    0.048   -3.187    0.001
##    .PosAFF31  (t3)   -0.063    0.046   -1.365    0.172
##    .PosAFF12  (t1)    0.216    0.045    4.791    0.000
##    .PosAFF22  (t2)   -0.154    0.048   -3.187    0.001
##    .PosAFF32  (t3)   -0.063    0.046   -1.365    0.172
##    .PosAFF13  (t1)    0.216    0.045    4.791    0.000
##    .PosAFF23  (t2)   -0.154    0.048   -3.187    0.001
##    .PosAFF33  (t3)   -0.063    0.046   -1.365    0.172
##    .NegAFF11 (tt1)    0.039    0.025    1.553    0.120
##    .NegAFF21 (tt2)    0.018    0.025    0.716    0.474
##    .NegAFF31 (tt3)   -0.056    0.023   -2.497    0.013
##    .NegAFF12 (tt1)    0.039    0.025    1.553    0.120
##    .NegAFF22 (tt2)    0.018    0.025    0.716    0.474
##    .NegAFF32 (tt3)   -0.056    0.023   -2.497    0.013
##    .NegAFF13 (tt1)    0.039    0.025    1.553    0.120
##    .NegAFF23 (tt2)    0.018    0.025    0.716    0.474
##    .NegAFF33 (tt3)   -0.056    0.023   -2.497    0.013
##    .Pos1              0.000                           
##    .Pos2              0.000                           
##    .Pos3              0.000                           
##    .Neg1              0.000                           
##    .Neg2              0.000                           
##    .Neg3              0.000                           
##     i.p               3.193    0.035   92.501    0.000
##     s.p               0.018    0.018    0.996    0.319
##     i.n               1.414    0.027   52.911    0.000
##     s.n              -0.047    0.015   -3.271    0.001
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .PosAFF11          0.103    0.011    9.281    0.000
##    .PosAFF21          0.129    0.013    9.683    0.000
##    .PosAFF31          0.092    0.012    7.837    0.000
##    .PosAFF12          0.086    0.009    9.582    0.000
##    .PosAFF22          0.109    0.011   10.042    0.000
##    .PosAFF32          0.059    0.009    6.869    0.000
##    .PosAFF13          0.073    0.008    8.877    0.000
##    .PosAFF23          0.101    0.010    9.715    0.000
##    .PosAFF33          0.081    0.009    8.640    0.000
##    .NegAFF11          0.105    0.011   10.019    0.000
##    .NegAFF21          0.086    0.009    9.635    0.000
##    .NegAFF31          0.061    0.008    7.271    0.000
##    .NegAFF12          0.060    0.006   10.358    0.000
##    .NegAFF22          0.042    0.005    9.127    0.000
##    .NegAFF32          0.032    0.004    7.515    0.000
##    .NegAFF13          0.085    0.008   10.966    0.000
##    .NegAFF23          0.045    0.005    8.959    0.000
##    .NegAFF33          0.036    0.005    7.399    0.000
##    .Pos1              0.234    0.041    5.641    0.000
##    .Pos2              0.190    0.021    9.025    0.000
##    .Pos3              0.157    0.034    4.600    0.000
##    .Neg1              0.148    0.022    6.683    0.000
##    .Neg2              0.054    0.008    6.944    0.000
##    .Neg3              0.084    0.015    5.626    0.000
##     i.p               0.223    0.041    5.464    0.000
##     s.p               0.013    0.018    0.686    0.493
##     i.n               0.140    0.022    6.382    0.000
##     s.n               0.010    0.009    1.108    0.268
## 
## Constraints:
##                                                |Slack|
##     L1 - (3-L2-L3)                               0.000
##     L4 - (3-L5-L6)                               0.000
##     t1 - (0-t2-t3)                               0.000
##     tt1 - (0-tt2-tt3)                            0.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(mv.secondorder)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-24-week-9_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;d-factor-of-curves-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.d Factor of curves model&lt;/h2&gt;
&lt;p&gt;This type of model takes multiple slope terms and asks is there a latent variable associated with the change?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;i.a =~ 1*Pos1 + 1*Pos2 + 1*Pos3 
s.a =~ 0*Pos1 + 1*Pos2 + 2*Pos3

i.b =~ 1*Neg1 + 1*Neg2 + 1*Neg3 
s.b =~ 0*Neg1 + 1*Neg2 + 2*Neg3

i.c =~ 1*hap1 + 1*hap2 + 1*hap3 
s.c =~ 0*hap1 + 1*hap2 + 2*hap3

i.d =~ 1*Arou1 + 1*Arou2 + 1*Arou3 
s.d =~ 0*Arou1 + 1*Arou2 + 2*Arou3

#Note that there are no constraints on these

Intercept =~ i.a + i.b + i.c + i.d 
Slope =~ s.a + s.b + s.c + s.d 

&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-groups&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Multiple groups&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group &amp;lt;- read_csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/group.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   P1 = col_double(),
##   P2 = col_double(),
##   P3 = col_double(),
##   N1 = col_double(),
##   N2 = col_double(),
##   N3 = col_double(),
##   Grade = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(group)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 7
##      P1    P2    P3    N1    N2    N3 Grade
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1  2.75  3.42  3.27 1.71  1.45  1.76  7th  
## 2  4.18  3.49  3.36 3.43  2.70  2.48  7th  
## 3  2.17  2.69  2.90 1.06  0.726 1.15  7th  
## 4  2.68  2.88  3.28 0.846 1.22  0.293 7th  
## 5  1.41  1.83  1.64 2.18  2.73  2.47  7th  
## 6  3.96  4.03  4.21 1.80  1.72  1.71  7th&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group1 &amp;lt;- &amp;#39;Positive =~ P1 + P2 + P3
Negative =~ N1 + N2 + N3 

Positive ~~ 1*Positive
Negative ~~ 1*Negative

Positive ~~ Negative
&amp;#39;

fit.group.1 &amp;lt;- cfa(group1, data=group, std.lv=TRUE, group = &amp;quot;Grade&amp;quot;)

summary(fit.group.1, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 42 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         38
## 
##   Number of observations per group         
##   7th                                              380
##   8th                                              379
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      46.102
##   Degrees of freedom                                16
##   P-value (Chi-square)                           0.000
## 
## Chi-square for each group:
## 
##   7th                                           17.124
##   8th                                           28.978
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             3598.760
##   Degrees of freedom                                30
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.992
##   Tucker-Lewis Index (TLI)                       0.984
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -2901.633
##   Loglikelihood unrestricted model (H1)      -2878.582
## 
##   Number of free parameters                         38
##   Akaike (AIC)                                5879.267
##   Bayesian (BIC)                              6055.283
##   Sample-size adjusted Bayesian (BIC)         5934.617
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.070
##   90 Percent Confidence Interval          0.047  0.095
##   P-value RMSEA &amp;lt;= 0.05                          0.072
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.029
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## 
## Group 1 [7th]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive =~                                                           
##     P1                0.571    0.028   20.208    0.000    0.571    0.856
##     P2                0.605    0.029   21.201    0.000    0.605    0.884
##     P3                0.628    0.029   21.416    0.000    0.628    0.890
##   Negative =~                                                           
##     N1                0.634    0.029   21.640    0.000    0.634    0.888
##     N2                0.585    0.027   21.458    0.000    0.585    0.884
##     N3                0.599    0.026   22.814    0.000    0.599    0.918
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive ~~                                                           
##     Negative         -0.069    0.056   -1.249    0.212   -0.069   -0.069
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .P1                3.136    0.034   91.663    0.000    3.136    4.702
##    .P2                2.991    0.035   85.211    0.000    2.991    4.371
##    .P3                3.069    0.036   84.777    0.000    3.069    4.349
##    .N1                1.701    0.037   46.482    0.000    1.701    2.384
##    .N2                1.527    0.034   44.944    0.000    1.527    2.306
##    .N3                1.545    0.033   46.195    0.000    1.545    2.370
##     Positive          0.000                               0.000    0.000
##     Negative          0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     Positive          1.000                               1.000    1.000
##     Negative          1.000                               1.000    1.000
##    .P1                0.119    0.012    9.705    0.000    0.119    0.267
##    .P2                0.102    0.012    8.337    0.000    0.102    0.218
##    .P3                0.103    0.013    8.006    0.000    0.103    0.207
##    .N1                0.107    0.012    9.214    0.000    0.107    0.211
##    .N2                0.096    0.010    9.468    0.000    0.096    0.219
##    .N3                0.067    0.009    7.307    0.000    0.067    0.157
## 
## 
## Group 2 [8th]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive =~                                                           
##     P1                0.638    0.028   22.652    0.000    0.638    0.909
##     P2                0.639    0.029   21.938    0.000    0.639    0.892
##     P3                0.713    0.030   23.800    0.000    0.713    0.937
##   Negative =~                                                           
##     N1                0.562    0.027   20.758    0.000    0.562    0.865
##     N2                0.544    0.024   22.313    0.000    0.544    0.906
##     N3                0.565    0.025   22.927    0.000    0.565    0.921
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive ~~                                                           
##     Negative         -0.321    0.050   -6.454    0.000   -0.321   -0.321
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .P1                3.073    0.036   85.224    0.000    3.073    4.378
##    .P2                2.847    0.037   77.322    0.000    2.847    3.972
##    .P3                2.979    0.039   76.197    0.000    2.979    3.914
##    .N1                1.717    0.033   51.485    0.000    1.717    2.645
##    .N2                1.580    0.031   51.175    0.000    1.580    2.629
##    .N3                1.550    0.032   49.195    0.000    1.550    2.527
##     Positive          0.000                               0.000    0.000
##     Negative          0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     Positive          1.000                               1.000    1.000
##     Negative          1.000                               1.000    1.000
##    .P1                0.085    0.009    9.009    0.000    0.085    0.173
##    .P2                0.105    0.011   10.032    0.000    0.105    0.205
##    .P3                0.071    0.010    6.908    0.000    0.071    0.123
##    .N1                0.106    0.010   10.403    0.000    0.106    0.252
##    .N2                0.065    0.008    8.293    0.000    0.065    0.180
##    .N3                0.057    0.008    7.233    0.000    0.057    0.152&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.group.1,&amp;#39;est&amp;#39;, panelGroups=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-24-week-9_files/figure-html/unnamed-chunk-40-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Need to put in labels to fix the different parameters across models&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group2 &amp;lt;- &amp;#39;Positive =~ c(L1,L1)*P1 + c(L2,L2)*P2 + c(L3,L3)*P3
Negative =~ N1 + N2 + N3 

Positive ~~ 1*Positive
Negative ~~ 1*Negative

Positive ~~ Negative
&amp;#39;

fit.group.2 &amp;lt;- cfa(group2, data=group, std.lv=TRUE, group = &amp;quot;Grade&amp;quot;)

summary(fit.group.2, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 41 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         38
##   Number of equality constraints                     3
##   Row rank of the constraints matrix                 3
## 
##   Number of observations per group         
##   7th                                              380
##   8th                                              379
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      51.058
##   Degrees of freedom                                19
##   P-value (Chi-square)                           0.000
## 
## Chi-square for each group:
## 
##   7th                                           19.934
##   8th                                           31.124
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             3598.760
##   Degrees of freedom                                30
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.991
##   Tucker-Lewis Index (TLI)                       0.986
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -2904.111
##   Loglikelihood unrestricted model (H1)      -2878.582
## 
##   Number of free parameters                         35
##   Akaike (AIC)                                5878.222
##   Bayesian (BIC)                              6040.342
##   Sample-size adjusted Bayesian (BIC)         5929.203
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.067
##   90 Percent Confidence Interval          0.045  0.089
##   P-value RMSEA &amp;lt;= 0.05                          0.098
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.048
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## 
## Group 1 [7th]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive =~                                                           
##     P1        (L1)    0.607    0.020   30.548    0.000    0.607    0.871
##     P2        (L2)    0.621    0.020   30.504    0.000    0.621    0.886
##     P3        (L3)    0.674    0.021   32.232    0.000    0.674    0.906
##   Negative =~                                                           
##     N1                0.634    0.029   21.642    0.000    0.634    0.888
##     N2                0.585    0.027   21.461    0.000    0.585    0.884
##     N3                0.599    0.026   22.816    0.000    0.599    0.918
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive ~~                                                           
##     Negative         -0.074    0.055   -1.343    0.179   -0.074   -0.074
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .P1                3.136    0.036   87.656    0.000    3.136    4.497
##    .P2                2.991    0.036   83.103    0.000    2.991    4.263
##    .P3                3.069    0.038   80.434    0.000    3.069    4.126
##    .N1                1.701    0.037   46.473    0.000    1.701    2.384
##    .N2                1.527    0.034   44.936    0.000    1.527    2.305
##    .N3                1.545    0.033   46.185    0.000    1.545    2.369
##     Positive          0.000                               0.000    0.000
##     Negative          0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     Positive          1.000                               1.000    1.000
##     Negative          1.000                               1.000    1.000
##    .P1                0.118    0.012    9.839    0.000    0.118    0.242
##    .P2                0.106    0.012    9.121    0.000    0.106    0.216
##    .P3                0.099    0.012    7.968    0.000    0.099    0.179
##    .N1                0.107    0.012    9.215    0.000    0.107    0.211
##    .N2                0.096    0.010    9.467    0.000    0.096    0.219
##    .N3                0.067    0.009    7.308    0.000    0.067    0.157
## 
## 
## Group 2 [8th]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive =~                                                           
##     P1        (L1)    0.607    0.020   30.548    0.000    0.607    0.901
##     P2        (L2)    0.621    0.020   30.504    0.000    0.621    0.888
##     P3        (L3)    0.674    0.021   32.232    0.000    0.674    0.928
##   Negative =~                                                           
##     N1                0.559    0.027   20.740    0.000    0.559    0.864
##     N2                0.542    0.024   22.303    0.000    0.542    0.905
##     N3                0.563    0.025   22.922    0.000    0.563    0.920
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive ~~                                                           
##     Negative         -0.308    0.050   -6.180    0.000   -0.308   -0.308
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .P1                3.073    0.035   88.799    0.000    3.073    4.561
##    .P2                2.847    0.036   79.220    0.000    2.847    4.069
##    .P3                2.979    0.037   79.802    0.000    2.979    4.099
##    .N1                1.717    0.033   51.640    0.000    1.717    2.653
##    .N2                1.580    0.031   51.345    0.000    1.580    2.637
##    .N3                1.550    0.031   49.363    0.000    1.550    2.536
##     Positive          0.000                               0.000    0.000
##     Negative          0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     Positive          1.000                               1.000    1.000
##     Negative          1.000                               1.000    1.000
##    .P1                0.085    0.009    9.240    0.000    0.085    0.188
##    .P2                0.104    0.010    9.940    0.000    0.104    0.212
##    .P3                0.074    0.010    7.434    0.000    0.074    0.140
##    .N1                0.106    0.010   10.399    0.000    0.106    0.254
##    .N2                0.065    0.008    8.288    0.000    0.065    0.181
##    .N3                0.057    0.008    7.227    0.000    0.057    0.153&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could now test whether constraining the groups makes the fit worse (similar to how we did it for measurement invariance)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(fit.group.1,fit.group.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Chi Square Difference Test
## 
##             Df    AIC    BIC  Chisq Chisq diff Df diff Pr(&amp;gt;Chisq)
## fit.group.1 16 5879.3 6055.3 46.102                              
## fit.group.2 19 5878.2 6040.3 51.058     4.9557       3     0.1751&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if you wanted to make everything the same for a particular type of parameter? Lavaan has an easier answer:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;group3 &amp;lt;- &amp;#39;Positive =~ P1 + P2 + P3
Negative =~ N1 + N2 + N3 

Positive ~~ 1*Positive
Negative ~~ 1*Negative

Positive ~~ Negative
&amp;#39;

fit.group.3 &amp;lt;- cfa(group3, data=group, std.lv=TRUE, group = &amp;quot;Grade&amp;quot;, group.equal = c(&amp;quot;loadings&amp;quot;, &amp;quot;intercepts&amp;quot;))

summary(fit.group.3, standardized=TRUE, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 63 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         40
##   Number of equality constraints                    12
##   Row rank of the constraints matrix                12
## 
##   Number of observations per group         
##   7th                                              380
##   8th                                              379
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      64.155
##   Degrees of freedom                                26
##   P-value (Chi-square)                           0.000
## 
## Chi-square for each group:
## 
##   7th                                           26.771
##   8th                                           37.384
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic             3598.760
##   Degrees of freedom                                30
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.989
##   Tucker-Lewis Index (TLI)                       0.988
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -2910.660
##   Loglikelihood unrestricted model (H1)      -2878.582
## 
##   Number of free parameters                         28
##   Akaike (AIC)                                5877.319
##   Bayesian (BIC)                              6007.015
##   Sample-size adjusted Bayesian (BIC)         5918.104
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.062
##   90 Percent Confidence Interval          0.043  0.082
##   P-value RMSEA &amp;lt;= 0.05                          0.137
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.059
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## 
## Group 1 [7th]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive =~                                                           
##     P1      (.p1.)    0.607    0.020   30.559    0.000    0.607    0.870
##     P2      (.p2.)    0.624    0.020   30.563    0.000    0.624    0.886
##     P3      (.p3.)    0.675    0.021   32.293    0.000    0.675    0.906
##   Negative =~                                                           
##     N1      (.p4.)    0.596    0.020   29.973    0.000    0.596    0.873
##     N2      (.p5.)    0.564    0.018   31.005    0.000    0.564    0.875
##     N3      (.p6.)    0.581    0.018   32.359    0.000    0.581    0.916
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive ~~                                                           
##     Negative         -0.072    0.055   -1.305    0.192   -0.072   -0.072
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .P1      (.16.)    3.153    0.034   92.887    0.000    3.153    4.521
##    .P2      (.17.)    2.966    0.035   85.406    0.000    2.966    4.211
##    .P3      (.18.)    3.077    0.037   83.037    0.000    3.077    4.133
##    .N1      (.19.)    1.697    0.033   50.786    0.000    1.697    2.485
##    .N2      (.20.)    1.545    0.031   49.198    0.000    1.545    2.399
##    .N3      (.21.)    1.535    0.032   48.359    0.000    1.535    2.420
##     Positiv           0.000                               0.000    0.000
##     Negativ           0.000                               0.000    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     Positive          1.000                               1.000    1.000
##     Negative          1.000                               1.000    1.000
##    .P1                0.118    0.012    9.870    0.000    0.118    0.243
##    .P2                0.107    0.012    9.109    0.000    0.107    0.215
##    .P3                0.099    0.012    7.946    0.000    0.099    0.178
##    .N1                0.111    0.011    9.751    0.000    0.111    0.237
##    .N2                0.097    0.010    9.681    0.000    0.097    0.234
##    .N3                0.065    0.009    7.382    0.000    0.065    0.162
## 
## 
## Group 2 [8th]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive =~                                                           
##     P1      (.p1.)    0.607    0.020   30.559    0.000    0.607    0.900
##     P2      (.p2.)    0.624    0.020   30.563    0.000    0.624    0.888
##     P3      (.p3.)    0.675    0.021   32.293    0.000    0.675    0.928
##   Negative =~                                                           
##     N1      (.p4.)    0.596    0.020   29.973    0.000    0.596    0.879
##     N2      (.p5.)    0.564    0.018   31.005    0.000    0.564    0.911
##     N3      (.p6.)    0.581    0.018   32.359    0.000    0.581    0.923
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   Positive ~~                                                           
##     Negative         -0.316    0.049   -6.434    0.000   -0.316   -0.316
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .P1      (.16.)    3.153    0.034   92.887    0.000    3.153    4.680
##    .P2      (.17.)    2.966    0.035   85.406    0.000    2.966    4.222
##    .P3      (.18.)    3.077    0.037   83.037    0.000    3.077    4.230
##    .N1      (.19.)    1.697    0.033   50.786    0.000    1.697    2.502
##    .N2      (.20.)    1.545    0.031   49.198    0.000    1.545    2.497
##    .N3      (.21.)    1.535    0.032   48.359    0.000    1.535    2.440
##     Positiv          -0.153    0.076   -2.028    0.043   -0.153   -0.153
##     Negativ           0.040    0.075    0.527    0.598    0.040    0.040
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##     Positive          1.000                               1.000    1.000
##     Negative          1.000                               1.000    1.000
##    .P1                0.086    0.009    9.277    0.000    0.086    0.189
##    .P2                0.104    0.010    9.930    0.000    0.104    0.211
##    .P3                0.074    0.010    7.418    0.000    0.074    0.139
##    .N1                0.104    0.010   10.299    0.000    0.104    0.227
##    .N2                0.065    0.008    8.576    0.000    0.065    0.170
##    .N3                0.058    0.008    7.722    0.000    0.058    0.147&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is nice is that the output will label what parameters are constrained. Go ahead and look at the lavaan tutorial for more (residual variances, latent means, etc)&lt;/p&gt;
&lt;div id=&#34;measurement-invariance-revisited&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Measurement invariance revisited&lt;/h3&gt;
&lt;p&gt;Can you see how MI tests are just a special type of multiple group analysis? We could set up our data to have variables assessed at two different time points with the time demarcated by a group variable. The exact same findings from MI would be found with multiple group. It all depends on how you structure your data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;when-to-use&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;When to use&lt;/h3&gt;
&lt;p&gt;For Longitudinal models you are faced with a choice: do you want to use group status to predict your slope, for example, or do you want to use multiple groups. The short answer is that it does not matter! The semi-longer answer is that including group as a predictor is simple while multiple groups analyses are more complex but also more flexible. Using one or the other depends on what your theory about where the group differences occur. If they occur in only the regression relationship indicating group mean differences then go with the easy option. If you think that the measurement model also may differ for your groups you have a more nuanced theory that necessitates multiple groups.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Demo.growth$group &amp;lt;- as.numeric(cut_number(Demo.growth$x1,2))


model.7 &amp;lt;- &amp;#39;
  # intercept and slope with fixed coefficients
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4
  # regressions
    i ~ group
    s ~ group
&amp;#39;
fit.7 &amp;lt;- growth(model.7, data = Demo.growth)
summary(fit.7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 36 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         11
## 
##   Number of observations                           400
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       8.760
##   Degrees of freedom                                 7
##   P-value (Chi-square)                           0.270
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     t1                1.000                           
##     t2                1.000                           
##     t3                1.000                           
##     t4                1.000                           
##   s =~                                                
##     t1                0.000                           
##     t2                1.000                           
##     t3                2.000                           
##     t4                3.000                           
## 
## Regressions:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~                                                 
##     group             1.148    0.142    8.060    0.000
##   s ~                                                 
##     group             0.563    0.079    7.150    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##  .i ~~                                                
##    .s                 0.456    0.062    7.413    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.000                           
##    .t2                0.000                           
##    .t3                0.000                           
##    .t4                0.000                           
##    .i                -1.107    0.225   -4.917    0.000
##    .s                 0.162    0.124    1.303    0.193
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.593    0.085    6.951    0.000
##    .t2                0.679    0.061   11.103    0.000
##    .t3                0.633    0.072    8.814    0.000
##    .t4                0.510    0.123    4.146    0.000
##    .i                 1.603    0.150   10.675    0.000
##    .s                 0.508    0.046   10.945    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.8 &amp;lt;- &amp;#39;
  # intercept and slope with fixed coefficients
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4

&amp;#39;
fit.8 &amp;lt;- growth(model.8, data = Demo.growth, group = &amp;quot;group&amp;quot;)
summary(fit.8)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 42 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         18
## 
##   Number of observations per group         
##   1                                                200
##   2                                                200
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      10.553
##   Degrees of freedom                                10
##   P-value (Chi-square)                           0.393
## 
## Chi-square for each group:
## 
##   1                                              7.914
##   2                                              2.639
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## 
## Group 1 [1]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     t1                1.000                           
##     t2                1.000                           
##     t3                1.000                           
##     t4                1.000                           
##   s =~                                                
##     t1                0.000                           
##     t2                1.000                           
##     t3                2.000                           
##     t4                3.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                 0.467    0.092    5.078    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.000                           
##    .t2                0.000                           
##    .t3                0.000                           
##    .t4                0.000                           
##     i                 0.038    0.103    0.371    0.711
##     s                 0.726    0.058   12.599    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.516    0.117    4.423    0.000
##    .t2                0.678    0.086    7.849    0.000
##    .t3                0.635    0.102    6.218    0.000
##    .t4                0.499    0.177    2.822    0.005
##     i                 1.753    0.223    7.853    0.000
##     s                 0.560    0.070    8.032    0.000
## 
## 
## Group 2 [2]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     t1                1.000                           
##     t2                1.000                           
##     t3                1.000                           
##     t4                1.000                           
##   s =~                                                
##     t1                0.000                           
##     t2                1.000                           
##     t3                2.000                           
##     t4                3.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                 0.448    0.082    5.459    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.000                           
##    .t2                0.000                           
##    .t3                0.000                           
##    .t4                0.000                           
##     i                 1.190    0.098   12.148    0.000
##     s                 1.287    0.054   24.037    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.675    0.125    5.413    0.000
##    .t2                0.676    0.086    7.853    0.000
##    .t3                0.628    0.101    6.239    0.000
##    .t4                0.530    0.171    3.109    0.002
##     i                 1.453    0.202    7.207    0.000
##     s                 0.453    0.061    7.381    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice how the slope and the intercept differ between groups. Is this meaningful? To test we will have to do the multiple groups comparison. But before we do that, take a look at the relationship between the intercept and slope for the two groups and the regression of group onto slope and intercept.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.9 &amp;lt;- &amp;#39;
  # intercept and slope with fixed coefficients
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4

&amp;#39;
fit.9 &amp;lt;- growth(model.9, data = Demo.growth, group = &amp;quot;group&amp;quot;, group.equal = &amp;quot;means&amp;quot;)
summary(fit.9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 45 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         18
##   Number of equality constraints                     2
##   Row rank of the constraints matrix                 2
## 
##   Number of observations per group         
##   1                                                200
##   2                                                200
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      94.244
##   Degrees of freedom                                12
##   P-value (Chi-square)                           0.000
## 
## Chi-square for each group:
## 
##   1                                             53.917
##   2                                             40.327
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## 
## Group 1 [1]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     t1                1.000                           
##     t2                1.000                           
##     t3                1.000                           
##     t4                1.000                           
##   s =~                                                
##     t1                0.000                           
##     t2                1.000                           
##     t3                2.000                           
##     t4                3.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                 0.660    0.108    6.093    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.000                           
##    .t2                0.000                           
##    .t3                0.000                           
##    .t4                0.000                           
##     i       (.20.)    0.662    0.077    8.632    0.000
##     s       (.21.)    1.034    0.042   24.857    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.523    0.118    4.446    0.000
##    .t2                0.672    0.086    7.804    0.000
##    .t3                0.640    0.103    6.200    0.000
##    .t4                0.493    0.178    2.764    0.006
##     i                 2.139    0.261    8.207    0.000
##     s                 0.655    0.079    8.299    0.000
## 
## 
## Group 2 [2]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     t1                1.000                           
##     t2                1.000                           
##     t3                1.000                           
##     t4                1.000                           
##   s =~                                                
##     t1                0.000                           
##     t2                1.000                           
##     t3                2.000                           
##     t4                3.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                 0.581    0.093    6.243    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.000                           
##    .t2                0.000                           
##    .t3                0.000                           
##    .t4                0.000                           
##     i       (.20.)    0.662    0.077    8.632    0.000
##     s       (.21.)    1.034    0.042   24.857    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.674    0.125    5.387    0.000
##    .t2                0.677    0.086    7.838    0.000
##    .t3                0.626    0.101    6.178    0.000
##    .t4                0.533    0.172    3.094    0.002
##     i                 1.733    0.229    7.582    0.000
##     s                 0.516    0.067    7.661    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does constraining the latent means lead to a worse fit?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(fit.8, fit.9)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Chi Square Difference Test
## 
##       Df    AIC    BIC  Chisq Chisq diff Df diff Pr(&amp;gt;Chisq)    
## fit.8 10 5459.9 5531.8 10.553                                  
## fit.9 12 5539.6 5603.5 94.244     83.691       2  &amp;lt; 2.2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes it does! Buut, how to interpret this? It is asking whether the model where we constained both the intercept and the mean differ from one that we don’t. What if we want to only look at the effect of slope?&lt;/p&gt;
&lt;p&gt;Testing just the slope difference without constraining anything else.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;model.10 &amp;lt;- &amp;#39;
  # intercept and slope with fixed coefficients
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4

s ~  c(m1, m1)*1

&amp;#39;
fit.10 &amp;lt;- growth(model.10, data = Demo.growth, group = &amp;quot;group&amp;quot;)
summary(fit.10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 44 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         18
##   Number of equality constraints                     1
##   Row rank of the constraints matrix                 1
## 
##   Number of observations per group         
##   1                                                200
##   2                                                200
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      58.413
##   Degrees of freedom                                11
##   P-value (Chi-square)                           0.000
## 
## Chi-square for each group:
## 
##   1                                             33.970
##   2                                             24.443
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## 
## Group 1 [1]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     t1                1.000                           
##     t2                1.000                           
##     t3                1.000                           
##     t4                1.000                           
##   s =~                                                
##     t1                0.000                           
##     t2                1.000                           
##     t3                2.000                           
##     t4                3.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                 0.507    0.098    5.153    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     s         (m1)    1.030    0.042   24.749    0.000
##    .t1                0.000                           
##    .t2                0.000                           
##    .t3                0.000                           
##    .t4                0.000                           
##     i                 0.179    0.102    1.766    0.077
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.502    0.117    4.291    0.000
##    .t2                0.684    0.087    7.856    0.000
##    .t3                0.640    0.104    6.152    0.000
##    .t4                0.490    0.182    2.686    0.007
##     i                 1.780    0.225    7.898    0.000
##     s                 0.654    0.079    8.317    0.000
## 
## 
## Group 2 [2]:
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i =~                                                
##     t1                1.000                           
##     t2                1.000                           
##     t3                1.000                           
##     t4                1.000                           
##   s =~                                                
##     t1                0.000                           
##     t2                1.000                           
##     t3                2.000                           
##     t4                3.000                           
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   i ~~                                                
##     s                 0.479    0.087    5.532    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##     s         (m1)    1.030    0.042   24.749    0.000
##    .t1                0.000                           
##    .t2                0.000                           
##    .t3                0.000                           
##    .t4                0.000                           
##     i                 1.076    0.097   11.116    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .t1                0.682    0.126    5.413    0.000
##    .t2                0.673    0.086    7.826    0.000
##    .t3                0.623    0.102    6.133    0.000
##    .t4                0.542    0.176    3.087    0.002
##     i                 1.464    0.203    7.209    0.000
##     s                 0.517    0.068    7.645    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(fit.8, fit.10)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Chi Square Difference Test
## 
##        Df    AIC    BIC  Chisq Chisq diff Df diff Pr(&amp;gt;Chisq)    
## fit.8  10 5459.9 5531.8 10.553                                  
## fit.10 11 5505.8 5573.6 58.413      47.86       1  4.578e-12 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What if you wanted to test differences in variance?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;planned-missing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Planned missing data&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missing data&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;power&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Power&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week #7 &amp; Week #8</title>
      <link>/workshops/week-6/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/week-6/</guid>
      <description>


&lt;div id=&#34;lavaan&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;lavaan&lt;/h1&gt;
&lt;p&gt;Easy to use SEM program in R&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is lavaan 0.6-4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan is BETA software! Please report any bugs.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Does most of what other sem packages do and just as well except for:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;advanced Multilevel SEM&lt;/li&gt;
&lt;li&gt;Latent class models/mixture models&lt;/li&gt;
&lt;li&gt;Bayesian SEM&lt;/li&gt;
&lt;li&gt;“Dynamic” SEM&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Two useful add on packages are&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(semTools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ###############################################################################&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is semTools 0.5-1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## All users of R (or SEM) are invited to submit functions or ideas for functions.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ###############################################################################&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(semPlot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 methods overwritten by &amp;#39;huge&amp;#39;:
##   method    from   
##   plot.sim  BDgraph
##   print.sim BDgraph&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A related package that uses similar syntax for Bayesian models is&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(blavaan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is blavaan 0.3-6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Major changes from blavaan 0.3-4:
##   - The default target is now &amp;#39;stan&amp;#39; instead of &amp;#39;jags&amp;#39;.
##   - To use JAGS, you now need to explicitly set target = &amp;#39;jags&amp;#39;.
##   - To use the old Stan approach, set target = &amp;#39;stanclassic&amp;#39;.
##   - Priors on variance parameters now default to gamma(1,.5) on the SD.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;keep-it-wide&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;keep it wide&lt;/h1&gt;
&lt;p&gt;Most SEM programs need wide data as opposed to the long data format we have been using for MLM.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lavaan-language&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;lavaan language&lt;/h1&gt;
&lt;p&gt;All you need to know (almost) is here:
&lt;a href=&#34;http://lavaan.ugent.be/tutorial/&#34; class=&#34;uri&#34;&gt;http://lavaan.ugent.be/tutorial/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A quick recap of that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Paths between variables is the same as our linear model syntax&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y ~ x1 + x2 + x3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;~ can be read as “is regressed on”&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;defining latent variables&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y =~ x1 + x2 + x3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;=~ can be read as “measured by”&lt;/p&gt;
&lt;p&gt;Y is measured by the variables x1 - x3. This will define the factor loadings.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;defining variances and covariances&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y ~~ x1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Y covaries with X1.&lt;/p&gt;
&lt;p&gt;The beautify of lavaan is that it will decide for you if you are interested in a variance or a covariance or a residual (co)variance.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;intercept&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y ~ 1 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Much as we saw with our lmer models where 1 served an important role, 1 here also is special in that it references the mean (intercept) of the variable. This will come in handy when we want to constrain or make the means of variables similar to one another.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;constraints&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y =~ NA*x1 + 1*x2 + a*x3 + a*x4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;NA serves to free a lavaan imposed constraint. Here, the default is to set the first factor loading to 1 to define the latent variable. NA* serves to say there is no constraint.&lt;/p&gt;
&lt;p&gt;1* pre-multiples the loading by a particular number. In this case it is 1, to define the latent variable, but it could be any number. R doesn’t know if it makes sense or not.&lt;/p&gt;
&lt;p&gt;a* (or and other character strings) serves as an equality constraint by estimating the same parameter for each term with that label. In this case x3 and x4 will have the same factor loading, referred to as a.&lt;/p&gt;
&lt;div id=&#34;how-to-run-lavaan&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to run lavaan&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specify your model&lt;/li&gt;
&lt;li&gt;Fit the model&lt;/li&gt;
&lt;li&gt;Display the summary output&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#1. Specify your model

HS.model &amp;lt;- &amp;#39; visual  =~ x1 + x2 + x3      
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 &amp;#39;

#2. Fit the model

fit &amp;lt;- cfa(HS.model, data=HolzingerSwineford1939)

  # other functions include sem, growth, and lavaan. All have different defaults (See below). we will use growth a lot. 


#3. Display the summary output

summary(fit, fit.measures=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lavaan-defaults&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;lavaan defaults&lt;/h2&gt;
&lt;p&gt;First, by default, the factor loading of the first indicator of a latent variable is fixed to 1, thereby fixing the scale of the latent variable. Second, residual variances are added automatically. And third, all exogenous latent variables are correlated by default.&lt;/p&gt;
&lt;p&gt;lets work with a dataset from the lavaan package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HolzingerSwineford1939 &amp;lt;- HolzingerSwineford1939

mod.1 &amp;lt;- &amp;#39;visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9&amp;#39;

fit.1 &amp;lt;- cfa(mod.1, data=HolzingerSwineford1939)

summary(fit.1, fit.measures=TRUE, standardized=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 35 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         21
## 
##   Number of observations                           301
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      85.306
##   Degrees of freedom                                24
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic              918.852
##   Degrees of freedom                                36
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.931
##   Tucker-Lewis Index (TLI)                       0.896
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3737.745
##   Loglikelihood unrestricted model (H1)      -3695.092
## 
##   Number of free parameters                         21
##   Akaike (AIC)                                7517.490
##   Bayesian (BIC)                              7595.339
##   Sample-size adjusted Bayesian (BIC)         7528.739
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.092
##   90 Percent Confidence Interval          0.071  0.114
##   P-value RMSEA &amp;lt;= 0.05                          0.001
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.065
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   visual =~                                                             
##     x1                1.000                               0.900    0.772
##     x2                0.554    0.100    5.554    0.000    0.498    0.424
##     x3                0.729    0.109    6.685    0.000    0.656    0.581
##   textual =~                                                            
##     x4                1.000                               0.990    0.852
##     x5                1.113    0.065   17.014    0.000    1.102    0.855
##     x6                0.926    0.055   16.703    0.000    0.917    0.838
##   speed =~                                                              
##     x7                1.000                               0.619    0.570
##     x8                1.180    0.165    7.152    0.000    0.731    0.723
##     x9                1.082    0.151    7.155    0.000    0.670    0.665
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   visual ~~                                                             
##     textual           0.408    0.074    5.552    0.000    0.459    0.459
##     speed             0.262    0.056    4.660    0.000    0.471    0.471
##   textual ~~                                                            
##     speed             0.173    0.049    3.518    0.000    0.283    0.283
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .x1                0.549    0.114    4.833    0.000    0.549    0.404
##    .x2                1.134    0.102   11.146    0.000    1.134    0.821
##    .x3                0.844    0.091    9.317    0.000    0.844    0.662
##    .x4                0.371    0.048    7.779    0.000    0.371    0.275
##    .x5                0.446    0.058    7.642    0.000    0.446    0.269
##    .x6                0.356    0.043    8.277    0.000    0.356    0.298
##    .x7                0.799    0.081    9.823    0.000    0.799    0.676
##    .x8                0.488    0.074    6.573    0.000    0.488    0.477
##    .x9                0.566    0.071    8.003    0.000    0.566    0.558
##     visual            0.809    0.145    5.564    0.000    1.000    1.000
##     textual           0.979    0.112    8.737    0.000    1.000    1.000
##     speed             0.384    0.086    4.451    0.000    1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;coding-revisited&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Coding revisited&lt;/h3&gt;
&lt;p&gt;Marker variable: if you are lazy; default. Residual variances won’t change, but the loadings do, as does the variance of the latent factor. The latent factors variance is the reliable variance of the marker variable, and the mean of the marker variable if you fit a mean. The latent variable takes on the identity, if you will, of the marker variable chosen. For this to work, one of the assumptions is that all indicators are equivalent to one another.&lt;/p&gt;
&lt;p&gt;Fixed factor: standardized, unit-free estimates. Has some nice-ities. Does not arbitrarily give more weight to one indicator. If more than one latent factor is estimated, the estimates between the factors gives the correlation. If you square the loadings and add the residual it equals 1.&lt;/p&gt;
&lt;p&gt;Effects coding: if the original metric is meaningful, keeps the latent variable in the metric of your scale. Residual variance is the same. Loadings average to 1. Latent variance is the average amount of reliable variance. To estimate you use the equation: indicator 1 = (number of indicators) - indicator N, indicator N-1… E.g. if you have three indicators (a,b,c) then: a = 3 - b - c. It would be the same if you changed it to b = 3 - a - c. &lt;/p&gt;
&lt;p&gt;Lets use a fixed factor approach rather than a marker variable approach&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2 &amp;lt;- &amp;#39;visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9&amp;#39;

fit.2 &amp;lt;- cfa(mod.2, std.lv=TRUE, data=HolzingerSwineford1939)

summary(fit.2, fit.measures=TRUE, standardized=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 20 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         21
## 
##   Number of observations                           301
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      85.306
##   Degrees of freedom                                24
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic              918.852
##   Degrees of freedom                                36
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.931
##   Tucker-Lewis Index (TLI)                       0.896
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3737.745
##   Loglikelihood unrestricted model (H1)      -3695.092
## 
##   Number of free parameters                         21
##   Akaike (AIC)                                7517.490
##   Bayesian (BIC)                              7595.339
##   Sample-size adjusted Bayesian (BIC)         7528.739
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.092
##   90 Percent Confidence Interval          0.071  0.114
##   P-value RMSEA &amp;lt;= 0.05                          0.001
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.065
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   visual =~                                                             
##     x1                0.900    0.081   11.128    0.000    0.900    0.772
##     x2                0.498    0.077    6.429    0.000    0.498    0.424
##     x3                0.656    0.074    8.817    0.000    0.656    0.581
##   textual =~                                                            
##     x4                0.990    0.057   17.474    0.000    0.990    0.852
##     x5                1.102    0.063   17.576    0.000    1.102    0.855
##     x6                0.917    0.054   17.082    0.000    0.917    0.838
##   speed =~                                                              
##     x7                0.619    0.070    8.903    0.000    0.619    0.570
##     x8                0.731    0.066   11.090    0.000    0.731    0.723
##     x9                0.670    0.065   10.305    0.000    0.670    0.665
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   visual ~~                                                             
##     textual           0.459    0.064    7.189    0.000    0.459    0.459
##     speed             0.471    0.073    6.461    0.000    0.471    0.471
##   textual ~~                                                            
##     speed             0.283    0.069    4.117    0.000    0.283    0.283
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .x1                0.549    0.114    4.833    0.000    0.549    0.404
##    .x2                1.134    0.102   11.146    0.000    1.134    0.821
##    .x3                0.844    0.091    9.317    0.000    0.844    0.662
##    .x4                0.371    0.048    7.779    0.000    0.371    0.275
##    .x5                0.446    0.058    7.642    0.000    0.446    0.269
##    .x6                0.356    0.043    8.277    0.000    0.356    0.298
##    .x7                0.799    0.081    9.823    0.000    0.799    0.676
##    .x8                0.488    0.074    6.573    0.000    0.488    0.477
##    .x9                0.566    0.071    8.003    0.000    0.566    0.558
##     visual            1.000                               1.000    1.000
##     textual           1.000                               1.000    1.000
##     speed             1.000                               1.000    1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now an effect coded one&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.3 &amp;lt;- &amp;#39; 
     visual  =~ NA*x1 + v1*x1 + v2*x2 + v3*x3 
     textual =~ NA*x4 + t1*x4 + t2*x5 + t3*x6 
     speed   =~ NA*x7 + s1*x7 + s2*x8 + s3*x9 
     
# constraints for loading
     v1 == 3 - v2 - v3 
     t1 == 3 - t2 - t3 
     s1 == 3 - s2 - s3 
&amp;#39; 

# Note that the number 3 will change depending on how many indicators you have

fit.3 &amp;lt;- cfa(mod.3, data=HolzingerSwineford1939) 
summary(fit.3, fit.measures=TRUE, standardized=TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 31 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         24
##   Number of equality constraints                     3
##   Row rank of the constraints matrix                 3
## 
##   Number of observations                           301
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      85.306
##   Degrees of freedom                                24
##   P-value (Chi-square)                           0.000
## 
## Model test baseline model:
## 
##   Minimum Function Test Statistic              918.852
##   Degrees of freedom                                36
##   P-value                                        0.000
## 
## User model versus baseline model:
## 
##   Comparative Fit Index (CFI)                    0.931
##   Tucker-Lewis Index (TLI)                       0.896
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -3737.745
##   Loglikelihood unrestricted model (H1)      -3695.092
## 
##   Number of free parameters                         21
##   Akaike (AIC)                                7517.490
##   Bayesian (BIC)                              7595.339
##   Sample-size adjusted Bayesian (BIC)         7528.739
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.092
##   90 Percent Confidence Interval          0.071  0.114
##   P-value RMSEA &amp;lt;= 0.05                          0.001
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.065
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   visual =~                                                             
##     x1        (v1)    1.314    0.101   13.037    0.000    0.900    0.772
##     x2        (v2)    0.727    0.091    8.006    0.000    0.498    0.424
##     x3        (v3)    0.958    0.089   10.744    0.000    0.656    0.581
##   textual =~                                                            
##     x4        (t1)    0.987    0.034   29.056    0.000    0.990    0.852
##     x5        (t2)    1.099    0.036   30.883    0.000    1.102    0.855
##     x6        (t3)    0.914    0.033   27.627    0.000    0.917    0.838
##   speed =~                                                              
##     x7        (s1)    0.920    0.078   11.725    0.000    0.619    0.570
##     x8        (s2)    1.085    0.081   13.381    0.000    0.731    0.723
##     x9        (s3)    0.995    0.078   12.789    0.000    0.670    0.665
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##   visual ~~                                                             
##     textual           0.315    0.055    5.736    0.000    0.459    0.459
##     speed             0.217    0.041    5.295    0.000    0.471    0.471
##   textual ~~                                                            
##     speed             0.191    0.051    3.775    0.000    0.283    0.283
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)   Std.lv  Std.all
##    .x1                0.549    0.114    4.833    0.000    0.549    0.404
##    .x2                1.134    0.102   11.146    0.000    1.134    0.821
##    .x3                0.844    0.091    9.317    0.000    0.844    0.662
##    .x4                0.371    0.048    7.779    0.000    0.371    0.275
##    .x5                0.446    0.058    7.642    0.000    0.446    0.269
##    .x6                0.356    0.043    8.277    0.000    0.356    0.298
##    .x7                0.799    0.081    9.823    0.000    0.799    0.676
##    .x8                0.488    0.074    6.573    0.000    0.488    0.477
##    .x9                0.566    0.071    8.003    0.000    0.566    0.558
##     visual            0.469    0.062    7.549    0.000    1.000    1.000
##     textual           1.005    0.093   10.823    0.000    1.000    1.000
##     speed             0.454    0.055    8.271    0.000    1.000    1.000
## 
## Constraints:
##                                                |Slack|
##     v1 - (3-v2-v3)                               0.000
##     t1 - (3-t2-t3)                               0.000
##     s1 - (3-s2-s3)                               0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;lets-try-with-means&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lets try with means&lt;/h3&gt;
&lt;p&gt;Now an effect coded one with means.&lt;/p&gt;
&lt;p&gt;First lest see what we get when we ask for a mean structures&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.1m &amp;lt;- &amp;#39;visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9

&amp;#39;

fit.1m &amp;lt;- cfa(mod.1m, meanstructure= TRUE, data=HolzingerSwineford1939)

summary(fit.1m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 35 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         30
## 
##   Number of observations                           301
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      85.306
##   Degrees of freedom                                24
##   P-value (Chi-square)                           0.000
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   visual =~                                           
##     x1                1.000                           
##     x2                0.554    0.100    5.554    0.000
##     x3                0.729    0.109    6.685    0.000
##   textual =~                                          
##     x4                1.000                           
##     x5                1.113    0.065   17.014    0.000
##     x6                0.926    0.055   16.703    0.000
##   speed =~                                            
##     x7                1.000                           
##     x8                1.180    0.165    7.152    0.000
##     x9                1.082    0.151    7.155    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   visual ~~                                           
##     textual           0.408    0.074    5.552    0.000
##     speed             0.262    0.056    4.660    0.000
##   textual ~~                                          
##     speed             0.173    0.049    3.518    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .x1                4.936    0.067   73.473    0.000
##    .x2                6.088    0.068   89.855    0.000
##    .x3                2.250    0.065   34.579    0.000
##    .x4                3.061    0.067   45.694    0.000
##    .x5                4.341    0.074   58.452    0.000
##    .x6                2.186    0.063   34.667    0.000
##    .x7                4.186    0.063   66.766    0.000
##    .x8                5.527    0.058   94.854    0.000
##    .x9                5.374    0.058   92.546    0.000
##     visual            0.000                           
##     textual           0.000                           
##     speed             0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .x1                0.549    0.114    4.833    0.000
##    .x2                1.134    0.102   11.146    0.000
##    .x3                0.844    0.091    9.317    0.000
##    .x4                0.371    0.048    7.779    0.000
##    .x5                0.446    0.058    7.642    0.000
##    .x6                0.356    0.043    8.277    0.000
##    .x7                0.799    0.081    9.823    0.000
##    .x8                0.488    0.074    6.573    0.000
##    .x9                0.566    0.071    8.003    0.000
##     visual            0.809    0.145    5.564    0.000
##     textual           0.979    0.112    8.737    0.000
##     speed             0.384    0.086    4.451    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note for both the marker and fixed factor give you the same means. This is because the latent variable means need to be scaled to zero to make the model identifiable (in this case).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2m &amp;lt;- &amp;#39;visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9


&amp;#39;

fit.2m &amp;lt;- cfa(mod.2m, std.lv=TRUE, meanstructure = TRUE, data=HolzingerSwineford1939)

summary(fit.2m)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 20 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         30
## 
##   Number of observations                           301
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      85.306
##   Degrees of freedom                                24
##   P-value (Chi-square)                           0.000
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   visual =~                                           
##     x1                0.900    0.081   11.128    0.000
##     x2                0.498    0.077    6.429    0.000
##     x3                0.656    0.074    8.817    0.000
##   textual =~                                          
##     x4                0.990    0.057   17.474    0.000
##     x5                1.102    0.063   17.576    0.000
##     x6                0.917    0.054   17.082    0.000
##   speed =~                                            
##     x7                0.619    0.070    8.903    0.000
##     x8                0.731    0.066   11.090    0.000
##     x9                0.670    0.065   10.305    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   visual ~~                                           
##     textual           0.459    0.064    7.189    0.000
##     speed             0.471    0.073    6.461    0.000
##   textual ~~                                          
##     speed             0.283    0.069    4.117    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .x1                4.936    0.067   73.473    0.000
##    .x2                6.088    0.068   89.855    0.000
##    .x3                2.250    0.065   34.579    0.000
##    .x4                3.061    0.067   45.694    0.000
##    .x5                4.341    0.074   58.452    0.000
##    .x6                2.186    0.063   34.667    0.000
##    .x7                4.186    0.063   66.766    0.000
##    .x8                5.527    0.058   94.854    0.000
##    .x9                5.374    0.058   92.546    0.000
##     visual            0.000                           
##     textual           0.000                           
##     speed             0.000                           
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .x1                0.549    0.114    4.833    0.000
##    .x2                1.134    0.102   11.146    0.000
##    .x3                0.844    0.091    9.317    0.000
##    .x4                0.371    0.048    7.779    0.000
##    .x5                0.446    0.058    7.642    0.000
##    .x6                0.356    0.043    8.277    0.000
##    .x7                0.799    0.081    9.823    0.000
##    .x8                0.488    0.074    6.573    0.000
##    .x9                0.566    0.071    8.003    0.000
##     visual            1.000                           
##     textual           1.000                           
##     speed             1.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can relax this assumption with the effect coding method using a newly implemented method&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.3m &amp;lt;- &amp;#39; 
     visual  =~  x1 + x2 + x3 


&amp;#39; 


fit.3m &amp;lt;- sem(mod.3m, meanstructure = TRUE, effect.coding = c(&amp;quot;loadings&amp;quot;, &amp;quot;intercepts&amp;quot;),  data=HolzingerSwineford1939) 
summary(fit.3m) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 46 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         11
##   Number of equality constraints                     2
##   Row rank of the constraints matrix                 2
## 
##   Number of observations                           301
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                       0.000
##   Degrees of freedom                                 0
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   visual =~                                           
##     x1                0.772    0.061   12.695    0.000
##     x2                0.601    0.077    7.783    0.000
##     x3                0.855    0.107    7.969    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .x1                0.249    0.371    0.671    0.502
##    .x2                2.442    0.471    5.181    0.000
##    .x3               -2.939    0.653   -4.502    0.000
##     visual            6.070    0.069   87.822    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .x1                0.835    0.118    7.064    0.000
##    .x2                1.065    0.105   10.177    0.000
##    .x3                0.633    0.129    4.899    0.000
##     visual            0.878    0.125    7.021    0.000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or with more code&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.3e &amp;lt;- &amp;#39; 
     visual  =~ NA*x1 + v1*x1 + v2*x2 + v3*x3 
     textual =~ NA*x4 + t1*x4 + t2*x5 + t3*x6
     speed   =~ NA*x7 + s1*x7 + s2*x8 + s3*x9

# specifying means
x1 ~ (m1)*1
x2 ~ (m2)*1
x3 ~ (m3)*1
x4 ~ (m4)*1
x5 ~ (m5)*1
x6 ~ (m6)*1
x7 ~ (m7)*1
x8 ~ (m8)*1
x9 ~ (m9)*1

visual ~ 1
textual ~ 1
speed ~ 1

# constraints for means
m1 == 0 - m2 - m3
m4 == 0 - m5 - m6
m7 == 0 - m8 - m9


&amp;#39; 


fit.3e &amp;lt;- sem(mod.3e, meanstructure = TRUE, data=HolzingerSwineford1939) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in lav_model_vcov(lavmodel = lavmodel2, lavsamplestats = lavsamplestats, : lavaan WARNING:
##     The variance-covariance matrix of the estimated parameters (vcov)
##     does not appear to be positive definite! The smallest eigenvalue
##     (= 9.868366e-18) is close to zero. This may be a symptom that the
##     model is not identified.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(fit.3e) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan 0.6-4 ended normally after 85 iterations
## 
##   Optimization method                           NLMINB
##   Number of free parameters                         36
##   Number of equality constraints                     3
##   Row rank of the constraints matrix                 3
## 
##   Number of observations                           301
## 
##   Estimator                                         ML
##   Model Fit Test Statistic                      85.306
##   Degrees of freedom                                21
##   P-value (Chi-square)                           0.000
## 
## Parameter Estimates:
## 
##   Information                                 Expected
##   Information saturated (h1) model          Structured
##   Standard Errors                             Standard
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   visual =~                                           
##     x1        (v1)    1.910    0.135   14.174    0.000
##     x2        (v2)    1.057    0.138    7.645    0.000
##     x3        (v3)    1.393    0.132   10.515    0.000
##   textual =~                                          
##     x4        (t1)    1.160    0.043   27.239    0.000
##     x5        (t2)    1.291    0.044   29.042    0.000
##     x6        (t3)    1.074    0.042   25.799    0.000
##   speed =~                                            
##     x7        (s1)    1.292    0.112   11.540    0.000
##     x8        (s2)    1.525    0.113   13.546    0.000
##     x9        (s3)    1.398    0.110   12.732    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##   visual ~~                                           
##     textual           0.184    0.031    5.909    0.000
##     speed             0.106    0.020    5.349    0.000
##   textual ~~                                          
##     speed             0.116    0.030    3.813    0.000
## 
## Intercepts:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .x1        (m1)   -0.879    0.448   -1.962    0.050
##    .x2        (m2)    2.870    0.405    7.085    0.000
##    .x3        (m3)   -1.991    0.397   -5.013    0.000
##    .x4        (m4)   -0.094    0.112   -0.832    0.405
##    .x5        (m5)    0.829    0.118    7.049    0.000
##    .x6        (m6)   -0.736    0.110   -6.716    0.000
##    .x7        (m7)   -0.440    0.397   -1.109    0.267
##    .x8        (m8)    0.069    0.409    0.168    0.867
##    .x9        (m9)    0.371    0.393    0.945    0.345
##     visual            3.044    0.026  117.478    0.000
##     textual           2.720    0.032   86.335    0.000
##     speed             3.579    0.012  293.501    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&amp;gt;|z|)
##    .x1                0.549    0.114    4.833    0.000
##    .x2                1.134    0.102   11.146    0.000
##    .x3                0.844    0.091    9.317    0.000
##    .x4                0.371    0.048    7.779    0.000
##    .x5                0.446    0.058    7.642    0.000
##    .x6                0.356    0.043    8.277    0.000
##    .x7                0.799    0.081    9.823    0.000
##    .x8                0.488    0.074    6.573    0.000
##    .x9                0.566    0.071    8.003    0.000
##     visual            0.222    0.028    7.794    0.000
##     textual           0.728    0.058   12.451    0.000
##     speed             0.230    0.027    8.357    0.000
## 
## Constraints:
##                                                |Slack|
##     m1 - (0-m2-m3)                               0.000
##     m4 - (0-m5-m6)                               0.000
##     m7 - (0-m8-m9)                               0.000&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing models&lt;/h3&gt;
&lt;p&gt;Can compare models as in mlm.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(model1, model2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Use AIC and BIC, just as with MLM. Smaller values indicate a better fit.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimators&lt;/h3&gt;
&lt;p&gt;Default in lavaan is the ML estimator, which we have seen before. There are many other options too, some of which require complete data (though see multiple imputation discussion next class).&lt;/p&gt;
&lt;p&gt;There are a number of “robust” estimates that are uniformly better. MLR is my personal choice if you go this route, but others are just as good and maybe better if you have complete data.&lt;/p&gt;
&lt;p&gt;To confuse things, there are other methods to get robust standard errors. When data are missing one can request standard errors via a number of different methods. To do so one needs to first specify that data are missing via missing = “ML” in the fitting function. Then use the se function to specify what you want.&lt;/p&gt;
&lt;p&gt;Bootstrapped estimates are also available with se = “bootstrap”&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week #7 &amp; week #8</title>
      <link>/lectures/week-6/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/week-6/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;longitudinal-structural-equation-modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Longitudinal Structural Equation Modeling&lt;/h1&gt;
&lt;p&gt;SEM is the broader umbrella from the GLM. With it we are able to do two interesting this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Fit a latent measurement model (e.g., CFA)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Fit a structural model (e.g,. path analysis)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two components allow us to address more difficult research questions involving but not limited to: multiple DVs, mediators, varying effects across time, unmeasured variables, constraints, and measurement invariance.&lt;/p&gt;
&lt;p&gt;Compared to MLM, and SEM approach to longitudinal data may be better suited to your research goals. We will see that at one level the two approaches can be equivalent.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sem-primer&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;SEM primer&lt;/h1&gt;
&lt;p&gt;In our studies some of the variables of interest will be measured and others will be latent or unobserved. Measured variables are the observed scores that are typically collected in a research study. That is, your variables in your dataset. This may include responses to survey items, reports of age or income, or performance on a recall task. We will use the general term “indicators” or “items” or “manifest variables” to refer to these measures variables. They will be indicated by boxes in a path diagram.&lt;/p&gt;
&lt;p&gt;Latent variables, in contrast, are not directly observed. A latent variable is something that we assume to exist but we cannot directly measure (see) it. They are typically postulated by theories or inferred from the statistical behavior of measured variables. Sounds like psychological variables! Examples of latent variables include “depression,” “job satisfaction,” and “working memory capacity.” Latent variables will be represented as a circle in path diagrams.&lt;/p&gt;
&lt;p&gt;For example, why does Sally like to go to parties, likes to talk a lot, and always tends to be a in a good mood? Maybe it is because her high levels of extraversion ( a latent variable that we cannot directly measure) is causing these tendencies.&lt;/p&gt;
&lt;p&gt;Key point: the variable/construct itself is not measurable, but the manifestations caused by the variable are measurable/observable.&lt;/p&gt;
&lt;p&gt;Interesting point: because variables are assumed to be causing indicators of the variable, SEM is sometimes referred to as causal modeling. (Also because in path models a directional relationship is hypothesized) Note that we cannot get any closer to causality than we can with regression.&lt;/p&gt;
&lt;div id=&#34;pretty-pictures&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pretty pictures&lt;/h2&gt;
&lt;p&gt;Circles = latent variables&lt;/p&gt;
&lt;p&gt;Boxes = observed indicator variables&lt;/p&gt;
&lt;p&gt;two headed arrows = correlations/covariances/variances&lt;/p&gt;
&lt;p&gt;single head arrows = regressions&lt;/p&gt;
&lt;p&gt;triangle = means&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lavaan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is lavaan 0.6-4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## lavaan is BETA software! Please report any bugs.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(semPlot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 methods overwritten by &amp;#39;huge&amp;#39;:
##   method    from   
##   plot.sim  BDgraph
##   print.sim BDgraph&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HolzingerSwineford1939 &amp;lt;- HolzingerSwineford1939

mod.1 &amp;lt;- &amp;#39;visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9&amp;#39;

fit.1 &amp;lt;- cfa(mod.1, data=HolzingerSwineford1939,meanstructure = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.1,  intercepts = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;semPaths(fit.1, &amp;quot;std&amp;quot;, intercepts = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measurement model&lt;/h2&gt;
&lt;p&gt;The first step of an SEM model with latent variables is to define them. This is called specifying the measurement model. It is up to you to specify how you think the latent variable is created. It is then up to you to compare that measurement model against alternative measurement models to see if it meaningful.&lt;/p&gt;
&lt;p&gt;The key components are the a) factor loadings, b) residuals, and c) variance of latent variable.&lt;/p&gt;
&lt;div id=&#34;classical-test-theory-interpretation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Classical test theory interpretation&lt;/h3&gt;
&lt;p&gt;How can we think of a latent construct:&lt;/p&gt;
&lt;p&gt;Latent construct = what the indicators share in common&lt;/p&gt;
&lt;p&gt;The indicators represent the sum of True Score variance + Item specific variance + Random error&lt;/p&gt;
&lt;p&gt;The variance of the latent variable represents the amount of common information in the latent variable. If your indicators are haphazardly chosen then there will be low variance. We want to maximize variance here as variance suggests meaningful differences can be made between people.&lt;/p&gt;
&lt;p&gt;The residual errors (sometimes referred to as disturbances) represent the amount of information unique to each indicator. A combination of error and item-specific variance.&lt;/p&gt;
&lt;p&gt;The extent of the connection between the latent variable and the indicators is represented as a factor loading.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;generizability-interpretation-of-latent-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generizability interpretation of latent variables&lt;/h3&gt;
&lt;p&gt;Same as above, but…&lt;/p&gt;
&lt;p&gt;True score variance can be thought of as consisting as a combination of
1. Construct variance- this is the truest true score variance
2. Method variance- see Campbell and Fiske or sludge factor of Meehl.
3. Occasion- important for longitudinal, aging, and cohort analyses–and for this class.&lt;/p&gt;
&lt;p&gt;For longitudinal models, occasion specific variance can lead to biased estimates. We want to separate the time specific variance from the overall construct variance. Or, we want to make sure that the time specific variance doesn’t make it appear that a construct is changing when really it is not.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;formative-indicators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Formative indicators&lt;/h3&gt;
&lt;p&gt;These pretty pictures imply that the latent variables “cause” the indicators. This is the standard view and are referred to as reflexive indicators. However, there is another approach, formative indicators, were indicators “cause” the latent variable. Or, in other words, the latent variable doesn’t actually exist. It is not real, only a combination of variables. An example of this is SES. SES does not ‘exist’ but is a socially constructed idea. Some people have argued that psychological constructs are of this kind, and that reflexive indicators are inappropriate. The arguments are mainly philosophical and thus is beyond the scope of our discussion. Suffice to say, that most of the time there are few analytic differences. You can think of this as similar to the factor analysis vs principal component analysis debates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-error&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Measurement error&lt;/h3&gt;
&lt;p&gt;A major advantage is that each latent variable does not contain measurement error. It is as is if we measured our variable with an alpha = 1.&lt;/p&gt;
&lt;p&gt;What does that do? Well, ideally that gets us closer to the population model, which could yield higher R2 and parameter estimates.&lt;/p&gt;
&lt;p&gt;How does this happen? It is a direct result of capturing what is shared among the indicators. The measurement error associated with each indicator is uncorrelated with the latent variable.&lt;/p&gt;
&lt;p&gt;Think about how this situation differs from creating a composite among variables. Think about how this differs from creating a factor score among variables within a simple factor analyses approach. How are all three different and similar? What does it mean if the error variances are correlated with one another?&lt;/p&gt;
&lt;p&gt;Remember however, that it is theoretically error free. The latent variable is not only filled with true score variance. Instead it could have method and occasion variance. Unless you have multiple methods and occasions it is hard to parse them apart.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regarding-means&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Regarding means&lt;/h3&gt;
&lt;p&gt;SEM is also known as covariance structure analysis. You can do SEM using only variance-covariance matrices. These do not necessarily involve any direct information about their means. Means in SEM are optional. This is cool because you can technically reproduce the analyses of a paper if they give you a correlation matrix of study variables.&lt;/p&gt;
&lt;p&gt;Given we are interested in change across time, however, we will be interested in means. Latent variables by themselves do not have any inherent metric, it is up to us to choose the scale they are on. We can standardize them, use the original metric, and more! More later on how we define the mean of a latent variable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;path-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Path model&lt;/h2&gt;
&lt;p&gt;The path model component can be in addition to a measurement model or separate from them. You have already worked with path models as a simple regression is a path model, so is a standard mediation. You can make the path models more complex than these though, by specifying relationships among many variables.&lt;/p&gt;
&lt;p&gt;An example with no measurement model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate data dataset:
X &amp;lt;- rnorm(100)
M &amp;lt;- rnorm(100)
Y &amp;lt;- rnorm(100) 
data &amp;lt;- data.frame(X, Y, M)

# Two regressions:
res1 &amp;lt;- lm(M ~ X, data = data)
res2 &amp;lt;- lm(Y ~ X + M, data = data)

# Plot mediation
semPaths(res1 + res2, &amp;quot;model&amp;quot;, &amp;quot;est&amp;quot;, intercepts = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;mediation model with measurement model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2 &amp;lt;- &amp;#39;visual =~ x1 + x2 + x3
textual =~ x4 + x5 + x6
speed =~ x7 + x8 + x9

speed ~ textual + visual
textual ~ visual&amp;#39;

fit.2 &amp;lt;- cfa(mod.2, data=HolzingerSwineford1939)
semPaths(fit.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-10-09-week-6_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-an-sem-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Estimating an SEM model&lt;/h2&gt;
&lt;p&gt;Our goal when doing SEM is a creation of a model that specifies certain relationship among variables. This is done by creating a measurement or path model that we think is driven by the data generating process we are trying to study. In addition to setting the measurement model and paths we may want to put apriori constraining parameters (variances/covariances/regressions) to reflect how we think variables are related.&lt;/p&gt;
&lt;p&gt;E.g., Should these two variables be correlated or not?&lt;/p&gt;
&lt;p&gt;Then we use or ML algorithm to get our model implied covariances/means as close as possible to the observed covariances/means.&lt;/p&gt;
&lt;p&gt;Of note, SEM can handle any time of measured DV/IV or construct/indicators. If you have categorical indicators you can do SEM. However, it is hard to measure change using categorical indicators. But, categorical indicators are used for many latent variable models such as in measuring psychopathology.&lt;/p&gt;
&lt;p&gt;If you have a categorical construct you can also do SEM. Here it is called latent transition analysis (if you also had categorical indicators) or latent class / latent mixture modeling if you had continuous indicators (i am counting ordinal as continuous).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setting-the-scale-and-defining-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setting the scale and defining variables&lt;/h2&gt;
&lt;p&gt;We are trying to measure clouds. How can we do this given that they are always moving? Need to define the scale of a latent variable because there is no inherent scale of measurement. Largely irrelevant as to what scale is chosen just as centering or standardizing yield no substantive changes. Instead, scaling serves to establish a point of reference so as to interpret other parameters (much like the justifications for centering and standardizing).&lt;/p&gt;
&lt;p&gt;3 options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Fixed factor. Here you fix the variance of the latent variable to 1 (standardized).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Marker variable. Here you fix one factor loading to 1. All other loadings are relative to this loading. The variance of the latent variable can thus be anything. This is often the default of software programs.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Effect coding. Here you constrain loading to average to 1. This will be helpful for us as we can then put the scale of measurement into our original metric. For longitudinal models this is helpful in terms of how to interpret the amount of change.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;identification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Identification&lt;/h2&gt;
&lt;p&gt;Note that we have multiple parameters we are trying to estimate. Paths (regression coefficients), means, variances (of manifest variables and latent variables). This makes estimating a little more tricky as you cannot have more unknowns than knowns. There are some tricks and rules of hand about what you can estimate or not. Typically however, it is suggested that you have 3 indicators per latent variable.&lt;/p&gt;
&lt;p&gt;If you are asking too much for a model you can also constrain parameters to be the same. For example, you may assume that all residual variances are the same. This assumption can be built into your model and reduces the number of parameters you have to estimate.&lt;/p&gt;
&lt;p&gt;More specifically, you need to compare the number of knows (variances and covariances) to the unknowns (model parameters).&lt;/p&gt;
&lt;p&gt;Foe example, a three indicator latent variable has 7 unknowns. 3 Loadings, 3 error variances and the variance of the latent variable&lt;/p&gt;
&lt;p&gt;The covariance matrix has 6 data points. Thus we need to add in one more known, in this case a fixed factor or a marker variable.&lt;/p&gt;
&lt;p&gt;Knowns - unknowns = df. Note that df in this case df will not directly relate to sample size, so it is a little different than typical degree of freedom concepts.&lt;/p&gt;
&lt;p&gt;However, we will use this version of a df because it corresponds to how we will test the fit of these models. Specifically, the difference in dfs between models is distributed as a chi-square.&lt;/p&gt;
&lt;div id=&#34;types-of-identification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Types of identification&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Just identified is where the number of knowns equal unknowns. Also known as saturated model. When you evaluate the fit of the model these will be perfect. So while these will estimate, we cannot examine whether or not our model is a good representation of the world, as we are simply recreating the observed covariance matrix (data). For some instances this may not be a problem, for others…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Over identified is when you have more knowns than unknowns. This is good as we can fit a model that is more parsimonious than our data. Moreover, we can examine fit stats.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Under identified is when you have problems and have more unknowns than knowns. this is because there is more than one solution available and the algorithm cannot decide e.g,. 2 + X = Y. If we add a constraint or a known value then it becomes manageable 2 + X = 12&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-indices&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit Indices&lt;/h3&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;residuals. Good to check.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;modification indices. Check to see if missing parameters that residuals may suggest you didn’t include or should include. Can test with more advanced techniques. But eh… makes your models non-theoretical, could be over fitting, relying too much on sig tests…&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;chi-square. (Statistical fit) Implied versus observed data, tests to see if model are exact fit with data. But eh…too impacted by sample size&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;RMSEA or SRMR (Absolute fit). Does not compare to a null model. Judges distance from perfect fit.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Above .10 poor fit
Below .08 acceptable&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;CFI, TFI (Relative fit models). Compares relative to a null model. Judges distance from the worse fit ie a null model. Null models have no covariance among observed and latent variables.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;range from 0-1. Indicate % of improvement from the null model to a saturated i.e. just identified model.&lt;/p&gt;
&lt;p&gt;Usually &amp;gt;.9 is okay. Some care about &amp;gt; .95&lt;/p&gt;
&lt;p&gt;Minor changes to the model can improve fit.&lt;/p&gt;
&lt;ol start=&#34;6&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Check the model parameters. Are they wonky? Easy to get negative variances or correlations above 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;parcels&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Parcels&lt;/h3&gt;
&lt;p&gt;It is often necessary to simplify your model. One option to do so is with parcels where you combine indicators into a composite. This simplifies the model in that you have fewer parameters to fit. In addition to being a way to get a model identified, it also has benefits in terms of the assumptions of the indicator variables.&lt;/p&gt;
&lt;p&gt;To do so, you can combine items however you want into 3 or 4 groups or parcels, averaging them together. You may balance highly loading with less highly loading items (item to construct technique) or you may pair pos and negatively keyed items together. It is up to you.&lt;/p&gt;
&lt;p&gt;Some dislike it because you are aggregating without taking into account the association between the indicators; it is a blind procedure based on theory/assumptions rather than maths. ¯_(ツ)_/¯&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Homework #3</title>
      <link>/homeworks/homework-3/</link>
      <pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/homeworks/homework-3/</guid>
      <description>


&lt;p&gt;This homework introduces you to brms. Because I want you to feel comfortable with the software first we will be reanalyzing past datasets that were done within &lt;code&gt;lme4&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Please send an email and Rmd (and pdf) files to: &lt;a href=&#34;mailto:Homewor.r53rwfxazn4wnxur@u.box.com&#34; class=&#34;email&#34;&gt;Homewor.r53rwfxazn4wnxur@u.box.com&lt;/a&gt; due date is: 10/18&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Rerun one model from question #1 from homework #2 in brms. Compare the output with the lme4 model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;What are the priors on this model? Graph one of them and explain what the distributions means about what you expect to happen.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Get the posterior samples into a dataframe. What does this dataframe contain? To answer describe why are there the number of rows there, why that many columns? What do each of the columns represent?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Graph the distribution of random effects of the slope parameter. Interpret what this graph tells you.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Change the default priors of this model to something that a) regularizes the model and b) that does the opposite, one that is wildly wrong/optomostic about the effects. What happens in each of these models?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduce a covariate to the model. Run the new model and empirically comapre the fit of the model to the model you ran in #1.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Hw#3 posted</title>
      <link>/post/hw-posted/</link>
      <pubDate>Fri, 04 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/hw-posted/</guid>
      <description>


&lt;p&gt;Due date is: 10/18&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Homework #3 is delayed.</title>
      <link>/post/homework-3-is-delayed/</link>
      <pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/homework-3-is-delayed/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>week 5 &amp;6 intensive longitudinal</title>
      <link>/lectures/week-5-intenslive-longitudinal/</link>
      <pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/week-5-intenslive-longitudinal/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intensive-longitudinal-designs&#34;&gt;Intensive Longitudinal Designs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#within-versus-between-person-processes&#34;&gt;Within versus between person processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#accounting-for-time&#34;&gt;Accounting for time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-importance-of-centering.&#34;&gt;The importance of centering.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lagged-associations&#34;&gt;Lagged associations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lag-as-moderator&#34;&gt;Lag as moderator?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#within-person-mediation&#34;&gt;Within person mediation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling-residual-correlation&#34;&gt;Modeling Residual Correlation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;intensive-longitudinal-designs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Intensive Longitudinal Designs&lt;/h1&gt;
&lt;p&gt;Often we are not interested in looking at trajectories across time. Instead we are interested in using time as a means to look at fluctuations in our DV. Fluctuations are especially helpful to understand within person processes versus between person processes.&lt;/p&gt;
&lt;p&gt;Typically, the focus of these sorts of analyses are with level 1 (time varying) covariates ie variables you have assessed more than once. The questions that can be answered with this design are many, but some examples: Are there associations between variation in X and variation in Y; is X associated with greater levels of Y; does X relate to later levels of Y; Does X relate to later levels of Y after accounting for concurrent levels of X. Notice how all of these have the flavor of standard regression interpretations rather than the models we have been working with. The reason is that that intensive longitudinal designs are less likely to focus on time as a meaningful metric. Instead the focus on accounting for time, or the processes that unfold across time, not time per se.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;within-versus-between-person-processes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Within versus between person processes&lt;/h1&gt;
&lt;p&gt;Get comfortable with playing around between these two levels. Does doing more homework lead to better grades? Can ask this at a between person and a within person level. Between: do people who on average study more tend to get higher grades? Within: When I study more do I get a higher grade? The answers are not necessarily the same.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1          ✔ purrr   0.3.2     
## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ readr   1.3.1          ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simp&amp;lt;- tribble(
  ~ID, ~group,  ~test.score, ~study,
1,1,5,1,
2,1,7,3,
3,2,4,1,
4,2,6,4,
5,3,3,3,
6,3,5,5,
7,4,2,4,
8,4,4,6,
9,5,1,5,
10,5,3,7)

ggplot(simp, aes(x=study, y=test.score, group = group)) +
    geom_point() +   
    geom_smooth(method=lm,  
                se=FALSE) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-26-week-5-intenslive-longitudinal_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Also the relationship between a within person association and a between person association are likely correlated with one another. Someone who studies a lot on average may be more or less likely to study for a particular test.&lt;/p&gt;
&lt;p&gt;Take another example: affect. Those who feel happy have more friends. How would you answer this? Well it is actually two questions. First, a between person one where those that are happier in general tend to have more friends. Second, if I go and get more friends do I become happier? These sorts of questions are useful to ask when, for example, you are thinking about interventions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;accounting-for-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Accounting for time&lt;/h1&gt;
&lt;p&gt;Usually time is not an important factor for studies that last for weeks. What happens to the time variable? Two options:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Ignore it.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use it to control, but mostly disregard.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Level 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}(Exercise_{ij}-\overline{Exercise_{j}}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}\overline{Exercise_{j}} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}\overline{Exercise_{j}} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20}  \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-importance-of-centering.&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The importance of centering.&lt;/h1&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} =  [\gamma_{00} +   \gamma_{10}Time_{ij}   + \gamma_{20}(Exercise_{ij}-\overline{Exercise_{j}}) + \gamma_{30}Mood_{ij} +\gamma_{40}(Mood*(Exercise_{ij}-\overline{Exercise_{j}}))_{ij} ] + [ U_{0j}  + U_{1j}Time_{ij}+ \varepsilon_{ij}] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What happens if we want to add a level 2 predictor?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} =  [\gamma_{00} +   \gamma_{10}Time_{ij}   + \gamma_{20}(Exercise_{ij}-\overline{Exercise_{j}}) + \gamma_{30}Mood_{ij} +\gamma_{31}(Mood*\overline{Exercise_{j}})_{j}  +\gamma_{40}(Mood*(Exercise_{ij}-\overline{Exercise_{j}}))_{ij} ] + [ U_{0j}  + U_{1j}Time_{ij}+ \varepsilon_{ij}] \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;lagged-associations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lagged associations&lt;/h1&gt;
&lt;p&gt;What if I want to predict something in the future? Such that my exercise today is associated with future health gains.&lt;/p&gt;
&lt;p&gt;Level 1
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}(Exercise_{(i-t)j}-\overline{Exercise_{j}}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}\overline{Exercise_{j}} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}\overline{Exercise_{j}} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20}  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Do I need to account for time also?&lt;/p&gt;
&lt;div id=&#34;lag-as-moderator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lag as moderator?&lt;/h2&gt;
&lt;p&gt;Is your X - &amp;gt; Y association dependent on time between assessments.&lt;/p&gt;
&lt;p&gt;Create a new lag variable that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Lag_{ij} + \beta_{2j}(Exercise_{(i-t)j}-\overline{Exercise_{j}}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}\overline{Exercise_{j}} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}\overline{Exercise_{j}} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20}  \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;within-person-mediation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Within person mediation&lt;/h1&gt;
&lt;p&gt;Pick your poison. Overall, Josh is very &lt;code&gt;blah&lt;/code&gt; about MLMM&lt;/p&gt;
&lt;p&gt;1-1-1
2-1-1
2-2-1?&lt;/p&gt;
&lt;p&gt;see:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://quantpsy.org/pubs/bauer_preacher_gil_2006.pdf&#34; class=&#34;uri&#34;&gt;http://quantpsy.org/pubs/bauer_preacher_gil_2006.pdf&lt;/a&gt; for a general overview&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://vuorre.netlify.com/pdf/2017-vuorre-bolger.pdf&#34; class=&#34;uri&#34;&gt;https://vuorre.netlify.com/pdf/2017-vuorre-bolger.pdf&lt;/a&gt; for a experimental plus Bayesian perspective&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.statmodel.com/download/pzz_012610_for_web.pdf&#34; class=&#34;uri&#34;&gt;https://www.statmodel.com/download/pzz_012610_for_web.pdf&lt;/a&gt; for why MLM might not be best and using SEM along with MLM is preferable.&lt;/p&gt;
&lt;p&gt;We will talk more about longitudinal mediation models when we cover SEM approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-residual-correlation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling Residual Correlation&lt;/h1&gt;
&lt;p&gt;Typically we model error assuming that there is $ {}_{ij} (0, ^{2}) $ such that there is no correlation among residuals. This is likely met when we have small number of repeated measures, as a linear trajectory likely captures the data well, and all of the deviations from that trajectory are likely noise. However, when we have a lot of repeated measures (and where we not be modeling time systematically) there is more likelihood that we will have correlated residuals. This is problematic. We got rid of a similar concern of correlated errors by fitting MLMs in the first place (ie nesting observations within person), but this different.&lt;/p&gt;
&lt;p&gt;This is hard to do within ‘lme4’ but we can do it easier within Bayesian frameworks&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Homework #2</title>
      <link>/homeworks/homework-2/</link>
      <pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/homeworks/homework-2/</guid>
      <description>


&lt;p&gt;Submit homework Rmd and pdf file to: &lt;a href=&#34;mailto:Homewor.c9u2tv5bm4mb6nbs@u.box.com&#34; class=&#34;email&#34;&gt;Homewor.c9u2tv5bm4mb6nbs@u.box.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Using example data set 4 from workshop #3 (Multiple Waves, Different Variables ), to answer the following questions. There are 7 waves of data. There are five different types of repeated measures variables, all from “the Big Five” of personality. Each of these five constructs have 9 items associated with them. See codebook for more details.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create composites of each of the 5 constructs and run growth models on each of these composite constructs. Include varying intercept and slopes for each of the five models. Bonus points for doing so in purrr. Interpret the parameters for 1 of the models that you are interested in.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduce age and sex as predictors to the slope and intercept to the same model you interpreted in #1. Interpret each of the parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the output to create a table, suitable for publication in a journal.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Plot the predicted fixed effects along with the random effects on the same plot.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduce one of the remaining 4 repeated measures as a time varying covariate (tvc) at level 1. First, run the model where the time varying covariate is constrained to be equal for everyone. Interpret the parameters of your model.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run a second tvc model where the new repeated measure is allowed to vary randomly across people. Interpret. Then compare this one with the one where it is constrained to be equal using a likelihood ratio test. Interpret.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Homework #2 is live</title>
      <link>/post/homework-2-is-live/</link>
      <pubDate>Fri, 20 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/homework-2-is-live/</guid>
      <description>&lt;p&gt;Hw#2 is posted. Please try to complete by next Thursday&amp;rsquo;s class. If you need extra time or questions answered,  reach out.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Workshop #4</title>
      <link>/workshops/workshop-4/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-4/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#workspace&#34;&gt;Workspace&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#packages&#34;&gt;Packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#read-in-data&#34;&gt;Read in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#restructure-data&#34;&gt;Restructure Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-basic-growth-model&#34;&gt;The Basic Growth Model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#predicted-values&#34;&gt;Predicted Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot&#34;&gt;Plot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-level-categorical-moderator&#34;&gt;Two-Level Categorical Moderator&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#three-level-categorical-moderator&#34;&gt;Three-Level Categorical Moderator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-time-invariant-moderator&#34;&gt;Continuous Time-Invariant Moderator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/emoriebeck/R-tutorials/master/ALDA/week_4_plotting/week_4_plotting.Rmd&#34; download&gt;Download .Rmd (won’t work in Safari or IE)&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://github.com/emoriebeck/R-tutorials/tree/master/ALDA/week_4_plotting&#34; target=&#34;_blank&#34;&gt;See GitHub Repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For a much more exhaustive tutorial, see &lt;a href=&#34;https://raw.githubusercontent.com/emoriebeck/R-tutorials/master/mlm/Conditional_Models_doc.Rmd&#34; download&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;workspace&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Workspace&lt;/h1&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;p&gt;First let’s load in our packages. We’re going to load in a couple of extras (&lt;code&gt;brms&lt;/code&gt; and &lt;code&gt;tidybayes&lt;/code&gt;) that we haven’t used before. These will let us make some pretty plots later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)
library(knitr)
library(kableExtra)
library(lme4)
library(broom.mixed)
library(brms)
library(tidybayes)
library(plyr)
library(tidyverse)

data_path &amp;lt;- &amp;quot;https://github.com/emoriebeck/R-tutorials/raw/master&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;read-in-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Read in Data&lt;/h2&gt;
&lt;p&gt;The National Longitudinal Study of Youths 1979 Child and Young Adult Sample (NLSYCYA) is a longitudinal study conducted by the National Bureau of Labor Statistics. The sample includes the children of the original 1979 sample, of which we will use a small subset. Here, we are going to use a subset of the more than 11,000 variables available that include the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Item Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Time-Varying?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;PROC_CID&lt;/td&gt;
&lt;td&gt;Participant ID&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Dem_DOB&lt;/td&gt;
&lt;td&gt;Year of Date of Birth&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;groups&lt;/td&gt;
&lt;td&gt;Jail, Community Service, None&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;DemPWeight&lt;/td&gt;
&lt;td&gt;Weight Percentile at age 10&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td&gt;Age of participant&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Year&lt;/td&gt;
&lt;td&gt;Year of Survey&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;age0&lt;/td&gt;
&lt;td&gt;Age of participant (centered)&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;SensSeek&lt;/td&gt;
&lt;td&gt;Sensation-Seeking Composite&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;CESD&lt;/td&gt;
&lt;td&gt;CESD Depression Composite&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(url(sprintf(&amp;quot;%s/ALDA/week_4_plotting/data/sample.RData&amp;quot;, data_path)))

sample_dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2,084 x 8
##    PROC_CID   age  year  age0 groups    CESD SensSeek DemPweight
##       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1     1601    16  2006     2 CommServ 0.429     3.67     0.816 
##  2     1601    18  2008     4 CommServ 2         3        0.816 
##  3     9102    16  2012     2 None     0.182     3.33     0.671 
##  4     9501    14  2000     0 Jail     0.5       3        0.548 
##  5     9501    18  2004     4 Jail     0.429     3        0.548 
##  6     9501    22  2008     8 Jail     0.429     3        0.548 
##  7     9502    14  2002     0 Jail     0.143     3        0.421 
##  8     9502    16  2004     2 Jail     0.286     3        0.421 
##  9     9502    20  2008     6 Jail     0         3        0.421 
## 10     9503    16  2004     2 Jail     1.71      3        0.0314
## # … with 2,074 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;restructure-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Restructure Data&lt;/h2&gt;
&lt;p&gt;These data are already largely cleaned as that is not our focus today. We just need to do a little bit of restructuring.&lt;/p&gt;
&lt;p&gt;To run our models using &lt;code&gt;purrr&lt;/code&gt;, we need to restrucutre the data to long. While we’re at it, we need to create a time variable centered at zero, so we can interpret our moderators later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df1_long &amp;lt;- sample_dat %&amp;gt;%
  gather(key = trait, value = value, CESD, SensSeek, na.rm = T) %&amp;gt;%
   mutate(wave = year - 1996,
          age0 = age-16)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4,168 x 9
##    PROC_CID   age  year  age0 groups   DemPweight trait value  wave
##       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1     1601    16  2006     0 CommServ     0.816  CESD  0.429    10
##  2     1601    18  2008     2 CommServ     0.816  CESD  2        12
##  3     9102    16  2012     0 None         0.671  CESD  0.182    16
##  4     9501    14  2000    -2 Jail         0.548  CESD  0.5       4
##  5     9501    18  2004     2 Jail         0.548  CESD  0.429     8
##  6     9501    22  2008     6 Jail         0.548  CESD  0.429    12
##  7     9502    14  2002    -2 Jail         0.421  CESD  0.143     6
##  8     9502    16  2004     0 Jail         0.421  CESD  0.286     8
##  9     9502    20  2008     4 Jail         0.421  CESD  0        12
## 10     9503    16  2004     0 Jail         0.0314 CESD  1.71      8
## # … with 4,158 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-basic-growth-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Basic Growth Model&lt;/h1&gt;
&lt;p&gt;We’ll start with the basic growth model (just looking at change in a single variable over time).&lt;/p&gt;
&lt;p&gt;we will use list columns to do it. We’ll start by using the &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;nest()&lt;/code&gt; functions from &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; to put the data for each trait into a cell of our data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df1_nested &amp;lt;- df1_long %&amp;gt;%
  group_by(trait) %&amp;gt;%
  nest())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
## # Groups:   trait [2]
##   trait              data
##   &amp;lt;chr&amp;gt;    &amp;lt;list&amp;lt;df[,8]&amp;gt;&amp;gt;
## 1 CESD        [2,084 × 8]
## 2 SensSeek    [2,084 × 8]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, our data frame is 2 x 2, with the elements in the second column each containing the data frame that corresponds to that trait. This makes it really easy to run our models using the &lt;code&gt;map()&lt;/code&gt; family of unctions from &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before we fit the full growth model, we will first fit the unconditional model. Below, we will add a new column to our data frame that will contain the unconditional model for each trait.&lt;/p&gt;
In this case the model will be in the form of:
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fit0 = map(data, ~lmer(value ~ 1 + (1 | PROC_CID), data = .))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
## # Groups:   trait [2]
##   trait              data fit0     
##   &amp;lt;chr&amp;gt;    &amp;lt;list&amp;lt;df[,8]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;   
## 1 CESD        [2,084 × 8] &amp;lt;lmerMod&amp;gt;
## 2 SensSeek    [2,084 × 8] &amp;lt;lmerMod&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can see we have a new list column in our data frame called fit0 that contains an S4 class lmerMod, which simply means your growth model. To understand model, I personally find it easiest to visualize it. What this model is telling us is the mean across all observations as well as the between-person variability in that estimate.&lt;/p&gt;
&lt;p&gt;Now, moving on to the growth model:&lt;/p&gt;
In this case the model will be in the form of:
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \beta_{1j}*time_{ij} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1j} = \gamma_{10} + U_{1j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fit2 = map(data, ~lmer(value ~ 1 + age0 + (age0 | PROC_CID), data = .))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
## # Groups:   trait [2]
##   trait              data fit0      fit2     
##   &amp;lt;chr&amp;gt;    &amp;lt;list&amp;lt;df[,8]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;   
## 1 CESD        [2,084 × 8] &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 2 SensSeek    [2,084 × 8] &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve run our model, we are ready to plot the results.&lt;/p&gt;
&lt;p&gt;To do so, we’ll write a short function that will get the predicted values both for group level effects, as well as for individual-level estimates. There’s also a hidden function for getting standard errors of our pointwise estimates that we’ll use to create confidence bands in our plots. Don’t worry about that.&lt;/p&gt;
&lt;div id=&#34;predicted-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predicted Values&lt;/h2&gt;
&lt;p&gt;Now we’ll use the predict function to get predicted values and the function I created to get pointwise standard errors for the fixed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function to get fixed effects
fixed_pred_fun &amp;lt;- function(m){
  frame &amp;lt;- tibble(Intercept = 1, age0 = seq(0, 8, .1)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16)
  frame$SE &amp;lt;- pv_fun(frame %&amp;gt;% select(Intercept, age0), m)
  frame %&amp;gt;% select(-Intercept)
}

# function to get random effects predictions  
ran_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
         PROC_CID = m@frame$PROC_CID) %&amp;gt;%
    mutate(ran_pred = predict(m, newdata = .),
           age = age0 + 16)
}

# get fixed and random efects, and also combine them.
(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fixed_pred = map(fit2, fixed_pred_fun),
         ran_pred = map(fit2, ran_pred_fun),
         combined_pred = map2(fixed_pred, ran_pred, full_join)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
## # Groups:   trait [2]
##   trait          data fit0   fit2   fixed_pred   ran_pred    combined_pred 
##   &amp;lt;chr&amp;gt;  &amp;lt;list&amp;lt;df[,8&amp;gt; &amp;lt;list&amp;gt; &amp;lt;list&amp;gt; &amp;lt;list&amp;gt;       &amp;lt;list&amp;gt;      &amp;lt;list&amp;gt;        
## 1 CESD    [2,084 × 8] &amp;lt;lmer… &amp;lt;lmer… &amp;lt;tibble [81… &amp;lt;tibble [8… &amp;lt;tibble [8,38…
## 2 SensS…  [2,084 × 8] &amp;lt;lmer… &amp;lt;lmer… &amp;lt;tibble [81… &amp;lt;tibble [8… &amp;lt;tibble [8,38…&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;Now let’s&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;%
  select(trait, combined_pred) %&amp;gt;%
  unnest(combined_pred) %&amp;gt;%
  ggplot(aes(x = age)) +
    geom_line(aes(y = ran_pred, color = trait, group = PROC_CID), size = .25, alpha = .5) +
    geom_line(aes(y = fixed_pred), size = 2) +
    lims(y = c(0,4)) +
    labs(x = &amp;quot;Age&amp;quot;, y = &amp;quot;Composite Rating&amp;quot;,
         title = &amp;quot;Simple Growth Models&amp;quot;) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And with confidence bands:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;% 
  mutate(df = map_dbl(fit2, df.residual)) %&amp;gt;%
  select(trait, fixed_pred, df) %&amp;gt;%
  unnest(fixed_pred) %&amp;gt;%
  ggplot(aes(x = age, y = fixed_pred, fill = trait)) +
    geom_ribbon(aes(ymin=fixed_pred-1.96*SE,ymax=fixed_pred+1.96*SE),alpha=0.2,fill=&amp;quot;blue&amp;quot;) +  
    geom_line(size = 2) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;% 
  mutate(df = map_dbl(fit2, df.residual)) %&amp;gt;%
  select(trait, fixed_pred, df) %&amp;gt;%
  unnest(fixed_pred) %&amp;gt;%
  ggplot(aes(x = age, y = fixed_pred, fill = trait)) +
    stat_dist_lineribbon(
      aes(dist = &amp;quot;student_t&amp;quot;, arg1 = unique(df), arg2 = fixed_pred, arg3 = SE),
      alpha = 1/4
    ) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;two-level-categorical-moderator&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Two-Level Categorical Moderator&lt;/h1&gt;
&lt;p&gt;Let’s start with the basic syntax:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \beta_{1j}*time_{1j} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + \gamma_{01}*X_{2j} + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1j} = \gamma_{10} + \gamma_{11}*X_{2j} + U_{1j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;p&gt;Now let’s swap that out for a 2 group sample from the present data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \beta_{1j}*age0_{ij} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + \gamma_{01}*groupsNone + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1j} = \gamma_{10} + \gamma_{11}*groupsNone + U_{1j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;D1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Jail&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(data_2g = map(data, function(x) x %&amp;gt;% filter(groups != &amp;quot;CommServ&amp;quot;)),
         fit3 = map(data_2g, ~lmer(value ~ 1 + age0*groups + (age0 | PROC_CID), data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we plot these, we are plotting the simple slopes. Subbing 1 and 0 into the equations above we end up with the following for the groups.&lt;/p&gt;
&lt;p&gt;None: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{10}*age\)&lt;/span&gt;&lt;br /&gt;
Jail: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{01} + (\gamma_{10} + \gamma_{11})*age\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixed_pred_fun &amp;lt;- function(m){
  frame &amp;lt;- crossing(Intercept = 1, age0 = seq(0, 8, .1),
           groups = c(&amp;quot;Jail&amp;quot;, &amp;quot;None&amp;quot;)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16,
           groupsn = as.numeric(mapvalues(groups, unique(groups), c(1,0))),
           Int = age0*groupsn)
  frame$SE &amp;lt;- pv_fun(frame %&amp;gt;% select(Intercept, age0, groupsn, Int), m)
  frame %&amp;gt;% select(-Intercept, -groupsn, -Int)
}

fixed_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
           groups = c(&amp;quot;Jail&amp;quot;, &amp;quot;None&amp;quot;)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16)
}

ran_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
         PROC_CID = m@frame$PROC_CID) %&amp;gt;%
    left_join(m@frame %&amp;gt;% tbl_df %&amp;gt;% select(PROC_CID, groups)) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(ran_pred = predict(m, newdata = .),
           age = age0 + 16)
}

(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fixed_pred3 = map(fit3, fixed_pred_fun),
         ran_pred3 = map(fit3, ran_pred_fun),
         combined_pred3 = map2(fixed_pred3, ran_pred3, full_join)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 12
## # Groups:   trait [2]
##   trait        data fit0  fit2  fixed_pred ran_pred combined_pred data_2g
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;   &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt; 
## 1 CESD  [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## 2 Sens… [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## # … with 4 more variables: fit3 &amp;lt;list&amp;gt;, fixed_pred3 &amp;lt;list&amp;gt;,
## #   ran_pred3 &amp;lt;list&amp;gt;, combined_pred3 &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;%
  select(trait, combined_pred3) %&amp;gt;%
  unnest(combined_pred3) %&amp;gt;%
  ggplot(aes(x = age)) +
    scale_color_manual(values = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;)) +
    geom_line(aes(y = ran_pred, color = groups, group = PROC_CID), size = .25, alpha = .1) +
    geom_line(aes(y = fixed_pred, color = groups), size = 2) +
    lims(y = c(0,4)) +
    labs(x = &amp;quot;Age&amp;quot;, y = &amp;quot;Composite Rating&amp;quot;,
         title = &amp;quot;Simple Growth Models&amp;quot;) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;three-level-categorical-moderator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Three-Level Categorical Moderator&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \beta_{1j}*age0_{ij} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + \gamma_{01}*D1 + \gamma_{02}*D2 + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1j} = \gamma_{10} + \gamma_{11}*D1 + \gamma_{12}*D2 + U_{1j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;D1&lt;/th&gt;
&lt;th&gt;D2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Jail&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;CommServ&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fit4 = map(data, ~lmer(value ~ 1 + age0*groups + (age0 | PROC_CID), data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we plot these, we are plotting the simple slopes. Subbing 1 and 0 into the equations above we end up with the following for the groups.&lt;/p&gt;
&lt;p&gt;None: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{10}*age\)&lt;/span&gt;&lt;br /&gt;
Jail: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{01} + (\gamma_{10} + \gamma_{11})*age\)&lt;/span&gt;&lt;br /&gt;
Community Service: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{02} + (\gamma_{10} + \gamma_{12})*age\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixed_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
           groups = c(&amp;quot;Jail&amp;quot;, &amp;quot;None&amp;quot;, &amp;quot;CommServ&amp;quot;)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16)
}

ran_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
         PROC_CID = m@frame$PROC_CID) %&amp;gt;%
    left_join(m@frame %&amp;gt;% tbl_df %&amp;gt;% select(PROC_CID, groups)) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(ran_pred = predict(m, newdata = .),
           age = age0 + 16)
}

(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fixed_pred4 = map(fit4, fixed_pred_fun),
         ran_pred4 = map(fit4, ran_pred_fun),
         combined_pred4 = map2(fixed_pred4, ran_pred4, full_join)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 16
## # Groups:   trait [2]
##   trait        data fit0  fit2  fixed_pred ran_pred combined_pred data_2g
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;   &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt; 
## 1 CESD  [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## 2 Sens… [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## # … with 8 more variables: fit3 &amp;lt;list&amp;gt;, fixed_pred3 &amp;lt;list&amp;gt;,
## #   ran_pred3 &amp;lt;list&amp;gt;, combined_pred3 &amp;lt;list&amp;gt;, fit4 &amp;lt;list&amp;gt;,
## #   fixed_pred4 &amp;lt;list&amp;gt;, ran_pred4 &amp;lt;list&amp;gt;, combined_pred4 &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;%
  select(trait, combined_pred4) %&amp;gt;%
  unnest(combined_pred4) %&amp;gt;%
  ggplot(aes(x = age)) +
    scale_color_manual(values = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;black&amp;quot;)) +
    geom_line(aes(y = ran_pred, color = groups, group = PROC_CID), size = .25, alpha = .1) +
    geom_line(aes(y = fixed_pred, color = groups), size = 2) +
    lims(y = c(0,4)) +
    labs(x = &amp;quot;Age&amp;quot;, y = &amp;quot;Composite Rating&amp;quot;,
         title = &amp;quot;Simple Growth Models&amp;quot;) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-time-invariant-moderator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous Time-Invariant Moderator&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fit5 = map(data, ~lmer(value ~ 1 + age0*DemPweight + (age0 | PROC_CID), data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixed_pred_fun &amp;lt;- function(m){
  desc &amp;lt;- Rmisc::summarySE(m@frame, measurevar = &amp;quot;DemPweight&amp;quot;)
    crossing(age0 = seq(0, 8, 1),
           DemPweight = c(desc$DemPweight - desc$sd, 
                          desc$DemPweight, 
                          desc$DemPweight + desc$sd)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16,
           Weight = factor(DemPweight, levels = unique(DemPweight), labels = c(&amp;quot;-1SD&amp;quot;, &amp;quot;0SD&amp;quot;, &amp;quot;1SD&amp;quot;)))
}

ran_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
         PROC_CID = m@frame$PROC_CID) %&amp;gt;%
    left_join(m@frame %&amp;gt;% tbl_df %&amp;gt;% select(PROC_CID, DemPweight)) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(ran_pred = predict(m, newdata = .),
           age = age0 + 16)
}

(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fixed_pred5 = map(fit5, fixed_pred_fun),
         ran_pred5 = map(fit5, ran_pred_fun),
         combined_pred5 = map2(fixed_pred5, ran_pred5, full_join)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 20
## # Groups:   trait [2]
##   trait        data fit0  fit2  fixed_pred ran_pred combined_pred data_2g
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;   &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt; 
## 1 CESD  [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## 2 Sens… [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## # … with 12 more variables: fit3 &amp;lt;list&amp;gt;, fixed_pred3 &amp;lt;list&amp;gt;,
## #   ran_pred3 &amp;lt;list&amp;gt;, combined_pred3 &amp;lt;list&amp;gt;, fit4 &amp;lt;list&amp;gt;,
## #   fixed_pred4 &amp;lt;list&amp;gt;, ran_pred4 &amp;lt;list&amp;gt;, combined_pred4 &amp;lt;list&amp;gt;,
## #   fit5 &amp;lt;list&amp;gt;, fixed_pred5 &amp;lt;list&amp;gt;, ran_pred5 &amp;lt;list&amp;gt;,
## #   combined_pred5 &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;%
  select(trait, combined_pred5) %&amp;gt;%
  unnest(combined_pred5) %&amp;gt;%
  ggplot(aes(x = age)) +
    scale_color_manual(values = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;black&amp;quot;)) +
    # geom_line(aes(y = ran_pred, group = PROC_CID), size = .25, alpha = .1) +
    geom_line(aes(y = fixed_pred, color = Weight), size = 1) +
    lims(y = c(0,4)) +
    labs(x = &amp;quot;Age&amp;quot;, y = &amp;quot;Composite Rating&amp;quot;,
         title = &amp;quot;Simple Growth Models&amp;quot;) +
    facet_wrap(~trait, scales = &amp;quot;free_y&amp;quot;) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 4</title>
      <link>/lectures/week-4/</link>
      <pubDate>Wed, 18 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/week-4/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#review-from-last-time&#34;&gt;Review from last time&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation&#34;&gt;Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimation&#34;&gt;Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing-significance-adapted-from-ben-bolker&#34;&gt;Testing significance (adapted from Ben Bolker)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#quick-aside-p-values-are-not-included&#34;&gt;Quick aside: P values are not included&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likelihood-ratio-test&#34;&gt;Likelihood ratio test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likelihood-tests-for-random-effects&#34;&gt;Likelihood tests for random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aic-and-bic&#34;&gt;AIC and BIC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-of-determination-equivalents&#34;&gt;Coefficient of determination equivalents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#level-1-predictors-aka-time-varying-covariates-tvcs&#34;&gt;Level 1 predictors AKA Time-varying covariates (TVCs)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introducing-a-random-slope-for-a-tvc&#34;&gt;Introducing a random slope for a TVC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interactions-among-level-1-variables&#34;&gt;Interactions among level 1 variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#centering-redux&#34;&gt;Centering redux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#readings&#34;&gt;Readings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#flexible-time-metrics&#34;&gt;Flexible time metrics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#categorical-structured-vs-continuous-unstructured&#34;&gt;Categorical (structured) vs continuous (unstructured)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#balanced-vs-unbalanced&#34;&gt;Balanced vs unbalanced&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#convergence-issues-or-other-warnings&#34;&gt;Convergence issues or other warnings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#polynomial-and-splines&#34;&gt;Polynomial and Splines&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#polynomial-example&#34;&gt;polynomial example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#importance-of-centering&#34;&gt;importance of centering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-terms&#34;&gt;random terms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#splines-aka-piecewise&#34;&gt;Splines aka piecewise&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#separate-curves&#34;&gt;separate curves&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#splines-polynomial-polynomial-piecewise&#34;&gt;splines + polynomial = polynomial piecewise&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;review-from-last-time&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Review from last time&lt;/h1&gt;
&lt;div id=&#34;interpretation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interpretation&lt;/h2&gt;
&lt;p&gt;Looked at between person predictors&lt;/p&gt;
&lt;p&gt;Can you to interpret each fixed and random effect?&lt;/p&gt;
&lt;p&gt;What do these different models look like graphically?&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}Exercise_{j} +  \gamma_{02}Intervention_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}Intervention_{j} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation&lt;/h1&gt;
&lt;p&gt;Now that we are able to build and visualize models, how do we test the parameters of interest?&lt;/p&gt;
&lt;p&gt;Maximum likelihood estimation. Uses a likelihood function that describes the probability of observing the sample data as a function of the parameters. Attempts to maximize the function through an iterative process. Because it is iterative, it might fail.&lt;/p&gt;
&lt;p&gt;There are fixed effects as well as random effects we need to count for. Maximum likelihood takes our assumptions about the model (normally distributed residuals, etc) and creates probability densities for each parameters. For example, based on certain fixed effects and sd of random effects, how likely is it that person x has a slope of z? The algorithm looks at the full sample to see how likely different parameters are, spits back the most likely, and gives you a number to show how likely they are (compared to others). This is akin to saying you rolled 10 dice, 5 came up as 2s. How likely is this dice fair? But instead of fair vs not fair it gives a likelihood to certain possibilities (e.g., a 2 comes up at 25%, 50% 75% rates).&lt;/p&gt;
&lt;p&gt;Restricted maximum likelihood (REML) vs Full Maximum likelihood (ML). Will give you similar parameters, the differences are in the standard errors. REML is similar to dividing by N - 1 for SE whereas ML is similar to dividing by N.&lt;/p&gt;
&lt;p&gt;Differences account for the fact that fixed effects are being estimated simultaneously with the variance parameters in ML. Estimates of the variance parameters assume that the fixed effects estimates are known and thus does not account for uncertainty in these estimates.&lt;/p&gt;
&lt;p&gt;REML accounts for uncertainty in the fixed effects before estimating residual variance. REML attempts to maximize the likelihood of the residuals whereas ML maximizes the sample data. REML can be thought of as an unbiased estimate of the residual variance.&lt;/p&gt;
&lt;p&gt;REML is good for small sample size both N and group. However, if you use REML you should be careful in testing fixed effects against each other (more down below). Deviance tests for fixed effects should be done with ML, but only random effects with REML. ML can also look at random effects too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-significance-adapted-from-ben-bolker&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Testing significance (adapted from Ben Bolker)&lt;/h1&gt;
&lt;p&gt;4 Methods for testing single parameters
From worst to best:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Wald Z-tests.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Wald t-tests&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Easy to compute - test statistic over standard error However, they are asymptotic standard error approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal and that (2) the sampling distribution of the log-likelihood is (proportional to) χ2.&lt;/p&gt;
&lt;p&gt;The above two are okay to do for single parameter estimates of fixed effects. But beware that a) degrees of freedom calculations are not straightforward and b) the assumptions for random effects are be hard to meet.&lt;/p&gt;
&lt;div id=&#34;quick-aside-p-values-are-not-included&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quick aside: P values are not included&lt;/h2&gt;
&lt;p&gt;Authors of the package we will be using first lme4 are not convinced of the utility of the general approach of testing with reference to an approximate null distribution. In general, it is not clear that the null distribution of the computed ratio of sums of squares is really an F distribution, for any choice of denominator degrees of freedom. While this is true for special cases that correspond to classical experimental designs (nested, split-plot, randomized block, etc.), it is apparently not true for more complex designs (unbalanced, GLMMs, temporal or spatial correlation, etc.).&lt;/p&gt;
&lt;p&gt;tl;dr: it gets messy with more complex models.&lt;/p&gt;
&lt;p&gt;If you really want p values&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(lmerTest)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Likelihood ratio test (also called deviance test).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Markov chain Monte Carlo (MCMC) or parametric bootstrap confidence intervals ( we will get to this later)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-ratio-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood ratio test&lt;/h2&gt;
&lt;p&gt;Used for model comparisons (often multiparameter comparisons) and for tests of random effects. REML can only be used if model compared have the same fixed parts and only differ in random. Otherwise ML must be used.&lt;/p&gt;
&lt;p&gt;How much more likely the data is under a more complex model than under the simpler model (these models need to be nested to compare this).&lt;/p&gt;
&lt;p&gt;Log Likelihood (LL) is derived from ML estimation. Logs are used because they are computationally simpler; logs of multiplications are reduced to adding the logs together.&lt;/p&gt;
&lt;p&gt;Larger the LL the better the fit.&lt;/p&gt;
&lt;p&gt;Deviance compares two LLs. Current model and a saturated model (that fits data perfectly). Asks how much worse the current model is to the best possible model. Deviance = -2[LL current - LL saturated]&lt;/p&gt;
&lt;p&gt;LL saturated = 1 for MLMs (probability it will perfectly recapture data). log of 1 is 0. So this term drops out. Deviance = -2(LL current model). AKA -2logL or -2LL&lt;/p&gt;
&lt;p&gt;Can compare two models via subtraction, often referred to as a full and reduced model. Differences is distributed as a chi square with a df equal to how many “constraints” are included. Constraints can be thought of as forcing a parameter to be zero ie removing it.&lt;/p&gt;
&lt;p&gt;Comparing 2 models is called a likelihood ratio test. Need to have:
1. same data
2. nested models (think of constraining a parameter to zero)&lt;/p&gt;
&lt;p&gt;Why work with deviances and not just log likelihoods? Why -2? Why a ratio test when you subtract deviances? Maths. Working with deviances allows us to subtract two from one another, which is equivalent to taking the ratio of likelihoods.&lt;/p&gt;
&lt;p&gt;You can test in r using the same procedure we would to test different regression models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.2, mod.2r)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-tests-for-random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood tests for random effects&lt;/h2&gt;
&lt;p&gt;Not listed in the output because it is harder to do this with variances. Remember variances do not have values below zero and thus the distributions get a wonky quickly. Needs mixture distributions (Cannot be easily done with chi square, for example)&lt;/p&gt;
&lt;p&gt;Can technically do anova comparisons for random effects, though that falls to many similar problems as trying to do a Wald test.&lt;/p&gt;
&lt;p&gt;The sampling distribution of variance estimates is in general strongly asymmetric: the standard error may be a poor characterization of the uncertainty. Thus the best way to handle is to do bootstrapped estimates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aic-and-bic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;AIC and BIC&lt;/h2&gt;
&lt;p&gt;Used when you want to compare non-nested data. Need to have the same data, however.&lt;/p&gt;
&lt;p&gt;AIC (Akaike’s Information Criterion) and the BIC (Bayesian Information Criterion) where “smaller is better.” This is the opposite of LL. As with the other types, these may give you wonky findings depending on some factors as they are related to LLs.&lt;/p&gt;
&lt;p&gt;AIC = 2(number of parameters) + (−2LL)
BIC = ln(n)(number of parameters) + (−2LL)&lt;/p&gt;
&lt;p&gt;BIC penalizes models with more parameters more than AIC does.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-determination-equivalents&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coefficient of determination equivalents&lt;/h2&gt;
&lt;p&gt;You want to get a model fit estimate. BIC and AIC are good to compare nested models but they aren’t standardized and thus make comparison across non nested models difficult.&lt;/p&gt;
&lt;p&gt;With MLM models we cannot directly compute R2. Instead we will use pseudo R2. Pseudo R2 is similar to R2 in that it can be thought of as the correlation between your predicted and actual scores. For example, assume we have three waves of data. The intercept is 1, the slope is 2 and time is coded 0,1,2. The predicted scores are: 1, 3, 5. We would then correlate everyone’s first, second and third wave scores with these predicted scores. This correlation squared is pseudo R2, telling us how much variance time explains in our DV.&lt;/p&gt;
&lt;p&gt;Yes, we typically think of this as a measure of variance explained divided by total variance. This is where things get tricky: should you include or exclude variation of different random-effects terms? These are error, but they are modeled in the sense that they are not unexplained. Is the effect size wanted after you are “controlling for” or do you want to talk about total variation. There are similarities here with regards to Eta and Partial Eta squared.&lt;/p&gt;
&lt;p&gt;The general idea is to be upfront about what you are comparing and what is included. Typically this is done with comparing models, much like a hierarchical regression. Taking the difference in variance between model 1 and model 2 and dividing it by model 1 makes it explicit what you are looking at and what you are including or not including.&lt;/p&gt;
&lt;p&gt;E.g,. residual variance in varying intercept model subtracted from growth model divided by intercept only model. This can tell you how much unexplained variance is explained by time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sigma(mod.1) - sigma(mod.2)) / sigma(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;level-1-predictors-aka-time-varying-covariates-tvcs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Level 1 predictors AKA Time-varying covariates (TVCs)&lt;/h1&gt;
&lt;p&gt;Thus far we have been talking about level 2, between person predictors. But we can extend this to level 1, within person, repeated measures as predictors and covariates.&lt;/p&gt;
&lt;p&gt;These are predictors that are assessed at level 1, which repeat. Note that there are some variables that are inherently level 2 (e.g. handedness), some that make sense more as a level 1 (e.g., mood) and some that could be considered either depending on your research question and/or your data (e.g. income). The latter type could conceivably change across time (And thus be appropriate for a level 1 variable; tvc) but may not change at the rate of your construct or not be important.&lt;/p&gt;
&lt;p&gt;What do level 1 predictors look like in your dataset?&lt;/p&gt;
&lt;p&gt;Consider health across time predicted by a level 1 exercise variable (1 = yes, exercised). Note that we had a similar model presented at the end of last class, but exercise was a level 2 predictor. Be comfortable with how these differ.&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}Exercise_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Combined:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} =  [\gamma_{00} +   \gamma_{10}Time_{ij}   + \gamma_{20}Exercise_{ij}] + [ U_{0j}  + U_{1j}Time_{ij}+ \varepsilon_{ij}] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Two things to keep in mind:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;These can be treated as another predictor with the effect of “controlling” for some TVC. Thus the regression coefficients in the model are conditional on this covariate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$ _{10} $ is the average rate of change in health, controlling for exercise&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\gamma_{20}\)&lt;/span&gt; is the average difference in health when exercising and when not. Ie the difference in health trajectory.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; is the average health at Time = 0 for those that do not exercise. Ie when both predictors are at zero.&lt;/p&gt;
&lt;p&gt;How would you visualize the fixed effects for varying combinations of exercise?&lt;/p&gt;
&lt;div id=&#34;introducing-a-random-slope-for-a-tvc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introducing a random slope for a TVC&lt;/h2&gt;
&lt;p&gt;Person specific residuals make the interpretation of parameters a little more difficult as the model says that the gap between exercise and not exercise is the same for everyone. Should we allow it to be this way?&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}Exercise_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} +  U_{2j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2 variance-covariance matrix:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \\ {U}_{2j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,  &amp;amp;  \tau_{0}^{2} &amp;amp; \tau_{01}   &amp;amp; \tau_{02}   \\ 
  0, &amp;amp; \tau_{10} &amp;amp; \tau_{1}^{2} &amp;amp; \tau_{12}  \\
  0, &amp;amp; \tau_{20} &amp;amp; \tau_{21} &amp;amp; \tau_{2}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Residual variance at level 1&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Compared to time invariant (level 2) predictors, tvc/level 1 predictors are likely to explain variance level 1 and level 2 variance terms as they differ between and within. Typically level 2 predictors tend to only reduce level 2 variance. It is possible, however, that including a level 1 predictor will increase the variance in level 2 variance components.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interactions-among-level-1-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interactions among level 1 variables&lt;/h2&gt;
&lt;p&gt;Couldn’t exercise levels influence the slope of health? The previous models constrained the slopes to be the same, saying that people differ on level when exercising vs not but not on rate of change.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} =  [\gamma_{00} +   \gamma_{10}Time_{ij}   + \gamma_{20}Exercise_{ij} + \gamma_{30}TimeXExercise_{ij}]] + [ U_{0j}  + U_{1j}Time_{ij}+ \varepsilon_{ij}] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How could you visualize this model?&lt;/p&gt;
&lt;p&gt;How do you interpret each of the terms (knowing what you know about interactions)?&lt;/p&gt;
&lt;p&gt;How would all of this change if our level 1 variable was continuous?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;centering-redux&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Centering redux&lt;/h2&gt;
&lt;p&gt;Especially when you are working with level 1 interactions, centering is important to interpret your lower order terms. How would $ _{10}$ be interpreted in the above if exercise was centered vs not? Also, be clear about what you mean by centering. Is it the person average or the grand mean average. These will differ in interpretation. Do you want to model a person’s average exercise or the grand mean exercise?&lt;/p&gt;
&lt;p&gt;Typically for level 1 we will want to within person-mean center.&lt;/p&gt;
&lt;p&gt;However, this gets rid of all mean level information for a person. The question at hand is not whether you exercise more or less it is compared to your typical levels, what happens when you exercise more or less. This is a within-person question and may be quite important for your theoretical tests.&lt;/p&gt;
&lt;p&gt;However, if you are including a level 1 person centered variable in the model, note that 1) the average level of exercise is not controlled for and 2) the variation around the level will likely be related to the persons mean score. In other words, the within and between person variance of exercise is not neatly decomposed. To do so, we will have to create a new variable out of the existing level 1 variable, a person mean.&lt;/p&gt;
&lt;p&gt;Level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}(Exercise_{ij}-\overline{Exercise_{j}}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}\overline{Exercise_{j}} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20}  \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;readings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Readings&lt;/h2&gt;
&lt;p&gt;Check out this article for more information on how to model level 1 predictors.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3095386/&#34; class=&#34;uri&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3095386/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;flexible-time-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Flexible time metrics&lt;/h1&gt;
&lt;p&gt;Thus far we have been talking about time relatively naively, assuming that time is fixed on equal assessments for everyone ie wave. This treatment of time can be made more complex in two ways.&lt;/p&gt;
&lt;div id=&#34;categorical-structured-vs-continuous-unstructured&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categorical (structured) vs continuous (unstructured)&lt;/h2&gt;
&lt;p&gt;Our repeated assessments are often collected based on some sort of structure. You have enough funding for three waves of data, and you proceed to call participants. These three waves may be specified to occur every 6 months, for example. However, it rarely works out that nicely. People don’t show up, people reschedule, your team is in holiday. The resulting time in between assessments thus differs between and within a person. What to do?&lt;/p&gt;
&lt;p&gt;Well, we could ignore the timing differences. Do we think that a few weeks difference will make or break your general conclusions? Sticking with wave is seen as treating time as categorical.&lt;/p&gt;
&lt;p&gt;We could also treat it as continuous. This is usually preferred because why get rid of meaningful information? Within MLMs there is practically no downside to doing so.&lt;/p&gt;
&lt;p&gt;Treating time as categorical, however, is standard with SEM based longitudinal methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;balanced-vs-unbalanced&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Balanced vs unbalanced&lt;/h2&gt;
&lt;p&gt;Balanced for longitudinal models means that everyone has the same number of repeated assessments. As with ANOVA/experimental designs, balance makes the math easier. In terms of interpretation of the results after doing said maths, there is no difference. In longitudinal designs especially, it is important is where this unbalance comes from. Does the unbalance occur because of dumb luck or is it systematically related to some variable e.g., attrition via death/health.&lt;/p&gt;
&lt;p&gt;The downfalls from unbalanced designs come from difficulties in convergence and interpretation. This is especially true when time is categorical rather than continuous (as continuous time makes estimation of variance components easier as it is more likely to be separated from the fixed effects).&lt;/p&gt;
&lt;p&gt;If you have less than 2 repeated measures for a person, they still can be used. They will be used to estimate relevant fixed effects that can be estimated (as they are similar to standard regression coefficients), but likely not the variance estimates. The slopes for these people will be based on their observed values and the model based trajectory (ie uses partial pooling/shrinkage). However, a number of these individuals will lead to convergence issues.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence-issues-or-other-warnings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Convergence issues or other warnings&lt;/h1&gt;
&lt;p&gt;If you have convergence issues it is likely because you have a) too few data points, b) too much imbalance in your repeated measures (ie missing data), c) too many parameters to estimate or d) a combination of all of the above.&lt;/p&gt;
&lt;p&gt;We will talk about fitting a “maximal model” – one that has as many variance components as possible. However, this may be asking too much of the data. Instead, we may have to get rid of some of these random terms to reduce model complexity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;polynomial-and-splines&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Polynomial and Splines&lt;/h1&gt;
&lt;p&gt;##Polynomials
level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}(Time_{ij} - \bar{X)} + \beta_{2j}(Time_{ij} - \bar{X)}^2 + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +   U_{0j}\]&lt;/span&gt;&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} +  U_{2j} \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;polynomial-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;polynomial example&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list = ls())

library(readr)
cdrs &amp;lt;- read_csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/cdrs.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   mapid = col_double(),
##   exclude = col_character(),
##   cdr = col_double(),
##   testdate = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality &amp;lt;- read_csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/Subject_personality.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   mapid = col_double(),
##   age = col_double(),
##   neodate = col_double(),
##   neuroticism = col_double(),
##   extraversion = col_double(),
##   openness = col_double(),
##   agreeablness = col_double(),
##   conscientiousness = col_double(),
##   gender = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2) 


gg1 &amp;lt;- ggplot(personality,
   aes(x = neodate, y = neuroticism, group = mapid)) + geom_line()  
gg1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ──────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ purrr   0.3.2          ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ──────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality&amp;lt;- personality %&amp;gt;% 
  group_by(mapid) %&amp;gt;%
  arrange(neodate) %&amp;gt;% 
  mutate(wave = seq_len(n())) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg2 &amp;lt;- ggplot(personality,
   aes(x = wave, y = neuroticism, group = mapid)) + geom_line()  
gg2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality$neodate &amp;lt;- as.Date(personality$neodate, origin = &amp;quot;1900-01-01&amp;quot;)

gg3 &amp;lt;- ggplot(personality,
   aes(x = neodate, y = neuroticism, group = mapid)) + geom_line()  
gg3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## convert to days from first assessment

personality.wide &amp;lt;- personality %&amp;gt;% 
  dplyr::select(mapid, wave, neodate) %&amp;gt;% 
  spread(wave, neodate) 

personality.wide$wave_1 &amp;lt;- personality.wide$&amp;#39;1&amp;#39;
personality.wide$wave_2 &amp;lt;- personality.wide$&amp;#39;2&amp;#39;
personality.wide$wave_3 &amp;lt;- personality.wide$&amp;#39;3&amp;#39;
personality.wide$wave_4 &amp;lt;- personality.wide$&amp;#39;4&amp;#39;
personality.wide$wave_5 &amp;lt;- personality.wide$&amp;#39;5&amp;#39;

personality.wide &amp;lt;- personality.wide %&amp;gt;% 
mutate (w_1 = (wave_1 - wave_1)/365,
          w_2 = (wave_2 - wave_1)/365,
          w_3 = (wave_3 - wave_1)/365,
          w_4 = (wave_4 - wave_1)/365,
        w_5 = (wave_5 - wave_1)/365)

personality.long &amp;lt;- personality.wide %&amp;gt;% 
  dplyr::select(mapid, w_1:w_5) %&amp;gt;% 
  gather(wave, year, -mapid) %&amp;gt;% 
  separate(wave, c(&amp;#39;weeks&amp;#39;, &amp;#39;wave&amp;#39; ), sep=&amp;quot;_&amp;quot;) %&amp;gt;% 
 dplyr::select(-weeks) 

personality.long$wave &amp;lt;-  as.numeric(personality.long$wave)


personality &amp;lt;- personality %&amp;gt;% 
   left_join(personality.long, by = c(&amp;#39;mapid&amp;#39;, &amp;#39;wave&amp;#39; )) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg4 &amp;lt;- ggplot(personality,
   aes(x = year, y = neuroticism, group = mapid)) + geom_line()  
gg4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Don&amp;#39;t know how to automatically pick scale for object of type difftime. Defaulting to continuous.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 1 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;Matrix&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- lmer(neuroticism ~ year + (1 | mapid), data=personality)
summary(p1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + (1 | mapid)
##    Data: personality
## 
## REML criterion at convergence: 13657.4
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7877 -0.4675 -0.0227  0.4289  3.3166 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  mapid    (Intercept) 42.16    6.493   
##  Residual             15.65    3.956   
## Number of obs: 2105, groups:  mapid, 1090
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 16.05632    0.22577  71.118
## year        -0.13204    0.03247  -4.067
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.247&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)
personality.s &amp;lt;- personality %&amp;gt;% 
  group_by(mapid) %&amp;gt;% 
  tally() %&amp;gt;% 
   filter(n &amp;gt;=2) 

 personality &amp;lt;- personality %&amp;gt;% 
   filter(mapid %in% personality.s$mapid)

p2 &amp;lt;- lmer(neuroticism ~ year + (1 | mapid), data=personality)
summary(p2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + (1 | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10396.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7542 -0.5122 -0.0282  0.4698  3.3369 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  mapid    (Intercept) 40.92    6.397   
##  Residual             15.61    3.950   
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  15.3797     0.2915  52.761
## year         -0.1083     0.0331  -3.271
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.320&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3 &amp;lt;- lmer(neuroticism ~ year + (year | mapid), data=personality)
summary(p3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + (year | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10389.2
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7440 -0.4825 -0.0304  0.4443  3.3453 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  mapid    (Intercept) 41.6916  6.4569        
##           year         0.0983  0.3135   -0.10
##  Residual             14.2561  3.7757        
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 15.37237    0.29136  52.760
## year        -0.10271    0.03602  -2.851
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.317&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;importance-of-centering&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;importance of centering&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality$year &amp;lt;- as.numeric(personality$year)
  
p4 &amp;lt;- lmer(neuroticism ~ year + I(year^2) + (year | mapid), data=personality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 0.00216391
## (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(p4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + I(year^2) + (year | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10395.8
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7664 -0.4836 -0.0251  0.4422  3.3259 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  mapid    (Intercept) 41.73017 6.4599        
##           year         0.09818 0.3133   -0.10
##  Residual             14.26174 3.7765        
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##              Estimate Std. Error t value
## (Intercept) 15.324317   0.297096  51.580
## year        -0.031791   0.092090  -0.345
## I(year^2)   -0.008789   0.010490  -0.838
## 
## Correlation of Fixed Effects:
##           (Intr) year  
## year      -0.300       
## I(year^2)  0.194 -0.920
## convergence code: 0
## Model failed to converge with max|grad| = 0.00216391 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# woah, how do I interpret this? WHy all of a sudden non-sig? 
# what would happen if I changed my time metric? &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;psych&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%, alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;describe(personality$year)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    vars    n mean   sd median trimmed  mad min   max range skew kurtosis
## X1    1 1635  3.1 3.29   2.45    2.66 3.63   0 12.78 12.78  0.8    -0.41
##      se
## X1 0.08&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality$year.c &amp;lt;- personality$year - 3.1

p5 &amp;lt;- lmer(neuroticism ~ year.c + I(year.c^2) + (year.c | mapid), data=personality)
summary(p5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year.c + I(year.c^2) + (year.c | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10395.8
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7663 -0.4836 -0.0251  0.4422  3.3258 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  mapid    (Intercept) 41.42901 6.4365       
##           year.c       0.09812 0.3132   0.05
##  Residual             14.26278 3.7766       
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##              Estimate Std. Error t value
## (Intercept) 15.141297   0.296070  51.141
## year.c      -0.086286   0.041061  -2.101
## I(year.c^2) -0.008789   0.010490  -0.838
## 
## Correlation of Fixed Effects:
##             (Intr) year.c
## year.c       0.226       
## I(year.c^2) -0.353 -0.480&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;random-terms&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;random terms&lt;/h3&gt;
&lt;p&gt;fitting a random slope plus a random quadratic leads to difficulties ie non-convergence. What does this model say?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p6 &amp;lt;- lmer(neuroticism ~ year + I(year^2) + ( I(year^2) | mapid), data=personality)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 0.167875
## (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl = control$checkConv, : Model is nearly unidentifiable: very large eigenvalue
##  - Rescale variables?&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(p6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: neuroticism ~ year + I(year^2) + (I(year^2) | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10398.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.7747 -0.4937 -0.0197  0.4525  3.3481 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev. Corr
##  mapid    (Intercept) 4.082e+01 6.38890      
##           I(year^2)   4.851e-04 0.02203  0.02
##  Residual             1.505e+01 3.87965      
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##              Estimate Std. Error t value
## (Intercept) 15.321498   0.296523  51.670
## year        -0.026955   0.093386  -0.289
## I(year^2)   -0.009488   0.010729  -0.884
## 
## Correlation of Fixed Effects:
##           (Intr) year  
## year      -0.300       
## I(year^2)  0.202 -0.928
## convergence code: 0
## Model failed to converge with max|grad| = 0.167875 (tol = 0.002, component 1)
## Model is nearly unidentifiable: very large eigenvalue
##  - Rescale variables?&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;splines-aka-piecewise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splines aka piecewise&lt;/h2&gt;
&lt;p&gt;Fit more than 1 trajectory. Best to use when we have a reason for a qualitative difference at some identified time point. For example, before your health event you may have a different trajectory than after it and thus you would want to model two separate trajectories. Splines allow you to do this in a single model. You can do this in simple regression and the logic follows for growth models.&lt;/p&gt;
&lt;p&gt;We simply replace time with dummy variables that represent different segments we wish to model. The point of separation is called a knot. You can have as many as you want and these can be pre-specified (usually for our case) or in more advanced treatments have the data specify it for you.&lt;/p&gt;
&lt;div id=&#34;separate-curves&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;separate curves&lt;/h3&gt;
&lt;p&gt;The most common is to create different trajectories that change across knots. The easiest example is to take your time variable and transform it into a Time1 and time2, that represent the different time periods. This is easiest to see if we choose our wave variable as our time metric, though you do not have to necessarily do it this way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t1 &amp;lt;- tribble(
  ~time, ~t0, ~t1,~t2,~t3,~t4,~t5,
  &amp;quot;time 1&amp;quot;, 0, 1,2,2,2,2,
  &amp;quot;time 2&amp;quot;, 0, 0,0,1,2,3
)
t1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   time      t0    t1    t2    t3    t4    t5
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 time 1     0     1     2     2     2     2
## 2 time 2     0     0     0     1     2     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The idea is that once you hit the knot your value stays the same. Same logic for the second knot, until you get to that knot you don’t have a trajectory.&lt;/p&gt;
&lt;p&gt;###incremental curves
This can be contrasted with a different type of coding, called incremental. Here the first trajectory keeps going, whereas the second trajectory starts at the position of the knot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t2 &amp;lt;- tribble(
  ~time, ~t0, ~t1,~t2,~t3,~t4,~t5,
  &amp;quot;time 1&amp;quot;, 0, 1,2,3,4,5,
  &amp;quot;time 2&amp;quot;, 0, 0,0,1,2,3
)
t2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
##   time      t0    t1    t2    t3    t4    t5
##   &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 time 1     0     1     2     3     4     5
## 2 time 2     0     0     0     1     2     3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The two coding schemes propose the same type of trajectory, the only thing that differs is the interpretation of the coefficients.&lt;/p&gt;
&lt;p&gt;In the first, the two slope coefficients represent the actual slope in the respective time period.&lt;/p&gt;
&lt;p&gt;In the second, the coefficient for time 2 represents the deviation from the slope in period 1. The positive of this second method is you can easily test whether these two slopes are different from one another.&lt;/p&gt;
&lt;p&gt;level 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time1_{ij} + \beta_{2j}Time2_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +  U_{0j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} +  U_{2j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;###splines example&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;personality$time1 &amp;lt;- recode(personality$wave, &amp;#39;1&amp;#39; = 0 , &amp;#39;2&amp;#39; = 1,  &amp;#39;3&amp;#39; = 1, &amp;#39;4&amp;#39; = 1,&amp;#39;5&amp;#39; = 1)      
personality$time2 &amp;lt;- recode(personality$wave, &amp;#39;1&amp;#39; = 0 , &amp;#39;2&amp;#39; = 0,  &amp;#39;3&amp;#39; = 1, &amp;#39;4&amp;#39; = 2,&amp;#39;5&amp;#39; = 3) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p7 &amp;lt;- lmer(conscientiousness ~ time1 + time2 + (time1   | mapid) , data=personality)
summary(p7)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: conscientiousness ~ time1 + time2 + (time1 | mapid)
##    Data: personality
## 
## REML criterion at convergence: 10003.8
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.2558 -0.4068  0.0272  0.4304  4.5854 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  mapid    (Intercept) 32.979   5.743         
##           time1        4.729   2.175    -0.13
##  Residual             10.702   3.271         
## Number of obs: 1635, groups:  mapid, 620
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  34.1871     0.2654 128.800
## time1        -0.5365     0.2018  -2.658
## time2         0.2184     0.1561   1.399
## 
## Correlation of Fixed Effects:
##       (Intr) time1 
## time1 -0.370       
## time2  0.000 -0.301&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg5 &amp;lt;- ggplot(personality, aes(x = wave, y = conscientiousness, group = mapid)) +  stat_smooth(method = &amp;#39;lm&amp;#39;, formula = y ~ poly(x,2, raw = TRUE),data = personality, aes(x = wave, y = conscientiousness, group=1)) + scale_y_continuous(limits = c(30, 40))
gg5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 609 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/2019-09-18-week-4_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;splines-polynomial-polynomial-piecewise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;splines + polynomial = polynomial piecewise&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time1_{ij} +  \beta_{2j}Time1_{ij}^2 + \beta_{3j}Time2_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +  U_{0j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} +  U_{1j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} +  U_{2j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{3j} = \gamma_{30} +  U_{3j}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Week 3</title>
      <link>/lectures/2019-08-14-lecture-3-test/</link>
      <pubDate>Thu, 12 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/2019-08-14-lecture-3-test/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-models&#34;&gt;Conditional models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#level-2-predictors&#34;&gt;Level 2 predictors&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#group-predictors-of-intercept&#34;&gt;Group predictors of intercept&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-of-fixed-effects&#34;&gt;Interpretation of fixed effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-of-random-effects&#34;&gt;Interpretation of random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#seperatinng-these-into-intercept-and-slope&#34;&gt;Seperatinng these into intercept and slope&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slope-and-intercept-group-predictors&#34;&gt;Slope and Intercept Group Predictors&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#cross-level-interactions&#34;&gt;Cross-level interactions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#equations-necessary-for-plotting&#34;&gt;Equations necessary for plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-predictors-of-intercept-and-slope&#34;&gt;Continuous predictors of intercept and slope&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#equations-necessary-for-plotting-1&#34;&gt;Equations necessary for plotting&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-more-level-2-predictors&#34;&gt;Adding more level 2 predictors&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#centering&#34;&gt;Centering&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-centering&#34;&gt;Types of centering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#time-centering&#34;&gt;Time centering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#level-2-centering&#34;&gt;Level 2 centering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects-and-residual-standard-assumptions&#34;&gt;Random effects and residual (standard) assumptions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-generating-process-dgp&#34;&gt;Data generating process (DGP)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#estimation&#34;&gt;Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing-significance-adapted-from-ben-bolker&#34;&gt;Testing significance (adapted from Ben Bolker)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#quick-aside-p-values-are-not-included&#34;&gt;Quick aside: P values are not included&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likelihood-ratio-test&#34;&gt;Likelihood ratio test&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#likelihood-tests-for-random-effects&#34;&gt;Likelihood tests for random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#aic-and-bic&#34;&gt;AIC and BIC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coefficient-of-determination-equivalents&#34;&gt;Coefficient of determination equivalents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#level-1-predictors-aka-time-varying-covariates-tvcs&#34;&gt;Level 1 predictors AKA Time-varying covariates (TVCs)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;conditional-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conditional models&lt;/h1&gt;
&lt;p&gt;We are now going to introduce predictors to our models. These predictors are similar to predictors in standard regression – dummy for nominal, interactions change lower order terms, etcetera. These predictors can occur at different levels. Just like in standard regression, there are different ways to interpret the resulting coefficients depending on the type and where the predictor is added.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;level-2-predictors&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Level 2 predictors&lt;/h1&gt;
&lt;div id=&#34;group-predictors-of-intercept&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Group predictors of intercept&lt;/h2&gt;
&lt;p&gt;Starting with the basic, let’s add a group level variable to the model that is dummy coded. Note that group here only is measured once, it is a between person variable. Thus it can only be meaningfully added to level 2. Here we are asking the question, does group 1 differ from group 2 in their…?&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;interpretation-of-fixed-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation of fixed effects&lt;/h3&gt;
&lt;p&gt;Notice we have a new gamma term, &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}\)&lt;/span&gt;. How do we interpret this new fixed effect, especially in the presense of other fixed effects? What is the slope and what is the effect of group on the slope? &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; is the intercept and can be considered the slope when G = 0 whereas the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}\)&lt;/span&gt; is the difference in slope between groups. What is then the slope for the group = 1? &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00} + \gamma_{01}\)&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation-of-random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Interpretation of random effects&lt;/h3&gt;
&lt;p&gt;One thing to keep in mind is that we are now changing the meaning of the random effect. Previously, the random effect was interpretted as the deviation from the mean of the interept. These random effects are interpretted almost like residual terms where it is what is left over. Now that we have a predictor in the model, the &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; is the person specific deviation from the group predicted intercept, not the grand mean intercept. It is the difference from what would be expected given all the terms. In other words, it is conditional on all other predictors in the model.&lt;/p&gt;
&lt;p&gt;Combined
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (Time_{ij}) + U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is helpful to start looking at your equation in terms of what to expect in your model output. Here you have 3 fixed effects, two random effects, and one residual term.&lt;/p&gt;
&lt;p&gt;Level 2 covariance matrix
&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     &amp;amp; \tau_{00}^{2} &amp;amp; \tau_{01}\\ 
  0, &amp;amp; \tau_{01} &amp;amp; \tau_{10}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Same as before in terms of struture, but the calculations will be slightly different. Why?&lt;/p&gt;
&lt;p&gt;Level 1 residual variance
&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Same as before too. But, would you expect the residual to be smaller or larger compared to a model without a group predictor of the itnercept?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seperatinng-these-into-intercept-and-slope&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Seperatinng these into intercept and slope&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = [\gamma_{00} + \gamma_{01}G_{j}+ U_{0j}]  + [(\gamma_{10}  + U_{1j})(Time_{ij})] + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Understanding how to re-write the equation will help for calculating estimated scores for your predictors in addition to being able to interpret the coefficients. This is going to be helpful for predictions and graphing, come later.&lt;/p&gt;
&lt;p&gt;What would differ between the two equations if calculating predicted scores for group coded = 0 versus a group = 1?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;slope-and-intercept-group-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Slope and Intercept Group Predictors&lt;/h2&gt;
&lt;p&gt;Predicting the intrecept only can only answer static questions, not about change. To do that we need to introduce predictions for the slope variable, as that is our variable that indexes how people change.&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}G_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}G_{j} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Similar to before, the interpretation of &lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; changes. The term is now what is left over after accounting for group differences in the mean slope.&lt;/p&gt;
&lt;p&gt;Can you visualize what &lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; captures? Can you visualize how &lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; differs in this model and one that does not have the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{11}G_{j}\)&lt;/span&gt; term?&lt;/p&gt;
&lt;p&gt;Combined
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{01}G_{j}+  \gamma_{10} (Time_{ij}) + \gamma_{11}(G_{j}*Time_{ij}) +  U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;cross-level-interactions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Cross-level interactions&lt;/h3&gt;
&lt;p&gt;Notice that when we combine Level 1 and Level 2, the slope effect predictor becomes an interaction with time. This is called “cross-level” interaction. Anytime you have a predictor of time that will be an interaction with time in that we are asking does group status (or what ever variable) differs in their trajectory across time. One of these is a level 2 predictor and one is a level 1 predictor, thus a “cross level” interaction. Even though we don’t explicitly model an interaction, it is there because you are inserting the level to prediction, within the level 1 model to get your combined model. As a result, you are replacing your $ {}&lt;em&gt;{1j} $ (that was originally multipled by your level 1 time variable), by $ &lt;/em&gt;{10} + &lt;em&gt;{11}G&lt;/em&gt;{j} + U_{1j} $. Each of these in turn must be multipled with time.&lt;/p&gt;
&lt;p&gt;Level 2 covariance matrix
&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     &amp;amp; \tau_{00}^{2} &amp;amp; \tau_{01}\\ 
  0, &amp;amp; \tau_{01} &amp;amp; \tau_{10}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How does your variance-covariance matrix change? What is the interpretation of &lt;span class=&#34;math inline&#34;&gt;\(\tau_{01}\)&lt;/span&gt;? It is the association between random effects after accounting for (controling) group differences in intercept and slope.&lt;/p&gt;
&lt;p&gt;Level 1 residual variance
&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How does your residual change relative to a model without group effects? Can you graph conceptually what this now captures?&lt;/p&gt;
&lt;p&gt;Alternative combined
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = [\gamma_{00} + U_{0j} +\gamma_{01}G_{j}] + [(\gamma_{10}  + \gamma_{11}G_{j}+  U_{1j})(Time_{ij})] + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is just rearranged so you can see that different groups have different intercepts and slopes – very much alike simple slopes analyses for interactions in standard regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;equations-necessary-for-plotting&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Equations necessary for plotting&lt;/h3&gt;
&lt;p&gt;Note that the above equation can be simplified to get rid of the random effects to focus only on fixed effects portion. This is what you would use to get an estimated trajectory. This can be easily lifted from your output.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{ij} = [\gamma_{00} +\gamma_{01}G_{j}] + [(\gamma_{10}  + \gamma_{11}G_{j})(Time_{ij})] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice how when G = 0, the equation simplifies:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[     \hat{Y}_{ij} = \gamma_{00} + \gamma_{10} (Time_{ij}) \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-predictors-of-intercept-and-slope&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous predictors of intercept and slope&lt;/h2&gt;
&lt;p&gt;Introducing a continuous predictor is similar to the group predictors, and is similar to how continuous predictors are used in regression – remember, MLM, is just fancy regression. Here the continuous predictor is again only measured once. It is thought of as a between person variable, one that is not assessed multiple times. As a result, it must go into a level 2 equation.&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}C_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}C_{j} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Combined:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{01}C_{j}+  \gamma_{10} (Time_{ij}) + \gamma_{11}(C_{j}*Time_{ij}) +  U_{0j} + U_{1j}(Time_{ij}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As with nominal level 2 predictors, the interpretation of our intercept is now when all preditors are at zero ie time AND C.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{01}C_{j}\)&lt;/span&gt; is now the effect of time when the continous predictor is zero. Zero is meaningful when you code for dummy or effect variables, but is not always straightforward with continuous variables. It is thus recommended to &lt;em&gt;always&lt;/em&gt; center your predictors to aide in interpretation. More on what we mean by this below.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{11}\)&lt;/span&gt; coefficient is now the difference in slopes for one unit of our C variable.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; Is the random effect for intercept after accounting for C.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; Is the random effect for the slope after accounting for C.&lt;/p&gt;
&lt;p&gt;The covariance between them is now accounting for or controlling for this predictor.&lt;/p&gt;
&lt;div id=&#34;equations-necessary-for-plotting-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Equations necessary for plotting&lt;/h3&gt;
&lt;p&gt;The same logic for plotting models with nominal variables applies to continuous predictor variables. Remembering back to decomposing interactions in standard regression models, it is important to plot predicted lines at different levels of interest. Usually plus minus one SD, but if other levels are interesting then you can do that too.&lt;/p&gt;
&lt;p&gt;As an example, lets say we have the mean of C = 0 with a SD of 1. What would our equation look like to plot a predicted trajectory a SD above and below, as well as mean trajectory?&lt;/p&gt;
&lt;p&gt;-1sd
&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{ij} = [\gamma_{00} +(\gamma_{01}*-1)] + [(\gamma_{10}  + (\gamma_{11}*-1))(Time_{ij})] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Mean
&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{ij} = \gamma_{00} + \gamma_{10}  (Time_{ij}) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;+1sd
&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{ij} = [\gamma_{00} +\gamma_{01}] + [(\gamma_{10}  + \gamma_{11})(Time_{ij})] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What would individual level trajectories look like?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = [\gamma_{00} + \gamma_{01}C_{j}+  U_{0j}] + [(\gamma_{10}  + \gamma_{11} + U_{1j})   (Time_{ij})] + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice how these are just the level 2 equations, to specify intercept and slope.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-more-level-2-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding more level 2 predictors&lt;/h2&gt;
&lt;p&gt;These same principles apply to more complex models. As the semester progresses we can continue to add in more complex models, as well as the ability to compare models that differ in complexity.&lt;/p&gt;
&lt;p&gt;It is important to be able to interpret and visualize what these more complex models may look like. For example, can you think about the interpretation of each parameter as well as the plots you would want to do for a model such as looking at health across time, examing the effects of an intervention, while controlling for initial exercise status?:&lt;/p&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Health}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2:
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + \gamma_{01}Exercise_{j} +  \gamma_{02}Intervention_{j} +   U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + \gamma_{11}Intervention_{j} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;centering&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Centering&lt;/h1&gt;
&lt;div id=&#34;types-of-centering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Types of centering&lt;/h2&gt;
&lt;p&gt;Changing the scale of your predictors changes the interpretation of your model. (Redux of how to interpret lower order terms in an interaction regression model.) We have more options for centering here compared to standard regression, however.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Original metric (no centering)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Group-mean centering (our group/nesting is person so this is also called person centering). This will be more appropriate when we talk about level 1 predictors.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Grand-mean centering (this is taking the average across everyone)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Centering on a value of theoretical or applied interest&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Importantly, centering can both change the interpretation of the coefficients, as well as the fit of the model. The latter is especially true when 1) people differ on the number of assessment points (ie grand mean =/= average person mean) and 2) the intercept is far away from a group or grand mean. The latter will influence the random effect variances and their covariances. You can see this with time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;time-centering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Time centering&lt;/h2&gt;
&lt;p&gt;Our time variable is our only level 1 predictor that we have worked with up to this point. Thus far we have centered it at the beginning. We typically center time around each person’s initial time to make the intercept more interpretable. However, this can cause correlations between an intercept and a slope. If high, the correlation can be problematic in terms of estimation. Often we center time in the middle of the repeated assessments to minimize this association. Doing so is especially important if you want to use some variable to predict intercept and slope (or use interecept/slope to predict some variable).&lt;/p&gt;
&lt;p&gt;Can you visualize why a slope may be more or less correlated with an intercept depending on how we scale time?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     &amp;amp; \tau_{00}^{2} &amp;amp; \tau_{01}\\ 
  0, &amp;amp; \tau_{01} &amp;amp; \tau_{10}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is sometimes helpful to center time as the last time point. Why? So as to use a predictor in the model that is trying to longitudinally predict from the initial point, not change, but a timepoint far in the future&lt;/p&gt;
&lt;p&gt;We will talk more about centering later, but it is important to note that it can get tricky for longitudinal models when people don’t have the same number of assessment waves or the same timespan. Where do you center? One option, the most clean, is to center within each person’s own time, regardless of whether it lines up with others. This is #2 above. This is nice because it makes the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; interpretable as the average score across people.&lt;/p&gt;
&lt;p&gt;However, what is the average score? If you are looking at longitudinal data where people span in age from 20 to 80 and the time each person was in the study differed from 1 to 10 years. How do you interpret the average person intercept? Data wise it is consistent but interpretation wise it may not be. Thus you may want to center on an age ie #4 above. The &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{00}\)&lt;/span&gt; can now easily be interpreted as age 40, for example. Buuut, this results in wonky residual terms, perhaps leading to greater covariance between intercept and slope.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;level-2-centering&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Level 2 centering&lt;/h2&gt;
&lt;p&gt;Because level 2 is involved with cross level interactions, it is always helpful to at least consider centering. For level 2, the centering options are much easier, as one can generally go with grand mean centering. As everyone has only 1 value to contribute to, the calculation and the interpretation is more straightforward.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects-and-residual-standard-assumptions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Random effects and residual (standard) assumptions&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Joint normal distribution of random effects&lt;/li&gt;
&lt;li&gt;Normally distributed residual&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Constant variance over time&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Random effects &lt;span class=&#34;math inline&#34;&gt;\(\pm U_{0j}\)&lt;/span&gt; and residual &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ij}\)&lt;/span&gt; are uncorrelated and have a mean of zero&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some of these we can relax, some of these are not too bad if we violate, some of these we cannot escape. A solution, to many of these standard assumptions is to change the model. The model that we are presenting is basic in that it is all defaults.&lt;/p&gt;
&lt;div id=&#34;data-generating-process-dgp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data generating process (DGP)&lt;/h2&gt;
&lt;p&gt;Our standard assumption is that the DV comes from a data generating process that results in normal distributions. This does not mean that it needs to result in an observed normal distribution. Instead, the default of assuming an Gaussian DGP is practical: it is robust against violations and the alternatives are sometimes harder to justify.&lt;/p&gt;
&lt;p&gt;If you think you have a non-Gaussian DGP (like a Poisson or a negative binomial if you are using some sort of count data) you will need to use a different estimation technique. You can do this somewhat with the package we will be working with primarily, lme4. However, the BRMS package – which uses Bayesian estimation – has many more possibilities: geometric, log normal, weibull, exponential, gamma, Beta, hurdle Poisson/gamma/negative binomial, zero inflated beta/Poisson/negative binomial, cumulative. We will fit some of these later in the semester. Currently, however, assume we are working with&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\({Y}_{ij} \sim \mathcal{N}(0, \sigma^{2})\)&lt;/span&gt;. Altering the assumed DGP will alter the assumptions we have.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation&lt;/h1&gt;
&lt;p&gt;Maximum likelihood estimation. Uses a likelihood function that describes the probability of observing the sample data as a function of the parameters. Attempts to maximize the function through an iterative process. Because it is iterative, it might fail.&lt;/p&gt;
&lt;p&gt;There are fixed effects as well as random effects we need to count for. Maximum likelihood takes our assumptions about the model (normally distributed residuals, etc) and creates probability densities for each parameters. For example, based on certain fixed effects and sd of random effects, how likely is it that person x has a slope of z? The algorithm looks at the full sample to see how likely different parameters are, spits back the most likely, and gives you a number to show how likely they are (compared to others). This is akin to saying you rolled 10 dice, 5 came up as 2s. How likely is this dice fair? But instead of fair vs not fair it gives a likelihood to certain possibilities (e.g., a 2 comes up at 25%, 50% 75% rates).&lt;/p&gt;
&lt;p&gt;Restricted maximum likelihood (REML) vs Full Maximum likelihood (ML). Will give you similar parameters, the differences are in the standard errors. REML is similar to dividing by N - 1 for SE whereas ML is similar to dividing by N.&lt;/p&gt;
&lt;p&gt;Differences account for the fact that fixed effects are being estimated simultaneously with the variance parameters in ML. Estimates of the variance parameters assume that the fixed effects estimates are known and thus does not account for uncertainty in these estimates.&lt;/p&gt;
&lt;p&gt;REML accounts for uncertainty in the fixed effects before estimating residual variance. REML attempts to maximize the likelihood of the residuals whereas ML maximizes the sample data. REML can be thought of as an unbiased estimate of the residual variance.&lt;/p&gt;
&lt;p&gt;REML is good for small sample size both N and group. However, if you use REML you should be careful in testing fixed effects against each other (more down below). Deviances tests for fixed effects should be done with ML, but only random effects with REML. ML can also look at random effects too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-significance-adapted-from-ben-bolker&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Testing significance (adapted from Ben Bolker)&lt;/h1&gt;
&lt;p&gt;4 Methods for testing single parameters
From worst to best:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Wald Z-tests.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Wald t-tests&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Easy to compute - test statistic over standard error However, they are asymptotic standard error approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal and that (2) the sampling distribution of the log-likelihood is (proportional to) χ2.&lt;/p&gt;
&lt;p&gt;The above two are okay to do for single parameter estimates of fixed effects. But beware that a) degrees of freedom calculations are not straightforward and b) the assumptions for random effects are be hard to meet.&lt;/p&gt;
&lt;div id=&#34;quick-aside-p-values-are-not-included&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quick aside: P values are not included&lt;/h2&gt;
&lt;p&gt;Authors of the package we will be using first lme4 are not convinced of the utility of the general approach of testing with reference to an approximate null distribution. In general, it is not clear that the null distribution of the computed ratio of sums of squares is really an F distribution, for any choice of denominator degrees of freedom. While this is true for special cases that correspond to classical experimental designs (nested, split-plot, randomized block, etc.), it is apparently not true for more complex designs (unbalanced, GLMMs, temporal or spatial correlation, etc.).&lt;/p&gt;
&lt;p&gt;tl;dr: it gets messy with more complex models.&lt;/p&gt;
&lt;p&gt;If you really want p values&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(lmerTest)&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Likelihood ratio test (also called deviance test).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Markov chain Monte Carlo (MCMC) or parametric bootstrap confidence intervals ( we will get to this later)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-ratio-test&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood ratio test&lt;/h2&gt;
&lt;p&gt;Used for model comparisons (often multiparameter comparisons) and for tests of random effects. REML can only be used if model compared have the same fixed parts and only differ in random. Otherwise ML must be used.&lt;/p&gt;
&lt;p&gt;How much more likely the data is under a more complex model than under the simpler model (these models need to be nested to compare this).&lt;/p&gt;
&lt;p&gt;Log Likelihood (LL) is derived from ML estimation. Logs are used because they are computationally simpler; logs of multiplications are reduced to adding the logs together.&lt;/p&gt;
&lt;p&gt;Larger the LL the better the fit.&lt;/p&gt;
&lt;p&gt;Deviance compares two LLs. Current model and a saturated model (that fits data perfectly). Asks how much worse the current model is to the best possible model. Deviance = -2[LL current - LL saturated]&lt;/p&gt;
&lt;p&gt;LL saturated = 1 for MLMs (probability it will perfectly recapture data). log of 1 is 0. So this term drops out. Deviance = -2(LL current model). AKA -2logL or -2LL&lt;/p&gt;
&lt;p&gt;Can compare two models via subtraction, often referred to as a full and reduced model. Differences is distributed as a chi square with a df equal to how many “constraints” are included. Constraints can be thought of as forcing a parameter to be zero ie removing it.&lt;/p&gt;
&lt;p&gt;Comparing 2 models is called a likelihood ratio test. Need to have:
1. same data
2. nested models (think of constraining a parameter to zero)&lt;/p&gt;
&lt;p&gt;Why work with deviances and not just log likelihoods? Why -2? Why a ratio test when you subtract deviances? Maths. Working with deviances allows us to subtract two from one another, which is equivalent to taking the ratio of likelihoods.&lt;/p&gt;
&lt;p&gt;You can test in r using the same procedure we would to test different regression models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.2, mod.2r)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;likelihood-tests-for-random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Likelihood tests for random effects&lt;/h2&gt;
&lt;p&gt;Not listed in the output because it is harder to do this with variances. Remember variances do not have values below zero and thus the distributions get a wonky quickly. Needs mixture distributions (Cannot be easily done with chi square, for example)&lt;/p&gt;
&lt;p&gt;Can technically do anova comparisons for random effects, though that falls to many similar problems as trying to do a Wald test.&lt;/p&gt;
&lt;p&gt;The sampling distribution of variance estimates is in general strongly asymmetric: the standard error may be a poor characterization of the uncertainty. Thus the best way to handle is to do bootstrapped estimates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aic-and-bic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;AIC and BIC&lt;/h2&gt;
&lt;p&gt;Used when you want to compare non-nested data. Need to have the same data, however.&lt;/p&gt;
&lt;p&gt;AIC (Akaike’s Information Criterion) and the BIC (Bayesian Information Criterion) where “smaller is better.” This is the opposite of LL. As with the other types, these may give you wonky findings depending on some factors as they are related to LLs.&lt;/p&gt;
&lt;p&gt;AIC = 2(number of parameters) + (−2LL)
BIC = ln(n)(number of parameters) + (−2LL)&lt;/p&gt;
&lt;p&gt;BIC penalizes models with more parameters more than AIC does.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficient-of-determination-equivalents&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Coefficient of determination equivalents&lt;/h1&gt;
&lt;p&gt;You want to get a model fit estimate. BIC and AIC are good to compare nested models but they aren’t standardized and thus make comparison across non nested models difficult.&lt;/p&gt;
&lt;p&gt;With MLM models we cannot directly compute R2. Instead we will use pseudo R2. Pseudo R2 is similar to R2 in that it can be thought of as the correlation between your predicted and actual scores. For example, assume we have three waves of data. The intercept is 1, the slope is 2 and time is coded 0,1,2. The predicted scores are: 1, 3, 5. We would then correlate everyone’s first, second and third wave scores with these predicted scores. This correlation squared is pseudo R2, telling us how much variance time explains in our DV.&lt;/p&gt;
&lt;p&gt;Yes, we typically think of this as a measure of variance explained divided by total variance. This is where things get tricky: should you include or exclude variation of different random-effects terms? These are error, but they are modeled in the sense that they are not unexplained. Is the effect size wanted after you are “controlling for” or do you want to talk about total variation. There are similarities here with regards to Eta and Partial Eta squared.&lt;/p&gt;
&lt;p&gt;The general idea is to be upfront about what you are comparing and what is included. Typically this is done with comparing models, much like a hierarchical regression. Taking the difference in variance between model 1 and model 2 and dividing it by model 1 makes it explicit what you are looking at and what you are including or not including.&lt;/p&gt;
&lt;p&gt;E.g,. residual variance in varying intercept model subtracted from growth model divided by intercept only model. This can tell you how much unexplained variance is explained by time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(sigma(mod.1) - sigma(mod.2)) / sigma(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;level-1-predictors-aka-time-varying-covariates-tvcs&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Level 1 predictors AKA Time-varying covariates (TVCs)&lt;/h1&gt;
&lt;p&gt;These are predictors that are assessed at level 1, which repeate. Note that there are some variables that are inherently level 2 (e.g. handedness), some that make sense more as a level 1 (e.g., mood) and some that could be considered either depending on your research question and/or your data (e.g. income). The latter type could concievably change across time (And thus be appropriate for a level 1 variable; tvc) but may not change at the rate of your construct or not be important.&lt;/p&gt;
&lt;p&gt;We will go into these in more depth in further weeks. The two points I want to discuss now are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;These can be treated as another predictor with the effect of “controlling” for some TVC. Thus the regression coefficents in the model are conditional on this covariate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For example, if you had group status (yes, no) as your TVC the fixed effect for this would indicate the difference in slope for the two conditions. The slope coefficient would be that average slope (depending on how the covariate is scaled)&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The level 1 and level 2 models are not that different from previous forms. Here is an example model with a TVC.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;level 1:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}Time_{ij} + \beta_{2j}Job_{ij} +\varepsilon_{ij} \]&lt;/span&gt;
Level 2:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} +    U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + U_{1j} \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{2j} = \gamma_{20} \]&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It is not necessary to specify a random effect for the TVC. Doing so would suggest that the differences in group membership within a person is not the same across people. For example, the effect of jobloss may effect some peoples development but not others.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key question is whether or not we think the variability across people in their TVC effects are systematic or not. If they are systematic, then maybe it is important to predict them by another variable. Can we go further and also fit a random effects term? This is a tricky issue in that this adds an additional parameter to the random effects and thus increases the number of covariances estimated. Often our data are not large enough to estimate the increased number of parameters and results in non-convergence.&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The introduction of the TVC can reduce &lt;span class=&#34;math inline&#34;&gt;\(\tau^2_{U_{0j}}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau^2_{U_{1j}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ij}\)&lt;/span&gt;. Normal time-invariant covariates only reduce the between person variance in intercept and slope and cannot account for the within person variance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But, but, because you are adding a new variable that changes the interpretation of the gamma terms, you may actually get increases in your variance components. As a result, it is difficult to directly compare models that have TVCs and those that do not.&lt;/p&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;You may need to seperate between person and within person effects for TVC. This is done through various centering techniques.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Workshop #3</title>
      <link>/workshops/workshop-3/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-3/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#purrr&#34;&gt;purrr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background-iteration&#34;&gt;Background: Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-reading&#34;&gt;Data Reading&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#many-subjects-same-variables-example-1&#34;&gt;Many Subjects, Same Variables (Example 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#many-subjects-different-variables-example-2&#34;&gt;Many Subjects, Different Variables (Example 2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-waves-same-variables-example-3&#34;&gt;Multiple Waves, Same Variables (Example 3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-waves-different-variables-example-4&#34;&gt;Multiple Waves, Different Variables (Example 4)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-waves-multiple-files-for-same-variables-example-5&#34;&gt;Multiple Waves, Multiple Files for Same Variables (Example 5)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#running-models&#34;&gt;Running Models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#read-in-data&#34;&gt;Read in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clean-data&#34;&gt;Clean Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#descriptives&#34;&gt;Descriptives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coding-time&#34;&gt;Coding Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fit-unconditional-models&#34;&gt;Fit Unconditional Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#icc&#34;&gt;ICC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fit-growth-models&#34;&gt;Fit Growth Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-comparisons&#34;&gt;Model Comparisons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tabling-values&#34;&gt;Tabling Values&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;purrr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;purrr&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/emoriebeck/R-tutorials/master/ALDA/week_3_purrr/week_3_purrr.Rmd&#34; download&gt;Download .Rmd (won’t work in Safari or IE)&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://github.com/emoriebeck/R-tutorials/tree/master/ALDA/week_3_purrr&#34; target=&#34;_blank&#34;&gt;See GitHub Repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;#&lt;code&gt;purrr&lt;/code&gt;&lt;br /&gt;
In my opinion, &lt;code&gt;purrr&lt;/code&gt; is one of the most underrated and under-utilized &lt;code&gt;R&lt;/code&gt; packages. It has completely revolutionized my own efficiency and workspace organization, particularly as someone who works with super messy data that comes in a variety of forms.&lt;/p&gt;
&lt;p&gt;In this tutorial, we are going to cover a number of what I believe are the most functional and important applications of &lt;code&gt;purrr&lt;/code&gt; in psychological research. Given the audience, in the first half of the tutorial, I will focus on working with the diverse forms of data that many of you work with, providing examples of how to load, clean, and merge data using &lt;code&gt;purrr&lt;/code&gt;. In the second half, I will focus on how we can use &lt;code&gt;purrr&lt;/code&gt; with longitudinal data analysis when we are working with multiple predictors and outcomes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background-iteration&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background: Iteration&lt;/h1&gt;
&lt;p&gt;Before we get there, though, I think it’s useful to think about when and where we would use &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Iteration is everywhere. It underpins much of mathematics and statistics. If you’ve ever seen the &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; symbol, then you’ve seen (and probably used) iteration.&lt;/p&gt;
&lt;p&gt;It’s also incredibly useful. Anytime you have to repeat some sort of action many times, iteration is your best friend. In psychology, this often means reading in a bunch of individual data files from an experiment, repeating an analysis with a series of different predictors or outcomes, or creating a series of figures.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)
library(knitr)
library(kableExtra)
library(lme4)
library(broom.mixed)
library(plyr)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Enter &lt;code&gt;for&lt;/code&gt; loops. &lt;code&gt;for&lt;/code&gt; loops are the “OG” form of iteration in computer science. The basic syntax is below. Basically, we can use a for loop to loop through and print a series of things.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in letters[1:5]){
  print(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot;
## [1] &amp;quot;b&amp;quot;
## [1] &amp;quot;c&amp;quot;
## [1] &amp;quot;d&amp;quot;
## [1] &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above “loops” through 5 times, printing the iteration letter.&lt;/p&gt;
&lt;p&gt;Essentially, like the &lt;code&gt;apply()&lt;/code&gt;, &lt;code&gt;lapply()&lt;/code&gt;, &lt;code&gt;sapply()&lt;/code&gt;, and &lt;code&gt;mapply()&lt;/code&gt; family of functions, &lt;code&gt;purrr&lt;/code&gt; is meant to be an alternative to iteration (i.e. &lt;code&gt;for&lt;/code&gt; loops) in &lt;code&gt;R&lt;/code&gt;. &lt;code&gt;for&lt;/code&gt; loops are great, but they aren’t as great in &lt;code&gt;R&lt;/code&gt; as they are in other programming languages. In &lt;code&gt;R&lt;/code&gt;, you’re better off vectorizing or building in C++ backends.&lt;/p&gt;
&lt;p&gt;There are a lot of functions in the &lt;code&gt;purrr&lt;/code&gt; package that I encourage you to check out. Today, though, we’ll focus on the &lt;code&gt;map()&lt;/code&gt; family of functions. The breakdown of map functions is pretty intuitive. The basic map function wants two things as input – a list or vector and a function. So the &lt;code&gt;purrr&lt;/code&gt; equivalent of the example above would be:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(letters[1:5], print)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot;
## [1] &amp;quot;b&amp;quot;
## [1] &amp;quot;c&amp;quot;
## [1] &amp;quot;d&amp;quot;
## [1] &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;a&amp;quot;
## 
## [[2]]
## [1] &amp;quot;b&amp;quot;
## 
## [[3]]
## [1] &amp;quot;c&amp;quot;
## 
## [[4]]
## [1] &amp;quot;d&amp;quot;
## 
## [[5]]
## [1] &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this returns a list, which we may not always want. With &lt;code&gt;purrr&lt;/code&gt;, we can change the kind of output of &lt;code&gt;map()&lt;/code&gt; by adding a predicate, like &lt;code&gt;lgl&lt;/code&gt;, &lt;code&gt;dbl&lt;/code&gt;, &lt;code&gt;chr&lt;/code&gt;, and &lt;code&gt;df&lt;/code&gt;. So in the example above, we may have wanted just the characters to print. To do that we’d call &lt;code&gt;map_chr()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_chr(letters[1:5], print)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot;
## [1] &amp;quot;b&amp;quot;
## [1] &amp;quot;c&amp;quot;
## [1] &amp;quot;d&amp;quot;
## [1] &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;d&amp;quot; &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that it also returns the concatenated character vector as well as printing each letter individually (i.e. iteratively).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;map()&lt;/code&gt; functions can also hand multiple inputs. Often we may need to input multiple pieces of information to a function, similarly to how we work with nested &lt;code&gt;for&lt;/code&gt; loops. In this case, we have &lt;code&gt;map2()&lt;/code&gt; and &lt;code&gt;pmap()&lt;/code&gt; that take additional arguments. &lt;code&gt;map2()&lt;/code&gt; shockingly takes two inputs and &lt;code&gt;pmap()&lt;/code&gt; takes p arguments that you feed in as list (e.g. &lt;code&gt;pmap(list(a, b, c, d), my_fun))&lt;/code&gt;. A simple printing example would be:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map2_chr(letters[1:5], 1:5, paste)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a 1&amp;quot; &amp;quot;b 2&amp;quot; &amp;quot;c 3&amp;quot; &amp;quot;d 4&amp;quot; &amp;quot;e 5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that we can use &lt;code&gt;map2()&lt;/code&gt; and &lt;code&gt;pmap()&lt;/code&gt; with the predicates from above.&lt;/p&gt;
&lt;p&gt;This likely makes little sense at this point, and that’s fine. The examples in the rest of this tutorial should elucidate their usage. The last note I’ll make is that thinking about the structure of your data is going to be very important when using &lt;code&gt;purrr&lt;/code&gt;. To use it effectively, you’ll need your data in specific forms, which will often require data manipulations. It just takes practice.&lt;/p&gt;
&lt;p&gt;Regardless of the programmatic form, iteration is everywhere. It underpins much of mathematics and statistics. If you’ve ever seen the &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; symbol, then you’ve seen (and probably used) iteration.&lt;/p&gt;
&lt;p&gt;It’s also incredibly useful. Anytime you have to repeat some sort of action many times, iteration is your best friend. In psychology, this could mean reading in a bunch of separate data files (with separate files for different people, variables, waves, etc.) or performing a number of regressions or other statistical tests.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-reading&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Reading&lt;/h1&gt;
&lt;p&gt;To demonstrate the first case in which I find &lt;code&gt;purrr&lt;/code&gt; useful, we are going to consider a five cases that, in my experience, capture many of the challenges we often face in working with psychological data. In each of these cases, we will use a codebook of the form we discussed in the previous tutorial on codebooks.&lt;/p&gt;
&lt;p&gt;All of these share a similar feature: multiple files. There are a variety of other techniques you could use to get your data into a usable form, such as those below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
Writing code to load in each file separately (not good).
&lt;/li&gt;
&lt;li&gt;
Copying each data file into one larger data set in Excel (worse)
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But let’s not do that. Let’s use iteration to make our process efficient and transparent.&lt;/p&gt;
&lt;div id=&#34;many-subjects-same-variables-example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Many Subjects, Same Variables (Example 1)&lt;/h2&gt;
&lt;p&gt;We will start with a data storage format that is very common in experimental studies in various fields of psychology as well as in observational studies of repeated assessments of individuals (i.e. ESM, EMA, etc.).&lt;/p&gt;
&lt;p&gt;For this first example, I’ll show you how this would look with a &lt;code&gt;for&lt;/code&gt; loop before I show you how it looks with &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Assuming you have all the data in a single folder and the format is reasonably similar, you have the following basic syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_path &amp;lt;- &amp;quot;&amp;quot;
files &amp;lt;- list.files(data_path)
data &amp;lt;- list()
for(i in files){
  data[[i]] &amp;lt;- read.csv(i, stringsAsFactors = F)
}
data &amp;lt;- combine(data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This works fine in this simple case, but where &lt;code&gt;purrr&lt;/code&gt; really shines in when you need to make modifications to your data before combining, whether this be recoding, removing missing cases, or renaming variables.&lt;/p&gt;
&lt;p&gt;But first, the simple case of reading data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_path &amp;lt;- &amp;quot;~/Documents/week_3_purrr&amp;quot;
df1 &amp;lt;- tibble(ID = list.files(sprintf(&amp;quot;%s/data/example_1&amp;quot;, data_path))) %&amp;gt;%
  mutate(path = sprintf(&amp;quot;%s/data/example_1/%s&amp;quot;, data_path, ID),
         data = map(path, read_csv),
         ID = str_remove(ID, &amp;quot;.csv&amp;quot;)) %&amp;gt;%
  unnest(data) %&amp;gt;%
  select(-path)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above creates a list of ID’s from the data path (files named for each person), reads the data in using the &lt;code&gt;map()&lt;/code&gt; function from &lt;code&gt;purrr&lt;/code&gt;, removes the “.csv” from the ID variable, then unnests the data, resulting in a data frame for each person.&lt;/p&gt;
&lt;p&gt;But often, we have variable names that aren’t super informative, so we want to rename them. In this case, we need to use our codebook to give them more informative variable names.&lt;/p&gt;
&lt;p&gt;In this case, where all people have the same variables, it’s easiest to just rename them after unnesting, so the full code would look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_path &amp;lt;- &amp;quot;https://github.com/emoriebeck/R-tutorials/raw/master&amp;quot;
(codebook &amp;lt;- sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/codebook_ex1.csv&amp;quot;, data_path) %&amp;gt;% read_csv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 2
##    new_name   old_name
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;   
##  1 O_AesSens  O_1     
##  2 E_Assert   E_1     
##  3 N_Depr     N_1     
##  4 N_EmoVol   N_2     
##  5 O_IntCur   O_2     
##  6 C_Org      C_1     
##  7 A_Rspct    A_1     
##  8 C_Rspnbl   C_2     
##  9 A_Cmpn     A_2     
## 10 O_CrtvImag O_3     
## 11 E_EnerLev  E_2     
## 12 C_Prdctv   C_3     
## 13 A_Trust    A_3     
## 14 N_Anxty    N_3     
## 15 E_Scblty   E_3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;old.names &amp;lt;- codebook$old_name
new.names &amp;lt;- codebook$new_name
df1 &amp;lt;- tibble(ID = list.files(sprintf(&amp;quot;%s/data/example_1&amp;quot;, data_path))) %&amp;gt;%
  mutate(path = sprintf(&amp;quot;%s/data/example_1/%s&amp;quot;, data_path, ID),
         data = map(path, read_csv),
         ID = str_remove(ID, &amp;quot;.csv&amp;quot;))%&amp;gt;%
  unnest(data) %&amp;gt;%
  select(ID, old.names) %&amp;gt;%
  setNames(c(&amp;quot;ID&amp;quot;, new.names))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;many-subjects-different-variables-example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Many Subjects, Different Variables (Example 2)&lt;/h2&gt;
&lt;p&gt;In some cases, participants may have different variables. This could be do to a skip rule in a study or intentionally different variable collection (e.g. in between-person experiments or idiographic work like I do). In this case, we might need to filter or rename variables within our iterative loop.&lt;/p&gt;
&lt;p&gt;In this case, all participants have the same set of core variables but were randomly assigned to complete one additional scale.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2 &amp;lt;- tibble(ID = list.files(sprintf(&amp;quot;%s/data/example_2&amp;quot;, data_path))) %&amp;gt;%
  mutate(path = sprintf(&amp;quot;%s/data/example_2/%s&amp;quot;, data_path, ID),
         data = map(path, read_csv),
         ID = str_remove(ID, &amp;quot;.csv&amp;quot;))%&amp;gt;%
  unnest(data) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-waves-same-variables-example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Waves, Same Variables (Example 3)&lt;/h2&gt;
&lt;p&gt;In some cases, instead of multiple files for each participant, we collect a single file for all participants across different waves (e.g. using Qualtrics). In this case, we need to index the files a little differently. Instead of reading in files for participants, we need to read in files for waves, which may be named in a variety of ways.&lt;/p&gt;
&lt;p&gt;Here, I’ll start with a simple example of data that were well-managed and nicely named the same except for wave content. This is a good practice to do. I’m in general against modifying data, but I am a fan of changing file &lt;em&gt;names&lt;/em&gt; because I think this actually helps with data management and prevents the need to actually go in and modify information within files.&lt;/p&gt;
&lt;p&gt;These data come from a longitudinal study of personality. We have seven waves, and the variable names for all items are consistent across waves. In this case, our code is almost identical to reading in multiple files for each participant, except that now we have wave info and will need to toss out part of the file names at the end.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codebook &amp;lt;- sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/codebook_ex3.csv&amp;quot;, data_path) %&amp;gt;% read_csv
old.names &amp;lt;- str_remove_all(codebook$old_name, &amp;quot;[ ]&amp;quot;)
new.names &amp;lt;- codebook$new_name

df3 &amp;lt;- tibble(wave = paste(&amp;quot;T&amp;quot;, 1:7, sep = &amp;quot;&amp;quot;),
              path = sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/example_3/%s.csv&amp;quot;, data_path, wave)) %&amp;gt;%
  mutate(data = map(path, read_csv),
         wave = as.numeric(str_extract_all(wave, &amp;quot;[0-9]&amp;quot;))) %&amp;gt;%
  select(-path) %&amp;gt;%
  unnest(data) %&amp;gt;%
  select(old.names) %&amp;gt;%
  setNames(new.names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only change from the code for reading in multiple files for participants is that we have “wave” as a variable instead of “ID” and we use the &lt;code&gt;str_extract_all()&lt;/code&gt; function from the &lt;code&gt;stringr&lt;/code&gt; package (part of &lt;code&gt;tidyverse&lt;/code&gt;) to get rid of everything except the numeric wave value.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-waves-different-variables-example-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Waves, Different Variables (Example 4)&lt;/h2&gt;
&lt;p&gt;Oftentimes, however, we do not have the same variables across waves or they do have the same names across waves. In those cases, we’ll have to do a little extra work to get our data into a form where we can &lt;code&gt;unnest()&lt;/code&gt; them – that is where shared column names will actually be shared.&lt;/p&gt;
&lt;p&gt;We’ll start with the case where we have some additional information (e.g. demographics) in the first wave.&lt;/p&gt;
&lt;p&gt;These data are the same as we used in the previous example except that I changed the names and added demographic information for this example. This means that we have slightly different information in wave one and need a way to match the same variables across waves. We’ll use our codebook to achieve this with little issue!&lt;/p&gt;
&lt;p&gt;However, because of this, we’ll need to use a function that take the year as input, so that we pull the correct variables from the codebook.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_fun &amp;lt;- function(Wave){
  old.names &amp;lt;- str_remove_all((codebook %&amp;gt;% filter(wave == &amp;quot;All&amp;quot; | wave == Wave))$old_name, &amp;quot;[ ]&amp;quot;)
  new.names &amp;lt;- (codebook %&amp;gt;% filter(wave == &amp;quot;All&amp;quot; | wave == Wave))$new_name
  
  sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/example_4/T%s.csv&amp;quot;, data_path, Wave) %&amp;gt;%
    read_csv() %&amp;gt;%
    select(old.names) %&amp;gt;%
    setNames(new.names) %&amp;gt;%
    gather(key = item, value = value, -SID)
}

codebook &amp;lt;- sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/codebook_ex4.csv&amp;quot;, data_path) %&amp;gt;% read_csv

df4 &amp;lt;- tibble(wave = 1:7) %&amp;gt;%
  mutate(data = map(wave, read_fun)) %&amp;gt;%
  unnest(data) %&amp;gt;%
  unite(tmp, item, wave, sep = &amp;quot;.&amp;quot;) %&amp;gt;%
  spread(tmp, value) %&amp;gt;%
  gather(key = item, value = value, -SID, -contains(&amp;quot;Dem&amp;quot;)) %&amp;gt;%
  separate(item, c(&amp;quot;item&amp;quot;, &amp;quot;wave&amp;quot;), sep = &amp;quot;[.]&amp;quot;) %&amp;gt;%
  spread(item, value) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-waves-multiple-files-for-same-variables-example-5&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Waves, Multiple Files for Same Variables (Example 5)&lt;/h2&gt;
&lt;p&gt;In other cases, we may have multiple types of files for different waves. Across waves, those variables may be the same or different, but we’ll focus on the case when we largely want the same variables.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Running Models&lt;/h1&gt;
&lt;p&gt;Another really powerful feature of &lt;code&gt;purrr&lt;/code&gt; is keeping your data, models, tables, plots, etc all conveniently indexed together. Often we need to do this for multiple DV’s or predictors, and you may end up with an environment that looks something like &lt;code&gt;E_fit1&lt;/code&gt;, &lt;code&gt;A_fit1&lt;/code&gt;, &lt;code&gt;E_fit2&lt;/code&gt;, &lt;code&gt;A_fit2&lt;/code&gt; and so on. There’s nothing wrong with this. But eventually you’ll want to pull out coefficients, plot results, etc., and it’s easy to make a copy and paste error or name different types of objects inconsistently, which can be difficult both for future you or someone else using your code.&lt;/p&gt;
&lt;p&gt;Before we can learn how to use &lt;code&gt;purrr&lt;/code&gt; for this, we need to understand what a nested data frame is. If you’ve ever worked with a list in R, you are halfway there. Basically a nested data frame takes the normal data frame you are probably familiar with and adds some new features. It still has columns, rows, and cells, but what makes up those cells isn’t restrictred to numbers, strings, or logicals. Instead, you can put essentially anything you want: lists, models, data frames, plots, etc!&lt;/p&gt;
&lt;p&gt;If this sounds scary, it will hopefully become clearer if we use our read in data from above to run, table, and plot some basic longitudinal models of our data.&lt;/p&gt;
&lt;div id=&#34;read-in-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Read in Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codebook &amp;lt;- sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/codebook_ex6.csv&amp;quot;, data_path) %&amp;gt;%
  read_csv %&amp;gt;%
  mutate(old_name = str_to_lower(old_name))

read_fun &amp;lt;- function(Year){
  old.names &amp;lt;- (codebook %&amp;gt;% filter(year == Year | year == 0))$old_name
  new.names &amp;lt;- (codebook %&amp;gt;% filter(year == Year | year == 0))$new_name
  set &amp;lt;- (codebook %&amp;gt;% filter(year == Year))$dataset[1]
  sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/example_6/%s.csv&amp;quot;, data_path, set) %&amp;gt;%
    read_csv %&amp;gt;%
    select(old.names) %&amp;gt;%
    setNames(new.names)
}

(df6 &amp;lt;- tibble(year = 2005:2015) %&amp;gt;%
  mutate(data = map(year, read_fun)) %&amp;gt;%
  select(-year) %&amp;gt;%
  unnest(data) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 256,663 x 151
##    Procedural__SID Procedural__hou… `Big 5__C_thoro… `Big 5__E_commu…
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1             901               94                6                4
##  2            1202              124                7                6
##  3            2301              230                7                6
##  4            2304              230                5                7
##  5            2302              230                6                5
##  6            4601              469                6                6
##  7            4701              477                6                6
##  8            4901              493                5                7
##  9            5201              523                7                5
## 10            5202              523                6                5
## # … with 256,653 more rows, and 147 more variables: `Big
## #   5__A_coarse.2005` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2005` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2005` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2005` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2005` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2005` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__DadDied.2005` &amp;lt;dbl&amp;gt;, `Life Event__Married.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2005` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__PartDied.2005` &amp;lt;dbl&amp;gt;, `Life Event__SepPart.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2006` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2006` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2006` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2006` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Married.2006` &amp;lt;dbl&amp;gt;, `Life Event__MomDied.2006` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MoveIn.2006` &amp;lt;dbl&amp;gt;, `Life Event__PartDied.2006` &amp;lt;dbl&amp;gt;, `Life
## #   Event__SepPart.2006` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2007` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__DadDied.2007` &amp;lt;dbl&amp;gt;, `Life Event__Married.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2007` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__PartDied.2007` &amp;lt;dbl&amp;gt;, `Life Event__SepPart.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2008` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2008` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2008` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Married.2008` &amp;lt;dbl&amp;gt;, `Life Event__MomDied.2008` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MoveIn.2008` &amp;lt;dbl&amp;gt;, `Life Event__PartDied.2008` &amp;lt;dbl&amp;gt;, `Life
## #   Event__SepPart.2008` &amp;lt;dbl&amp;gt;, `Big 5__C_thorough.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_communic.2009` &amp;lt;dbl&amp;gt;, `Big 5__A_coarse.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_original.2009` &amp;lt;dbl&amp;gt;, `Big 5__N_worry.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_forgive.2009` &amp;lt;dbl&amp;gt;, `Big 5__C_lazy.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_sociable.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_artistic.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_nervous.2009` &amp;lt;dbl&amp;gt;, `Big 5__C_efficient.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_reserved.2009` &amp;lt;dbl&amp;gt;, `Big 5__A_friendly.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_imagin.2009` &amp;lt;dbl&amp;gt;, `Big 5__N_dealStress.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2009` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2009` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Married.2009` &amp;lt;dbl&amp;gt;, `Life Event__MomDied.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MoveIn.2009` &amp;lt;dbl&amp;gt;, `Life Event__PartDied.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__SepPart.2009` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2010` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__DadDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__Married.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__PartDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__SepPart.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2011` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2011` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__NewPart.2011` &amp;lt;dbl&amp;gt;, `Life Event__Married.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2011` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__PartDied.2011` &amp;lt;dbl&amp;gt;, `Life Event__SepPart.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2012` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2012` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2012` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2012` &amp;lt;dbl&amp;gt;, `Life
## #   Event__NewPart.2012` &amp;lt;dbl&amp;gt;, `Life Event__Married.2012` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2012` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2012` &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clean Data&lt;/h2&gt;
&lt;p&gt;Now the data are all loaded in and have been given informative variable names, but we still need to do some data cleaning for the personality data.&lt;/p&gt;
&lt;p&gt;We’ll start by selecting only the personality variables and reverse-scoring them. Then we’ll create composites. To do so, we’ll again use our codebook.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# reverse code
(df6_long &amp;lt;- df6 %&amp;gt;%
  select(Procedural__SID, contains(&amp;quot;Big 5&amp;quot;)) %&amp;gt;%
  gather(key = item, value = value, -Procedural__SID, na.rm = T) %&amp;gt;%
  left_join(codebook %&amp;gt;% select(item = new_name, reverse, mini, maxi)) %&amp;gt;%
  mutate(value = ifelse(reverse == 1, value, 
                        reverse.code(-1, value, mini = mini, maxi = maxi))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 544,830 x 6
##    Procedural__SID item                   value reverse  mini  maxi
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 Big 5__C_thorough.2005     6       1     1     8
##  2            1202 Big 5__C_thorough.2005     7       1     1     8
##  3            2301 Big 5__C_thorough.2005     7       1     1     8
##  4            2304 Big 5__C_thorough.2005     5       1     1     8
##  5            2302 Big 5__C_thorough.2005     6       1     1     8
##  6            4601 Big 5__C_thorough.2005     6       1     1     8
##  7            4701 Big 5__C_thorough.2005     6       1     1     8
##  8            4901 Big 5__C_thorough.2005     5       1     1     8
##  9            5201 Big 5__C_thorough.2005     7       1     1     8
## 10            5202 Big 5__C_thorough.2005     6       1     1     8
## # … with 544,820 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create compoistes  
(df6_long &amp;lt;- df6_long %&amp;gt;%
  mutate(item = str_remove(item, &amp;quot;Big 5__&amp;quot;)) %&amp;gt;%
  separate(item, c(&amp;quot;trait&amp;quot;, &amp;quot;item&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  separate(item, c(&amp;quot;item&amp;quot;, &amp;quot;year&amp;quot;), sep = &amp;quot;[.]&amp;quot;) %&amp;gt;%
  group_by(Procedural__SID, trait, year) %&amp;gt;%
  summarize(value = mean(value, na.rm = T)) %&amp;gt;%
  ungroup())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 181,610 x 4
##    Procedural__SID trait year  value
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 A     2005   5   
##  2             901 A     2009   5.33
##  3             901 A     2013   5   
##  4             901 C     2005   5.33
##  5             901 C     2009   3.33
##  6             901 C     2013   6   
##  7             901 E     2005   4   
##  8             901 E     2009   4   
##  9             901 E     2013   4   
## 10             901 N     2005   4   
## # … with 181,600 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;descriptives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Descriptives&lt;/h2&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Descriptive Statistics of Study Variables
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Extraversion
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Agreeablness
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Conscientiousness
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Neuroticism
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Openness
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Year
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2005
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2009
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.36
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.08
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.04
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;coding-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coding Time&lt;/h2&gt;
&lt;p&gt;It’s important to remember how we code time. There are several ways we can do it. For now, for simplicity, we will create a new wave variable where 2005 = 0, 2009 = 1, and 2013 = 3, but we could make a lot of other choices depending on our goals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_long &amp;lt;- df6_long %&amp;gt;%
  mutate(wave = as.numeric(mapvalues(year, from = seq(2005, 2013, 4), to = seq(0, 2, 1)))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 181,610 x 5
##    Procedural__SID trait year  value  wave
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 A     2005   5        0
##  2             901 A     2009   5.33     1
##  3             901 A     2013   5        2
##  4             901 C     2005   5.33     0
##  5             901 C     2009   3.33     1
##  6             901 C     2013   6        2
##  7             901 E     2005   4        0
##  8             901 E     2009   4        1
##  9             901 E     2013   4        2
## 10             901 N     2005   4        0
## # … with 181,600 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s going to get mad later when I run growth models if I keep people with only one wave, so we’re going to remove them now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_long &amp;lt;- df6_long %&amp;gt;%
  group_by(trait, Procedural__SID) %&amp;gt;%
  filter(n() &amp;gt; 1) %&amp;gt;%
  ungroup())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 110,935 x 5
##    Procedural__SID trait year  value  wave
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 A     2005   5        0
##  2             901 A     2009   5.33     1
##  3             901 A     2013   5        2
##  4             901 C     2005   5.33     0
##  5             901 C     2009   3.33     1
##  6             901 C     2013   6        2
##  7             901 E     2005   4        0
##  8             901 E     2009   4        1
##  9             901 E     2013   4        2
## 10             901 N     2005   4        0
## # … with 110,925 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-unconditional-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit Unconditional Models&lt;/h2&gt;
&lt;p&gt;Now, here we could run separate unconditional growth models for each of the Big 5 like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit0_E &amp;lt;- lmer(value ~ 1 + (1 | Procedural__SID), data = df6_long %&amp;gt;% filter(trait == &amp;quot;E&amp;quot;))
summary(fit0_E)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: value ~ 1 + (1 | Procedural__SID)
##    Data: df6_long %&amp;gt;% filter(trait == &amp;quot;E&amp;quot;)
## 
## REML criterion at convergence: 62339.1
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.8959 -0.4997  0.0144  0.5498  3.3758 
## 
## Random effects:
##  Groups          Name        Variance Std.Dev.
##  Procedural__SID (Intercept) 0.7865   0.8868  
##  Residual                    0.5308   0.7286  
## Number of obs: 22187, groups:  Procedural__SID, 8592
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  5.11572    0.01078   474.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this would be tedious and prone to error. So instead we will use list columns to do it. We’ll start by using the &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;nest()&lt;/code&gt; functions from &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; to put the data for each trait into a cell of our data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_long %&amp;gt;%
  group_by(trait) %&amp;gt;%
  nest())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
## # Groups:   trait [5]
##   trait           data
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt;
## 1 A       [22,187 × 4]
## 2 C       [22,187 × 4]
## 3 E       [22,187 × 4]
## 4 N       [22,187 × 4]
## 5 O       [22,187 × 4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, our data frame is 5 x 2, with the elements in the second column each containing the data frame that corresponds to that trait. This makes it really easy to run our models using the &lt;code&gt;map()&lt;/code&gt; family of unctions from &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Below, we will add a new column to our data frame that will contain the unconditional model for each trait.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(fit0 = map(data, ~lmer(value ~ 1 + (1 | Procedural__SID), data = .))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
## # Groups:   trait [5]
##   trait           data fit0     
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;   
## 1 A       [22,187 × 4] &amp;lt;lmerMod&amp;gt;
## 2 C       [22,187 × 4] &amp;lt;lmerMod&amp;gt;
## 3 E       [22,187 × 4] &amp;lt;lmerMod&amp;gt;
## 4 N       [22,187 × 4] &amp;lt;lmerMod&amp;gt;
## 5 O       [22,187 × 4] &amp;lt;lmerMod&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can see we have a new list column in our data frame called fit0 that contains an S4 class lmerMod, which simply means your growth model. To understand model, I personally find it easiest to visualize it. What this model is telling us is the mean across all observations as well as the between-person variability in that estimate. I find it easiest to plot this. We’ll go over the code for it next week.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/week_3_purrr_files/figure-html/unc%20plot%20ex6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;icc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ICC&lt;/h2&gt;
&lt;p&gt;If you remember, what we’re often intersted in with the unconditional model is the ICC (relative between v. within variance), so let’s extract that from the models using the &lt;code&gt;ICC()&lt;/code&gt; function from the &lt;code&gt;reghelper&lt;/code&gt; package. In this case, we will use a version of &lt;code&gt;map()&lt;/code&gt; called &lt;code&gt;map_dbl&lt;/code&gt; because we want our result to be a regular numeric column, not a list column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(ICC = map_dbl(fit0, reghelper::ICC)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 4
## # Groups:   trait [5]
##   trait           data fit0        ICC
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 A       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.484
## 2 C       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.490
## 3 E       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.597
## 4 N       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.543
## 5 O       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.545&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-growth-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit Growth Models&lt;/h2&gt;
&lt;p&gt;What we’re starting to see is that we still have a tidy working environment, but we’re still holding onto a lot of info that we can access with relative ease.&lt;/p&gt;
&lt;p&gt;But before we get to things like pulling info from our models, let’s go ahead and run our basic growth model with and without a random slope.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(fit1 = map(data, ~lmer(value ~ 1 + wave + (1 | Procedural__SID), data = .)),
         fit2 = map(data, ~lmer(value ~ 1 + wave + (wave | Procedural__SID), data = .))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 6
## # Groups:   trait [5]
##   trait           data fit0        ICC fit1      fit2     
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;   
## 1 A       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.484 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 2 C       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.490 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 3 E       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.597 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 4 N       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.543 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 5 O       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.545 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our data frame has expanded to have two more columns.&lt;/p&gt;
&lt;p&gt;Let’s visualize the difference between these two models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_fun &amp;lt;- function(m){
  crossing(wave = seq(0, 2, .5), 
           Procedural__SID = m@frame$Procedural__SID) %&amp;gt;%
    mutate(pred = predict(m, newdata = .))
}

subs &amp;lt;- sample(df6_nested$fit1[[1]]@frame$Procedural__SID, 50)
df6_nested %&amp;gt;%
  select(trait, fit1, fit2) %&amp;gt;%
  gather(model, fit, fit1, fit2) %&amp;gt;%
  mutate(model = mapvalues(model, c(&amp;quot;fit1&amp;quot;, &amp;quot;fit2&amp;quot;), c(&amp;quot;Random Intercept&amp;quot;, &amp;quot;Random Intercept + Slope&amp;quot;)),
         pred = map(fit, pred_fun)) %&amp;gt;%
  select(trait, model, pred) %&amp;gt;%
  unnest(pred) %&amp;gt;%
  mutate(Procedural__SID = as.character(Procedural__SID)) %&amp;gt;%
  filter(Procedural__SID %in% subs) %&amp;gt;%
  ggplot(aes(x = wave, y = pred, color = Procedural__SID, group = Procedural__SID)) + 
    geom_line(alpha = .5, size = .25) +
    facet_grid(model ~ trait) +
    theme_classic() +
    theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/week_3_purrr_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparisons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Comparisons&lt;/h2&gt;
&lt;p&gt;To decide if we should have a random slope, we typically do nested model comparisons. We can do that here, too. Here, we need to use both fit1 and fit2, so we’ll use the &lt;code&gt;map2()&lt;/code&gt; function from &lt;code&gt;purrr&lt;/code&gt; to take 2 inputs and use the &lt;code&gt;anova()&lt;/code&gt; function to compare them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(anova1 = map2(fit1, fit2, anova)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 7
## # Groups:   trait [5]
##   trait           data fit0        ICC fit1      fit2      anova1          
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;          
## 1 A       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.484 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;
## 2 C       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.490 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;
## 3 E       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.597 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;
## 4 N       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.543 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;
## 5 O       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.545 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see the results, we can do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df6_nested$anova1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)
## .x[[1L]]  4 58533 58565 -29263    58525                         
## .y[[1L]]  6 58535 58583 -29261    58523 2.8338      2     0.2425
## 
## [[2]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)   
## .x[[1L]]  4 57394 57426 -28693    57386                            
## .y[[1L]]  6 57389 57437 -28688    57377 9.3688      2   0.009238 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## [[3]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)   
## .x[[1L]]  4 62305 62337 -31149    62297                            
## .y[[1L]]  6 62298 62346 -31143    62286 11.531      2   0.003134 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## [[4]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)    
## .x[[1L]]  4 66171 66203 -33081    66163                             
## .y[[1L]]  6 66146 66194 -33067    66134 28.585      2  6.207e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## [[5]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)    
## .x[[1L]]  4 67780 67812 -33886    67772                             
## .y[[1L]]  6 67766 67814 -33877    67754 18.507      2  9.579e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tabling-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tabling Values&lt;/h2&gt;
&lt;p&gt;Looks like we have enough slope variance for all traits but Agreeableness to proceed with the random slope models. We’re going to proceed with the random slope models for all traits for consistency.&lt;/p&gt;
&lt;p&gt;The next thing we want to do is actually examine the model coefficients. To do that, I prefer to use the &lt;code&gt;tidy()&lt;/code&gt; function from the &lt;code&gt;broom.mixed&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(tidy = map(fit2, ~tidy(., conf.int = T))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 8
## # Groups:   trait [5]
##   trait           data fit0      ICC fit1    fit2    anova1      tidy      
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;  &amp;lt;list&amp;gt;  &amp;lt;list&amp;gt;      &amp;lt;list&amp;gt;    
## 1 A       [22,187 × 4] &amp;lt;lmerM… 0.484 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […
## 2 C       [22,187 × 4] &amp;lt;lmerM… 0.490 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […
## 3 E       [22,187 × 4] &amp;lt;lmerM… 0.597 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […
## 4 N       [22,187 × 4] &amp;lt;lmerM… 0.543 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […
## 5 O       [22,187 × 4] &amp;lt;lmerM… 0.545 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a new column called tidy that contains a data frame. But we want to be able to see those values. This is where &lt;code&gt;purrr&lt;/code&gt; will really shine once again, especially when coupled with the &lt;code&gt;unnest()&lt;/code&gt; from &lt;code&gt;tidyr&lt;/code&gt;. Watch:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df6_nested %&amp;gt;%
  select(trait, tidy) %&amp;gt;%
  unnest(tidy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 x 9
## # Groups:   trait [5]
##    trait effect group term  estimate std.error statistic conf.low conf.high
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 A     fixed  &amp;lt;NA&amp;gt;  (Int…   5.77     0.0107     539.     5.75      5.79  
##  2 A     fixed  &amp;lt;NA&amp;gt;  wave   -0.0576   0.00646     -8.91  -0.0702   -0.0449
##  3 A     ran_p… Proc… sd__…   0.701   NA           NA     NA        NA     
##  4 A     ran_p… Proc… sd__…   0.107   NA           NA     NA        NA     
##  5 A     ran_p… Proc… cor_…  -0.121   NA           NA     NA        NA     
##  6 A     ran_p… Resi… sd__…   0.707   NA           NA     NA        NA     
##  7 C     fixed  &amp;lt;NA&amp;gt;  (Int…   6.22     0.0105     592.     6.20      6.24  
##  8 C     fixed  &amp;lt;NA&amp;gt;  wave   -0.0461   0.00633     -7.29  -0.0586   -0.0337
##  9 C     ran_p… Proc… sd__…   0.699   NA           NA     NA        NA     
## 10 C     ran_p… Proc… sd__…   0.143   NA           NA     NA        NA     
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is fine, but kind of ugly (I can’t publish this table, and it should be clear right now that I do not like copying and pasting).&lt;/p&gt;
&lt;p&gt;The code below is going to clean this up a bit. See if you can figure out what’s going on:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;terms &amp;lt;- tibble(old = c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;wave&amp;quot;, &amp;quot;sd__(Intercept)&amp;quot;, &amp;quot;sd__wave&amp;quot;,
                        &amp;quot;cor__(Intercept).wave&amp;quot;, &amp;quot;sd__Observation&amp;quot;),
                new = c(&amp;quot;Intercept&amp;quot;, &amp;quot;Slope&amp;quot;, &amp;quot;SD Intercept&amp;quot;, &amp;quot;SD Slope&amp;quot;, &amp;quot;Intercept-Slope Correlation&amp;quot;, &amp;quot;SD Residual&amp;quot;))

(tab &amp;lt;- df6_nested %&amp;gt;%
  select(trait, tidy) %&amp;gt;%
  unnest(tidy) %&amp;gt;% 
  mutate(term = mapvalues(term, from = terms$old, to = terms$new)) %&amp;gt;%
  select(trait, effect, term, estimate, conf.low, conf.high))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 x 6
## # Groups:   trait [5]
##    trait effect   term                        estimate conf.low conf.high
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;                          &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 A     fixed    Intercept                     5.77     5.75      5.79  
##  2 A     fixed    Slope                        -0.0576  -0.0702   -0.0449
##  3 A     ran_pars SD Intercept                  0.701   NA        NA     
##  4 A     ran_pars SD Slope                      0.107   NA        NA     
##  5 A     ran_pars Intercept-Slope Correlation  -0.121   NA        NA     
##  6 A     ran_pars SD Residual                   0.707   NA        NA     
##  7 C     fixed    Intercept                     6.22     6.20      6.24  
##  8 C     fixed    Slope                        -0.0461  -0.0586   -0.0337
##  9 C     ran_pars SD Intercept                  0.699   NA        NA     
## 10 C     ran_pars SD Slope                      0.143   NA        NA     
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We extracted some elements, but we still aren’t quite ready for publication. Let’s do some reshaping so that we have different rows for terms and different columns for traits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levs &amp;lt;- paste(rep(c(&amp;quot;E&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;O&amp;quot;), each = 2), rep(c(&amp;quot;b&amp;quot;, &amp;quot;CI&amp;quot;), 5), sep = &amp;quot;.&amp;quot;)
(tab &amp;lt;- tab %&amp;gt;%
  mutate(sig = ifelse(sign(conf.low) == sign(conf.high), &amp;quot;sig&amp;quot;, &amp;quot;ns&amp;quot;)) %&amp;gt;%
  mutate_at(vars(estimate:conf.high), ~sprintf(&amp;quot;%.2f&amp;quot;, .)) %&amp;gt;%
  mutate_at(vars(conf.low, conf.high), ~ifelse(. == &amp;quot;NA&amp;quot;, &amp;quot;&amp;quot;, .)) %&amp;gt;%
  mutate(CI = ifelse(effect == &amp;quot;fixed&amp;quot;, sprintf(&amp;quot;[%s, %s]&amp;quot;, conf.low, conf.high), &amp;quot;&amp;quot;)) %&amp;gt;%
  mutate_at(vars(estimate, CI), ~ifelse(is.na(sig), .,
              ifelse(sig == &amp;quot;sig&amp;quot;, sprintf(&amp;quot;&amp;lt;strong&amp;gt;%s&amp;lt;/strong&amp;gt;&amp;quot;, .), .))) %&amp;gt;%
  select(trait:term, b = estimate, CI) %&amp;gt;%
  gather(key = est, value = value, b, CI) %&amp;gt;%
  unite(tmp, trait, est, sep = &amp;quot;.&amp;quot;) %&amp;gt;%
  mutate(tmp = factor(tmp, levels = levs)) %&amp;gt;%
  spread(tmp, value))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 12
##   effect  term  E.b   E.CI  A.b   A.CI  C.b   C.CI  N.b   N.CI  O.b   O.CI 
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
## 1 fixed   Inte… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str…
## 2 fixed   Slope &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str…
## 3 ran_pa… Inte… -0.20 &amp;quot;&amp;quot;    -0.12 &amp;quot;&amp;quot;    -0.19 &amp;quot;&amp;quot;    -0.28 &amp;quot;&amp;quot;    -0.16 &amp;quot;&amp;quot;   
## 4 ran_pa… SD I… 0.91  &amp;quot;&amp;quot;    0.70  &amp;quot;&amp;quot;    0.70  &amp;quot;&amp;quot;    0.94  &amp;quot;&amp;quot;    0.95  &amp;quot;&amp;quot;   
## 5 ran_pa… SD R… 0.71  &amp;quot;&amp;quot;    0.71  &amp;quot;&amp;quot;    0.68  &amp;quot;&amp;quot;    0.79  &amp;quot;&amp;quot;    0.83  &amp;quot;&amp;quot;   
## 6 ran_pa… SD S… 0.16  &amp;quot;&amp;quot;    0.11  &amp;quot;&amp;quot;    0.14  &amp;quot;&amp;quot;    0.22  &amp;quot;&amp;quot;    0.20  &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have it formatted, but let’s make it pretty using the &lt;code&gt;kable()&lt;/code&gt; function from the &lt;code&gt;knitr&lt;/code&gt; package and the &lt;code&gt;kableExtra&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab %&amp;gt;%
  mutate(effect = mapvalues(effect, c(&amp;quot;fixed&amp;quot;, &amp;quot;ran_pars&amp;quot;), c(&amp;quot;Fixed&amp;quot;, &amp;quot;Random&amp;quot;))) %&amp;gt;%
  kable(., &amp;quot;html&amp;quot;, escape = F, booktabs = T, 
        col.names = c(&amp;quot;Effect&amp;quot;, &amp;quot;Term&amp;quot;, rep(c(&amp;quot;b&amp;quot;, &amp;quot;CI&amp;quot;), times = 5)),
        align = c(&amp;quot;r&amp;quot;, &amp;quot;r&amp;quot;, rep(&amp;quot;c&amp;quot;,10)),
        caption = &amp;quot;Growth Model Terms for the Big 5&amp;quot;) %&amp;gt;%
  kable_styling(full_width = F) %&amp;gt;%
  collapse_rows(1, valign = &amp;quot;top&amp;quot;) %&amp;gt;%
  add_header_above(c(&amp;quot; &amp;quot; = 2, &amp;quot;Extraversion&amp;quot; = 2, &amp;quot;Agreeablness&amp;quot; = 2, 
                     &amp;quot;Conscientiousness&amp;quot; = 2, &amp;quot;Neuroticism&amp;quot; = 2, &amp;quot;Openness&amp;quot; = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
(#tab:kable ex6)Growth Model Terms for the Big 5
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;2&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Extraversion
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Agreeablness
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Conscientiousness
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Neuroticism
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Openness
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Effect
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Term
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;vertical-align: top !important;&#34; rowspan=&#34;2&#34;&gt;
Fixed
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
Intercept
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;5.15&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[5.13, 5.18]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;5.77&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[5.75, 5.79]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;6.22&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[6.20, 6.24]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;4.74&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[4.72, 4.77]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;4.46&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[4.44, 4.49]&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
Slope
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;-0.04&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[-0.05, -0.03]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;-0.06&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[-0.07, -0.04]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;-0.05&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[-0.06, -0.03]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;0.08&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[0.07, 0.10]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;-0.04&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[-0.06, -0.03]&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;vertical-align: top !important;&#34; rowspan=&#34;4&#34;&gt;
Random
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
Intercept-Slope Correlation
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.12
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.19
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.28
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.16
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
SD Intercept
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.70
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.70
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.94
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
SD Residual
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.68
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.83
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
SD Slope
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.16
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.22
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Homework is posted! </title>
      <link>/post/homework-is-posted/</link>
      <pubDate>Fri, 06 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/homework-is-posted/</guid>
      <description>



</description>
    </item>
    
    <item>
      <title>Week 2</title>
      <link>/lectures/03-growth-curves/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/03-growth-curves/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#growth-curves&#34;&gt;Growth curves&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#between-person-models-and-cross-sectional-data&#34;&gt;Between person models and cross sectional data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#within-person-models-e.g.-2-level-models&#34;&gt;Within person models e.g., 2-level models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#thinking-about-random-effects&#34;&gt;Thinking about random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects&#34;&gt;Random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#empty-model-equation&#34;&gt;Empty model equation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#putting-it-together&#34;&gt;Putting it together&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#visualize-what-you-are-doing&#34;&gt;Visualize what you are doing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#icc&#34;&gt;ICC&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-time&#34;&gt;Adding time&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-does-this-look-like-graphically&#34;&gt;What does this look like graphically?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-a-random-slope&#34;&gt;Adding a random slope?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#individual-level-random-effects&#34;&gt;Individual level random effects&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#calculation-of-individual-level-random-effects&#34;&gt;Calculation of individual level random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-are-these-random-effects-calculated&#34;&gt;How are these random effects calculated?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effect-decomposition&#34;&gt;Random effect decomposition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;growth-curves&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Growth curves&lt;/h1&gt;
&lt;div id=&#34;between-person-models-and-cross-sectional-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Between person models and cross sectional data&lt;/h2&gt;
&lt;p&gt;You already know this, but it gives us a chance to review regression&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... +\epsilon_{i} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \hat{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Parameters are considered fixed where one regression value corresponds to everyone. I.e., that association between X1 and Y is the same for everyone.&lt;/p&gt;
&lt;p&gt;Each person has a Y, denoted by the subscript i, and each has a residual associated with them, also designated by i.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
example &amp;lt;- read_csv(&amp;quot;~/Box/5165 Applied Longitudinal Data Analysis/ALDA/example copy.csv&amp;quot;)
example$ID &amp;lt;- as.factor(example$ID)
# you can find the data on my github at: https://github.com/josh-jackson/ALDA/example%20copy.csv&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets look at some data. These data examine older adults who came into a study up to six times over a six year period. Multiple cognitive, psychiatric and imaging assessments were done. Let’s look at functional connectivity network called SMN7.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ───────────────────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1           ✔ dplyr   0.8.3      
## ✔ tibble  2.1.3           ✔ stringr 1.4.0      
## ✔ tidyr   0.8.99.9000     ✔ forcats 0.4.0      
## ✔ purrr   0.3.2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ───────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
gg1 &amp;lt;- ggplot(example,
   aes(x = week, y = SMN7)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;)   
print(gg1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/03-Growth-curves_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What happens if we run a regression?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regression &amp;lt;- lm(SMN7 ~ week, data = example)
summary(regression)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = SMN7 ~ week, data = example)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.099294 -0.039929 -0.005938  0.032715  0.169885 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 0.100161   0.005261  19.039   &amp;lt;2e-16 ***
## week        0.004087   0.002563   1.595    0.112    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.05562 on 214 degrees of freedom
##   (9 observations deleted due to missingness)
## Multiple R-squared:  0.01174,    Adjusted R-squared:  0.007124 
## F-statistic: 2.543 on 1 and 214 DF,  p-value: 0.1123&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;within-person-models-e.g.-2-level-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Within person models e.g., 2-level models&lt;/h2&gt;
&lt;p&gt;We saw this last time where we can think of everyone being run in a separate regression model. Here the lines connect the dots of the same people across time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
gg2 &amp;lt;- ggplot(example,
   aes(x = week, y = SMN7, group = ID)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)   

gg3 &amp;lt;- gg2 +  stat_smooth(data = example, aes(x = week, y = SMN7, group=1, colour=&amp;quot;#990000&amp;quot;), method = &amp;quot;lm&amp;quot;, size = 3, se=FALSE) + theme(legend.position = &amp;quot;none&amp;quot;)
print(gg3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/03-Growth-curves_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each person has multiple assessments, so we need to distinguish between people and their assessments. In normal regression we wouldn’t think about this as everyone datapoint is assumed to be independent. However, this is not the case here. Failing to distinguish would lead to violation of independence, an important assumption of the standard regression model.&lt;/p&gt;
&lt;p&gt;As seen in the graph above, what we have now is both individual level slopes as well as an average level slope. The average level slope is going to be the average of the individual level slopes, which will look like our average slope ignoring all dependencies. Same for the intercept.&lt;/p&gt;
&lt;p&gt;One way to do this is to run separate regressions for each person. Then we could just pool (or average) together where people start and how much they change to get the average intercept (starting value) and trajectory (how much people change). We will see later that this is a somewhat poor approach.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;ID&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regressions &amp;lt;- example2 %&amp;gt;% 
  group_by(ID) %&amp;gt;% 
  do(tidy(lm(SMN7 ~ week, data = .)))

head(regressions)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
## # Groups:   ID [3]
##   ID    term        estimate std.error statistic  p.value
##   &amp;lt;fct&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 67    (Intercept)  0.0921    0.0161       5.72   0.0292
## 2 67    week         0.00662   0.00657      1.01   0.420 
## 3 75    (Intercept)  0.126   NaN          NaN    NaN     
## 4 75    week         0.00771 NaN          NaN    NaN     
## 5 87    (Intercept)  0.0787  NaN          NaN    NaN     
## 6 87    week        -0.0227  NaN          NaN    NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to the average intercept and the average trajectory there is also the amount of variation around each of these estimates. Do people tend to change the same? Are there individual differences in the initial assessment?&lt;/p&gt;
&lt;p&gt;This type of meaningful variation is lost when we have a between subjects only model that ignores the individual level. This variation will be called Random Effects (or variance estimates in SEM).&lt;/p&gt;
&lt;p&gt;[Side note: note how some people do not have se estimates for their regression coefficients. The reason for this will impact our ability to fit longitudinal models later on.]&lt;/p&gt;
&lt;p&gt;There is another important source of variation different from standard regression models. The within-subjects error that can be seen in the below graph. If we did not take people into account and just collapsed across people to get a between subjects assessment of change, this error would be confounded with individual differences in change. We will discuss this error more in depth later, but one way to think about our goal is to utilize our repeated assessments to make better predictions. A way to do that is to create additional buckets of explained variance, resulting in a smaller bucket of unexplained variance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example3 &amp;lt;- example2 %&amp;gt;% 
  filter(ID == &amp;quot;67&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg4 &amp;lt;-  ggplot(example3, aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;)

gg4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/03-Growth-curves_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;thinking-about-random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Thinking about random effects&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random effects&lt;/h3&gt;
&lt;p&gt;Within subjects variability in either starting value or slope/trajectory is referenced in terms of random effects. How do we represent this in our equation? Easy, we just say that the typical regression parameters we have are not the same for everyone – that they are random (in contrast to fixed).&lt;/p&gt;
&lt;p&gt;In general, when would we want to use random effects? If there is some sort of selection (random or not) of many possible values of the predictor (e.g., stimuli are 3 random depression drugs, three semi random chosen levels of a drug). With longitudinal data this is (random) people.&lt;/p&gt;
&lt;p&gt;Side bar: Even in situations where these levels are not random (eg working with U.S. states) it is still useful to use MLM and we still call them random effects. To be consistent with language, random here can refer to as random from the population average, not randomly selected. When talking about “random effects” you can mean either of these definitions (and a few more). Luckily we can mostly ignore these semantic issues.&lt;/p&gt;
&lt;p&gt;What is necessary for modeling random effects? For longitudinal models, there needs to be multiple assessments per your grouping category (people, schools, neighborhoods, trials).&lt;/p&gt;
&lt;p&gt;We are assuming these random effects are sampled from some population and thus vary from group to group (or person to person). This means that your coefficients (like traditional regression coefficients) are estimates of some population parameter and thus have error associated with them. This error is not like like a standard residual, which represents error for your overall model. Nor is it like the standard error for a point estimate. Random effects can best be thought of as deviation of individual regression lines from the group regression line (though it technically is not this).&lt;/p&gt;
&lt;p&gt;To facilitate the multiple assessments per person we will now use both i and j subscripts. We will see that the random effects are part of the overall error term in the model. Counterintuitively, the main focus of these types of models will be the fixed effects, with less attention paid to the random effects. That said, the random effects are necessary to account for dependency in the data. One can think about these models as normal fixed effects regressions, with the random effects there to account for the longitudinal nature of the data. They are made up of a number of standard regression equations, each for a single individual. Doing so side steps the trouble of having correlated errors, and thus allows us to interpret our findings without concern.&lt;/p&gt;
&lt;p&gt;To facilitate adding random effects to our model it is helpful to think about two “levels” to our regression equation. We are going to put a regression equation within our regression equation. (Que Xzhibit joke). The first level will be the within-person model, in that it described how people differ across time. The second level will be the between person level. Note that these do not correspond to fixed or random effects. Instead they can be thought to model either within person differences or between person differences. Mastering thinking at these two levels will help make sense of these MLM models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empty-model-equation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Empty model equation&lt;/h3&gt;
&lt;p&gt;Let’s start with the most basic model and then expand from there.&lt;/p&gt;
&lt;p&gt;Level 1 - within person
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  +\varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that we have multiple responses per individual j, noted with an i to refer to specific times.&lt;/p&gt;
&lt;p&gt;Also note that the intercept has a subscript. In typical regression it does not. This suggests that not everyone has the same intercept.&lt;/p&gt;
&lt;p&gt;The residuals at this level are thought of as measurement error OR as something that can be explained by time varying predictors.&lt;/p&gt;
&lt;p&gt;Level 2 - between person
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + U_{0j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2 takes the intercept (or other parameter) at level 1 and breaks it down into an equation for each individual, j. An overall group average (the gamma) and a residual term specific to deviation around the intercept (see below).&lt;/p&gt;
&lt;p&gt;And two variance components:
1. a random effect of the intercept
&lt;span class=&#34;math display&#34;&gt;\[ {U}_{0j} \sim \mathcal{N}(0, \tau_{00}^{2})  \]&lt;/span&gt;
The subscript of the &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; refers to the number of the parameter where 0 is the intercept, 1 is the first regression coefficient, and so on. The second refers to the individual, j. So &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; refers to the intercept whereas &lt;span class=&#34;math inline&#34;&gt;\(U_{1j}\)&lt;/span&gt; would refer to the random effect of the first regression coefficient.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; random effect is said to be normally distributed with a mean of zero and a variance of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the residual error term
&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;
Much like in normal regression there is an error term for all of the variation we cannot account for. What is unique here is that we took that normal variation and split it into two components. One that is attributable to variation around the intercept &lt;span class=&#34;math inline&#34;&gt;\({U}_{0j}\)&lt;/span&gt; and a catch all residual.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Technically this is not a growth model, nor one that is inherently longitudinal. However, it does serve as a nice starting point to identify random effects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-together&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Putting it together&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-what-you-are-doing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualize what you are doing&lt;/h3&gt;
&lt;p&gt;Imagine the raw data plotted without knowing person j, how would &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{i}\)&lt;/span&gt; be calculated?&lt;/p&gt;
&lt;p&gt;Now think about the data plotted again but with knowing each person has their own intercept. How would &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ij}\)&lt;/span&gt; be calculated?&lt;/p&gt;
&lt;p&gt;Finally, how is &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; calculated?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;icc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ICC&lt;/h3&gt;
&lt;p&gt;If the ICC is greater than zero, we are breaking standard regression assumptions.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{U_{0j}}{U_{0j}+ \varepsilon_{ij}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Is defined as % variation between over total variance.&lt;/p&gt;
&lt;p&gt;ICC can also be interpreted as the average (or expected) correlation within a nested group, in this case a person. On other words, the ICC is the correlation between any person’s repeated measures (technically residuals).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding time&lt;/h2&gt;
&lt;p&gt;Here is the basic growth model where our predictor is a time variable&lt;/p&gt;
&lt;p&gt;Level 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note how similar this looks like to a normal regression equation. Again, the differences are due to those pesky subscripts. Like before, think of this as a normal regression equation at the level of a person. Each person would have one of these equations with, in addition to a unique Y, X and residual, a unique &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt;. Look above to those individual regressions we did at the start of this section.&lt;/p&gt;
&lt;p&gt;Level 2:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2 takes the parameters at level 1 and decomposes them into a fixed component that reflects that average and then the individual deviations around that fixed effect. &lt;span class=&#34;math inline&#34;&gt;\(U_{0j}\)&lt;/span&gt; is not error in the traditional sense. It describes how much variation there is around that parameter. Do some people start higher while some start lower, for example.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The new level 2 term refers to the first predictor in the level 1 regression equation ie the slope. This slope is fixed in that the level 2 equation only has a gamma term and no U residual term.&lt;/p&gt;
&lt;p&gt;Putting it together:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{10} (X_{1j})+ U_{0j}  + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that in computing a single individuals Y, it depends on the two fixed effects, the Xj, and the random effect for the intercept.&lt;/p&gt;
&lt;div id=&#34;what-does-this-look-like-graphically&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What does this look like graphically?&lt;/h3&gt;
&lt;p&gt;And how does this differ from the random intercept model?&lt;/p&gt;
&lt;p&gt;Can you draw out the sources of error? The random effects for each participant? The fixed effects?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-a-random-slope&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding a random slope?&lt;/h3&gt;
&lt;p&gt;What happens when we add a random slope?
Level 1:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{1j} + \varepsilon_{ij} \]&lt;/span&gt;
Level 2:&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + U_{0j}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{1j} = \gamma_{10} + U_{1j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Putting it together:
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + \gamma_{10}(X_{ij})+ U_{0j} + U_{1j}(X_{ij}) + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Can think of a persons score divided up into a fixed component as well as the random component.&lt;/p&gt;
&lt;p&gt;These random effects are likely related to one another. For example, if someone starts high on a construct they are then less likely to increase across time. This negative correlation can be seen in the residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     &amp;amp; \tau_{00}^{2} &amp;amp; \tau_{01}\\ 
  0, &amp;amp; \tau_{01} &amp;amp; \tau_{10}^{2}
\end{pmatrix} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that it is possible to have a different error structures, one where there is no relationship between the intercept and the slope, for example. We will discuss this more later in the semester. Right now just know that the default is to have correlated random effects.&lt;/p&gt;
&lt;p&gt;We also have the within subject variance term that accounts for deviations that are not accounted for by time variable and other level 1 predictors.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that it is possible to model these level 1 residuals with different structures. This specification implies that there is no correlation across an individuals residuals, once you account for level 1 predictors (ie growth trajectories). Having a specific level 1 autoregressive or other type of pattern is common in other treatments of longitudinal models (panel models) but is not necessary with growth models (but possible).&lt;/p&gt;
&lt;p&gt;This is the basic format of the growth model. It will be expanded later on by adding variables to the level 1 model and to the level 2 model. Adding to the level 1 model is only possible with repeated variables.&lt;/p&gt;
&lt;p&gt;Level 1 regression coefficients are added to the level 2 model. These coefficients are decomposed into a fixed effect, a random effect (possibly), and between person predictors. As with any regression model, each of these only have a single error term.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;individual-level-random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Individual level random effects&lt;/h2&gt;
&lt;div id=&#34;calculation-of-individual-level-random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculation of individual level random effects&lt;/h3&gt;
&lt;p&gt;Random effects are often thought in terms of variance components. We can see this if we think of individual level regressions for each person where we then have a mean and a variance for both the intercept or the slope. The greater the variance around the intercept and the slope means that not everyone starts at the same position and not everyone changes at the same rate.&lt;/p&gt;
&lt;p&gt;If you want to look at a specific person’s random effect you can think of it as a deviation from the fixed effect where subject 6’s intercept can be thought of as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{06} = \gamma_{00} \pm U_{06}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;e.g 2.2 = 3 - .8&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-are-these-random-effects-calculated&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How are these random effects calculated?&lt;/h3&gt;
&lt;p&gt;It isn’t as straightforward as calculating a slope for each person and then using the difference between that slope and the average slope. Instead, the estimates are partially pooled towards the overall mean of the sample, the fixed effect. We do this to get a better estimate of the parameters, the same way that using regression to predict y-hat given an X is better than binning X and calculating y-hat. More information = better.&lt;/p&gt;
&lt;p&gt;Why not full pooling ie give everyone the same slope? Because it ignores individual differences in change. Often individual differences in (intraindividual) change is what we care about.&lt;/p&gt;
&lt;p&gt;The result is that the variance of the change trajectories (using MLM) will be smaller than the variance of the fitted linear models. Trajectories are “regressed” towards the average trajectory under the assumption that extreme scores are extreme because of (measurement) error, not that people are actually extreme.&lt;/p&gt;
&lt;p&gt;Can think about this in terms of creating an average for your intercept. Do you want the average to be the grand mean average, ignoring group? Do you want it to be the person average, ignoring that some people have more data points and thus are better assessed? No right answer, so maybe lets meet in the middle? This is sometimes called an empirical Bayes estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effect-decomposition&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random effect decomposition&lt;/h3&gt;
&lt;p&gt;Think of the original total variance in a scatter plot of our DVs. Adding random effects takes that variance and trims it down.&lt;/p&gt;
&lt;p&gt;The intercept only MLM separates it into a level 1 variance (which at this stage is treated as error) and a level 2 random intercept variance.&lt;/p&gt;
&lt;p&gt;Creating a random slopes model takes the Level 1 residual variance and creates a new “pile” of explained or accounted for variance.&lt;/p&gt;
&lt;p&gt;We can then further explain the variance or reduce the pile by predictors at level 1 and level 2. Our goal isn’t necessarily to explain all of the variance but it is helpful to reduce the unexplained variance &lt;span class=&#34;math inline&#34;&gt;\(\varepsilon_{ij}\)&lt;/span&gt; to improve model fit.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>homework #1</title>
      <link>/homeworks/homework-1/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/homeworks/homework-1/</guid>
      <description>


&lt;p&gt;Answer the questions below using your own data (for question 1) and the dataset from this file for the remaining questions:&lt;a href=&#34;https://raw.github.com/josh-jackson/ALDA/master/Hw1&#34; class=&#34;uri&#34;&gt;https://raw.github.com/josh-jackson/ALDA/master/Hw1&lt;/a&gt; If you have your own longitudinal data, please feel free to use that instead.&lt;/p&gt;
&lt;p&gt;If using the provided dataset note that we are looking at depression across 5 waves during college. Waves are signified by a letter at the start of the variable name, starting with A, then B, then C, etcetera. Not all waves collected depression during college.&lt;/p&gt;
&lt;p&gt;Please email your answers via a) an Rmd file and b) a pdf to: &lt;a href=&#34;mailto:Homewor.dk7huzwn6n5aohka@u.box.com&#34; class=&#34;email&#34;&gt;Homewor.dk7huzwn6n5aohka@u.box.com&lt;/a&gt; If you are using your own data please also attach the dataset.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Create a codebook with an actual dataset you are working with (it does not need to be longitudinal). Feed it into R to change variable names.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Take a wide dataset (such as the provided one) and make it long&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run separate linear models on all of the participants subjects (a basic regression). What is the average intercept, the average slope?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Now run a mlm/lmer model with only a random intercept. What is the ICC? Interpret.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Introduce a fixed slope term. What is the difference in interpretation for the fixed effects estimates in this estimate and the previous? Of the residual standard error?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run an additional model with a random slope. How does this change compare to the previous fixed slope model? Should you keep the random slope or not?&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interpret the correlation between the slope and the intercept.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Create a density plot of the random effects from your final model.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Workshop #2</title>
      <link>/workshops/workshop-2/</link>
      <pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-2/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tidyr&#34;&gt;tidyr&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#wide-and-long-form&#34;&gt;Wide and Long form&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lme4&#34;&gt;lme4&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-calculate-icc&#34;&gt;How to calculate ICC?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-beyond-the-summary&#34;&gt;Exploring beyond the summary&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-do-the-random-effects-look-like&#34;&gt;what do the random effects look like?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-time-to-the-mlm&#34;&gt;Adding time to the MLM&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-slope&#34;&gt;Fixed slope&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-slope&#34;&gt;Random slope&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing-models&#34;&gt;Testing models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#why-treating-time-is-so-important&#34;&gt;Why treating time is so important&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects&#34;&gt;Random effects&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#calculation-of-random-effect-confidence-interval&#34;&gt;Calculation of random effect confidence interval&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-simulations-to-get-better-estimates-of-confidence-around-our-estimates&#34;&gt;Using simulations to get better estimates of confidence around our estimates&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#caterpillar-plots&#34;&gt;Caterpillar plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-of-individual-random-effects&#34;&gt;Density of individual random effects&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-to-a-standard-linear-model&#34;&gt;Comparing to a standard linear model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-models&#34;&gt;Comparing models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predictions-and-prediction-intervals&#34;&gt;Predictions and prediction intervals&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#predictions-and-prediction-intervals-1&#34;&gt;Predictions and prediction intervals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;tidyr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;tidyr&lt;/h1&gt;
&lt;div id=&#34;wide-and-long-form&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wide and Long form&lt;/h2&gt;
&lt;p&gt;Depending on what type of analysis you want to perform you may need to restructure your data. I recommend the combination of tidyr and dplyr (among others) to restructure and manage your dataframes. The first decision you need to make is whether you want your data structured in a long or a wide format. There are multiple names to refer to these two types: multivariate vs univariate, person-level vs person-period, etc but they all refer to the same idea. How to structure your data depends on both what level of analysis (individual, dyad, household) and what type of analyses (MLM/SEM). Typically our focus is on individuals.&lt;/p&gt;
&lt;p&gt;Wide form is common among non-longitudinal data. It has one line per individual with all of their repeated measures in the same row, each with some name to distinguish which assessment wave the data came from. In general, this format is used for SEM.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##      ID ext_1 ext_2 ext_3
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     4     4     4
## 2     2     6     5     4
## 3     3     4     5     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In contrast, long format has a row per observation. Thus, participants likely have many rows, each one referring to a different assessment wave. There are fewer variables in this format which makes organization somewhat easier. Thus this has been referred to as “Tidy” data. Graphing with ggplot is facilitated when using tidy data such as being in the long format.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##      ID  time   ext
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     1     4
## 2     1     2     4
## 3     1     3     4
## 4     2     1     6
## 5     2     2     5
## 6     2     3     4
## 7     3     1     4
## 8     3     2     5
## 9     3     3     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do you go back and forth? We use the tidyr package! As of a few months ago, you would use the gather and the spread functions. Now these are being phased out and are being replaced by pivot_longer and pivot_wider. The functions work similar but the newer ones are a little more intuitive both in terms of remembering the correct function name as well as well adding more bells and whistles.&lt;/p&gt;
&lt;p&gt;Gather and pivot_longer goes from wide to long.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)

wide_to_long &amp;lt;- wide %&amp;gt;% 
  gather(ext_1:ext_3,key = &amp;quot;time&amp;quot;, value = &amp;quot;ext&amp;quot;) 
wide_to_long&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##      ID time    ext
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 ext_1     4
## 2     2 ext_1     6
## 3     3 ext_1     4
## 4     1 ext_2     4
## 5     2 ext_2     5
## 6     3 ext_2     5
## 7     1 ext_3     4
## 8     2 ext_3     4
## 9     3 ext_3     6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)

wide_to_long.p &amp;lt;- wide %&amp;gt;% 
  pivot_longer(-ID, names_to = &amp;quot;time&amp;quot;, values_to = &amp;quot;ext&amp;quot;) 
wide_to_long.p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##      ID time    ext
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 ext_1     4
## 2     1 ext_2     4
## 3     1 ext_3     4
## 4     2 ext_1     6
## 5     2 ext_2     5
## 6     2 ext_3     4
## 7     3 ext_1     4
## 8     3 ext_2     5
## 9     3 ext_3     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the similarities. The key in all three is to 1. identify which columns need to be reshaped. Here is it all of them besides ID. 2. we need to name the newly created variable that consists of the old column names (here time). 3. we need to name what those values represent (here levels of extraversion)&lt;/p&gt;
&lt;p&gt;The separate function could be used to get only the assessment wave number. This might be useful when combining data together or for creating a common time metric for everyone.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wide_to_long2 &amp;lt;- wide_to_long %&amp;gt;% 
  separate(time, into = c(&amp;quot;omit&amp;quot;, &amp;quot;wave&amp;quot;), sep = &amp;quot;_&amp;quot;, convert = TRUE) %&amp;gt;%
  dplyr::select(-omit) %&amp;gt;% 
  arrange(ID)
wide_to_long2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##      ID  wave   ext
##   &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     1     4
## 2     1     2     4
## 3     1     3     4
## 4     2     1     6
## 5     2     2     5
## 6     2     3     4
## 7     3     1     4
## 8     3     2     5
## 9     3     3     6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Note that the seperate function will identify non numeric characters and use that to seperate the values. You can omit the sep = function to check yourself. &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One issue that comes up here is that we have differing dates for each assessment. Ideally we would like to utilize that extra information.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 7
##      ID ext_1 ext_2 ext_3 date_1 date_2  date_3 
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  
## 1     1     4     4     4 1/1/10 5/1/10  8/1/10 
## 2     2     6     5     4 1/6/10 4/10/10 9/1/10 
## 3     3     4     5     6 1/8/10 4/25/10 9/13/10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we fix it? The same way we would with multiple variables we want to convert. Wave, along with ID helps us keep track of what variables go with which person at which time. Together, the two serve as a unique identifier. To better understand the code go through each line to see what the intervening data frame looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long.date &amp;lt;- wide.date %&amp;gt;% 
  gather(-ID, key = &amp;quot;time&amp;quot;, value = &amp;quot;value&amp;quot;) %&amp;gt;% 
  separate(time, into = c(&amp;quot;variable&amp;quot;, &amp;quot;wave&amp;quot;)) %&amp;gt;% 
  spread(variable,value) 
long.date&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 4
##      ID wave  date    ext  
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;
## 1     1 1     1/1/10  4    
## 2     1 2     5/1/10  4    
## 3     1 3     8/1/10  4    
## 4     2 1     1/6/10  6    
## 5     2 2     4/10/10 5    
## 6     2 3     9/1/10  4    
## 7     3 1     1/8/10  4    
## 8     3 2     4/25/10 5    
## 9     3 3     9/13/10 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One difficulty of creating a wave variable is whether or not the variables are named in a manner such that 1) assessment wave is easily identifiable (e.g. does _a always refer to the first wave whereas _b always refer to the second?) and 2) if that is consistent across variables. Having a wave identifier for your variables is important/necessary. Having an easily selected one (ie at the end of the variable name, hopefully separated by an underscore or a period). If assessment wave separators are embedded within the variable name it will be harder to covert your data. Often, variable data is attached at the end of a name such as SWB_4 to refer to the fourth item in a scale. This may obscure wave identification as in SWB_A_4. A similar naming problem can occur with multiple reports e.g,. SWB_4_parent. I recommend putting wave identification last. The difficulties become partly moot when working in long format (read: entering data in) as opposed to wide. This also becomes moot when stored in a separate dataset. This is another reason why you should use codebooks!&lt;/p&gt;
&lt;p&gt;In the above code we used spread to go from long to wide as a means of creating a long dataset where there were multiple variables. Technically this is not a tidy dataset in that it comprises of both long and wide information, but it is the typical format used for MLM analyses.&lt;/p&gt;
&lt;p&gt;Going from long to wide uses spread or pivot_wider function. We will utilize this when converting our MLM models to SEM models, but try the code below to see what happens.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long_to_wide &amp;lt;- long %&amp;gt;% 
  spread(time, ext)
long_to_wide&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##      ID   `1`   `2`   `3`
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     4     4     4
## 2     2     6     5     4
## 3     3     4     5     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this is technically the dataframe format that we want. The problem is that our variable names are numeric. This often causes problems. When working with tibbles use backticks ’ to refer to the column e.g., select(‘1’). I’d recode into a more usable variable name.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lme4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;lme4&lt;/h1&gt;
&lt;p&gt;The basic function we will work with is lmer from the lme4 package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;Matrix&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The package was developed to be similar to the lm function. The code will be similar to the formula for the combined model&lt;/p&gt;
&lt;p&gt;Code for empty/null/intercept only model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmer(Y ~ 1 + (1 | subjects), data=example)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Level 1
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  +\varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + U_{0j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Combined
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;1 is the way to reference the intercept. All additional fixed effects go outside the parentheses. Inside the parentheses are the random effects and residual terms. To the right of the vertical line is our level 1 residual term, which references the grouping variable. In this case, as with almost all longitudinal work, is the subject ID. To the left of the vertical line is the random effects we want to estimate. Right now this estimates only one random effect, one for the intercept.&lt;/p&gt;
&lt;p&gt;It is possible to suppress a random intercept by putting a zero instead of a 1. If you do not put anything there the 1 is implied.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmer(y ~ 1 + time + (1 + time | subjects), data=data)

lmer(y ~ time + (time | subjects), data=data)
# both are equivalent&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.1 &amp;lt;- lmer(SMN7 ~ 1 + (1 | ID), data=example)
summary(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: SMN7 ~ 1 + (1 | ID)
##    Data: example
## 
## REML criterion at convergence: -714.1
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.1575 -0.4728 -0.0232  0.4512  3.2750 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  ID       (Intercept) 0.001823 0.04270 
##  Residual             0.001302 0.03608 
## Number of obs: 225, groups:  ID, 91
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.106972   0.005106   20.95&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-calculate-icc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to calculate ICC?&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.001823/(0.001823 + 0.001302)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.58336&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-beyond-the-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring beyond the summary&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;lmerMod&amp;quot;
## attr(,&amp;quot;package&amp;quot;)
## [1] &amp;quot;lme4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;what-do-the-random-effects-look-like&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;what do the random effects look like?&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sjPlot)
plot_model(mod.1, type = &amp;quot;re&amp;quot;, sort.est = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(ranef(mod.1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ID
##       (Intercept)
## 6   -0.0597240676
## 29  -0.0101119688
## 34  -0.0103698893
## 36  -0.0035902640
## 37  -0.0082433829
## 48   0.0455797808
## 53  -0.0222710793
## 54  -0.0066548052
## 58  -0.0060624543
## 61  -0.0271347235
## 66  -0.0123359896
## 67  -0.0026491341
## 69   0.0348398944
## 71  -0.0486040243
## 74   0.0484338355
## 75   0.0224228634
## 76  -0.0021583228
## 78   0.0224780927
## 79  -0.0054325535
## 80  -0.0194707993
## 81   0.0712662731
## 82   0.0053695094
## 85  -0.0532215425
## 86  -0.0388885304
## 87  -0.0387411472
## 89  -0.0208712287
## 91   0.0123812011
## 92  -0.0078125821
## 93   0.0430219016
## 94  -0.0543390588
## 96   0.0233440081
## 97  -0.0497003277
## 98  -0.0432302582
## 99   0.0104394983
## 101  0.0508032394
## 102 -0.0104344307
## 103 -0.0206130188
## 104 -0.0482473609
## 105 -0.0478231980
## 106 -0.0028045239
## 110  0.0418641247
## 112 -0.0109089622
## 114 -0.0549314098
## 115 -0.0013505715
## 116  0.0062422910
## 120  0.0300499418
## 122  0.0793976365
## 125  0.0532803435
## 127 -0.0105050866
## 129 -0.0448207025
## 135  0.0406255726
## 136 -0.0364069792
## 137 -0.0444890904
## 140 -0.0153440709
## 141  0.0770651692
## 142  0.0817077387
## 143  0.0072423981
## 144  0.0001680065
## 146 -0.0551006778
## 149 -0.0137965477
## 150  0.0091583792
## 152 -0.0187707293
## 153  0.0490992150
## 155 -0.0233396072
## 156 -0.0218943803
## 159 -0.0488368935
## 160  0.0024524455
## 162  0.0911638809
## 163  0.0155327007
## 165  0.0320764602
## 167 -0.0025217361
## 169  0.0647586755
## 171 -0.0397728293
## 174  0.0259232134
## 182 -0.0154177625
## 187 -0.0581588783
## 189  0.0348767402
## 190 -0.0030744230
## 193  0.0636533019
## 194  0.0099321407
## 201  0.0104848276
## 204  0.0414352908
## 205  0.0353188897
## 208  0.0033367444
## 209 -0.0346144188
## 211  0.0168223034
## 214 -0.0374146988
## 219  0.0564683729
## 222 -0.0262135788
## 223 -0.0228606119
## 229 -0.0484315899&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(coef(mod.1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ID
##     (Intercept)
## 6    0.04724795
## 29   0.09686005
## 34   0.09660212
## 36   0.10338175
## 37   0.09872863
## 48   0.15255179
## 53   0.08470093
## 54   0.10031721
## 58   0.10090956
## 61   0.07983729
## 66   0.09463602
## 67   0.10432288
## 69   0.14181191
## 71   0.05836799
## 74   0.15540585
## 75   0.12939488
## 76   0.10481369
## 78   0.12945011
## 79   0.10153946
## 80   0.08750121
## 81   0.17823829
## 82   0.11234152
## 85   0.05375047
## 86   0.06808348
## 87   0.06823087
## 89   0.08610079
## 91   0.11935322
## 92   0.09915943
## 93   0.14999392
## 94   0.05263296
## 96   0.13031602
## 97   0.05727169
## 98   0.06374176
## 99   0.11741151
## 101  0.15777525
## 102  0.09653758
## 103  0.08635900
## 104  0.05872465
## 105  0.05914882
## 106  0.10416749
## 110  0.14883614
## 112  0.09606305
## 114  0.05204060
## 115  0.10562144
## 116  0.11321430
## 120  0.13702196
## 122  0.18636965
## 125  0.16025236
## 127  0.09646693
## 129  0.06215131
## 135  0.14759759
## 136  0.07056503
## 137  0.06248292
## 140  0.09162794
## 141  0.18403718
## 142  0.18867975
## 143  0.11421441
## 144  0.10714002
## 146  0.05187134
## 149  0.09317547
## 150  0.11613039
## 152  0.08820128
## 153  0.15607123
## 155  0.08363241
## 156  0.08507763
## 159  0.05813512
## 160  0.10942446
## 162  0.19813589
## 163  0.12250471
## 165  0.13904847
## 167  0.10445028
## 169  0.17173069
## 171  0.06719918
## 174  0.13289523
## 182  0.09155425
## 187  0.04881314
## 189  0.14184875
## 190  0.10389759
## 193  0.17062532
## 194  0.11690415
## 201  0.11745684
## 204  0.14840730
## 205  0.14229090
## 208  0.11030876
## 209  0.07235760
## 211  0.12379432
## 214  0.06955732
## 219  0.16344039
## 222  0.08075844
## 223  0.08411140
## 229  0.05854042&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##    0.106972&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do these relate? Lets calculate ID 6 intercept random effect&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#coef = fixef + raneff

# coef for ID = 6 is 0.04724795  
0.106972 -0.0597240676 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04724793&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get residuals and fitted scores&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)
example.aug&amp;lt;- augment(mod.1, data = example)


# .fitted    = predicted values
# .resid    = residuals/errors
# .fixed     = predicted values with no random effects&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-time-to-the-mlm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding time to the MLM&lt;/h2&gt;
&lt;div id=&#34;fixed-slope&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fixed slope&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2f &amp;lt;- lmer(SMN7 ~ 1 + year + (1  | ID), data=example)
summary(mod.2f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: SMN7 ~ 1 + year + (1 | ID)
##    Data: example
## 
## REML criterion at convergence: -675.4
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.2308 -0.4868 -0.0377  0.4542  3.2337 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  ID       (Intercept) 0.001815 0.04261 
##  Residual             0.001300 0.03606 
## Number of obs: 216, groups:  ID, 88
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.104041   0.005733  18.147
## year        0.001331   0.001755   0.758
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.426&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does this look like graphically?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-slope&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random slope&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2 &amp;lt;- lmer(SMN7 ~ 1 + year + (year  | ID), data=example)
summary(mod.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: SMN7 ~ 1 + year + (year | ID)
##    Data: example
## 
## REML criterion at convergence: -678.1
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -1.93345 -0.47015 -0.00405  0.46985  2.67965 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev. Corr
##  ID       (Intercept) 1.688e-03 0.041085     
##           year        5.999e-05 0.007745 0.11
##  Residual             1.114e-03 0.033378     
## Number of obs: 216, groups:  ID, 88
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.104719   0.005466  19.157
## year        0.000489   0.001913   0.256
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.339&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the intercept change from the random intercept only model? It may change because the intercept is now conditional on time ie after accounting for time. It is not the predicted outcome when time = 0. You can think of the previous intercept as the grand mean of person means. If our year variable here changed across time then there would be a larger change in the intercept.&lt;/p&gt;
&lt;p&gt;How do you interpret year?&lt;/p&gt;
&lt;p&gt;How did the random effects change?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing models&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.2f, mod.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## refitting model(s) with ML (instead of REML)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data: example
## Models:
## mod.2f: SMN7 ~ 1 + year + (1 | ID)
## mod.2: SMN7 ~ 1 + year + (year | ID)
##        Df     AIC     BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)
## mod.2f  4 -686.93 -673.43 347.46  -694.93                         
## mod.2   6 -685.45 -665.20 348.73  -697.45 2.5248      2      0.283&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why is there a 2df difference?&lt;/p&gt;
&lt;p&gt;also you can see the non-REML fit info here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glance(mod.2f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##    sigma logLik   AIC   BIC deviance df.residual
##    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;int&amp;gt;
## 1 0.0361   338. -667. -654.    -695.         212&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;why-treating-time-is-so-important&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why treating time is so important&lt;/h3&gt;
&lt;p&gt;Time with a different scale. How do we interpret? And what changes?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example$year.n &amp;lt;- (example$year - 30)
  
mod.2n &amp;lt;- lmer(SMN7 ~ 1 + year.n + (year.n  | ID), data=example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 1.99825 (tol
## = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.2n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: SMN7 ~ 1 + year.n + (year.n | ID)
##    Data: example
## 
## REML criterion at convergence: -674.8
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.2053 -0.4841 -0.0423  0.4505  3.2646 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev.  Corr 
##  ID       (Intercept) 1.001e-03 0.0316327      
##           year.n      1.333e-07 0.0003652 -1.00
##  Residual             1.320e-03 0.0363343      
## Number of obs: 216, groups:  ID, 88
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.146910   0.050598   2.904
## year.n      0.001432   0.001763   0.812
## 
## Correlation of Fixed Effects:
##        (Intr)
## year.n 0.995 
## convergence code: 0
## Model failed to converge with max|grad| = 1.99825 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random effects&lt;/h2&gt;
&lt;div id=&#34;calculation-of-random-effect-confidence-interval&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculation of random effect confidence interval&lt;/h3&gt;
&lt;p&gt;Conveys the predicted range around each fixed effect in which 95% of the sample individuals are predicted to fall.&lt;/p&gt;
&lt;p&gt;95% random effect = fixed effect plus minus 1.96 * random standard deviation&lt;/p&gt;
&lt;p&gt;How to calculate?
1. Intercept &lt;span class=&#34;math display&#34;&gt;\[ \gamma_{00} \pm  1.96  *  \tau_{U_{0j}}  \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.1193933 + (1.96 * 0.240217) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5902186&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.1193933 - (1.96 * 0.240217) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.351432&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Slope &lt;span class=&#34;math display&#34;&gt;\[ \gamma_{10} \pm  1.96  *  \tau_{U_{1j}}  \]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.0004891 + (1.96 * 0.007745) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0156693&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.0004891 - (1.96 * 0.007745) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.0146911&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;###Individual level random effects&lt;/p&gt;
&lt;p&gt;Are the intercept random effects the same as the model with only the intercept? Why or why not?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(ranef(mod.1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ID
##       (Intercept)
## 6   -0.0597240676
## 29  -0.0101119688
## 34  -0.0103698893
## 36  -0.0035902640
## 37  -0.0082433829
## 48   0.0455797808
## 53  -0.0222710793
## 54  -0.0066548052
## 58  -0.0060624543
## 61  -0.0271347235
## 66  -0.0123359896
## 67  -0.0026491341
## 69   0.0348398944
## 71  -0.0486040243
## 74   0.0484338355
## 75   0.0224228634
## 76  -0.0021583228
## 78   0.0224780927
## 79  -0.0054325535
## 80  -0.0194707993
## 81   0.0712662731
## 82   0.0053695094
## 85  -0.0532215425
## 86  -0.0388885304
## 87  -0.0387411472
## 89  -0.0208712287
## 91   0.0123812011
## 92  -0.0078125821
## 93   0.0430219016
## 94  -0.0543390588
## 96   0.0233440081
## 97  -0.0497003277
## 98  -0.0432302582
## 99   0.0104394983
## 101  0.0508032394
## 102 -0.0104344307
## 103 -0.0206130188
## 104 -0.0482473609
## 105 -0.0478231980
## 106 -0.0028045239
## 110  0.0418641247
## 112 -0.0109089622
## 114 -0.0549314098
## 115 -0.0013505715
## 116  0.0062422910
## 120  0.0300499418
## 122  0.0793976365
## 125  0.0532803435
## 127 -0.0105050866
## 129 -0.0448207025
## 135  0.0406255726
## 136 -0.0364069792
## 137 -0.0444890904
## 140 -0.0153440709
## 141  0.0770651692
## 142  0.0817077387
## 143  0.0072423981
## 144  0.0001680065
## 146 -0.0551006778
## 149 -0.0137965477
## 150  0.0091583792
## 152 -0.0187707293
## 153  0.0490992150
## 155 -0.0233396072
## 156 -0.0218943803
## 159 -0.0488368935
## 160  0.0024524455
## 162  0.0911638809
## 163  0.0155327007
## 165  0.0320764602
## 167 -0.0025217361
## 169  0.0647586755
## 171 -0.0397728293
## 174  0.0259232134
## 182 -0.0154177625
## 187 -0.0581588783
## 189  0.0348767402
## 190 -0.0030744230
## 193  0.0636533019
## 194  0.0099321407
## 201  0.0104848276
## 204  0.0414352908
## 205  0.0353188897
## 208  0.0033367444
## 209 -0.0346144188
## 211  0.0168223034
## 214 -0.0374146988
## 219  0.0564683729
## 222 -0.0262135788
## 223 -0.0228606119
## 229 -0.0484315899&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(ranef(mod.2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ID
##       (Intercept)          year
## 6   -5.657190e-02 -0.0036945459
## 29  -8.481756e-03 -0.0012401120
## 34  -6.559720e-03 -0.0036272361
## 36  -7.107377e-03  0.0047529824
## 37  -7.584156e-03  0.0003779790
## 48   3.760800e-02  0.0079905136
## 53  -1.989445e-02 -0.0015608099
## 54   1.298061e-03 -0.0046572183
## 58   3.763976e-04 -0.0054304028
## 61  -2.523030e-02 -0.0013740463
## 66  -1.692834e-02  0.0022291440
## 67  -4.435808e-03  0.0020525694
## 71  -4.790346e-02  0.0002829010
## 75   2.147451e-02  0.0025670702
## 76  -1.178132e-03  0.0005880005
## 78   1.590422e-02  0.0031440504
## 79  -4.554999e-03  0.0005646937
## 80  -1.789820e-02 -0.0007154790
## 81   5.310404e-02  0.0082155118
## 82   4.163353e-03  0.0008114128
## 85  -5.229368e-02 -0.0013883325
## 86  -3.747004e-02 -0.0020088462
## 87  -3.530494e-02 -0.0042629307
## 89  -1.906497e-02 -0.0008530205
## 91   1.547560e-02 -0.0021933791
## 92  -5.127067e-03 -0.0012825084
## 93   3.779074e-02  0.0055466427
## 94  -5.318192e-02 -0.0005568113
## 96   2.535874e-02 -0.0004379578
## 97  -4.654850e-02 -0.0019515143
## 98  -3.875698e-02 -0.0027541673
## 99   9.785177e-03  0.0016155143
## 101  4.073120e-02  0.0116039230
## 103 -1.910831e-02 -0.0013254022
## 104 -4.656380e-02 -0.0034349757
## 105 -4.532176e-02 -0.0021913974
## 106  4.024883e-05 -0.0015454111
## 110  5.266477e-02 -0.0065630548
## 112 -5.482604e-03 -0.0041413140
## 114 -5.187791e-02 -0.0023737578
## 115 -6.018565e-04  0.0004676209
## 116  7.916722e-03 -0.0002939168
## 120  3.156554e-02  0.0021828094
## 122  7.845607e-02  0.0030220350
## 125  5.565560e-02 -0.0004030572
## 127 -1.035814e-02  0.0009947753
## 129 -4.383714e-02 -0.0011901610
## 135  4.517320e-02 -0.0027851051
## 136 -3.257406e-02 -0.0027536989
## 137 -4.307457e-02 -0.0026649722
## 140 -1.389973e-02 -0.0009470411
## 141  7.828180e-02  0.0011246004
## 142  7.656429e-02  0.0071700560
## 143  8.802167e-03  0.0001782843
## 144  5.023920e-04  0.0007247389
## 146 -5.190868e-02 -0.0030652978
## 149 -1.315714e-02  0.0001913378
## 150  1.329264e-02 -0.0028911814
## 152 -1.719482e-02 -0.0013669192
## 153  4.865193e-02  0.0022929034
## 155 -2.212352e-02 -0.0004931690
## 156 -2.041537e-02 -0.0004522343
## 159 -4.783022e-02 -0.0015435325
## 160  3.459146e-03  0.0001563020
## 162  8.771544e-02  0.0058967120
## 163  1.575853e-02  0.0011486036
## 165  3.244233e-02  0.0013196820
## 167 -5.436807e-04 -0.0009083619
## 169  6.576873e-02  0.0011926808
## 171 -3.475852e-02 -0.0047511867
## 174  2.798144e-02 -0.0004759192
## 182 -5.231973e-03 -0.0081378967
## 187 -5.609959e-02 -0.0019680493
## 189  3.456648e-02  0.0020405469
## 190 -1.620723e-03 -0.0003743333
## 193  5.854085e-02  0.0071786529
## 194  4.241261e-03  0.0066159420
## 201  1.187752e-02 -0.0001340229
## 204  3.799808e-02  0.0055309494
## 205  4.004323e-02 -0.0029413522
## 208  3.040083e-04  0.0043109412
## 209 -3.367017e-02 -0.0004225192
## 211  1.856239e-02 -0.0003192506
## 214 -3.676234e-02 -0.0001666149
## 219  4.541593e-02  0.0143735740
## 222 -1.947934e-02 -0.0063364890
## 223 -1.719018e-02 -0.0051038253
## 229 -4.254994e-02 -0.0060019177&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-simulations-to-get-better-estimates-of-confidence-around-our-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using simulations to get better estimates of confidence around our estimates&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom.mixed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 methods overwritten by &amp;#39;broom.mixed&amp;#39;:
##   method         from 
##   augment.lme    broom
##   augment.merMod broom
##   glance.lme     broom
##   glance.merMod  broom
##   glance.stanreg broom
##   tidy.brmsfit   broom
##   tidy.gamlss    broom
##   tidy.lme       broom
##   tidy.merMod    broom
##   tidy.rjags     broom
##   tidy.stanfit   broom
##   tidy.stanreg   broom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;broom.mixed&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:broom&amp;#39;:
## 
##     tidyMCMC&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;random_params &amp;lt;- tidy(mod.2,  effects = &amp;quot;ran_vals&amp;quot;, conf.int=TRUE)
head(random_params)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##   effect   group level term        estimate std.error   conf.low conf.high
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 ran_vals ID    6     (Intercept) -0.0566     0.0181 -0.0920      -0.0212
## 2 ran_vals ID    29    (Intercept) -0.00848    0.0205 -0.0487       0.0317
## 3 ran_vals ID    34    (Intercept) -0.00656    0.0211 -0.0479       0.0348
## 4 ran_vals ID    36    (Intercept) -0.00711    0.0214 -0.0491       0.0349
## 5 ran_vals ID    37    (Intercept) -0.00758    0.0191 -0.0451       0.0299
## 6 ran_vals ID    48    (Intercept)  0.0376     0.0192 -0.0000131    0.0752&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(merTools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: arm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;MASS&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## arm (Version 1.10-1, built: 2018-4-12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Working directory is /Users/jackson/Box/5165 Applied Longitudinal Data Analysis/ALDA/content/Workshops&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FEsim(mod.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          term         mean       median          sd
## 1 (Intercept) 0.1049025608 0.1051183677 0.005295477
## 2        year 0.0007514429 0.0008749235 0.002054281&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;re.sim &amp;lt;- REsim(mod.2)
head(re.sim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   groupFctr groupID        term         mean       median         sd
## 1        ID       6 (Intercept) -0.054039874 -0.054006147 0.01849033
## 2        ID      29 (Intercept) -0.005934701 -0.004010852 0.02089182
## 3        ID      34 (Intercept) -0.006448982 -0.007406987 0.01989969
## 4        ID      36 (Intercept) -0.006238323 -0.006724604 0.02380377
## 5        ID      37 (Intercept) -0.008978693 -0.008864176 0.02005051
## 6        ID      48 (Intercept)  0.038832930  0.037810554 0.01781832&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be used to create CIs for each individual random effect (and fixed effect). What is the confidence interval around person 6’s intercept estimate compared to person 2000 who has 25 repeated measurements?&lt;/p&gt;
&lt;div id=&#34;caterpillar-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Caterpillar plots&lt;/h3&gt;
&lt;p&gt;Look through these different methods of getting random effects. Note that they are not all exactly the same.&lt;/p&gt;
&lt;p&gt;caterpillar plots&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- plotREsim(re.sim)
p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-of-individual-random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density of individual random effects&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1.gg1 &amp;lt;- re.sim %&amp;gt;% 
  filter(term == &amp;quot;(Intercept)&amp;quot;) 

ggplot(p1.gg1, aes(mean)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1.gg2 &amp;lt;- re.sim %&amp;gt;% 
  filter(term == &amp;quot;year&amp;quot;) 


ggplot(p1.gg2, aes(mean)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-to-a-standard-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing to a standard linear model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.1 &amp;lt;- lm(SMN7 ~ 1 + year, data = example)
summary(lm.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = SMN7 ~ 1 + year, data = example)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.099294 -0.039929 -0.005938  0.032715  0.169885 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 0.100161   0.005261  19.039   &amp;lt;2e-16 ***
## year        0.004087   0.002563   1.595    0.112    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.05562 on 214 degrees of freedom
##   (9 observations deleted due to missingness)
## Multiple R-squared:  0.01174,    Adjusted R-squared:  0.007124 
## F-statistic: 2.543 on 1 and 214 DF,  p-value: 0.1123&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;comparing-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing models&lt;/h3&gt;
&lt;p&gt;LRT&lt;/p&gt;
&lt;p&gt;Parametric bootstrap for CIs&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(mod.1, method=&amp;quot;boot&amp;quot;, nsim=1000)
summary(mod.1)

# uses SDs of random effects
# sigma = residual standard error&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparing two models. fit the reduced model, then repeatedly simulate from it and compute the differences between the deviance of the reduced and the full model for each simulated data set. Compare this null distribution to the observed deviance difference.&lt;/p&gt;
&lt;p&gt;This procedure is implemented in the pbkrtest package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pbkrtest)
#pb &amp;lt;- PBmodcomp(mod.2,mod.2r)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;predictions-and-prediction-intervals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictions and prediction intervals&lt;/h2&gt;
&lt;p&gt;Predict function is deterministic and uses only the fixed effects (i.e. does not include random effects in the predictions). It does not do prediction in the typical sense where you are predicting &lt;em&gt;new&lt;/em&gt; individual’s scores.&lt;/p&gt;
&lt;p&gt;Simulate is non-deterministic because it samples random effect values for all subjects and then samples from the conditional distribution. Simulation is needed to create true predictions.&lt;/p&gt;
&lt;div id=&#34;predictions-and-prediction-intervals-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predictions and prediction intervals&lt;/h3&gt;
&lt;p&gt;Predict function is deterministic and uses only the fixed effects (i.e. does not include random effects in the predictions). It does not do prediction in the typical sense where you are predicting &lt;em&gt;new&lt;/em&gt; individual’s scores.&lt;/p&gt;
&lt;p&gt;Simulate is non-deterministic because it samples random effect values for all subjects and then samples from the conditional distribution. Simulation is needed to create true predictions.&lt;/p&gt;
&lt;p&gt;Short of a fully Bayesian analysis, bootstrapping is the gold-standard for deriving prediction intervals/bands (ie where would a new person score given X), but the time required is typically high.&lt;/p&gt;
&lt;p&gt;In order to generate a proper prediction (for either a new person or a new observation within a person), a prediction must account for three sources of uncertainty in mixed models:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the residual (observation-level) variance,&lt;/li&gt;
&lt;li&gt;the uncertainty in the fixed coefficients, and&lt;/li&gt;
&lt;li&gt;the uncertainty in the variance parameters for the random effects&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Does so by:
1. extracting the fixed and random coefficients
2. takes n draws from the multivariate normal distribution of the fixed and random coefficients (separately)
3. calculates the linear predictor for each row in newdata based on these draws, and
4. incorporates the residual variation&lt;br /&gt;
then:
5. returns newdata with the lower and upper limits of the prediction interval and the mean or median of the simulated predictions&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(merTools)
# see also their shiny app: shinyMer(mod.1)

PI &amp;lt;- predictInterval(merMod = mod.2, newdata = example, level = 0.9, n.sims = 100, stat = &amp;quot;median&amp;quot;, include.resid.var = TRUE)
head(PI)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          fit       upr         lwr
## 1 0.05637403 0.1097137 -0.01587163
## 2 0.04308280 0.1112285 -0.01217230
## 3 0.02906342 0.1003347 -0.02107285
## 4 0.08963131 0.1538768  0.02283071
## 5 0.09504809 0.1510902  0.03936437
## 6 0.10191796 0.1657385  0.02358774&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice for bringing in confidence bands around your prediction (And we might use this later)&lt;/p&gt;
&lt;p&gt;Broom offers the fitted (predicted) values already if you just want to plot your trajectory. But note that these are not typical prediction intervals (what happens if you get a new participant with a certain value of X). The bands fit in ggplot are for predicted &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;|X&lt;/p&gt;
&lt;p&gt;Broom offers the fitted (predicted) values already if you just want to plot your trajectory. But note that these are not typical prediction intervals (what happens if you get a new participant with a certain value of X). The bands fit in ggplot are for predicted &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;|X&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;P.gg &amp;lt;- ggplot(example.aug, aes(x= year, y = .fitted)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;)   

P.gg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Can also explicitly simulate new data (rather than rely on another function to do so), which will be useful for power calculations later. In the simulated data, the subject means are different from the means in the original data because simulate samples by-subject random effect values using the variance components in the fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.1&amp;lt;- simulate(mod.2)
head(sim.1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Welcome to longitudinal data analysis! </title>
      <link>/post/welcome-to-longitudinal-data-analysis/</link>
      <pubDate>Wed, 28 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/welcome-to-longitudinal-data-analysis/</guid>
      <description>


&lt;p&gt;Please take a look at the syllabus for important class details.&lt;/p&gt;
&lt;p&gt;Any updates or announcements to the class will be found here on the main page. Otherwise, the lectures and workshop pages will where you will spend most of your time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Week 1</title>
      <link>/lectures/02-lda-basics/</link>
      <pubDate>Thu, 15 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/lectures/02-lda-basics/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lda-basics&#34;&gt;LDA basics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#procedural&#34;&gt;Procedural&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#goals&#34;&gt;Goals&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#motivation-terms-concepts&#34;&gt;Motivation, terms, concepts&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#why-longitudinal-what-can-we-ask&#34;&gt;Why longitudinal? What can we ask?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#types-of-change-most-common&#34;&gt;Types of change (most common)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#between-person-versus-within-person-variables&#34;&gt;Between person versus within person variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modeling-frameworks-mlm-sem&#34;&gt;Modeling frameworks: MLM &amp;amp; SEM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#meaningful-time-metric&#34;&gt;Meaningful time metric&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thinking-through-longitudinal-data-example&#34;&gt;Thinking through longitudinal data example&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#person-level&#34;&gt;Person level&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#doing-this-with-mlm-and-sem&#34;&gt;Doing this with MLM (and SEM)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#design-considerations&#34;&gt;Design considerations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#number-of-assessment-waves&#34;&gt;1. Number of assessment waves&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#measurement&#34;&gt;2. Measurement&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#threats-to-validity&#34;&gt;Threats to validity&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#missing-data&#34;&gt;1. Missing data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attritionmortality&#34;&gt;2. Attrition/Mortality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#historycohort&#34;&gt;3. History/cohort&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#maturation&#34;&gt;4. Maturation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing&#34;&gt;5. Testing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#selection&#34;&gt;6. Selection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-not-rm-anova&#34;&gt;Why not RM ANOVA?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;lda-basics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;LDA basics&lt;/h1&gt;
&lt;div id=&#34;procedural&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Procedural&lt;/h2&gt;
&lt;p&gt;By virtue of being on this page, you know the class website. You may access all of the code and datasets and everything that is used to create the lectures through my github: &lt;a href=&#34;https://github.com/josh-jackson/ALDA&#34; class=&#34;uri&#34;&gt;https://github.com/josh-jackson/ALDA&lt;/a&gt;. The provided code should be enough but if you want to search farther go for it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;goals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goals&lt;/h2&gt;
&lt;p&gt;This first class is set to orientate you to the world of longitudinal data and MLM models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;motivation-terms-concepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation, terms, concepts&lt;/h2&gt;
&lt;div id=&#34;why-longitudinal-what-can-we-ask&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why longitudinal? What can we ask?&lt;/h3&gt;
&lt;p&gt;At least 7 reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Identification of intraindividual change (and stability). Do people increase or decrease with time or age. Is this pattern monotonic? Should this best be conceptualized as a stable process or something that is more dynamic? On average how do people change? Ex: people decline in cognitive ability across time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Inter-individual differences in intraindividual change. Does everyone change the same? Do some people start higher but change less? Do some increase while some decrease? Ex: not all people people decline in cognitive ability across time, but some do.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Examine joint relationship among intraindividual change for two or more constructs. If variable X goes up does variable Y also go up across time? Does this always happen or only during certain times? Is this association due to a third variable or does it mean that change occurs for similar reasons? Ex: changes in cognitive ability are associated with changes in health across time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Determinants of intraindividual change. What are the repeated experiences that can push construct X around. Do these have similar effects at all times? Ex: people declinein cognitive ability across time. Ex: I have better memory compared to other times when I engage in cognitive activities vs times that I do not.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Determinants of interindividual differences in intraindividual change. Do events, background characteristics, interventions or other between person characteristic shape why certain people change while others don’t? Ex: people decline less in cognitive ability across time if tend to do cognitively engaging activities.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Inter-individual differences in intraindividual fluctuation and determinants of intraindividual fluctuation. Does everyone vary the same? Why are some more variable than others? Ex: Someone who is depressed fluctuates more in happiness than someone who is not depressed&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Are there different classes/populations/mixtures of intraindividual change? Ex: do people who decrease vs don’t in cognitive ability across time exist as different groups? (Vs construing differences as on a continuum).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;types-of-change-most-common&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Types of change (most common)&lt;/h3&gt;
&lt;p&gt;There are many ways to think of change and stability. We will only have time to go into a few of these types, but it is helpful to think about what type you are interested in when you plan a project or sit down to analyze data. “Change” can mean different things. The above questions you can ask mostly map onto #3 definition of change below.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Differential / rank order consistency/ rank order stability. Goes by many names but in the end it is just a correlation. This is a group/sample/population level variable and indexes the relative standing of a person with regard to the rest of the members in the sample. Does not take into account mean structure. Best used with heterotypic continuity where the construct may be the same but the measurement of the construct changes e.g., childhood IQ or acting out in school versus when you are an adult.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A specialized case of this is ipsative change, which looks at the rank order of constructs within a person. This is not done on a single variable (depression) but on a broad number of them (all PD symptoms). Often uses profiles.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Mean level/ absolute change. Takes into account mean structure and indexes absolute levels of a construct. A strong assumption is that the construct means (not a pun) the same thing across time. That is, my measure of depression is interpreted the same for a 40 year old and a 90 year old if I want to look at absolute differences between the two ages.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mean level change is not dependent at all on rank order consistency. Can have no mean level change and high rank order consistency and vice versa.&lt;/p&gt;
&lt;p&gt;Perfect rank order, mean level increase&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ────────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1          ✔ purrr   0.3.2     
## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ readr   1.3.1          ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simp&amp;lt;- tribble(
  ~ID,  ~Y, ~time,
1,5,1,
1,7,2,
2,4,1,
2,6,2,
3,3,1,
3,5,2,
4,2,1,
4,4,2,
5,1,1,
5,3,2)

ro.ml &amp;lt;- ggplot(simp, aes(x=time, y=Y)) +
    geom_point() +   
   stat_summary(fun.y = mean, geom=&amp;quot;line&amp;quot;, size = 4) +
    geom_smooth(aes(group = ID), method=lm,  
                se=FALSE)  

ggsave(&amp;quot;ro.ml.png&amp;quot;, ro.ml)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Saving 7 x 5 in image&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No rank order, mean level increase&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
simp&amp;lt;- tribble(
  ~ID,  ~Y, ~time,
1,1,1,
1,5,2,
2,1.5,1,
2,4.5,2,
3,2,1,
3,4,2,
4,2.5,1,
4,3.5,2,
5,3,1,
5,3,2)

noro.ml&amp;lt;- ggplot(simp, aes(x=time, y=Y)) +
    geom_point() +   
   stat_summary(fun.y = mean, geom=&amp;quot;line&amp;quot;, size = 4) +
    geom_smooth(aes(group = ID), method=lm,  
                se=FALSE)  
ggsave(&amp;quot;noro.ml.png&amp;quot;, noro.ml)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Saving 7 x 5 in image&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Individual differences in change. Rank order and mean level provide an index of change and or stability for the sample. Here this provides an assessment of change for an individual. For example, if it is typical to decline in cognitive ability do some people buck the trend and stay at their past level? Individual differences in change get at both mean level changes as well as the tendency of the sample to show stability. It is the type of change that we will focus on the most.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Structural. Does the construct (or measure) change across time? The assumption for mean level change assumes that the measurement properties stays the same. But maybe it is theoretically interesting to ask whether what you are measuring changes. Examples include practice effects, age effects (cog ability in kids vs adults), differences due to health and life events.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Variance. Does your experiment lead to an increas in variability in response? You may show no mean levels (which is what is looked at in typical t-tests and ANOVAs) but you could see people increase or decrease in their expected range of response.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So how do we refer to ‘change’? Usually it is easier to refer to pictorially or in terms of an equation. Putting a word onto it usually causes some confusion, which is why there are a lot of redundant terms in the literature. All of these might refer to the same thing when used within a model. However, the names of some models use these terms differently and thus can refer to different models or conditions that you are working with. In this class I will try to point out the important differences but you will be fine if you supplement your terms with graphs or equations. Math is the great equalizer.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;between-person-versus-within-person-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Between person versus within person variables&lt;/h3&gt;
&lt;p&gt;Between-person versus within-person are the shortened version of interindividaul differences in change versus intraindividaul differences in change. Refers to across people versus within a particular person. Do you care about how people differ from their previous and future self or do you care about how people differ from other people? (See examples in the 7 types of questions we can ask)&lt;/p&gt;
&lt;p&gt;Often we are interested in modeling both between person and within person variables simultaneously. This is related to Level 1 and Level 2 (for those of you familiar with this terminology). It is helpful to start thinking about what variables you are working with and whether they are within or between person variables. For predictors, it is typically the case that between person effects are constant (between person) variables (e.g., gender) that do not change from assessment to assessment or are only assessed once. In contrast, within person questions are best understood by time varying predictors (within person variables e.g., daily mood) that are assessed more than once.&lt;/p&gt;
&lt;p&gt;We will incorporate both time invariant (between person) and time varying (within person) predictors into our eventual model. In addition to thinking about the types of questions you want to ask it is important to think about what “type” or variables you are working with. Your choice of questions you can ask depends on how often you assess variables or how you conceptualize them.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-frameworks-mlm-sem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modeling frameworks: MLM &amp;amp; SEM&lt;/h3&gt;
&lt;p&gt;In this class (and in the field) two primary techniques are used with longitudinal models: MLM and SEM. At some levels they are completely equivalent. At others, one is better than the other and vice versa.&lt;/p&gt;
&lt;p&gt;MLM/HLM is a simple extension of standard regression models. As a result it is easy to interpret and implement. In terms of longitudinal data it is best suited to run models when the time of measurement differs from person to person (compared to equal intervals). For this class we will use lme4 and brms as our MLM program but there are many others we could use e.g., nlme.&lt;/p&gt;
&lt;p&gt;SEM is related to regression in that regression is a subset of SEM techniques. In other words, an SEM program could run a simple regression analysis.&lt;/p&gt;
&lt;p&gt;The primary advantage of MLM is that you may have assessment waves that vary in length between participants. An assumption of SEM models is that everyone has the same amount of time between assessment waves (though this assumption can be relaxed). MLM is also better suited for complex error structures and complex nesting above and beyond assessments within person. It is also easier to model interactions. Currently, it is easier to do MLM within a Bayesian framework too.&lt;/p&gt;
&lt;p&gt;SEM primary advantage is the ability to account for measurement error via latent assessment of the repeated measures. Other advantages include the ability to model multiple DVs at once, and do so in a flexible manner to look at, for example, the associations between change in one construct and change in the another (though these are also possible with MLMs). Another major advantage is the ability to look at latent groups via latent class or mixture models.&lt;/p&gt;
&lt;p&gt;Bottom line: MLM is probably best suited for “basic” or “standard” growth models. More complex analyses of change with multiple variables would benefit from an SEM approach. This is also an oversimplification.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;meaningful-time-metric&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Meaningful time metric&lt;/h2&gt;
&lt;p&gt;Time is the most important part of a longitudinal analyses. Without some sort of explicit operationalization of time or thought into how you handle time in your analyses you are not conducting longitudinal analyses. The key to interpreting your output is to know how you handled your time variable. What units is it in? Does everyone have the same differences between assessments? Is time something you are explicitly interested in or merely there as a means to collect repeated measures? We will discuss more of these as the semester progresses. Right now however an important distinction is what should the scale of our x-axis variable, time, be in?&lt;/p&gt;
&lt;p&gt;At one level, the distinction is relevant to what is the process that is changing someone? Is it a naturally occurring developmental process? Then maybe age is the best metric. What about tracking child’s cognitive ability, something that might be influenced by level of schooling? Here grade may be more important than age. Another common metric is time in study. This may be useful if you are running an intervention or if you want to put everyone on the same starting metric and then control for nuisance variables like age or schooling level. Similarly, year of study as a prime time candidate may be useful if you are working from panel studies and interested in historical events and or cohort effects. A wave variable (ie study measurement occasion) may be good enough to use as a time metric (though this makes some assumptions about the regularity of assessments both within and across people).&lt;/p&gt;
&lt;p&gt;Depending on your choice of time metric you may see different rates of change and variability in change. For psychological applications the most common would be age and time in study (followed by grades for assessments of kids). Age is nice because it captures a number of developmental processes thought to drive change (maturation, history, time-in-study) but does not identify a single reason. Time in study is the opposite in that it does not index any other type of change but that simplicity aides in testing different reasons for change (e.g, age moderation). Thus choosing one type of time metric will naturally guide the types of questions you are able to address. E.g. if you use age as your time metric you won’t be able to control for age or examine the effects of age as simply as if you used time in study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;thinking-through-longitudinal-data-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thinking through longitudinal data example&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using some resting state imaging data, lets think about how we can model and think about this data using our current skills (ie standard regression and plotting)&lt;/p&gt;
&lt;p&gt;We defined time as year in study. How would this look if we used age?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg1 &amp;lt;- ggplot(example,
   aes(x = year, y = SMN7, group = ID)) + geom_point()  
print(gg1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The above graph just plots datapoints. Do we have repeated assessments per person? Lets find out.&lt;/p&gt;
&lt;div id=&#34;person-level&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Person level&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg2 &amp;lt;- ggplot(example,
   aes(x = year, y = SMN7, group = ID)) + geom_line()  
gg2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
Note that some people start at different levels. Some people have more data in terms of assessment points and years. Note that the shape of change isn’t necessarily a straight line.&lt;/p&gt;
&lt;p&gt;We often want to look at this at a per person level to get more info.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg3 &amp;lt;- ggplot(example,
   aes(x = year, y = SMN7, group = ID)) + geom_line() +  geom_point() + facet_wrap( ~ ID)
gg3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;
As part of our dataset we have different groups. A question we may have is if they change differently across time. Lets take a look at this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg4 &amp;lt;- ggplot(example,
   aes(x = year, y = SMN7, group = ID)) + geom_line() + facet_grid(. ~ group)
gg4&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re not in Kansas anymore - look at the technocolor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg5 &amp;lt;-  gg2 + aes(colour = factor(ID)) + guides(colour=FALSE) 
gg5&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Okay, beside the occular technique, we’re going to need to do something more to address our theoretical questions. Lets look at some random people in the sample and run some regressions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(11)
ex.random &amp;lt;- example %&amp;gt;% 
  dplyr::select(ID) %&amp;gt;% 
  distinct %&amp;gt;% 
  sample_n(10) 

example2 &amp;lt;-
  left_join(ex.random, example)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;ID&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg6 &amp;lt;- ggplot(example2,
   aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method=&amp;quot;lm&amp;quot;) + facet_wrap( ~ID)
gg6&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced

## Warning in qt((1 - level)/2, df): NaNs produced&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Lets look at individual level regressions&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(broom)

regressions &amp;lt;- example2 %&amp;gt;% 
  group_by(ID) %&amp;gt;% 
  do(tidy(lm(SMN7 ~ week, data=.)))

regressions&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20 x 6
## # Groups:   ID [10]
##       ID term        estimate std.error statistic  p.value
##    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1    67 (Intercept)  0.0921    0.0161      5.72    0.0292
##  2    67 week         0.00662   0.00657     1.01    0.420 
##  3    75 (Intercept)  0.126   NaN         NaN     NaN     
##  4    75 week         0.00771 NaN         NaN     NaN     
##  5    87 (Intercept)  0.0787  NaN         NaN     NaN     
##  6    87 week        -0.0227  NaN         NaN     NaN     
##  7    99 (Intercept)  0.111     0.0236      4.69    0.0426
##  8    99 week         0.00545   0.0122      0.446   0.699 
##  9   101 (Intercept)  0.111     0.0424      2.62    0.232 
## 10   101 week         0.0421    0.0217      1.94    0.303 
## 11   103 (Intercept)  0.0887  NaN         NaN     NaN     
## 12   103 week        -0.0168  NaN         NaN     NaN     
## 13   105 (Intercept)  0.0465    0.00658     7.06    0.0896
## 14   105 week         0.00122   0.00467     0.261   0.838 
## 15   142 (Intercept)  0.197   NaN         NaN     NaN     
## 16   142 week         0.0130  NaN         NaN     NaN     
## 17   149 (Intercept)  0.0801  NaN         NaN     NaN     
## 18   149 week         0.00497 NaN         NaN     NaN     
## 19   152 (Intercept)  0.0921  NaN         NaN     NaN     
## 20   152 week        -0.0172  NaN         NaN     NaN&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What can we see? Estimates give us an intercept and regression coefficient for each person. Some people increase across time, some decrease. Some we cannot do statistical tests on – why?&lt;/p&gt;
&lt;p&gt;Well that is per person. Lets get the average starting value and change per week&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;regressions %&amp;gt;% 
  group_by(term) %&amp;gt;% 
  summarise(avg.reg = mean(estimate))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   term        avg.reg
##   &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;
## 1 (Intercept) 0.102  
## 2 week        0.00244&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lets plot the average trend across everyone. Start with a best fit line not taking into account that people have repeated measures.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg7 &amp;lt;-  gg1 &amp;lt;- ggplot(example, aes(x = year, y = SMN7)) + geom_point() + stat_smooth() 
gg7&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using method = &amp;#39;loess&amp;#39; and formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That lowess line is a little strange. How about a linear estimate.&lt;/p&gt;
&lt;p&gt;Split up by group&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg8 &amp;lt;-  ggplot(example, aes(x = year, y = SMN7)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;) + facet_grid(. ~ group)
gg8&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But I also want to see the individual slopes, not just the average.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg9 &amp;lt;- ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_point(alpha = 0.05) + stat_smooth(method = &amp;quot;lm&amp;quot;, se = FALSE)   

gg10 &amp;lt;- gg9 +  stat_smooth(data = example, aes(x = year, y = SMN7, group=1, color = &amp;quot;black&amp;quot;), method = &amp;quot;lm&amp;quot;, size = 2) + guides(fill=FALSE)


gg11 &amp;lt;- gg10 + facet_grid(.~ group) + theme(legend.position=&amp;quot;none&amp;quot;)

gg11&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing non-finite values (stat_smooth).

## Warning: Removed 9 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Lectures/02-LDA-basics_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;doing-this-with-mlm-and-sem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Doing this with MLM (and SEM)&lt;/h3&gt;
&lt;p&gt;These regressions and plots are how you should begin to think about longitudinal data analysis. These growth models (the simplest form of longitudinal data analysis) are just a bunch of regressions for each person plus a little extra stuff. MLM is just a fancy regression equation. We want to create a line for everyone. Does someone go up? Does someone go down? On average, do people’s lines indciate that this construct increases or decreases? That is it. Seriously.&lt;/p&gt;
&lt;p&gt;What is different from normal regression? Extra error terms, mostly. For regression, we think of error as existing in one big bucket. For MLMs (and other longitudinal models) we will be breaking up unexplained variance (error) into multiple buckets.&lt;/p&gt;
&lt;p&gt;This is where fixed effects and random effects come into play. We will discuss this more next class, but the gist is that fixed effects are the regression coefficients you are used to. Fixed effects index group level change. Do people decline in memory across time, on average? Random effects vary among individuals (in the longitudinal models we are talking about) and index variation from the group. While most people decline in their memory, does everyone? In other words, an average trajectory will be termed a fixed effect and the random effect indexes how much variability there is around that group level effect. This extra variability we measure through random effects means that we are explaining more variance and thus there is less unexplained variance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;design-considerations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Design considerations&lt;/h2&gt;
&lt;div id=&#34;number-of-assessment-waves&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Number of assessment waves&lt;/h3&gt;
&lt;p&gt;Remember high school algebra: two points define a line. But, that assumes we can measure constructs without error. Three assessment points will better define changes in psychological variables. As a default, you need three waves of data to use MLM models. However, some simplifications can be made with MLM. Two wave assessments are mostly better with SEM approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Measurement&lt;/h3&gt;
&lt;div id=&#34;scale-of-measurement&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Scale of measurement&lt;/h4&gt;
&lt;p&gt;Measurement is always the basis for good quantiative analysis. Without good measurement you are just spitting into the wind. Standard measurement concerns remain (reliability, dimensionality) but extra concerns exist with longitudinal data.&lt;/p&gt;
&lt;p&gt;What does it mean for categorical variables to change over time? Can you imagine a trajectory for what this is measuring? How would dichotomous responses impact ability to measure change?&lt;/p&gt;
&lt;p&gt;What about ranks, such as in preference for school subjects? What if the class composition changes – what is this assessing? Given that ranks are related such that if I increase someone has to decrease, how does that impact change assessments?&lt;/p&gt;
&lt;p&gt;Can I analyze childhood and adult variables simultaneously if assess the same construct, even though they may be measured differently? How can you measure change in the same construct but with different measures? To assess math ability in 5 year olds you can ask them about addition, can you do that in a sample of 20 year olds? Does that measure continue to assess math ability?&lt;/p&gt;
&lt;p&gt;Often the answer to these is to use a different form of the longitudinal model. In general, the better measured the construct (continuous, not dependent on others, using the same scale) the more complex/sophisticated analysis you can run. Worse measurment leads to simplification both in terms of the models and the types of conclusions you can make.&lt;/p&gt;
&lt;p&gt;As with all of science, everything rests on measurement. Poor measurement results in poor conclusions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;standardizing&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Standardizing&lt;/h4&gt;
&lt;p&gt;It is standard practice to z-score to get standardized responses. However, it is not straight forward to do so when using longitudinal data. Why would z-scoring your variables be problematic?&lt;/p&gt;
&lt;p&gt;First, if you scale for age, for example, this takes out a potential explanatory variable.&lt;/p&gt;
&lt;p&gt;Second, more worriesome, it also can add error if not everyone is standardized consistently (say if standardization is across age groups and someone just misses a cut). Or if the sample changes due to attrition.&lt;/p&gt;
&lt;p&gt;Third, is that you take away the mean for each assessment such that the expected change across time is zero. We will talk more about solutions to this problem as the semester progresses but the short answer is to avoid or to use SEM.&lt;/p&gt;
&lt;p&gt;While helpful for cross sectional analyses, z-scoring adds in layers of computational and interpretational problems.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reliability&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Reliability&lt;/h4&gt;
&lt;p&gt;The goal of longitudinal analyses is to understand why some construct changes or stays the same across time. A major difficulty in addressing this goal is whether you are able to accurately assess the construct of interest. One of the key characteristics (but not the only characteristic) is whether or not your assessment would be consistent if you gave an alternative measure or if you retook it immediately after your first assessment. This is known as reliability of measurement. To the extent that your measure is reliable it assesses true score variance as opposed to error variance. The amount of error score variance assessed is important given that error variance will masquerade as change across time given that error can correlate with anything else. The more error in your measurement the more change you will find. Of course this is unreliable change – change that is not true change, just stochastic noise.&lt;/p&gt;
&lt;p&gt;We can think of reliability two ways. First, reliability of the change estimate. This depends on how much error there is in the assessment and the number of waves. These two components are similar to inter item correlation and number of items being the two main components that effect reliability in cross sectional analyses. Increase the number of items (waves) you increase your alpha. Increase the average correlation among items, you increase your alpha. The parallel to average correlation among items is our ability to accurately assess the construct. When comparing a construct across time we examine this with measurement invariance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measurement-invariance&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Measurement invariance&lt;/h4&gt;
&lt;p&gt;The second way to think of reliability is in terms of how consistently the measure is assessed across time. Or, do you assess the same construct at each time? What would happen if we looked at change in IQ from 1st grade to 12 grade and used the first grade IQ test at each time? The construct that you assessed at the first wave is likely not the same assessed later.
To test this formally is called measurement invariance and is typically done through SEM. We will talk more about this later in the semester. Until we get there we make a large assumption that what we are measuring now is the same at each wave of assessment.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;threats-to-validity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Threats to validity&lt;/h2&gt;
&lt;div id=&#34;missing-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1. Missing data&lt;/h3&gt;
&lt;div id=&#34;types-of-missing-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Types of missing data&lt;/h4&gt;
&lt;p&gt;On a scale from 1 to you’re completely screwed, how confident are you that the missingness is not related to your study variables?&lt;/p&gt;
&lt;p&gt;Missing completely at random (MCAR) means that the missingness pattern is due entirely to randomness&lt;/p&gt;
&lt;p&gt;Missing at random (MAR) means that there is conditional randomness. Missingness may be due to other variables in the dataset. Pretty standard for longitudinal data.&lt;/p&gt;
&lt;p&gt;Not missing at random (NMAR) means that the missingness is systematic based on the missing values and not associated with measured variables. For example, in a study of reading ability, kids with low reading ability drop out, due to not liking to take tests on reading ability. However, if reading ability is associated with other variables in the model, then this missingness becomes closer in kind to MAR, and thus somewhat less problematic.&lt;/p&gt;
&lt;p&gt;Typically, we make the assumption we are working under MAR and thus we will have unbiased estimates when predictors of missingness are incorporated into the model.&lt;/p&gt;
&lt;p&gt;There are tests to distinguish MAR from NMAR but you cannot distinguish MCAR from MAR because it is based entirely on knowing something that you dont have.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-handle-missing-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;How to handle missing data&lt;/h4&gt;
&lt;p&gt;Listwise? Nah&lt;/p&gt;
&lt;p&gt;Full information maximum likelihood and other ML approaches? Sure.
Multiple imputation? Cannot hurt. More on these approaches later.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;attritionmortality&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2. Attrition/Mortality&lt;/h3&gt;
&lt;p&gt;Major contributor to missing data&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;historycohort&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3. History/cohort&lt;/h3&gt;
&lt;p&gt;Know that the processes driving change can be due to a specific event or cohort.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maturation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;4. Maturation&lt;/h3&gt;
&lt;p&gt;Change may occur because of natural processes. Thus if you just follow someone across time they will likely change irregardless of say, if they are in the control group.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;5. Testing&lt;/h3&gt;
&lt;p&gt;Having people take the same survey, test or interview multiple times may lead them to respond differently. Does that change result from development or does it result from them being familiar with the test?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;selection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;6. Selection&lt;/h3&gt;
&lt;p&gt;If you are looking at life events, know that life events are not distributed randomly. Moreover, people who stay in studies and even sign up for studies are different from those that do not. As a result, it is often hard to make internally valid inferences with longitudinal data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;why-not-rm-anova&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why not RM ANOVA?&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Cannot handle missing data&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Assumes rate of change is the same for all individuals.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Time is usually done with orthogonal polynomials, making it difficult to interpret or to model non-linear. In other words, you have flexibility on how you want to model time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Accounting for correlation across time uses up many parameters, MLM is more efficient.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Can accommodate differences in time between assessment waves across participants&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Handles various types of predictors - continuous vs nominal &amp;amp; static vs dynamic&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bottom line: this is an old way of doing these analyses with no upside. Don’t do them.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Workshop Week 1</title>
      <link>/workshops/workshop-1/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-1/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-are-data&#34;&gt;What Are Data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#workspace&#34;&gt;Workspace&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#packages&#34;&gt;Packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#codebook&#34;&gt;Codebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clean-data&#34;&gt;Clean Data&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#recode-variables&#34;&gt;Recode Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reverse-scoring&#34;&gt;Reverse-Scoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#create-composites&#34;&gt;Create Composites&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bfi-s&#34;&gt;BFI-S&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#life-events&#34;&gt;Life Events&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#descriptives&#34;&gt;Descriptives&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#scale-reliability&#34;&gt;Scale Reliability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#zero-order-correlations&#34;&gt;Zero-Order Correlations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;what-are-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What Are Data?&lt;/h1&gt;
&lt;p&gt;Data are the core of everything that we do in statistical analysis. Data come in many forms, and I don’t just mean &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.sav&lt;/code&gt;, etc. Data can be wide, long, documented, fragmented, messy, and about anything else that you can imagine.&lt;/p&gt;
&lt;p&gt;Although data could arguably be more means than end in psychology, the importance of understanding the structure and format of your data cannot overstated. Failure to understand your data could end in improper techniques and flagrantly wrong inferences at worst. This is especially important for longitudinal data.&lt;/p&gt;
&lt;p&gt;In this workshop, we are going to talk data management and basic data cleaning. Other tutorials will go more in depth into data cleaning and reshaping. This tutorial is meant to prepare you to think about those in more nuanced ways and to help you develop a functional workflow for conducting your own research.&lt;/p&gt;
&lt;p&gt;The workshop applies to ALL of your data/projects/analysis, not just longitudinal data. These are practices that will accomplish three goals: 1) efficiently load and leave your data in the right form to be analyzed, 2) have the organization so as to follow what you did and so others can understand you did, and 3. share the data/code/plots/analyses easily and effectively. None of the following are the absolute necessary way to accomplish your data analytic goals. However, we feel that people mostly don’t think through these steps. Hammstringing them later. In other words, if you know an alternative that means you already know what we are trying to convey.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workspace&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Workspace&lt;/h1&gt;
&lt;p&gt;When I create an &lt;code&gt;rmarkdown&lt;/code&gt; document for my own research projects, I always start by setting up my my workspace. This involves 3 steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Packages&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Codebook(s)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below, we will step through each of these separately, setting ourselves up to (hopefully) flawlessly communicate with &lt;code&gt;R&lt;/code&gt; and our data.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;p&gt;Packages seems like the most basic step, but it is actually very important. &lt;strong&gt;ALWAYS LOAD YOUR PACKAGES IN A VERY INTENTIONAL ORDER AT THE BEGINNING OF YOUR SCRIPT.&lt;/strong&gt; Package conflicts suck, so it needs to be shouted. (Note: Josh will often reload or not follow this advice for didactic reasons, choosing to put library calls above the code. )&lt;/p&gt;
&lt;p&gt;For this tutorial, we are going to quite simple. We will load the &lt;code&gt;psych&lt;/code&gt; package for data descriptives, some options for cleaning and reverse coding, and some evaluations of our scales. The &lt;code&gt;plyr&lt;/code&gt; package is the predecessor of the &lt;code&gt;dplyr&lt;/code&gt; package, which is a core package of the &lt;code&gt;tidyverse&lt;/code&gt;, which you will become quite familiar with in these tutorials. I like the plyr package because it contains a couple of functions (e.g. &lt;code&gt;mapvalues()&lt;/code&gt;) that I find quite useful. Finally, we load the &lt;code&gt;tidyverse&lt;/code&gt; package, which is actually a complilation of 8 packages. Some of these we will use today and some we will use in later tutorials. All are very useful and are arguably some of the most powerful tools R offers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load packages
library(psych)
library(plyr)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1           ✔ purrr   0.3.2      
## ✔ tibble  2.1.3           ✔ dplyr   0.8.3      
## ✔ tidyr   0.8.99.9000     ✔ stringr 1.4.0      
## ✔ readr   1.3.1           ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ ggplot2::%+%()     masks psych::%+%()
## ✖ ggplot2::alpha()   masks psych::alpha()
## ✖ dplyr::arrange()   masks plyr::arrange()
## ✖ purrr::compact()   masks plyr::compact()
## ✖ dplyr::count()     masks plyr::count()
## ✖ dplyr::failwith()  masks plyr::failwith()
## ✖ dplyr::filter()    masks stats::filter()
## ✖ dplyr::id()        masks plyr::id()
## ✖ dplyr::lag()       masks stats::lag()
## ✖ dplyr::mutate()    masks plyr::mutate()
## ✖ dplyr::rename()    masks plyr::rename()
## ✖ dplyr::summarise() masks plyr::summarise()
## ✖ dplyr::summarize() masks plyr::summarize()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;codebook&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Codebook&lt;/h2&gt;
&lt;p&gt;The second step is a codebook. Arguably, this is the first step because you should &lt;em&gt;create&lt;/em&gt; the codebook long before you open &lt;code&gt;R&lt;/code&gt; and load your data.&lt;/p&gt;
&lt;p&gt;In this case, we are going to using some data from the &lt;a href=&#34;https://www.diw.de/en/soep/&#34;&gt;German Socioeconomic Panel Study (GSOEP)&lt;/a&gt;, which is an ongoing Panel Study in Germany. Note that these data are for teaching purposes only, shared under the license for the Comprehensive SOEP teaching dataset, which I, as a contracted SOEP user, can use for teaching purposes. These data represent select cases from the full data set and should not be used for the purpose of publication. The full data are available for free at &lt;a href=&#34;https://www.diw.de/en/diw_02.c.222829.en/access_and_ordering.html&#34; class=&#34;uri&#34;&gt;https://www.diw.de/en/diw_02.c.222829.en/access_and_ordering.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For this tutorial, I created the codebook for you, and included what I believe are the core columns you may need. Some of these columns will not be particularly helpful for this dataset. For example, many of you likely work with datasets that have only a single file while others work with datasetsspread across many files. As a result, the “dataset” column of the codebook may only have a single value whereas for others it may have multiple. With longitudinal data it is likely you will have multiple.&lt;/p&gt;
&lt;p&gt;Here are my core columns that are based on the original data:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;dataset&lt;/strong&gt;: this column indexes the &lt;strong&gt;name&lt;/strong&gt; of the dataset that you will be pulling the data from. This is important because we will use this info later on (see &lt;code&gt;purrr&lt;/code&gt; tutorial) to load and clean specific data files. Even if you don’t have multiple data sets, I believe consistency is more important and suggest using this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;old_name&lt;/strong&gt;: this column is the name of the variable in the data you are pulling it from. This should be exact. The goal of this column is that it will allow us to &lt;code&gt;select()&lt;/code&gt; variables from the original data file and rename them something that is more useful to us. If you have worked with qualtrics (really any data) you know why this is important.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;item_text&lt;/strong&gt;: this column is the original text that participants saw or a description of the item.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;scale&lt;/strong&gt;: this column tells you what the scale of the variable is. Is it a numeric variable, a text variable, etc. This is helpful for knowing the plausible range.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;reverse&lt;/strong&gt;: this column tells you whether items in a scale need to be reverse coded. I recommend coding this as 1 (leave alone) and -1 (reverse) for reasons that will become clear later.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;mini&lt;/strong&gt;: this column represents the minimum value of scales that are numeric. Leave blank otherwise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;maxi&lt;/strong&gt;: this column represents the maximumv alue of scales that are numeric. Leave blank otherwise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;recode&lt;/strong&gt;: sometimes, we want to recode variables for analyses (e.g. for categorical variables with many levels where sample sizes for some levels are too small to actually do anything with it). I use this column to note the kind of recoding I’ll do to a variable for transparency.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here are additional columns that will make our lives easier or are applicable to some but not all data sets:&lt;/p&gt;
&lt;ol start=&#34;9&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;category&lt;/strong&gt;: broad categories that different variables can be put into. I’m a fan of naming them things like “outcome”, “predictor”, “moderator”, “demographic”, “procedural”, etc. but sometimes use more descriptive labels like “Big 5” to indicate the model from which the measures are derived.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;label&lt;/strong&gt;: label is basically one level lower than category. So if the category is Big 5, the label would be, or example, “A” for Agreeableness, “SWB” for subjective well-being, etc. This column is most important and useful when you have multiple items in a scales, so I’ll typically leave this blank when something is a standalone variable (e.g. sex, single-item scales, etc.).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;item_name&lt;/strong&gt;: This is the lowest level and most descriptive variable. It indicates which item in scale something is. So it may be “kind” for Agreebleness or “sex” for the demographic biological sex variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;year&lt;/strong&gt;: for longitudinal data, we have several waves of data and the name of the same item across waves is often different, so it’s important to note to which wave an item belongs. You can do this by noting the wave (e.g. 1, 2, 3), but I prefer the actual year the data were collected (e.g. 2005, 2009, etc.) if that is appropriate. See Lecture #1 on discussion of meaningful time metrics. Note that this differs from that discussion in codebook describes how you collected the data, not necessarily how you want to analyze the data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;new_name&lt;/strong&gt;: This is a column that brings together much of the information we’ve already collected. It’s purpose is to be the new name that we will give to the variable that is more useful and descriptive to us. This is a constructed variable that brings together others. I like to make it a combination of “category”, “label”, “item_name”, and year using varying combos of &#34;_&#34; and “.” that we can use later with tidyverse functions. I typically construct this variable in Excel using the &lt;code&gt;CONCATENATE()&lt;/code&gt; function, but it could also be done in &lt;code&gt;R&lt;/code&gt;. The reason I do it in Excel is that it makes it easier for someone who may be reviewing my codebook.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is a seperate discussion to be had on naming conventions for your variables, but the important idea to remember is that names convey important information and we want to use this information later on to make our life easier. By coding these variables using this information AND systematically using different seperators we can accomplish this goal.&lt;/p&gt;
&lt;ol start=&#34;14&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;meta&lt;/strong&gt;: Some datasets have a meta name, which essentially means a name that variable has across all waves to make it clear which variables are the same. They are not always useful as some data sets have meta names but no great way of extracting variables using them. But they’re still typically useful to include in your codebook regardless.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below, I’ll load in the codebook we will use for this study, which will include all of the above columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set the path
wd &amp;lt;- &amp;quot;https://github.com/emoriebeck/R-tutorials/blob/master/ALDA/week_1_descriptives&amp;quot;

# load the codebook
(codebook &amp;lt;- url(sprintf(&amp;quot;%s/codebook.csv?raw=true&amp;quot;, wd)) %&amp;gt;% 
    read_csv(.) %&amp;gt;%
    mutate(old_name = str_to_lower(old_name)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   dataset = col_character(),
##   old_name = col_character(),
##   item_text = col_character(),
##   scale = col_character(),
##   category = col_character(),
##   label = col_character(),
##   item_name = col_character(),
##   year = col_double(),
##   new_name = col_character(),
##   reverse = col_double(),
##   mini = col_double(),
##   maxi = col_double(),
##   recode = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 153 x 13
##    dataset old_name item_text scale category label item_name  year new_name
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   
##  1 &amp;lt;NA&amp;gt;    persnr   Never Ch… &amp;lt;NA&amp;gt;  Procedu… &amp;lt;NA&amp;gt;  SID           0 Procedu…
##  2 &amp;lt;NA&amp;gt;    hhnr     househol… &amp;lt;NA&amp;gt;  Procedu… &amp;lt;NA&amp;gt;  household     0 Procedu…
##  3 ppfad   gebjahr  Year of … nume… Demogra… &amp;lt;NA&amp;gt;  DOB           0 Demogra…
##  4 ppfad   sex      Sex       &amp;quot;\n1… Demogra… &amp;lt;NA&amp;gt;  Sex           0 Demogra…
##  5 vp      vp12501  Thorough… &amp;lt;NA&amp;gt;  Big 5    C     thorough   2005 Big 5__…
##  6 zp      zp12001  Thorough… &amp;lt;NA&amp;gt;  Big 5    C     thorough   2009 Big 5__…
##  7 bdp     bdp15101 Thorough… &amp;lt;NA&amp;gt;  Big 5    C     thorough   2013 Big 5__…
##  8 vp      vp12502  Am commu… &amp;lt;NA&amp;gt;  Big 5    E     communic   2005 Big 5__…
##  9 zp      zp12002  Am commu… &amp;lt;NA&amp;gt;  Big 5    E     communic   2009 Big 5__…
## 10 bdp     bdp15102 Am commu… &amp;lt;NA&amp;gt;  Big 5    E     communic   2013 Big 5__…
## # … with 143 more rows, and 4 more variables: reverse &amp;lt;dbl&amp;gt;, mini &amp;lt;dbl&amp;gt;,
## #   maxi &amp;lt;dbl&amp;gt;, recode &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;First, we need to load in the data. We’re going to use three waves of data from the German Socioeconomic Panel Study, which is a longitudinal study of German households that has been conducted since 1984. We’re going to use more recent data from three waves of personality data collected between 2005 and 2013.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: we will be using the teaching set of the GSOEP data set. I will not be pulling from the raw files as a result of this. I will also not be mirroring the format that you would usually load the GSOEP from because that is slightly more complicated and somethng we will return to in a later tutorial after we have more skills. I’ve left that code for now, but it won’t make a lot of sense right now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path &amp;lt;- &amp;quot;~/Box/network/other projects/PCLE Replication/data/sav_files&amp;quot;
ref &amp;lt;- sprintf(&amp;quot;%s/cirdef.sav&amp;quot;, path) %&amp;gt;% haven::read_sav(.) %&amp;gt;% select(hhnr, rgroup20)
read_fun &amp;lt;- function(Year){
  vars &amp;lt;- (codebook %&amp;gt;% filter(year == Year | year == 0))$old_name
  set &amp;lt;- (codebook %&amp;gt;% filter(year == Year))$dataset[1]
  sprintf(&amp;quot;%s/%s.sav&amp;quot;, path, set) %&amp;gt;% haven::read_sav(.) %&amp;gt;%
    full_join(ref) %&amp;gt;%
    filter(rgroup20 &amp;gt; 10) %&amp;gt;%
    select(one_of(vars)) %&amp;gt;%
    gather(key = item, value = value, -persnr, -hhnr, na.rm = T)
}

vars &amp;lt;- (codebook %&amp;gt;% filter(year == 0))$old_name
dem &amp;lt;- sprintf(&amp;quot;%s/ppfad.sav&amp;quot;, path) %&amp;gt;% 
  haven::read_sav(.) %&amp;gt;%
  select(vars)
  
tibble(year = c(2005:2015)) %&amp;gt;%
  mutate(data = map(year, read_fun)) %&amp;gt;%
  select(-year) %&amp;gt;% 
  unnest(data) %&amp;gt;%
  distinct() %&amp;gt;% 
  filter(!is.na(value)) %&amp;gt;%
  spread(key = item, value = value) %&amp;gt;%
  left_join(dem) %&amp;gt;%
  write.csv(., file = &amp;quot;~/Documents/Github/R-tutorials/ALDA/week_1_descriptives/data/wdeek_1_data.csv&amp;quot;, row.names = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code below shows how I would read in and rename a wide-format data set using the codebook I created.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;old.names &amp;lt;- codebook$old_name # get old column names
new.names &amp;lt;- codebook$new_name # get new column names

(soep &amp;lt;- url(sprintf(&amp;quot;%s/data/week_1_data.csv?raw=true&amp;quot;, wd)) %&amp;gt;% # path to data
  read_csv(.) %&amp;gt;% # read in data
  select(old.names) %&amp;gt;% # select the columns from our codebook
  setNames(new.names)) # rename columns with our new names&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   .default = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See spec(...) for full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 28,290 x 153
##    Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1             901               94             1951                2
##  2            1202              124             1913                2
##  3            2301              230             1946                1
##  4            2302              230             1946                2
##  5            2304              230             1978                1
##  6            2305              230             1946                2
##  7            4601              469             1933                2
##  8            4701              477             1919                2
##  9            4901              493             1925                2
## 10            5201              523             1955                1
## # … with 28,280 more rows, and 149 more variables: `Big
## #   5__C_thorough.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_thorough.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_thorough.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_communic.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_communic.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_communic.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_coarse.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_coarse.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_coarse.2013` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_original.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_worry.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2013` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_forgive.2009` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_lazy.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_sociable.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2005` &amp;lt;dbl&amp;gt;, `Big 5__O_artistic.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2013` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_nervous.2009` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_efficient.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_reserved.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_friendly.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2013` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_imagin.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_dealStress.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2013` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2006` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2010` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2012` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2013` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2014` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2015` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2005` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2006` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2011` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2013` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2014` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2015` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2005` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2006` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2007` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2008` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2009` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2010` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2011` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2013` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2014` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2015` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2005` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2006` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2007` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2008` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2011` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2012` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2013` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2014` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2015` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2011` &amp;lt;dbl&amp;gt;, `Life Event__NewPart.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2013` &amp;lt;dbl&amp;gt;, `Life Event__NewPart.2014` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2015` &amp;lt;dbl&amp;gt;, `Life Event__Married.2005` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2006` &amp;lt;dbl&amp;gt;, `Life Event__Married.2007` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2008` &amp;lt;dbl&amp;gt;, `Life Event__Married.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2010` &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Clean Data&lt;/h1&gt;
&lt;div id=&#34;recode-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recode Variables&lt;/h2&gt;
&lt;p&gt;Many of the data we work with have observations that are missing for a variety of reasons. In &lt;code&gt;R&lt;/code&gt;, we treat missing values as &lt;code&gt;NA&lt;/code&gt;, but many other programs from which you may be importing your data may use other codes (e.g. 999, -999, etc.). Large panel studies tend to use small negative values to indicate different types of missingness. This is why it is important to note down the scale in your codebook. That way you can check which values may need to be recoded to explicit &lt;code&gt;NA&lt;/code&gt; values.&lt;/p&gt;
&lt;p&gt;In the GSOEP, &lt;code&gt;-1&lt;/code&gt; to &lt;code&gt;-7&lt;/code&gt; indicate various types of missing values, so we will recode these to &lt;code&gt;NA&lt;/code&gt;. To do this, we will use one of my favorite functions, &lt;code&gt;mapvalues()&lt;/code&gt;, from the &lt;code&gt;plyr&lt;/code&gt; package. In later tutorials where we read in and manipulate more complex data sets, we will use &lt;code&gt;mapvalues()&lt;/code&gt; a lot. Basically, mapvalues takes 4 key arguments: (1) the variable you are recoding, (2) a vector of initial values &lt;code&gt;from&lt;/code&gt; which you want to (3) recode your variable &lt;code&gt;to&lt;/code&gt; using a vector of new values in the same order as the old values, and (4) a way to turn off warnings if some levels are not in your data (&lt;code&gt;warn_missing = F&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(soep &amp;lt;- soep %&amp;gt;%
  mutate_all(~as.numeric(mapvalues(., from = seq(-1,-7, -1), # recode negative 
                to = rep(NA, 7), warn_missing = F)))) # values to NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 28,290 x 153
##    Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1             901               94             1951                2
##  2            1202              124             1913                2
##  3            2301              230             1946                1
##  4            2302              230             1946                2
##  5            2304              230             1978                1
##  6            2305              230             1946                2
##  7            4601              469             1933                2
##  8            4701              477             1919                2
##  9            4901              493             1925                2
## 10            5201              523             1955                1
## # … with 28,280 more rows, and 149 more variables: `Big
## #   5__C_thorough.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_thorough.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_thorough.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_communic.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_communic.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_communic.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_coarse.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_coarse.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_coarse.2013` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_original.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_worry.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2013` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_forgive.2009` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_lazy.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_sociable.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2005` &amp;lt;dbl&amp;gt;, `Big 5__O_artistic.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2013` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_nervous.2009` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_efficient.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_reserved.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_friendly.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2013` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_imagin.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_dealStress.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2013` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2006` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2010` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2012` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2013` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2014` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2015` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2005` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2006` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2011` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2013` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2014` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2015` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2005` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2006` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2007` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2008` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2009` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2010` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2011` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2013` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2014` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2015` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2005` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2006` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2007` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2008` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2011` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2012` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2013` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2014` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2015` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2011` &amp;lt;dbl&amp;gt;, `Life Event__NewPart.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2013` &amp;lt;dbl&amp;gt;, `Life Event__NewPart.2014` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2015` &amp;lt;dbl&amp;gt;, `Life Event__Married.2005` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2006` &amp;lt;dbl&amp;gt;, `Life Event__Married.2007` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2008` &amp;lt;dbl&amp;gt;, `Life Event__Married.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2010` &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reverse-scoring&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reverse-Scoring&lt;/h2&gt;
&lt;p&gt;Many scales we use have items that are positively or negatively keyed. High ratings on positively keyed items are indicative of being high on a construct. In contrast, high ratings on negatively keyed items are indicative of being low on a construct. Thus, to create the composite scores of constructs we often use, we must first “reverse” the negatively keyed items so that high scores indicate being higher on the construct.&lt;/p&gt;
&lt;p&gt;There are a few ways to do this in &lt;code&gt;R&lt;/code&gt;. Below, I’ll demonstrate how to do so using the &lt;code&gt;reverse.code()&lt;/code&gt; function in the &lt;code&gt;psych&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;. This function was built to make reverse coding more efficient (i.e. please don’t run every item that needs to be recoded with separate lines of code!!).&lt;/p&gt;
&lt;p&gt;Before we can do that, though, we need to restructure the data a bit in order to bring in the reverse coding information from our codebook. We will talk more about what’s happening here in later tutorials on &lt;code&gt;tidyr&lt;/code&gt;, so for now, just bear with me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(soep_long &amp;lt;- soep %&amp;gt;%
  gather(key = item, value = value, -contains(&amp;quot;Procedural&amp;quot;), # change to long format
         -contains(&amp;quot;Demographic&amp;quot;), na.rm = T) %&amp;gt;%
  left_join(codebook %&amp;gt;% select(item = new_name, reverse, mini, maxi)) %&amp;gt;% # bring in codebook
  separate(item, c(&amp;quot;type&amp;quot;, &amp;quot;item&amp;quot;), sep = &amp;quot;__&amp;quot;) %&amp;gt;% # separate category
  separate(item, c(&amp;quot;item&amp;quot;, &amp;quot;year&amp;quot;), sep = &amp;quot;[.]&amp;quot;) %&amp;gt;% # seprate year
  separate(item, c(&amp;quot;item&amp;quot;, &amp;quot;scrap&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;% # separate scale and item
  mutate(value = as.numeric(value), # change to numeric
         value = ifelse(reverse == -1, 
            reverse.code(-1, value, mini = mini, maxi = maxi), value)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;item&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 19618 rows
## [452105, 452106, 452107, 452108, 452109, 452110, 452111, 452112, 452113,
## 452114, 452115, 452116, 452117, 452118, 452119, 452120, 452121, 452122,
## 452123, 452124, ...].&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 471,722 x 12
##    Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex type 
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
##  1             901               94             1951                2 Big 5
##  2            1202              124             1913                2 Big 5
##  3            2301              230             1946                1 Big 5
##  4            2302              230             1946                2 Big 5
##  5            2304              230             1978                1 Big 5
##  6            4601              469             1933                2 Big 5
##  7            4701              477             1919                2 Big 5
##  8            4901              493             1925                2 Big 5
##  9            5201              523             1955                1 Big 5
## 10            5202              523             1956                2 Big 5
## # … with 471,712 more rows, and 7 more variables: item &amp;lt;chr&amp;gt;, scrap &amp;lt;chr&amp;gt;,
## #   year &amp;lt;chr&amp;gt;, value &amp;lt;dbl&amp;gt;, reverse &amp;lt;dbl&amp;gt;, mini &amp;lt;dbl&amp;gt;, maxi &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-composites&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create Composites&lt;/h2&gt;
&lt;p&gt;Now that we have reverse coded our items, we can create composites.&lt;/p&gt;
&lt;div id=&#34;bfi-s&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;BFI-S&lt;/h3&gt;
&lt;p&gt;We’ll start with our scale – in this case, the Big 5 from the German translation of the BFI-S.&lt;/p&gt;
&lt;p&gt;Here’s the simplest way, which is also the long way because you’d have to do it for each scale in each year, which I don’t recommend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;soep$C.2005 &amp;lt;- with(soep, rowMeans(cbind(`Big 5__C_thorough.2005`, `Big 5__C_lazy.2005`, `Big 5__C_efficient.2005`), na.rm = T)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But personally, I don’t have a desire to do that 15 times (5 traits times 3 waves), so we can use our codebook and &lt;code&gt;dplyr&lt;/code&gt; to make our lives a whole lot easier. In general, trying to run everything simultanously saves from copy-paste errors, makes your code more readable, and reduces the total amount of code. So while the below code may not make intuiative sense immediately, it is nonetheless what we are working towards.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;soep &amp;lt;- soep %&amp;gt;% select(-C.2005) # get rid of added column

(b5_soep_long &amp;lt;- soep_long %&amp;gt;%
  filter(type == &amp;quot;Big 5&amp;quot;) %&amp;gt;% # keep Big 5 variables
  group_by(Procedural__SID, item, year) %&amp;gt;% # group by person, construct, &amp;amp; year
  summarize(value = mean(value, na.rm = T)) %&amp;gt;% # calculate means
  ungroup() %&amp;gt;% # ungroup
  left_join(soep_long %&amp;gt;% # bring demographic info back in 
    select(Procedural__SID, DOB = Demographic__DOB, Sex = Demographic__Sex) %&amp;gt;%
    distinct()))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;Procedural__SID&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 151,186 x 6
##    Procedural__SID item  year  value   DOB   Sex
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 A     2005   5     1951     2
##  2             901 A     2009   5.33  1951     2
##  3             901 A     2013   5     1951     2
##  4             901 C     2005   5.33  1951     2
##  5             901 C     2009   5.5   1951     2
##  6             901 C     2013   6     1951     2
##  7             901 E     2005   4     1951     2
##  8             901 E     2009   4     1951     2
##  9             901 E     2013   4     1951     2
## 10             901 N     2005   4     1951     2
## # … with 151,176 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;life-events&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Life Events&lt;/h3&gt;
&lt;p&gt;We also want to create a variable that indexes whether our participants experienced any of the life events during the years of interest (2005-2015).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(events_long &amp;lt;- soep_long %&amp;gt;%
  filter(type == &amp;quot;Life Event&amp;quot;) %&amp;gt;% # keep only life events
  group_by(Procedural__SID, item) %&amp;gt;% # group by person and event
  summarize(value = sum(value, na.rm = T), # sum up whether they experiened the event at all
            value = ifelse(value &amp;gt; 1, 1, 0))) # if more than once 1, otherwise 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15,061 x 3
## # Groups:   Procedural__SID [10,019]
##    Procedural__SID item      value
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
##  1             901 MomDied       1
##  2            2301 MoveIn        0
##  3            2301 PartDied      1
##  4            2305 MoveIn        0
##  5            4601 PartDied      0
##  6            5201 ChldMvOut     1
##  7            5201 DadDied       0
##  8            5202 ChldMvOut     1
##  9            5203 MoveIn        1
## 10            5303 MomDied       0
## # … with 15,051 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;descriptives&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Descriptives&lt;/h1&gt;
&lt;p&gt;Descriptives of your data are incredibly important. They are your first line of defense against things that could go wrong later on when you run inferential stats. They help you check the distribution of your variables (e.g. non-normally distributed), look for implausible values made through coding or participant error, and allow you to anticipate what your findings will look like.&lt;/p&gt;
&lt;p&gt;There are lots of ways to create great tables of descriptives. My favorite way is using &lt;code&gt;dplyr&lt;/code&gt;, but we will save that for a later lesson on creating great APA style tables in &lt;code&gt;R&lt;/code&gt;. For now, we’ll use a wonderfully helpful function from the &lt;code&gt;psych&lt;/code&gt; package called &lt;code&gt;describe()&lt;/code&gt; in conjunction with a small amount of &lt;code&gt;tidyr&lt;/code&gt; to reshape the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b5_soep_long  %&amp;gt;%
  unite(tmp, item, year, sep = &amp;quot;_&amp;quot;) %&amp;gt;% # make new column that joins item and year
  spread(tmp, value) %&amp;gt;% # make wide because that helps describe
  describe(.) # call describe&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 vars     n       mean          sd     median    trimmed
## Procedural__SID    1 16719 8321022.91 10677731.19 3105002.00 6487615.30
## DOB                2 16719    1960.03       18.48    1960.00    1960.22
## Sex                3 16719       1.52        0.50       2.00       1.53
## A_2005             4 10419       5.79        0.98       6.00       5.83
## A_2009             5 10294       5.68        0.99       5.67       5.72
## A_2013             6  9535       5.74        0.96       5.67       5.78
## C_2005             7 10412       6.23        0.96       6.33       6.34
## C_2009             8 10290       6.16        0.95       6.33       6.25
## C_2013             9  9530       6.17        0.91       6.33       6.25
## E_2005            10 10416       5.15        1.15       5.33       5.18
## E_2009            11 10291       5.11        1.15       5.00       5.13
## E_2013            12  9533       5.20        1.11       5.33       5.24
## N_2005            13 10413       4.71        1.23       4.67       4.72
## N_2009            14 10294       4.84        1.22       5.00       4.87
## N_2013            15  9534       4.92        1.21       5.00       4.93
## O_2005            16 10408       4.51        1.22       4.67       4.53
## O_2009            17 10287       4.40        1.22       4.33       4.41
## O_2013            18  9530       4.60        1.18       4.67       4.62
##                        mad     min        max       range  skew kurtosis
## Procedural__SID 3735541.17  901.00 3.5022e+07 35021101.00  1.50     0.56
## DOB                  20.76 1909.00 1.9950e+03       86.00 -0.08    -0.84
## Sex                   0.00    1.00 2.0000e+00        1.00 -0.10    -1.99
## A_2005                0.99    1.00 7.5000e+00        6.50 -0.40    -0.14
## A_2009                0.99    1.33 8.0000e+00        6.67 -0.36    -0.19
## A_2013                0.99    1.33 8.0000e+00        6.67 -0.43     0.05
## C_2005                0.99    1.00 8.0000e+00        7.00 -0.95     0.84
## C_2009                0.99    1.00 8.0000e+00        7.00 -0.82     0.49
## C_2013                0.99    1.33 8.0000e+00        6.67 -0.72     0.17
## E_2005                0.99    1.00 7.5000e+00        6.50 -0.27    -0.16
## E_2009                0.99    1.00 7.3300e+00        6.33 -0.23    -0.18
## E_2013                0.99    1.33 7.3300e+00        6.00 -0.29    -0.18
## N_2005                1.48    1.50 8.0000e+00        6.50 -0.07    -0.32
## N_2009                0.99    1.50 7.6700e+00        6.17 -0.17    -0.30
## N_2013                1.48    1.50 8.0000e+00        6.50 -0.15    -0.30
## O_2005                1.48    1.00 7.0000e+00        6.00 -0.23    -0.18
## O_2009                1.48    1.00 7.0000e+00        6.00 -0.10    -0.30
## O_2013                0.99    1.00 7.0000e+00        6.00 -0.21    -0.20
##                       se
## Procedural__SID 82579.80
## DOB                 0.14
## Sex                 0.00
## A_2005              0.01
## A_2009              0.01
## A_2013              0.01
## C_2005              0.01
## C_2009              0.01
## C_2013              0.01
## E_2005              0.01
## E_2009              0.01
## E_2013              0.01
## N_2005              0.01
## N_2009              0.01
## N_2013              0.01
## O_2005              0.01
## O_2009              0.01
## O_2013              0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For count variables, like life events, we need to use something slightly different. We’re typically more interested in counts – in this case, how many people experienced each life event in the 10 years we’re considering?&lt;/p&gt;
&lt;p&gt;To do this, we’ll use a little bit of &lt;code&gt;dplyr&lt;/code&gt; rather than the base &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;table()&lt;/code&gt; that is often used for count data. Instead, we’ll use a combination of &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;n()&lt;/code&gt; to get the counts by group. In the end, we’re left with a nice little table of counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;events_long %&amp;gt;%
  group_by(item, value) %&amp;gt;% 
  summarize(N = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  spread(value, N)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##    item        `0`   `1`
##    &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1 ChldBrth   1600   735
##  2 ChldMvOut  1555   830
##  3 DadDied     953   213
##  4 Divorce     414   122
##  5 Married    1646   331
##  6 MomDied     929   219
##  7 MoveIn     1403   419
##  8 NewPart    1207   420
##  9 PartDied    402    76
## 10 SepPart    1172   415&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;scale-reliability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scale Reliability&lt;/h2&gt;
&lt;p&gt;When we work with scales, it’s often a good idea to check the internal consistency of your scale. If the scale isn’t performing how it should be, that could critically impact the inferences you make from your data.&lt;/p&gt;
&lt;p&gt;To check the internal consistency of our Big 5 scales, we will use the &lt;code&gt;alpha()&lt;/code&gt; function from the &lt;code&gt;psych&lt;/code&gt; package, which will give us Cronbach’s as well as a number of other indicators of internal consistency.&lt;/p&gt;
&lt;p&gt;Here’s the way you may have seen / done this in the past.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha.C.2005 &amp;lt;- with(soep, psych::alpha(x = cbind(`Big 5__C_thorough.2005`, 
                                  `Big 5__C_lazy.2005`, `Big 5__C_efficient.2005`)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in psych::alpha(x = cbind(`Big 5__C_thorough.2005`, `Big 5__C_lazy.2005`, : Some items were negatively correlated with the total scale and probably 
## should be reversed.  
## To do this, run the function again with the &amp;#39;check.keys=TRUE&amp;#39; option&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Some items ( Big 5__C_lazy.2005 ) were negatively correlated with the total scale and 
## probably should be reversed.  
## To do this, run the function again with the &amp;#39;check.keys=TRUE&amp;#39; option&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But again, doing this 15 times would be quite a pain and would open you up to the possibility of a lot of copy and paste errors.&lt;/p&gt;
&lt;p&gt;So instead, to do this, I’m going to use a mix of the tidyverse. At first glance, it may seem complex but as you move through other tutorials (particularly the &lt;code&gt;purrr&lt;/code&gt; tutorial), it will begin to make much more sense.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# short function to reshape data and run alpha
alpha_fun &amp;lt;- function(df){
  df %&amp;gt;% spread(scrap,value) %&amp;gt;% psych::alpha(.)
}

(alphas &amp;lt;- soep_long %&amp;gt;%
  filter(type == &amp;quot;Big 5&amp;quot;) %&amp;gt;% # filter out Big 5
  select(Procedural__SID, item:value) %&amp;gt;% # get rid of extra columns
  group_by(item, year) %&amp;gt;% # group by construct and year
  nest() %&amp;gt;% # nest the data
  mutate(alpha_res = map(data, alpha_fun), # run alpha
         alpha = map(alpha_res, ~.$total[2])) %&amp;gt;% # get the alpha value
  unnest(alpha)) # pull it out of the list column&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 5
## # Groups:   item, year [15]
##    item  year            data alpha_res std.alpha
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,3]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 C     2005    [31,117 × 3] &amp;lt;psych&amp;gt;       0.515
##  2 C     2009    [30,728 × 3] &amp;lt;psych&amp;gt;       0.494
##  3 C     2013    [28,496 × 3] &amp;lt;psych&amp;gt;       0.482
##  4 E     2005    [31,188 × 3] &amp;lt;psych&amp;gt;       0.540
##  5 E     2009    [30,770 × 3] &amp;lt;psych&amp;gt;       0.530
##  6 E     2013    [28,532 × 3] &amp;lt;psych&amp;gt;       0.544
##  7 A     2005    [31,184 × 3] &amp;lt;psych&amp;gt;       0.418
##  8 A     2009    [30,796 × 3] &amp;lt;psych&amp;gt;       0.410
##  9 A     2013    [28,529 × 3] &amp;lt;psych&amp;gt;       0.401
## 10 O     2005    [31,091 × 3] &amp;lt;psych&amp;gt;       0.527
## 11 O     2009    [30,722 × 3] &amp;lt;psych&amp;gt;       0.510
## 12 O     2013    [28,451 × 3] &amp;lt;psych&amp;gt;       0.497
## 13 N     2005    [31,162 × 3] &amp;lt;psych&amp;gt;       0.473
## 14 N     2009    [30,802 × 3] &amp;lt;psych&amp;gt;       0.490
## 15 N     2013    [28,536 × 3] &amp;lt;psych&amp;gt;       0.480&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;zero-order-correlations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Zero-Order Correlations&lt;/h2&gt;
&lt;p&gt;Finally, we often want to look at the zero-order correlation among study variables to make sure they are performing as we think they should.&lt;/p&gt;
&lt;p&gt;To run the correlations, we will need to have our data in wide format, so we’re going to do a little bit of reshaping before we do.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b5_soep_long %&amp;gt;%
  unite(tmp, item, year, sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  spread(key = tmp, value = value) %&amp;gt;% 
  select(-Procedural__SID) %&amp;gt;%
  cor(., use = &amp;quot;pairwise&amp;quot;) %&amp;gt;%
  round(., 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          DOB   Sex A_2005 A_2009 A_2013 C_2005 C_2009 C_2013 E_2005 E_2009
## DOB     1.00  0.00  -0.08  -0.07  -0.06  -0.13  -0.12  -0.14   0.10   0.12
## Sex     0.00  1.00   0.18   0.17   0.18   0.05   0.07   0.09   0.08   0.08
## A_2005 -0.08  0.18   1.00   0.50   0.50   0.32   0.20   0.19   0.10   0.06
## A_2009 -0.07  0.17   0.50   1.00   0.55   0.19   0.28   0.18   0.05   0.08
## A_2013 -0.06  0.18   0.50   0.55   1.00   0.18   0.19   0.29   0.04   0.06
## C_2005 -0.13  0.05   0.32   0.19   0.18   1.00   0.52   0.48   0.19   0.10
## C_2009 -0.12  0.07   0.20   0.28   0.19   0.52   1.00   0.55   0.12   0.16
## C_2013 -0.14  0.09   0.19   0.18   0.29   0.48   0.55   1.00   0.13   0.14
## E_2005  0.10  0.08   0.10   0.05   0.04   0.19   0.12   0.13   1.00   0.61
## E_2009  0.12  0.08   0.06   0.08   0.06   0.10   0.16   0.14   0.61   1.00
## E_2013  0.10  0.11   0.04   0.04   0.07   0.10   0.10   0.18   0.59   0.65
## N_2005  0.06 -0.18   0.10   0.06   0.02   0.09   0.06   0.03   0.18   0.10
## N_2009  0.03 -0.22   0.07   0.09   0.03   0.06   0.08   0.05   0.13   0.16
## N_2013  0.02 -0.21   0.06   0.06   0.10   0.04   0.06   0.08   0.10   0.10
## O_2005  0.11  0.06   0.12   0.09   0.07   0.17   0.12   0.08   0.40   0.29
## O_2009  0.10  0.05   0.05   0.11   0.07   0.06   0.13   0.08   0.26   0.36
## O_2013  0.05  0.07   0.08   0.09   0.13   0.07   0.08   0.15   0.24   0.28
##        E_2013 N_2005 N_2009 N_2013 O_2005 O_2009 O_2013
## DOB      0.10   0.06   0.03   0.02   0.11   0.10   0.05
## Sex      0.11  -0.18  -0.22  -0.21   0.06   0.05   0.07
## A_2005   0.04   0.10   0.07   0.06   0.12   0.05   0.08
## A_2009   0.04   0.06   0.09   0.06   0.09   0.11   0.09
## A_2013   0.07   0.02   0.03   0.10   0.07   0.07   0.13
## C_2005   0.10   0.09   0.06   0.04   0.17   0.06   0.07
## C_2009   0.10   0.06   0.08   0.06   0.12   0.13   0.08
## C_2013   0.18   0.03   0.05   0.08   0.08   0.08   0.15
## E_2005   0.59   0.18   0.13   0.10   0.40   0.26   0.24
## E_2009   0.65   0.10   0.16   0.10   0.29   0.36   0.28
## E_2013   1.00   0.11   0.13   0.15   0.26   0.28   0.35
## N_2005   0.11   1.00   0.55   0.53   0.09   0.08   0.06
## N_2009   0.13   0.55   1.00   0.60   0.06   0.07   0.07
## N_2013   0.15   0.53   0.60   1.00   0.05   0.05   0.05
## O_2005   0.26   0.09   0.06   0.05   1.00   0.58   0.55
## O_2009   0.28   0.08   0.07   0.05   0.58   1.00   0.61
## O_2013   0.35   0.06   0.07   0.05   0.55   0.61   1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a lot of values and a little hard to make sense of, so as a bonus, I’m going to give you a little bit of more complex code that makes this more readable (and publishable ).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- b5_soep_long %&amp;gt;%
  unite(tmp, item, year, sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  spread(key = tmp, value = value) %&amp;gt;% 
  select(-Procedural__SID, -DOB, -Sex) %&amp;gt;%
  cor(., use = &amp;quot;pairwise&amp;quot;) 

r[upper.tri(r, diag = T)] &amp;lt;- NA
diag(r) &amp;lt;- (alphas %&amp;gt;% arrange(item, year))$std.alpha

r %&amp;gt;% data.frame %&amp;gt;%
  rownames_to_column(&amp;quot;V1&amp;quot;) %&amp;gt;%
  gather(key = V2, value = r, na.rm = T, -V1) %&amp;gt;%
  separate(V1, c(&amp;quot;T1&amp;quot;, &amp;quot;Year1&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  separate(V2, c(&amp;quot;T2&amp;quot;, &amp;quot;Year2&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  mutate_at(vars(Year1), ~factor(., levels = c(2013, 2009, 2005))) %&amp;gt;%
  ggplot(aes(x = Year2, y = Year1, fill = r)) +
    geom_raster() + 
  scale_fill_gradient2(low = &amp;quot;blue&amp;quot;, high = &amp;quot;red&amp;quot;, mid = &amp;quot;white&amp;quot;, 
   midpoint = 0, limit = c(-1,1), space = &amp;quot;Lab&amp;quot;, 
   name=&amp;quot;Correlations&amp;quot;) +  
  geom_text(aes(label = round(r,2))) +
    facet_grid(T1 ~ T2) +
    theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/Workshops/2019-08-14-workshop-1_files/figure-html/unnamed-chunk-5-1.png&#34; alt=&#34;Correlations among Personality Indicators. Values on the diagonal represent Chronbach&#39;s alpha for each scale in each year. Within-trait correlations represent test-retest correlations.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Correlations among Personality Indicators. Values on the diagonal represent Chronbach’s alpha for each scale in each year. Within-trait correlations represent test-retest correlations.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Terms</title>
      <link>/terms/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>/terms/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Syllabus</title>
      <link>/syllabus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/syllabus/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;Instructor&lt;/strong&gt;: Joshua Jackson&lt;br /&gt;
&lt;strong&gt;Office&lt;/strong&gt;: 315B&lt;br /&gt;
&lt;strong&gt;Office hours&lt;/strong&gt;: 1-2 Wednesday and by appointment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AI&lt;/strong&gt;: Emorie Beck
&lt;strong&gt;Office hours&lt;/strong&gt;: Wednesday 2-3&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Course Description&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This course covers modern methods of handling longitudinal, repeated measures. The class will introduce the rationale of measuring change and stability over time to study phenomena, as well as how within-person designs can increase statistical power and precision compared to more traditional designs. Most the course will use multi-level models and latent (growth) curve models to specify patterns of change across time. Additional topics include: visualization, measurement invariance, time-to- event models and power. PREREQ: Use of R will be required, Familiarity with MLM and/or Structural Equation Models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Class textbook&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Readings will be provided&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Structure of class&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Each class will cover a specific type of longitudinal model. During that class, I will provide an overview of the important considerations or motivation for this analysis. This will take the first half of the class or a little longer. The final hour of class will be devoted to walking through code and results. We’ll call this the workshop portion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Grading&lt;/strong&gt;
Grading consists of 3 aspects:
1. Weekly “pop” quiz (36% of grade - 12 @ 3% each). Quizes consist of 1-3 questions based on the previous lecture and reading.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Homework take homes (64% of grade - 8 @ 8% each). Homeworks will be presented in the workshop portion of the class and due 1 week later.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Schedule&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Week&lt;/th&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;th&gt;Topic&lt;/th&gt;
&lt;th&gt;Workshop&lt;/th&gt;
&lt;th&gt;Homework&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;8/29&lt;/td&gt;
&lt;td&gt;Motivation, terms, concepts, and graphing&lt;/td&gt;
&lt;td&gt;Data Descriptives&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;9/5&lt;/td&gt;
&lt;td&gt;Growth curve basics; MLM in R: packages and procedures&lt;/td&gt;
&lt;td&gt;tidyr/lme4&lt;/td&gt;
&lt;td&gt;Hw#1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;9/12&lt;/td&gt;
&lt;td&gt;Conditional (Level 1 and 2 predictors) MLM models&lt;/td&gt;
&lt;td&gt;purrr&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;9/19&lt;/td&gt;
&lt;td&gt;Polynomial, piecewise and spline models&lt;/td&gt;
&lt;td&gt;plotting redux&lt;/td&gt;
&lt;td&gt;Hw#2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;9/26&lt;/td&gt;
&lt;td&gt;Intensive data analysis/within person fluctuations p1&lt;/td&gt;
&lt;td&gt;brms&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;10/03&lt;/td&gt;
&lt;td&gt;Intensive data analysis/within person fluctuations p1&lt;/td&gt;
&lt;td&gt;brms&lt;/td&gt;
&lt;td&gt;Hw#3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;10/10&lt;/td&gt;
&lt;td&gt;SEM and simple path models&lt;/td&gt;
&lt;td&gt;tidybayes and bayesplot&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;10/17&lt;/td&gt;
&lt;td&gt;SEM intro cont.&lt;/td&gt;
&lt;td&gt;lavaan&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;10/24&lt;/td&gt;
&lt;td&gt;Latent Grown (curve) Models&lt;/td&gt;
&lt;td&gt;amelia&lt;/td&gt;
&lt;td&gt;Hw#4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;10/31&lt;/td&gt;
&lt;td&gt;2 measurement points&lt;/td&gt;
&lt;td&gt;semPlot&lt;/td&gt;
&lt;td&gt;Hw#5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;11/7&lt;/td&gt;
&lt;td&gt;Multiple group models&lt;/td&gt;
&lt;td&gt;lavaan&lt;/td&gt;
&lt;td&gt;Hw#6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;11/14&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Class canceled&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;11/21&lt;/td&gt;
&lt;td&gt;Flexible SEM models (LCM, STATE-TRAIT; ALT-SR)&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Hw#7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;11/28&lt;/td&gt;
&lt;td&gt;&lt;em&gt;Tofurkey day&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;12/5&lt;/td&gt;
&lt;td&gt;Mixture Models&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Hw#8&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Other topics: Longitudinal mediation (and multilevel mediation), two wave data, experimental approaches&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>workshop#5 &amp; 6</title>
      <link>/workshops/workshop-5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-5/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#how-is-this-different&#34;&gt;How is this different?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-basic-parts&#34;&gt;The basic parts&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#posterior-predictive-distribution&#34;&gt;Posterior predictive distribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-think-about-these-models&#34;&gt;How to think about these models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mcmc-estimation&#34;&gt;MCMC Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-lmer-with-brms&#34;&gt;Comparing lmer with brms&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-priors-did-we-use&#34;&gt;What priors did we use?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#differences-in-random-effects-with-brms&#34;&gt;Differences in random effects with brms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-models&#34;&gt;More models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ex-1&#34;&gt;Ex 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#intercepts-are-different-because-of-priors&#34;&gt;Intercepts are different because of priors&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lets-fit-it-again-without-that-intercept.&#34;&gt;Lets fit it again without that intercept.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-does-the-posterior-look-like&#34;&gt;What does the posterior look like?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects&#34;&gt;Random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex-2&#34;&gt;Ex 2&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#using-priors&#34;&gt;Using priors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calculate-icc.&#34;&gt;Calculate ICC.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-predictors&#34;&gt;Adding predictors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#marginal-effects&#34;&gt;Marginal effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#priors-choice&#34;&gt;Priors choice&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-models&#34;&gt;Comparing models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects-revisited&#34;&gt;Random effects revisited&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tidybayes&#34;&gt;tidybayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bayesplot&#34;&gt;Bayesplot&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#level-1-predictors&#34;&gt;Level 1 predictors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance-explained&#34;&gt;Variance explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hypothesis-function&#34;&gt;Hypothesis function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#posterior-predictive-checks&#34;&gt;Posterior-predictive checks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#missing-data&#34;&gt;Missing data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nonlinear&#34;&gt;Nonlinear&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thanks&#34;&gt;Thanks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;#Why Bayesian?
The models we have been working with can easily be done in a Bayesian Framework. Why Bayes? For at least 3 reasons: 1. Better convergence. 2. More flexibility. 3. Fewer assumptions. We will go through each of these ideas throughout the course in more detail, so what I want to do is sell you not on the benefits but on the lack of difference.&lt;/p&gt;
&lt;p&gt;Many places to read up on this. Try Kruschke’s Bayesian new statistics: &lt;a href=&#34;https://rdcu.be/bRUvW&#34; class=&#34;uri&#34;&gt;https://rdcu.be/bRUvW&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Simple blog post on why Bayesian should be default:
&lt;a href=&#34;http://babieslearninglanguage.blogspot.com/2018/02/mixed-effects-models-is-it-time-to-go.html&#34; class=&#34;uri&#34;&gt;http://babieslearninglanguage.blogspot.com/2018/02/mixed-effects-models-is-it-time-to-go.html&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;how-is-this-different&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How is this different?&lt;/h1&gt;
&lt;p&gt;Bayesian analysis differs in two major ways from our traditional MLMs that we have been working with. First, the end result is different in that it uses probability differently to derive estimates and to interpret the results. The lucky thing for us is that this will not be drastically different if we dont want it to be. In other words, we can keep with our focuses on estimates and precision around those estimates, same way as we would in standard stats world. Once you progress further, you can better understand the nuances, but right now lets not get bogged down by technical differences.&lt;/p&gt;
&lt;p&gt;The second major difference is that prior are used. Priors are a way to incorporate your beliefs into the model. At first blush it feels as if this is wrong, and is the justification for many for why the standard approach is correct and Bayesian is wrong. I mean, most people are taught frequentest approaches, thus how can 10 million SPSS users be wrong? (The reason for the popularity of frequentest approaches is two fold: computers and history. Computation power is needed, which was lacking until recently and thus curtailed general use, the same way that computation power held back adoption of SEM and before that multivariate approaches like factor analysis, and before that multiple regression with continuous predictors. The second reason, history, is, like much of history, driven by disagreements between dead white guys. Fisher, you’ve heard of him, populated the p-value and disliked Bayes, so long story short, it went out of fashion)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-basic-parts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The basic parts&lt;/h1&gt;
&lt;p&gt;Bayes Theorem
&lt;span class=&#34;math display&#34;&gt;\[ P(\theta | y) =  \frac{P(y | \theta) P(\theta)}{P(y)}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is often rewritten for when we want to estimate some value, $ $. Note that the &lt;span class=&#34;math inline&#34;&gt;\(P(y)\)&lt;/span&gt; drops out, as it does not vary across $ $. In our case y is will be the data, and the data will be collected only once, thus considered fixed. The probability o y was included in the above equation to re scale the equation and create some nice properties eg integrating to 1. We don’t care about that as we are going to look at relative likelihoods of each theta conditioned on y.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(\theta | y) \propto P(y | \theta) P(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This reads as the conditional probability of theta, given y is proportional to the probability of y given theta, multiplied by the probability of theta. The this, likely, sounds like gobbledygook unless you have taken a bunch of probability classes. Instead the more helpful way to think about this is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(hypothesis|data) \propto p(data|hypothesis)p(hypothesis)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is what we get: the probability our hypothesis. Not the standard p value interpretation of the probability of our data given some hypothesis like the null distribution.&lt;/p&gt;
&lt;p&gt;Bayesian stats gives names to each of these terms:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ posterior \propto likelihood * prior \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Prior - Allows us to provide a priori intuition about what the findings are. It is of the form of a probability distribution. E.g., “Extraordinary claims require extraordinary evidence.” Note we will note use this as “guesses” of what will happen. Instead we will use this as a regularization tool, similar to partial pooling in MLM.&lt;/p&gt;
&lt;p&gt;Likelihood - Distribution of the likelihood of various hypothesis. Probability attaches to possible results; likelihood attaches to hypotheses. It is akin to flipping coins, something we did with binomials.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ───────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1          ✔ purrr   0.3.2     
## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ readr   1.3.1          ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For 7 successes out of 10 trials, what is the likelihood of different probabilities?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(prob = seq(from = 0, to = 1, by = .01)) %&amp;gt;% 
  ggplot(aes(x = prob,
             y = dbinom(x = 7, size = 10, prob = prob))) +
  geom_line() +
  labs(x = &amp;quot;probability&amp;quot;,
       y = &amp;quot;binomial likelihood&amp;quot;) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(prob = seq(from = 0, to = 1, by = .01)) %&amp;gt;% 
  ggplot(aes(x = prob,
             y = dbinom(x = 4, size = 10, prob = prob))) +
  geom_line() +
  labs(x = &amp;quot;probability&amp;quot;,
       y = &amp;quot;binomial likelihood&amp;quot;) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It can be made similar to the maximum likelihood estimation (with flat priors), but think about this as nothing different than your standard estimation procedure.&lt;/p&gt;
&lt;p&gt;Posterior – distribution of our belief about the parameter values after taking into account the likelihood and one’s priors. In regression terms, it is not a specific value of b that would make the data most likely, but a probability distribution for b that serves as a weighted combination of the likelihood and prior.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sequence_length &amp;lt;- 1e3

d &amp;lt;-
  tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %&amp;gt;% 
  expand(probability, row = c(&amp;quot;flat&amp;quot;, &amp;quot;stepped&amp;quot;, &amp;quot;Laplace&amp;quot;)) %&amp;gt;% 
  arrange(row, probability) %&amp;gt;% 
  mutate(prior = ifelse(row == &amp;quot;flat&amp;quot;, 1,
                        ifelse(row == &amp;quot;stepped&amp;quot;, rep(0:1, each = sequence_length / 2),
                               exp(-abs(probability - .5) / .25) / ( 2 * .25))),
         likelihood = dbinom(x = 6, size = 9, prob = probability)) %&amp;gt;% 
  group_by(row) %&amp;gt;% 
  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %&amp;gt;% 
  gather(key, value, -probability, -row) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(key = factor(key, levels = c(&amp;quot;prior&amp;quot;, &amp;quot;likelihood&amp;quot;, &amp;quot;posterior&amp;quot;)),
         row = factor(row, levels = c(&amp;quot;flat&amp;quot;, &amp;quot;stepped&amp;quot;, &amp;quot;Laplace&amp;quot;)))
p1 &amp;lt;-
  d %&amp;gt;%
  filter(key == &amp;quot;prior&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = &amp;quot;prior&amp;quot;) +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = &amp;quot;free_y&amp;quot;, ncol = 1)

p2 &amp;lt;-
  d %&amp;gt;%
  filter(key == &amp;quot;likelihood&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = &amp;quot;likelihood&amp;quot;) +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = &amp;quot;free_y&amp;quot;, ncol = 1)

p3 &amp;lt;-
  d %&amp;gt;%
  filter(key == &amp;quot;posterior&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = &amp;quot;posterior&amp;quot;) +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = &amp;quot;free_y&amp;quot;, ncol = 1)

library(gridExtra)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gridExtra&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid.arrange(p1, p2, p3, ncol = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can describe our&lt;/p&gt;
&lt;p&gt;Another way to think about it:
updated belief = current evidence ∗ prior belief or evidence&lt;/p&gt;
&lt;div id=&#34;posterior-predictive-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior predictive distribution&lt;/h2&gt;
&lt;p&gt;Once we have the posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, we can then feed new or unobserved data into the data generating process and get new distributions for any potential observation. We can use this to make predictions, check if the model is correct, and to evaluate model fit.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-think-about-these-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to think about these models&lt;/h1&gt;
&lt;p&gt;In standard regression, we can state some expectations up front:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
h_i  \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i  = \alpha + \beta x_i \\
\alpha  \sim \text{Normal}(0, 10) \\
\beta  \sim \text{Normal}(0, 10) \\
\sigma  \sim \text{Half Cauchy}(0, 50)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_{i}  \sim \text{Normal}(\mu_i, \sigma) \]&lt;/span&gt;
This specifies the DGP and the family argument within &lt;code&gt;brms&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \mu_i  = \beta \times Predictor_i \]&lt;/span&gt;
This specifies the model we are fitting. Here it is just a standard regression.&lt;/p&gt;
&lt;p&gt;The above two lines are what is implicitly implied in almost all regressions. Namely, we have some variable that is being generated from a normal distribution, with a mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. That &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is going to be described as the relationship between some (fixed) regression coefficient and a person specific predictor variable.&lt;/p&gt;
&lt;p&gt;Below is where we get into some new stuff, where we describe what we think the estimated parameters will look like (ie format a prior).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha \sim \text{Normal}(0, 10) \\\)&lt;/span&gt;
This says we have a prior idea bout the distribution of the intercept. Namely that it is centered around 0 with a possible range above and below that.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta \sim \text{Normal}(0, 10)\)&lt;/span&gt;
This says that we have a prior idea about the possible distribution of the regression coefficient. We think it is most likely zero, but we aren’t that confident. As it could be higher or lower.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  aes(x = c(-40, 40)) +
  stat_function(fun = dnorm, n = 200, args = list(0, 10)) +
  labs(title = &amp;quot;Normal (Gaussian) distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now for the priors for our residual.
$(0, 1) $
Same idea for the residual, the prior specifies what we expect is plausible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  aes(x = c(0, 10)) +
  stat_function(fun = dcauchy, n = 200, args = list(0, 1)) +
  labs(title = &amp;quot;Half Cauchy distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that we do two important things here. First, we describe the data generating processes (DGP) we think our data are coming from. Often we will just assume Gaussian normal, but this does not need to be the case. For our residual we specify a half Cauchy to make sure it is above zero and that lower terms are more likely than higher terms.&lt;/p&gt;
&lt;p&gt;Second, we are specifying a prior. Here we are saying we expect our regression parameter to be centered around zero but that it could vary widely from that and we wouldn’t be surprised. The prior distribution for Beta is defined by a mean of 0 and an SD of 10. But we have control if we want to make this different. Same thing with the half Cauchy for the variance. We expect that it cannot be negative, that it is likely closer to zero than not. Again, it is up to us in terms of how we want to specify it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc-estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;MCMC Estimation&lt;/h1&gt;
&lt;p&gt;Bayesian estimation, like maximum likelihood, starts with initial guesses as starting points and then runs in an iterative fashion, producing simulated draws from the posterior distribution. Think of taking one of the posterior samples above and randomly selecting values from it, again and again. Lets take a sample the size of your study and call that sample 1. Then do that same procedure 4000 times. The result is that you have a lot of potential samples that can be combined in multiple ways. This part is key and different from frequentist statistics. We are aiming for creating a distribution of end goals as our end goal, not just a point estimate.&lt;/p&gt;
&lt;p&gt;The simulation process is referred to as Markov Chain Monte Carlo, or MCMC for short. In MCMC, all of the simulated draws from the posterior are based on and correlated with previous draws. We will typically allow the process to “warm up”, as the initial random starting point may be way off of being plausible. As it runs, however, these estimates will get better and better, creating a distribution of plausible values. As a safety check, we will run the the process multiple times, what is known as having multiple chains. If multiple chains converge towards the same answer we are more confident in our results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-lmer-with-brms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing lmer with brms&lt;/h1&gt;
&lt;p&gt;Lets do a mixed effects model to test this out. We will use the sleepstudy dataset that is loaded with lme4&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;Matrix&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;sleepstudy&amp;quot;)
head(sleepstudy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Reaction Days Subject
## 1 249.5600    0     308
## 2 258.7047    1     308
## 3 250.8006    2     308
## 4 321.4398    3     308
## 5 356.8519    4     308
## 6 414.6901    5     308&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(sleepstudy, file = &amp;quot;sleepstudy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sleep_lmer &amp;lt;- lmer(Reaction ~ Days + (1 + Days|Subject), data = sleepstudy)
summary(sleep_lmer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: Reaction ~ Days + (1 + Days | Subject)
##    Data: sleepstudy
## 
## REML criterion at convergence: 1743.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.9536 -0.4634  0.0231  0.4633  5.1793 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  Subject  (Intercept) 611.90   24.737       
##           Days         35.08    5.923   0.07
##  Residual             654.94   25.592       
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  251.405      6.824  36.843
## Days          10.467      1.546   6.771
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.138&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;xts&amp;#39;:
##   method     from
##   as.zoo.xts zoo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading &amp;#39;brms&amp;#39; package (version 2.10.0). Useful instructions
## can be found by typing help(&amp;#39;brms&amp;#39;). A more detailed introduction
## to the package is available through vignette(&amp;#39;brms_overview&amp;#39;).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;brms&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:lme4&amp;#39;:
## 
##     ngrps&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sleep_brm &amp;lt;- brm(Reaction ~ Days + (1 + Days|Subject), data = sleepstudy, file = &amp;quot;fit1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sleep_brm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;xts&amp;#39;:
##   method     from
##   as.zoo.xts zoo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Reaction ~ Days + (1 + Days | Subject) 
##    Data: sleepstudy (Number of observations: 180) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~Subject (Number of levels: 18) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)          27.20      6.94    16.21    43.45 1.00     1745
## sd(Days)                6.65      1.57     4.14    10.36 1.00     1437
## cor(Intercept,Days)     0.08      0.30    -0.48     0.68 1.00     1096
##                     Tail_ESS
## sd(Intercept)           2348
## sd(Days)                1878
## cor(Intercept,Days)     1800
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   251.16      7.62   235.92   266.63 1.00     1813     1960
## Days         10.40      1.68     7.19    13.65 1.00     1478     2246
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    25.86      1.57    23.04    29.16 1.00     3680     3030
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;what-priors-did-we-use&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What priors did we use?&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brms::get_prior(Reaction ~ Days + (1 + Days|Subject), data = sleepstudy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    prior     class      coef   group resp dpar nlpar bound
## 1                                b                                        
## 2                                b      Days                              
## 3                 lkj(1)       cor                                        
## 4                              cor           Subject                      
## 5  student_t(3, 289, 59) Intercept                                        
## 6    student_t(3, 0, 59)        sd                                        
## 7                               sd           Subject                      
## 8                               sd      Days Subject                      
## 9                               sd Intercept Subject                      
## 10   student_t(3, 0, 59)     sigma&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;*Note that there are flat priors as the default for fixed effects. This essentially disregards the prior and spits back the likelihood, equivalent to the ML estimate.&lt;/p&gt;
&lt;p&gt;The ts have three parameters: degrees of freedom, mean, and then an SD.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;differences-in-random-effects-with-brms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Differences in random effects with brms&lt;/h2&gt;
&lt;p&gt;We often summarize the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{0j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{1j}\)&lt;/span&gt; deviations as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2\)&lt;/span&gt;, respectively. And importantly, these variance parameters have a covariance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{01}\)&lt;/span&gt;. However, &lt;strong&gt;brms&lt;/strong&gt; parameterizes these in the standard-deviation metric. That is, in &lt;strong&gt;brms&lt;/strong&gt;, these are expressed as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;. Similarly, the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{01}\)&lt;/span&gt; presented in &lt;strong&gt;brms&lt;/strong&gt; output is in a correlation metric, rather than a covariance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\begin{bmatrix} U_{0i} \\ U_{1i} \end{bmatrix} &amp;amp; 
\sim \text{N} 
\bigg ( \begin{bmatrix} 0 \\ 0 \end{bmatrix},  
\begin{bmatrix} \sigma_0^2 &amp;amp; \sigma_{01}\\ \sigma_{01} &amp;amp; \sigma_1^2 \end{bmatrix}
\bigg )
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note, that in many Bayesian texts, the Level 2 covariance matrix above is often rewritten as below.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
U \sim \text{N} (\mathbf{0}, \mathbf{\Sigma})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{0}\)&lt;/span&gt; is the vector of 0 means and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\Sigma}\)&lt;/span&gt; is the variance/covariance matrix. In &lt;strong&gt;Stan&lt;/strong&gt;, and thus &lt;strong&gt;brms&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\Sigma}\)&lt;/span&gt; is decomposed&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\mathbf{\Sigma} &amp;amp; = \mathbf{D} \mathbf{\Omega} \mathbf{D}, \text{where} \\
\mathbf{D}      &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \text{and} \\
\mathbf{\Omega} &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix}
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\)&lt;/span&gt; is the diagonal matrix of standard deviations and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\Omega}\)&lt;/span&gt; is the correlation matrix. As we will see later, we will need to specify priors for each of these components, the SDs of our random effects and the correlation between them. By splitting them into these two different matrices we can handle that easier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(sleep_brm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-12-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;more-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;More models&lt;/h1&gt;
&lt;div id=&#34;ex-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ex 1&lt;/h2&gt;
&lt;p&gt;Here we’ll use a dataset from Singer &amp;amp; Willet, chapter 3, looking at cognitive performance across time for children who under went an intervention or not.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       zeta_0     zeta_1
## 1 10.7586672 -3.0908765
## 2  3.4258938 -0.4186497
## 3 -3.0770183  0.2140130
## 4 12.5303603 -4.9043416
## 5 -2.1114641  0.8936950
## 6 -0.5521597 -0.6310265&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##      id gamma_00 gamma_01 gamma_10 gamma_11
##   &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1     1     108.     6.85    -21.1     5.27
## 2     2     108.     6.85    -21.1     5.27
## 3     3     108.     6.85    -21.1     5.27
## 4     4     108.     6.85    -21.1     5.27
## 5     5     108.     6.85    -21.1     5.27
## 6     6     108.     6.85    -21.1     5.27&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 13
##      id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c
##   &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0  
## 2     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0.5
## 3     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   1  
## 4     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0  
## 5     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0.5
## 6     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   1  
## # … with 4 more variables: epsilon &amp;lt;dbl&amp;gt;, pi_0 &amp;lt;dbl&amp;gt;, pi_1 &amp;lt;dbl&amp;gt;,
## #   cog &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 14
##      id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c
##   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0  
## 2     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0.5
## 3     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   1  
## 4     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0  
## 5     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0.5
## 6     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   1  
## # … with 5 more variables: epsilon &amp;lt;dbl&amp;gt;, pi_0 &amp;lt;dbl&amp;gt;, pi_1 &amp;lt;dbl&amp;gt;,
## #   cog &amp;lt;dbl&amp;gt;, age &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;id&amp;quot;, &amp;quot;age&amp;quot;, &amp;quot;cog&amp;quot;, &amp;quot;program&amp;quot;, &amp;quot;age_c&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 309
## Variables: 5
## $ id      &amp;lt;dbl&amp;gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,…
## $ age     &amp;lt;dbl&amp;gt; 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2…
## $ cog     &amp;lt;dbl&amp;gt; 117, 113, 109, 108, 112, 102, 112, 113, 85, 138, 110, 97…
## $ program &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ age_c   &amp;lt;dbl&amp;gt; 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;early_int_sim.rda&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;early_int_sim &amp;lt;-
  early_int_sim %&amp;gt;% 
  mutate(label = str_c(&amp;quot;program = &amp;quot;, program)) 

early_int_sim %&amp;gt;% 
  ggplot(aes(x = age, y = cog, color = label)) +
  stat_smooth(aes(group = id),
              method = &amp;quot;lm&amp;quot;, se = F, size = 1/6) +
  stat_smooth(method = &amp;quot;lm&amp;quot;, se = F, size = 2) +
  scale_x_continuous(breaks = c(1, 1.5, 2)) +
  scale_color_viridis_d(option = &amp;quot;B&amp;quot;, begin = .33, end = .67) +
  ylim(50, 150) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;) +
  facet_wrap(~label)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is our assumed data generating model, with some relatively random priors thrown in.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
cog  \sim \text{Normal} (\mu_{ij}, \sigma_{ij}) \\
\mu_{ij}    = \gamma_{00j} + \gamma_{10j} ({age}_{ij} - 1) + \gamma_{11j} Program \\
\gamma_{00}      \sim \text{Normal}(0, 20) \\
\gamma_{10}      \sim \text{Normal}(0, 5) \\
\gamma_{11}      \sim \text{Normal}(0, 5) \\
\sigma_{ij}  \sim \text{Student-t} (3, 0, 1)\\ 
\sigma_0         \sim \text{Student-t} (3, 0, 1) \\
\sigma_1         \sim \text{Student-t} (3, 0, 1) \\
\rho_{01}        \sim \text{LKJ} (4) \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex1 &amp;lt;-
  brm(data = early_int_sim,
      file = &amp;quot;fit2&amp;quot;,
      family = gaussian,
      formula = cog ~ 0 + intercept + age_c + program + age_c:program + (1 + age_c | id),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.9),
      seed = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;intercepts-are-different-because-of-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intercepts are different because of priors&lt;/h2&gt;
&lt;p&gt;Another important part of the syntax concerns the intercept. Normally we include a 1 (or lmer does it for us automatically) to reflect we want to fit an intercept. If we did that here, we would have made the assumption that our predictors are mean centered. The default priors set by &lt;code&gt;brms::brm()&lt;/code&gt; are set based on this assumption. SO, if we want to use non mean centered predictors (eg setting time at initial wave or dummy variables), we need to respecify our model. Neither our variables are mean centered.&lt;/p&gt;
&lt;p&gt;With a &lt;code&gt;0 + intercept&lt;/code&gt;, we told &lt;code&gt;brm()&lt;/code&gt; to suppress the default intercept and replace it with our smartly-named &lt;code&gt;intercept&lt;/code&gt; parameter. This is our fixed effect for the population intercept and, importantly, &lt;code&gt;brms()&lt;/code&gt; will assign default priors to it based on the data themselves without assumptions about centering. We will speak later about changing these default priors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(ex1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: There were 29 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help.
## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: cog ~ 0 + intercept + age_c + program + age_c:program + (1 + age_c | id) 
##    Data: early_int_sim (Number of observations: 309) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 103) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)            9.67      1.16     7.60    11.97 1.00     1043
## sd(age_c)                3.85      2.39     0.20     9.00 1.01      386
## cor(Intercept,age_c)    -0.48      0.36    -0.96     0.53 1.00     3234
##                      Tail_ESS
## sd(Intercept)            2084
## sd(age_c)                 487
## cor(Intercept,age_c)     1903
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## intercept       106.53      1.95   102.45   110.26 1.00     1176     1359
## age_c           -20.52      1.98   -24.42   -16.64 1.00     2217     2872
## program           9.21      2.54     4.14    14.09 1.00     1244     1938
## age_c:program     3.20      2.66    -2.03     8.38 1.00     1767     1347
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     8.59      0.52     7.54     9.59 1.01      729      606
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  brms::get_prior(cog ~ 0 + intercept + age_c + program + age_c:program + (1 + age_c | id), data = early_int_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  prior class          coef group resp dpar nlpar bound
## 1                          b                                          
## 2                          b         age_c                            
## 3                          b age_c:program                            
## 4                          b     intercept                            
## 5                          b       program                            
## 6               lkj(1)   cor                                          
## 7                        cor                  id                      
## 8  student_t(3, 0, 16)    sd                                          
## 9                         sd                  id                      
## 10                        sd         age_c    id                      
## 11                        sd     Intercept    id                      
## 12 student_t(3, 0, 16) sigma&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;lets-fit-it-again-without-that-intercept.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lets fit it again without that intercept.&lt;/h3&gt;
&lt;p&gt;What is the difference?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex1.nc &amp;lt;-
  brm(data = early_int_sim,
      file = &amp;quot;fit2.nc&amp;quot;,
      family = gaussian,
      formula = cog ~ 1 + age.c + program + age.c:program + (1 + age | id),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.9),
      seed = 3)
print(ex1.nc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  brms::get_prior(cog ~  1 + age_c + program + age_c:program + (1 + age_c | id), data = early_int_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    prior     class          coef group resp dpar nlpar
## 1                                b                                    
## 2                                b         age_c                      
## 3                                b age_c:program                      
## 4                                b       program                      
## 5                 lkj(1)       cor                                    
## 6                              cor                  id                
## 7  student_t(3, 102, 16) Intercept                                    
## 8    student_t(3, 0, 16)        sd                                    
## 9                               sd                  id                
## 10                              sd         age_c    id                
## 11                              sd     Intercept    id                
## 12   student_t(3, 0, 16)     sigma                                    
##    bound
## 1       
## 2       
## 3       
## 4       
## 5       
## 6       
## 7       
## 8       
## 9       
## 10      
## 11      
## 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Is 102 the right prior we should have?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;psych&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     %+%, alpha&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;describe(early_int_sim$cog)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    vars   n   mean    sd median trimmed   mad min max range skew kurtosis
## X1    1 309 102.35 15.42    102  102.27 16.31  64 155    91 0.12    -0.16
##      se
## X1 0.88&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A few take aways: 1. not much has changed. Why? Because our data overwhelm the prior. Again, priors are not &lt;em&gt;that&lt;/em&gt; big of a deal.
2. We can forget about all of this if we do not use default priors. More on that later.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-the-posterior-look-like&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What does the posterior look like?&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(ex1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-22-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(ex1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 Estimate Est.Error       Q2.5      Q97.5
## intercept     106.529202  1.946755 102.448293 110.258352
## age_c         -20.516543  1.982355 -24.422934 -16.641683
## program         9.213920  2.536958   4.140112  14.087900
## age_c:program   3.201103  2.655993  -2.030914   8.378437&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading &amp;#39;brms&amp;#39; package (version 2.10.0). Useful instructions
## can be found by typing help(&amp;#39;brms&amp;#39;). A more detailed introduction
## to the package is available through vignette(&amp;#39;brms_overview&amp;#39;).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;brms&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:psych&amp;#39;:
## 
##     cs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:lme4&amp;#39;:
## 
##     ngrps&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- brms::posterior_samples(ex1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a look at the first 10 columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; post[, 1:10] %&amp;gt;%
glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 4,000
## Variables: 10
## $ b_intercept              &amp;lt;dbl&amp;gt; 107.8432, 105.9106, 106.5840, 105.9220,…
## $ b_age_c                  &amp;lt;dbl&amp;gt; -22.15160, -18.79935, -20.28713, -21.92…
## $ b_program                &amp;lt;dbl&amp;gt; 8.058549, 7.426575, 7.172656, 7.401558,…
## $ `b_age_c:program`        &amp;lt;dbl&amp;gt; 6.71653288, 1.54733881, 2.46308672, 6.0…
## $ sd_id__Intercept         &amp;lt;dbl&amp;gt; 7.934120, 9.182119, 9.680560, 9.891759,…
## $ sd_id__age_c             &amp;lt;dbl&amp;gt; 0.07493328, 3.29208929, 0.46960055, 1.4…
## $ cor_id__Intercept__age_c &amp;lt;dbl&amp;gt; 0.11099642, -0.85142856, -0.62951652, -…
## $ sigma                    &amp;lt;dbl&amp;gt; 9.009290, 8.263876, 8.559613, 8.511837,…
## $ `r_id[1,Intercept]`      &amp;lt;dbl&amp;gt; 3.9047820, 6.2320897, 4.9022182, 8.3357…
## $ `r_id[2,Intercept]`      &amp;lt;dbl&amp;gt; -2.6429448, -3.2966948, 5.0002671, 2.64…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We saved our results as &lt;code&gt;post&lt;/code&gt;, which is a data frame with 4000 rows (i.e., 1000 post-warmup iterations times 4 chains) and 215 columns, each depicting one of the model parameters. With &lt;strong&gt;brms&lt;/strong&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; parameters (i.e., the fixed effects or population parameters) get &lt;code&gt;b_&lt;/code&gt; prefixes in the &lt;code&gt;posterior_samples()&lt;/code&gt; output. So we can isolate them like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
select(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_intercept   b_age_c b_program b_age_c:program
## 1    107.8432 -22.15160  8.058549        6.716533
## 2    105.9106 -18.79935  7.426575        1.547339
## 3    106.5840 -20.28713  7.172656        2.463087
## 4    105.9220 -21.92054  7.401558        6.062974
## 5    104.3962 -18.09924  9.998274        1.475840
## 6    108.5692 -22.13250  2.936269        7.693650&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;%
  select(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%

  ggplot(aes(x = value)) +
  geom_density(color = &amp;quot;transparent&amp;quot;, fill = &amp;quot;grey&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Notice how this is exactly the same as using the &lt;code&gt;plot&lt;/code&gt; function. But you have more control now.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random effects&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(ex1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $id
## $id$sd
##           Estimate Est.Error      Q2.5     Q97.5
## Intercept 9.667999  1.155713 7.6023029 11.965095
## age_c     3.845584  2.394552 0.1988863  8.997617
## 
## $id$cor
## , , Intercept
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept  1.0000000 0.0000000  1.0000000 1.0000000
## age_c     -0.4809751 0.3646166 -0.9641151 0.5332108
## 
## , , age_c
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept -0.4809751 0.3646166 -0.9641151 0.5332108
## age_c      1.0000000 0.0000000  1.0000000 1.0000000
## 
## 
## $id$cov
## , , Intercept
## 
##            Estimate Est.Error      Q2.5     Q97.5
## Intercept  94.80554  22.79074  57.79501 143.16349
## age_c     -21.32497  18.96069 -65.51948   4.40548
## 
## , , age_c
## 
##            Estimate Est.Error        Q2.5    Q97.5
## Intercept -21.32497  18.96069 -65.5194793  4.40548
## age_c      20.52097  22.16587   0.0395562 80.95712
## 
## 
## 
## $residual__
## $residual__$sd
##  Estimate Est.Error     Q2.5    Q97.5
##  8.585042 0.5183284 7.535189 9.592982&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case that output is confusing, &lt;code&gt;VarCorr()&lt;/code&gt; returned a 2-element list of lists.&lt;/p&gt;
&lt;p&gt;If you just want the &lt;span class=&#34;math inline&#34;&gt;\(U_j\)&lt;/span&gt;s, subset the first list of the first list. Note this is included in the standard summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(ex1)[[1]][[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Estimate Est.Error      Q2.5     Q97.5
## Intercept 9.667999  1.155713 7.6023029 11.965095
## age_c     3.845584  2.394552 0.1988863  8.997617&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how to get their correlation matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(ex1)[[1]][[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , Intercept
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept  1.0000000 0.0000000  1.0000000 1.0000000
## age_c     -0.4809751 0.3646166 -0.9641151 0.5332108
## 
## , , age_c
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept -0.4809751 0.3646166 -0.9641151 0.5332108
## age_c      1.0000000 0.0000000  1.0000000 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;variance/covariance matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(ex1)[[1]][[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , Intercept
## 
##            Estimate Est.Error      Q2.5     Q97.5
## Intercept  94.80554  22.79074  57.79501 143.16349
## age_c     -21.32497  18.96069 -65.51948   4.40548
## 
## , , age_c
## 
##            Estimate Est.Error        Q2.5    Q97.5
## Intercept -21.32497  18.96069 -65.5194793  4.40548
## age_c      20.52097  22.16587   0.0395562 80.95712&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(ex1) %&amp;gt;%
  transmute(`sigma[0]^2`  = sd_id__Intercept^2,
            `sigma[1]^2`  = sd_id__age_c^2,
            `sigma[0][1]` = sd_id__Intercept * cor_id__Intercept__age_c * sd_id__age_c,
            `sigma[epsilon]^2` = sigma^2) %&amp;gt;%
  gather(key, posterior) %&amp;gt;%

  ggplot(aes(x = posterior)) +
  geom_density(color = &amp;quot;transparent&amp;quot;, fill = &amp;quot;grey&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank(),
        strip.text = element_text(size = 12)) +
  facet_wrap(~key, scales = &amp;quot;free&amp;quot;, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ex-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ex 2&lt;/h2&gt;
&lt;p&gt;Load the data, here from chapter 4 of Singer and Willet. It is a three wave longitudinal study of adolescents. We are looking at alcohol use during the previous year, measured from 0 - 7. COA is a variable indicating the child’s parents are alcoholics.&lt;/p&gt;
&lt;p&gt;Data generating model we are fitting. You can also write this with L1 and L2 convention.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
alcuse  \sim \text{Normal} (\mu_{ij}, \sigma_{ij}) \\
\mu_{ij}  =  \gamma_{00} +  U_{0j} + \epsilon_{ij} \\
\gamma_{00} \sim \text{Student t} (1, 10) \\
U_{0i}  \sim \text{Student t} (0, 10) \\
\epsilon_{ij}  \sim \text{Student t} (0, 10) \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What are the default priors?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  get_prior(data = alcohol1_pp, 
           family = gaussian,
           alcuse ~ 1 + (1 | id))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 prior     class      coef group resp dpar nlpar bound
## 1 student_t(3, 1, 10) Intercept                                      
## 2 student_t(3, 0, 10)        sd                                      
## 3                            sd              id                      
## 4                            sd Intercept    id                      
## 5 student_t(3, 0, 10)     sigma&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;using-priors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using priors&lt;/h3&gt;
&lt;p&gt;How can we put that in directly to our code?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2 &amp;lt;-
  brm(data = alcohol1_pp,
      file = &amp;quot;fit3&amp;quot;,
      family = gaussian,
      alcuse ~ 1 + (1 | id),
      prior = c(prior(student_t(3, 1, 10), class = Intercept),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualizing priors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(metRology)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;metRology&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     cbind, rbind&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(x = seq(from = -100, to = 100, length.out = 1e3)) %&amp;gt;%
  mutate(density = metRology::dt.scaled(x, df = 3, mean = 1, sd = 10)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = density)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_line() +
  labs(title = expression(paste(&amp;quot;prior for &amp;quot;, gamma[0][0])),
       x = &amp;quot;parameter space&amp;quot;) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note a few things: First, this is a broad space for our intercept based on what we are looking at. This would be considering minimally informative.&lt;/p&gt;
&lt;p&gt;Second, consider the variance priors – they go below zero. Does this make sense?&lt;/p&gt;
&lt;p&gt;Here are the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: alcuse ~ 1 + (1 | id) 
##    Data: alcohol1_pp (Number of observations: 246) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 82) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.77      0.08     0.62     0.94 1.00     1662     2274
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.92      0.10     0.72     1.12 1.00     2602     2734
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.76      0.04     0.68     0.84 1.00     3219     3327
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post2 &amp;lt;- posterior_samples(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since all we’re interested in are the variance components, we’ll &lt;code&gt;select()&lt;/code&gt; out the relevant columns from &lt;code&gt;post2&lt;/code&gt;, and save the results in a mini data frame, &lt;code&gt;v&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;v &amp;lt;-
  post2 %&amp;gt;%
  select(sigma, sd_id__Intercept)

head(v)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       sigma sd_id__Intercept
## 1 0.7657679        0.9317019
## 2 0.7630844        0.9259266
## 3 0.8073889        0.7670859
## 4 0.6964495        0.6670863
## 5 0.7029354        0.7075109
## 6 0.7410712        0.7324726&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(v)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4000    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note these are in SD units&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;v %&amp;gt;%
  gather() %&amp;gt;%
  ggplot(aes(x = value)) +
  geom_vline(xintercept = c(.25, .5, .75, 1), color = &amp;quot;white&amp;quot;) +
  geom_density(size = 0, fill = &amp;quot;grey&amp;quot;) +
  scale_x_continuous(NULL, limits = c(0, 1.25),
                     breaks = seq(from = 0, to = 1.25, by = .25)) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-icc.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculate ICC.&lt;/h3&gt;
&lt;p&gt;Note that the formula uses variances. ‘brms’ gives us SDs
&lt;span class=&#34;math display&#34;&gt;\[
ICC = \frac{\sigma_0^2}{\sigma_0^2 + \sigma_\epsilon^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;v %&amp;gt;%
  transmute(ICC = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&amp;gt;%
  ggplot(aes(x = ICC)) +
  geom_density(size = 0, fill = &amp;quot;grey&amp;quot;) +
  scale_x_continuous( limits = 0:1) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note we get a distribution of ICCs, not just a singular score! Not all of our samples show as strong of between person association. Note that measuring this dispersion is a feature, not a problem. With standard MLM we are not taking into account potential sampling variance that may influence our estimates. Bayesian does.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-predictors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding predictors&lt;/h3&gt;
&lt;p&gt;Using the composite formula, our next model, the unconditional growth model, follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{alcuse}_{ij} &amp;amp; = \gamma_{00} + \gamma_{10} \text{age_14}_{ij} + U_{0j} + U_{1j} \text{age_14}_{ij} + e_{ij} \\
\epsilon_{ij} &amp;amp; \sim \text{Normal} (0, \sigma_\epsilon^2) \\
\begin{bmatrix} U_{0j} \\ U_{1j} \end{bmatrix} &amp;amp; \sim \text{MVN} 
\Bigg ( 
\begin{bmatrix} 0 \\ 0 \end{bmatrix}, 
\begin{bmatrix} \sigma_0^2 &amp;amp; \sigma_{01} \\ \sigma_{01} &amp;amp; \sigma_1^2 \end{bmatrix}
\Bigg )
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_prior(data = alcohol1_pp,
          family = gaussian,
          alcuse ~ 0 + intercept + age_14 + (1 + age_14 | id))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  prior class      coef group resp dpar nlpar bound
## 1                          b                                      
## 2                          b    age_14                            
## 3                          b intercept                            
## 4               lkj(1)   cor                                      
## 5                        cor              id                      
## 6  student_t(3, 0, 10)    sd                                      
## 7                         sd              id                      
## 8                         sd    age_14    id                      
## 9                         sd Intercept    id                      
## 10 student_t(3, 0, 10) sigma&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;*Note that there are flat priors as the default for fixed effects. This essentially disregards the prior and spits back the likelihood, equivalent to the ML estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2.fit2 &amp;lt;-
  brm(data = alcohol1_pp, 
      file = &amp;quot;fit4&amp;quot;,
      family = gaussian,
      alcuse ~ 0 + intercept + age_14 + (1 + age_14 | id),
      prior = c(prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(1), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(ex2.fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-44-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Marginal effects&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(ex2.fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2.fit3 &amp;lt;-
  brm(data = alcohol1_pp, 
      file = &amp;quot;fit5&amp;quot;,
      family = gaussian,
      alcuse ~ 0 + intercept + age_14 + coa + age_14:coa + (1 + age_14 | id),
      prior = c(prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(1), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(ex2.fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: alcuse ~ 0 + intercept + age_14 + coa + age_14:coa + (1 + age_14 | id) 
##    Data: alcohol1_pp (Number of observations: 246) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 82) 
##                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)             0.70      0.10     0.50     0.90 1.00      763
## sd(age_14)                0.37      0.09     0.14     0.53 1.01      339
## cor(Intercept,age_14)    -0.10      0.28    -0.51     0.66 1.01      486
##                       Tail_ESS
## sd(Intercept)             1455
## sd(age_14)                 278
## cor(Intercept,age_14)      342
## 
## Population-Level Effects: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## intercept      0.32      0.13     0.06     0.57 1.00     2043     2563
## age_14         0.29      0.09     0.13     0.46 1.00     2902     2796
## coa            0.74      0.20     0.34     1.14 1.00     1992     2295
## age_14:coa    -0.05      0.13    -0.30     0.20 1.00     2931     3067
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.61      0.05     0.52     0.72 1.01      416      672
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(ex2.fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-47-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-47-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(ex2.fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/marg.ex2.fit3-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/marg.ex2.fit3-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/marg.ex2.fit3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3_f &amp;lt;-
  update(ex2.fit3,
         newdata = alcohol1_pp %&amp;gt;% mutate(coa = factor(coa)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(fit3_f)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-48-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-48-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-48-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(fit3_f,
                 effects = &amp;quot;coa&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-49-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(fit3_f,
                 effects = &amp;quot;coa:age_14&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-50-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(fit3_f,
                 effects = &amp;quot;age_14:coa&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-51-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use fitted function to create “predicted” values, much like we did with lmer&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nd &amp;lt;- 
  tibble(age_14 = seq(from = 0, to = 2, length.out = 30))

f &amp;lt;- 
  fitted(ex2.fit2, 
         newdata = nd,
         re_formula = NA) %&amp;gt;%
  data.frame() %&amp;gt;%
  bind_cols(nd) %&amp;gt;% 
  mutate(age = age_14 + 14)

head(f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate Est.Error      Q2.5     Q97.5     age_14      age
## 1 0.6464935 0.1110443 0.4285175 0.8709599 0.00000000 14.00000
## 2 0.6652664 0.1091008 0.4513743 0.8852507 0.06896552 14.06897
## 3 0.6840392 0.1073183 0.4766966 0.8986808 0.13793103 14.13793
## 4 0.7028121 0.1057051 0.4992086 0.9142175 0.20689655 14.20690
## 5 0.7215850 0.1042690 0.5198633 0.9306017 0.27586207 14.27586
## 6 0.7403579 0.1030174 0.5407352 0.9446840 0.34482759 14.34483&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f %&amp;gt;%
  ggplot(aes(x = age)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),
              fill = &amp;quot;grey75&amp;quot;, alpha = 3/4) +
  geom_line(aes(y = Estimate)) +
  scale_y_continuous(&amp;quot;alcuse&amp;quot;, breaks = 0:2, limits = c(0, 2)) +
  coord_cartesian(xlim = 13:17) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-53-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;priors-choice&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Priors choice&lt;/h3&gt;
&lt;p&gt;Here we can change the prior to our regression coefficients&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2.fit3.prior &amp;lt;-
  brm(data = alcohol1_pp, 
      file = &amp;quot;fit6&amp;quot;,
      family = gaussian,
      alcuse ~ 0 + intercept + age_14 + coa + age_14:coa + (1 + age_14 | id),
      prior = c(prior(student_t(3, 0, 10), class = b),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(1), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(ex2.fit3.prior)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-55-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-55-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;compare that to the default&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(ex2.fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-56-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-56-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;can we make it break?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2.fit3.prior2 &amp;lt;-
  brm(data = alcohol1_pp, 
      file = &amp;quot;fit7&amp;quot;,
      family = gaussian,
      alcuse ~ 0 + intercept + age_14 + coa + age_14:coa + (1 + age_14 | id),
      prior = c(prior(student_t(3, 5, 2), class = b),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(1), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(ex2.fit3.prior2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-58-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-58-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So what priors should you use?&lt;/p&gt;
&lt;p&gt;I will suggest the use of regularizing priors. Regularizing priors are centered around no effect, with a relatively large band around it. This will in effect pull your estimates closer to zero, especially if you have limited data. The result will be better out of sample model validity.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing models&lt;/h3&gt;
&lt;p&gt;As it turns out, we Bayesian use the log-likelihood (LL), too. Recall how the numerator in the right-hand side of Bayes’ Theorem was &lt;span class=&#34;math inline&#34;&gt;\(p(\text{data} | \theta) p(\theta)\)&lt;/span&gt;? That first part, &lt;span class=&#34;math inline&#34;&gt;\(p(\text{data} | \theta)\)&lt;/span&gt;, is the likelihood. In words, the likelihood is the &lt;em&gt;probability of the data given the parameters&lt;/em&gt;. And we take the log of the likelihood rather than the likelihood itself because it’s easier to work with statistically.&lt;/p&gt;
&lt;p&gt;When you’re working with &lt;strong&gt;brms&lt;/strong&gt;, you can extract the LL with the &lt;code&gt;log_lik()&lt;/code&gt; function. Here’s an example with &lt;code&gt;fit1&lt;/code&gt;, our unconditional means model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_lik(ex2) %&amp;gt;%
  str()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  num [1:4000, 1:246] -0.654 -0.788 -0.795 -0.65 -0.587 ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   ..$ : NULL
##   ..$ : NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may have noticed we didn’t just get a single value back. Rather, we got an array of 4000 rows and 246 columns. The reason we got 4000 rows is because that’s how many post-warmup iterations we drew from the posterior. I.e., we set &lt;code&gt;brm(..., iter = 2000, warmup = 1000, chains = 4)&lt;/code&gt;. With respect to the 246 columns, that’s how many rows there are in the &lt;code&gt;alcohol1_pp&lt;/code&gt; data. So for each person in the data set, we get an entire posterior distribution of LL values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll &amp;lt;-
  log_lik(ex2) %&amp;gt;%
  data.frame() %&amp;gt;%
  mutate(sums     = rowSums(.)) %&amp;gt;%
  mutate(deviance = -2 * sums) %&amp;gt;%
  select(sums, deviance, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll %&amp;gt;%
  ggplot(aes(x = deviance)) +
  geom_density(fill = &amp;quot;grey25&amp;quot;, size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-61-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The AIC is frequentist and cannot handle models with priors. The BIC isis a misnomer as it is not Bayesian. The Widely Applicable Information Criterion (WAIC) is used instead.&lt;/p&gt;
&lt;p&gt;The distinguishing feature of WAIC is that it is &lt;em&gt;point wise&lt;/em&gt;. This means that uncertainty in prediction is considered case-by-case, or point-by-point, in the data. This is useful, because some observations are much harder to predict than others and may also have different uncertainty. You can think of WAIC as handling uncertainty where it actually matters: for each independent observation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;waic(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 4000 by 246 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -312.2 12.0
## p_waic        54.8  4.7
## waic         624.5 24.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: 42 (17.1%) p_waic estimates greater than 0.4. We recommend trying
## loo instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the statistic in each row, you get a point estimate and a standard error. The WAIC is on the bottom. The effective number of parameters, the &lt;span class=&#34;math inline&#34;&gt;\(p_\text{WAIC}\)&lt;/span&gt;, is in the middle. Notice the &lt;code&gt;elpd_waic&lt;/code&gt; on the top. That’s what you get without the &lt;span class=&#34;math inline&#34;&gt;\(-2 \times ...\)&lt;/span&gt; in the formula. Remember how that part is just to put things in a metric amenable to &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; difference testing? Well, not all Bayesians like that and within the &lt;strong&gt;Stan&lt;/strong&gt; ecosystem you’ll also see the WAIC expressed instead as the &lt;span class=&#34;math inline&#34;&gt;\(\text{elpd}_\text{WAIC}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The current recommended workflow within &lt;strong&gt;brms&lt;/strong&gt; is to attach the WAIC information to the model fit. You do it with the &lt;code&gt;add_criterion()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2 &amp;lt;- add_criterion(ex2, &amp;quot;waic&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2$waic&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 4000 by 246 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -312.2 12.0
## p_waic        54.8  4.7
## waic         624.5 24.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: 42 (17.1%) p_waic estimates greater than 0.4. We recommend trying
## loo instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Leave-one-out cross-validation (LOO-CV).&lt;/p&gt;
&lt;p&gt;Cross validation is quickly becoming the primary method to examine fit and utility of one’s model. The hope is our findings would generalize to other data we could have collected or may collect in the future. We’d like our findings to tell us something more general about the world at large. But we don’t have all the data and we typically don’t even know what all the relevant variables are. That is where validation comes in.&lt;/p&gt;
&lt;p&gt;k-fold is a common type of CV. As &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; increases, the number of cases with a fold get smaller. In the extreme, &lt;span class=&#34;math inline&#34;&gt;\(k = N\)&lt;/span&gt;, the number of cases within the data. At that point, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-fold cross-validation turns into leave-one-out cross-validation (LOO-CV).&lt;/p&gt;
&lt;p&gt;But there’s a practical difficulty with LOO-CV: it’s costly. As you may have noticed, it takes some time to fit a Bayesian multilevel model. For large data and/or complicated models, sometimes it takes hours or days. Most of us just don’t have enough time or computational resources to fit that many models. Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO) as an efficient way to approximate true LOO-CV.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l_fit1 &amp;lt;- loo(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Found 3 observations with a pareto_k &amp;gt; 0.7 in model &amp;#39;ex2&amp;#39;. It is
## recommended to set &amp;#39;reloo = TRUE&amp;#39; in order to calculate the ELPD without
## the assumption that these observations are negligible. This will refit
## the model 3 times to compute the ELPDs for the problematic observations
## directly.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(l_fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 4000 by 246 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -315.5 12.3
## p_loo        58.0  4.9
## looic       630.9 24.6
## ------
## Monte Carlo SE of elpd_loo is NA.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     221   89.8%   715       
##  (0.5, 0.7]   (ok)        22    8.9%   395       
##    (0.7, 1]   (bad)        3    1.2%   99        
##    (1, Inf)   (very bad)   0    0.0%   &amp;lt;NA&amp;gt;      
## See help(&amp;#39;pareto-k-diagnostic&amp;#39;) for details.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparing models with the WAIC and LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2 &amp;lt;- add_criterion(ex2, c(&amp;quot;loo&amp;quot;, &amp;quot;waic&amp;quot;))
ex2.fit2 &amp;lt;- add_criterion(ex2.fit2, c(&amp;quot;loo&amp;quot;, &amp;quot;waic&amp;quot;))
ex2.fit3 &amp;lt;- add_criterion(ex2.fit3, c(&amp;quot;loo&amp;quot;, &amp;quot;waic&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The point to focus on, here, is we can use the &lt;code&gt;loo_compare()&lt;/code&gt; function to compare fits by their WAIC or LOO. Let’s practice with the WAIC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ws &amp;lt;- loo_compare(ex2, ex2.fit2, ex2.fit3, criterion = &amp;quot;waic&amp;quot;)

print(ws)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          elpd_diff se_diff
## ex2.fit3   0.0       0.0  
## ex2.fit2  -0.5       2.2  
## ex2      -35.7       7.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if you wanted a more focused comparison, say between &lt;code&gt;ex2&lt;/code&gt; and &lt;code&gt;ex2.fit2&lt;/code&gt;, you’d just simplify your input.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(ex2, ex2.fit2, criterion = &amp;quot;loo&amp;quot;) %&amp;gt;%
  print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic 
## ex2.fit2    0.0       0.0  -290.3     12.8        95.8    7.8    580.6
## ex2       -25.1       7.6  -315.5     12.3        58.0    4.9    630.9
##          se_looic
## ex2.fit2   25.7  
## ex2        24.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects-revisited&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random effects revisited&lt;/h3&gt;
&lt;p&gt;For one person&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alcohol1_pp %&amp;gt;% 
  select(id:coa, cpeer, alcuse) %&amp;gt;% 
  filter(id == 23)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
##      id   age   coa cpeer alcuse
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1    23    14     1 -1.02   1   
## 2    23    15     1 -1.02   1   
## 3    23    16     1 -1.02   1.73&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23 &amp;lt;-
  posterior_samples(ex2.fit3) %&amp;gt;%
  select(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;%
  mutate(`gamma[0][&amp;quot;,23&amp;quot;]` = b_intercept + b_coa * 1 ,
         `gamma[1][&amp;quot;,23&amp;quot;]` = b_age_14 + `b_age_14:coa`)

head(post_23)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_intercept  b_age_14     b_coa b_age_14:coa gamma[0][&amp;quot;,23&amp;quot;]
## 1   0.5346809 0.2726632 0.3404332  -0.10266363       0.8751141
## 2   0.5003572 0.2943097 0.4494779  -0.14723165       0.9498351
## 3   0.6032724 0.2168968 0.1844755   0.05751361       0.7877478
## 4   0.5964837 0.2549591 0.5012584  -0.11002381       1.0977422
## 5   0.1802073 0.3580173 0.8257751  -0.03640028       1.0059824
## 6   0.1855054 0.2323935 0.6888844  -0.02800347       0.8743898
##   gamma[1][&amp;quot;,23&amp;quot;]
## 1       0.1699996
## 2       0.1470781
## 3       0.2744105
## 4       0.1449353
## 5       0.3216170
## 6       0.2043900&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23 %&amp;gt;%
  select(starts_with(&amp;quot;gamma&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%
  group_by(key) %&amp;gt;%
  summarise(mean = mean(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %&amp;gt;%
  mutate_if(is.double, round, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
##   key                  mean    ll    ul
##   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 &amp;quot;gamma[0][\&amp;quot;,23\&amp;quot;]&amp;quot; 1.05  0.765 1.34 
## 2 &amp;quot;gamma[1][\&amp;quot;,23\&amp;quot;]&amp;quot; 0.244 0.06  0.425&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23 %&amp;gt;%
  select(starts_with(&amp;quot;gamma&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%

  ggplot(aes(x = value)) +
  geom_density(size = 0, fill = &amp;quot;grey25&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;participant-specific parameter estimates&amp;quot;) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, labeller = label_parsed, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-72-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yet this approach neglects the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;s. That is, it would be the same for everyone with COA = 1. We’ve been extracting the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;s with &lt;code&gt;ranef()&lt;/code&gt;. We also get them when we use &lt;code&gt;posterior_samples()&lt;/code&gt;. Here we’ll extract both the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;s as well as the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;s for &lt;code&gt;id == 23&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23.r &amp;lt;-
  posterior_samples(ex2.fit3) %&amp;gt;%
  select(starts_with(&amp;quot;b_&amp;quot;), contains(&amp;quot;23&amp;quot;))

glimpse(post_23.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 4,000
## Variables: 6
## $ b_intercept          &amp;lt;dbl&amp;gt; 0.53468090, 0.50035715, 0.60327237, 0.59648…
## $ b_age_14             &amp;lt;dbl&amp;gt; 0.27266322, 0.29430973, 0.21689684, 0.25495…
## $ b_coa                &amp;lt;dbl&amp;gt; 0.3404332, 0.4494779, 0.1844755, 0.5012584,…
## $ `b_age_14:coa`       &amp;lt;dbl&amp;gt; -0.102663630, -0.147231646, 0.057513614, -0…
## $ `r_id[23,Intercept]` &amp;lt;dbl&amp;gt; -0.207329355, 0.396994028, -0.131291582, -0…
## $ `r_id[23,age_14]`    &amp;lt;dbl&amp;gt; 0.12645762, -0.04757970, 0.23943539, 0.5509…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;code&gt;r_id&lt;/code&gt; prefix, &lt;strong&gt;brms&lt;/strong&gt; tells you these are residual estimates for the levels in the &lt;code&gt;id&lt;/code&gt; grouping variable. Within the brackets, we learn these particular columns are for &lt;code&gt;id == 23&lt;/code&gt;, the first with respect to the &lt;code&gt;Intercept&lt;/code&gt; and second with respect to the &lt;code&gt;age_14&lt;/code&gt; parameter. Let’s put them to use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23.r &amp;lt;-
  post_23.r %&amp;gt;%
  mutate(`beta[0][&amp;quot;,23&amp;quot;]` = b_intercept + b_coa * 1  + `r_id[23,Intercept]`,
         `beta[1][&amp;quot;,23&amp;quot;]` = b_age_14 + `r_id[23,age_14]`)

glimpse(post_23.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 4,000
## Variables: 8
## $ b_intercept          &amp;lt;dbl&amp;gt; 0.53468090, 0.50035715, 0.60327237, 0.59648…
## $ b_age_14             &amp;lt;dbl&amp;gt; 0.27266322, 0.29430973, 0.21689684, 0.25495…
## $ b_coa                &amp;lt;dbl&amp;gt; 0.3404332, 0.4494779, 0.1844755, 0.5012584,…
## $ `b_age_14:coa`       &amp;lt;dbl&amp;gt; -0.102663630, -0.147231646, 0.057513614, -0…
## $ `r_id[23,Intercept]` &amp;lt;dbl&amp;gt; -0.207329355, 0.396994028, -0.131291582, -0…
## $ `r_id[23,age_14]`    &amp;lt;dbl&amp;gt; 0.12645762, -0.04757970, 0.23943539, 0.5509…
## $ `beta[0][&amp;quot;,23&amp;quot;]`     &amp;lt;dbl&amp;gt; 0.6677847, 1.3468291, 0.6564562, 0.5999346,…
## $ `beta[1][&amp;quot;,23&amp;quot;]`     &amp;lt;dbl&amp;gt; 0.39912085, 0.24673003, 0.45633223, 0.80588…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23.r %&amp;gt;%
  select(starts_with(&amp;quot;beta&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%
  group_by(key) %&amp;gt;%
  summarise(mean = mean(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %&amp;gt;%
  mutate_if(is.double, round, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
##   key                 mean     ll    ul
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 &amp;quot;beta[0][\&amp;quot;,23\&amp;quot;]&amp;quot; 0.979  0.247 1.73 
## 2 &amp;quot;beta[1][\&amp;quot;,23\&amp;quot;]&amp;quot; 0.332 -0.198 0.883&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23.r %&amp;gt;%
  select(starts_with(&amp;quot;beta&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%

  ggplot(aes(x = value)) +
  geom_density(size = 0, fill = &amp;quot;grey25&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;participant-specific parameter estimates&amp;quot;) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, labeller = label_parsed, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-76-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tidybayes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;tidybayes&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidybayes)
get_variables(ex1.nc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1] &amp;quot;b_Intercept&amp;quot;              &amp;quot;b_age_c&amp;quot;                 
##   [3] &amp;quot;b_program&amp;quot;                &amp;quot;b_age_c:program&amp;quot;         
##   [5] &amp;quot;sd_id__Intercept&amp;quot;         &amp;quot;sd_id__age_c&amp;quot;            
##   [7] &amp;quot;cor_id__Intercept__age_c&amp;quot; &amp;quot;sigma&amp;quot;                   
##   [9] &amp;quot;r_id[1,Intercept]&amp;quot;        &amp;quot;r_id[2,Intercept]&amp;quot;       
##  [11] &amp;quot;r_id[3,Intercept]&amp;quot;        &amp;quot;r_id[4,Intercept]&amp;quot;       
##  [13] &amp;quot;r_id[5,Intercept]&amp;quot;        &amp;quot;r_id[6,Intercept]&amp;quot;       
##  [15] &amp;quot;r_id[7,Intercept]&amp;quot;        &amp;quot;r_id[8,Intercept]&amp;quot;       
##  [17] &amp;quot;r_id[9,Intercept]&amp;quot;        &amp;quot;r_id[10,Intercept]&amp;quot;      
##  [19] &amp;quot;r_id[11,Intercept]&amp;quot;       &amp;quot;r_id[12,Intercept]&amp;quot;      
##  [21] &amp;quot;r_id[13,Intercept]&amp;quot;       &amp;quot;r_id[14,Intercept]&amp;quot;      
##  [23] &amp;quot;r_id[15,Intercept]&amp;quot;       &amp;quot;r_id[16,Intercept]&amp;quot;      
##  [25] &amp;quot;r_id[17,Intercept]&amp;quot;       &amp;quot;r_id[18,Intercept]&amp;quot;      
##  [27] &amp;quot;r_id[19,Intercept]&amp;quot;       &amp;quot;r_id[20,Intercept]&amp;quot;      
##  [29] &amp;quot;r_id[21,Intercept]&amp;quot;       &amp;quot;r_id[22,Intercept]&amp;quot;      
##  [31] &amp;quot;r_id[23,Intercept]&amp;quot;       &amp;quot;r_id[24,Intercept]&amp;quot;      
##  [33] &amp;quot;r_id[25,Intercept]&amp;quot;       &amp;quot;r_id[26,Intercept]&amp;quot;      
##  [35] &amp;quot;r_id[27,Intercept]&amp;quot;       &amp;quot;r_id[28,Intercept]&amp;quot;      
##  [37] &amp;quot;r_id[29,Intercept]&amp;quot;       &amp;quot;r_id[30,Intercept]&amp;quot;      
##  [39] &amp;quot;r_id[31,Intercept]&amp;quot;       &amp;quot;r_id[32,Intercept]&amp;quot;      
##  [41] &amp;quot;r_id[33,Intercept]&amp;quot;       &amp;quot;r_id[34,Intercept]&amp;quot;      
##  [43] &amp;quot;r_id[35,Intercept]&amp;quot;       &amp;quot;r_id[36,Intercept]&amp;quot;      
##  [45] &amp;quot;r_id[37,Intercept]&amp;quot;       &amp;quot;r_id[38,Intercept]&amp;quot;      
##  [47] &amp;quot;r_id[39,Intercept]&amp;quot;       &amp;quot;r_id[40,Intercept]&amp;quot;      
##  [49] &amp;quot;r_id[41,Intercept]&amp;quot;       &amp;quot;r_id[42,Intercept]&amp;quot;      
##  [51] &amp;quot;r_id[43,Intercept]&amp;quot;       &amp;quot;r_id[44,Intercept]&amp;quot;      
##  [53] &amp;quot;r_id[45,Intercept]&amp;quot;       &amp;quot;r_id[46,Intercept]&amp;quot;      
##  [55] &amp;quot;r_id[47,Intercept]&amp;quot;       &amp;quot;r_id[48,Intercept]&amp;quot;      
##  [57] &amp;quot;r_id[49,Intercept]&amp;quot;       &amp;quot;r_id[50,Intercept]&amp;quot;      
##  [59] &amp;quot;r_id[51,Intercept]&amp;quot;       &amp;quot;r_id[52,Intercept]&amp;quot;      
##  [61] &amp;quot;r_id[53,Intercept]&amp;quot;       &amp;quot;r_id[54,Intercept]&amp;quot;      
##  [63] &amp;quot;r_id[68,Intercept]&amp;quot;       &amp;quot;r_id[70,Intercept]&amp;quot;      
##  [65] &amp;quot;r_id[71,Intercept]&amp;quot;       &amp;quot;r_id[72,Intercept]&amp;quot;      
##  [67] &amp;quot;r_id[902,Intercept]&amp;quot;      &amp;quot;r_id[904,Intercept]&amp;quot;     
##  [69] &amp;quot;r_id[906,Intercept]&amp;quot;      &amp;quot;r_id[908,Intercept]&amp;quot;     
##  [71] &amp;quot;r_id[955,Intercept]&amp;quot;      &amp;quot;r_id[956,Intercept]&amp;quot;     
##  [73] &amp;quot;r_id[957,Intercept]&amp;quot;      &amp;quot;r_id[958,Intercept]&amp;quot;     
##  [75] &amp;quot;r_id[959,Intercept]&amp;quot;      &amp;quot;r_id[960,Intercept]&amp;quot;     
##  [77] &amp;quot;r_id[961,Intercept]&amp;quot;      &amp;quot;r_id[962,Intercept]&amp;quot;     
##  [79] &amp;quot;r_id[963,Intercept]&amp;quot;      &amp;quot;r_id[964,Intercept]&amp;quot;     
##  [81] &amp;quot;r_id[965,Intercept]&amp;quot;      &amp;quot;r_id[966,Intercept]&amp;quot;     
##  [83] &amp;quot;r_id[967,Intercept]&amp;quot;      &amp;quot;r_id[968,Intercept]&amp;quot;     
##  [85] &amp;quot;r_id[969,Intercept]&amp;quot;      &amp;quot;r_id[970,Intercept]&amp;quot;     
##  [87] &amp;quot;r_id[971,Intercept]&amp;quot;      &amp;quot;r_id[972,Intercept]&amp;quot;     
##  [89] &amp;quot;r_id[973,Intercept]&amp;quot;      &amp;quot;r_id[974,Intercept]&amp;quot;     
##  [91] &amp;quot;r_id[975,Intercept]&amp;quot;      &amp;quot;r_id[976,Intercept]&amp;quot;     
##  [93] &amp;quot;r_id[977,Intercept]&amp;quot;      &amp;quot;r_id[978,Intercept]&amp;quot;     
##  [95] &amp;quot;r_id[979,Intercept]&amp;quot;      &amp;quot;r_id[980,Intercept]&amp;quot;     
##  [97] &amp;quot;r_id[981,Intercept]&amp;quot;      &amp;quot;r_id[982,Intercept]&amp;quot;     
##  [99] &amp;quot;r_id[983,Intercept]&amp;quot;      &amp;quot;r_id[984,Intercept]&amp;quot;     
## [101] &amp;quot;r_id[985,Intercept]&amp;quot;      &amp;quot;r_id[986,Intercept]&amp;quot;     
## [103] &amp;quot;r_id[987,Intercept]&amp;quot;      &amp;quot;r_id[988,Intercept]&amp;quot;     
## [105] &amp;quot;r_id[989,Intercept]&amp;quot;      &amp;quot;r_id[990,Intercept]&amp;quot;     
## [107] &amp;quot;r_id[991,Intercept]&amp;quot;      &amp;quot;r_id[992,Intercept]&amp;quot;     
## [109] &amp;quot;r_id[993,Intercept]&amp;quot;      &amp;quot;r_id[994,Intercept]&amp;quot;     
## [111] &amp;quot;r_id[995,Intercept]&amp;quot;      &amp;quot;r_id[1,age_c]&amp;quot;           
## [113] &amp;quot;r_id[2,age_c]&amp;quot;            &amp;quot;r_id[3,age_c]&amp;quot;           
## [115] &amp;quot;r_id[4,age_c]&amp;quot;            &amp;quot;r_id[5,age_c]&amp;quot;           
## [117] &amp;quot;r_id[6,age_c]&amp;quot;            &amp;quot;r_id[7,age_c]&amp;quot;           
## [119] &amp;quot;r_id[8,age_c]&amp;quot;            &amp;quot;r_id[9,age_c]&amp;quot;           
## [121] &amp;quot;r_id[10,age_c]&amp;quot;           &amp;quot;r_id[11,age_c]&amp;quot;          
## [123] &amp;quot;r_id[12,age_c]&amp;quot;           &amp;quot;r_id[13,age_c]&amp;quot;          
## [125] &amp;quot;r_id[14,age_c]&amp;quot;           &amp;quot;r_id[15,age_c]&amp;quot;          
## [127] &amp;quot;r_id[16,age_c]&amp;quot;           &amp;quot;r_id[17,age_c]&amp;quot;          
## [129] &amp;quot;r_id[18,age_c]&amp;quot;           &amp;quot;r_id[19,age_c]&amp;quot;          
## [131] &amp;quot;r_id[20,age_c]&amp;quot;           &amp;quot;r_id[21,age_c]&amp;quot;          
## [133] &amp;quot;r_id[22,age_c]&amp;quot;           &amp;quot;r_id[23,age_c]&amp;quot;          
## [135] &amp;quot;r_id[24,age_c]&amp;quot;           &amp;quot;r_id[25,age_c]&amp;quot;          
## [137] &amp;quot;r_id[26,age_c]&amp;quot;           &amp;quot;r_id[27,age_c]&amp;quot;          
## [139] &amp;quot;r_id[28,age_c]&amp;quot;           &amp;quot;r_id[29,age_c]&amp;quot;          
## [141] &amp;quot;r_id[30,age_c]&amp;quot;           &amp;quot;r_id[31,age_c]&amp;quot;          
## [143] &amp;quot;r_id[32,age_c]&amp;quot;           &amp;quot;r_id[33,age_c]&amp;quot;          
## [145] &amp;quot;r_id[34,age_c]&amp;quot;           &amp;quot;r_id[35,age_c]&amp;quot;          
## [147] &amp;quot;r_id[36,age_c]&amp;quot;           &amp;quot;r_id[37,age_c]&amp;quot;          
## [149] &amp;quot;r_id[38,age_c]&amp;quot;           &amp;quot;r_id[39,age_c]&amp;quot;          
## [151] &amp;quot;r_id[40,age_c]&amp;quot;           &amp;quot;r_id[41,age_c]&amp;quot;          
## [153] &amp;quot;r_id[42,age_c]&amp;quot;           &amp;quot;r_id[43,age_c]&amp;quot;          
## [155] &amp;quot;r_id[44,age_c]&amp;quot;           &amp;quot;r_id[45,age_c]&amp;quot;          
## [157] &amp;quot;r_id[46,age_c]&amp;quot;           &amp;quot;r_id[47,age_c]&amp;quot;          
## [159] &amp;quot;r_id[48,age_c]&amp;quot;           &amp;quot;r_id[49,age_c]&amp;quot;          
## [161] &amp;quot;r_id[50,age_c]&amp;quot;           &amp;quot;r_id[51,age_c]&amp;quot;          
## [163] &amp;quot;r_id[52,age_c]&amp;quot;           &amp;quot;r_id[53,age_c]&amp;quot;          
## [165] &amp;quot;r_id[54,age_c]&amp;quot;           &amp;quot;r_id[68,age_c]&amp;quot;          
## [167] &amp;quot;r_id[70,age_c]&amp;quot;           &amp;quot;r_id[71,age_c]&amp;quot;          
## [169] &amp;quot;r_id[72,age_c]&amp;quot;           &amp;quot;r_id[902,age_c]&amp;quot;         
## [171] &amp;quot;r_id[904,age_c]&amp;quot;          &amp;quot;r_id[906,age_c]&amp;quot;         
## [173] &amp;quot;r_id[908,age_c]&amp;quot;          &amp;quot;r_id[955,age_c]&amp;quot;         
## [175] &amp;quot;r_id[956,age_c]&amp;quot;          &amp;quot;r_id[957,age_c]&amp;quot;         
## [177] &amp;quot;r_id[958,age_c]&amp;quot;          &amp;quot;r_id[959,age_c]&amp;quot;         
## [179] &amp;quot;r_id[960,age_c]&amp;quot;          &amp;quot;r_id[961,age_c]&amp;quot;         
## [181] &amp;quot;r_id[962,age_c]&amp;quot;          &amp;quot;r_id[963,age_c]&amp;quot;         
## [183] &amp;quot;r_id[964,age_c]&amp;quot;          &amp;quot;r_id[965,age_c]&amp;quot;         
## [185] &amp;quot;r_id[966,age_c]&amp;quot;          &amp;quot;r_id[967,age_c]&amp;quot;         
## [187] &amp;quot;r_id[968,age_c]&amp;quot;          &amp;quot;r_id[969,age_c]&amp;quot;         
## [189] &amp;quot;r_id[970,age_c]&amp;quot;          &amp;quot;r_id[971,age_c]&amp;quot;         
## [191] &amp;quot;r_id[972,age_c]&amp;quot;          &amp;quot;r_id[973,age_c]&amp;quot;         
## [193] &amp;quot;r_id[974,age_c]&amp;quot;          &amp;quot;r_id[975,age_c]&amp;quot;         
## [195] &amp;quot;r_id[976,age_c]&amp;quot;          &amp;quot;r_id[977,age_c]&amp;quot;         
## [197] &amp;quot;r_id[978,age_c]&amp;quot;          &amp;quot;r_id[979,age_c]&amp;quot;         
## [199] &amp;quot;r_id[980,age_c]&amp;quot;          &amp;quot;r_id[981,age_c]&amp;quot;         
## [201] &amp;quot;r_id[982,age_c]&amp;quot;          &amp;quot;r_id[983,age_c]&amp;quot;         
## [203] &amp;quot;r_id[984,age_c]&amp;quot;          &amp;quot;r_id[985,age_c]&amp;quot;         
## [205] &amp;quot;r_id[986,age_c]&amp;quot;          &amp;quot;r_id[987,age_c]&amp;quot;         
## [207] &amp;quot;r_id[988,age_c]&amp;quot;          &amp;quot;r_id[989,age_c]&amp;quot;         
## [209] &amp;quot;r_id[990,age_c]&amp;quot;          &amp;quot;r_id[991,age_c]&amp;quot;         
## [211] &amp;quot;r_id[992,age_c]&amp;quot;          &amp;quot;r_id[993,age_c]&amp;quot;         
## [213] &amp;quot;r_id[994,age_c]&amp;quot;          &amp;quot;r_id[995,age_c]&amp;quot;         
## [215] &amp;quot;lp__&amp;quot;                     &amp;quot;accept_stat__&amp;quot;           
## [217] &amp;quot;stepsize__&amp;quot;               &amp;quot;treedepth__&amp;quot;             
## [219] &amp;quot;n_leapfrog__&amp;quot;             &amp;quot;divergent__&amp;quot;             
## [221] &amp;quot;energy__&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex1.nc %&amp;gt;% 
spread_draws(`b_age_c`, r_id[id,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 824,000 x 6
## # Groups:   id [103]
##    .chain .iteration .draw b_age_c    id   r_id
##     &amp;lt;int&amp;gt;      &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;  &amp;lt;dbl&amp;gt;
##  1      1          1     1   -21.4     1   5.31
##  2      1          1     1   -21.4     2   1.72
##  3      1          1     1   -21.4     3  -1.54
##  4      1          1     1   -21.4     4  14.0 
##  5      1          1     1   -21.4     5   4.89
##  6      1          1     1   -21.4     6   7.20
##  7      1          1     1   -21.4     7   1.42
##  8      1          1     1   -21.4     8 -12.2 
##  9      1          1     1   -21.4     9  12.0 
## 10      1          1     1   -21.4    10  -5.30
## # … with 823,990 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex1.nc %&amp;gt;% 
spread_draws(`b_age_c`, r_id[id,]) %&amp;gt;% 
   median_qi(slope_mean = b_age_c + r_id) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 103 x 7
##       id slope_mean .lower .upper .width .point .interval
##    &amp;lt;int&amp;gt;      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    
##  1     1      -18.8  -27.7  -7.07   0.95 median qi       
##  2     2      -20.2  -30.4 -10.0    0.95 median qi       
##  3     3      -21.6  -32.5 -12.7    0.95 median qi       
##  4     4      -18.9  -34.1  -2.63   0.95 median qi       
##  5     5      -21.1  -33.2 -10.5    0.95 median qi       
##  6     6      -22.4  -35.0 -12.4    0.95 median qi       
##  7     7      -20.3  -31.4  -8.73   0.95 median qi       
##  8     8      -27.2  -45.8 -11.1    0.95 median qi       
##  9     9      -17.4  -27.7  -3.78   0.95 median qi       
## 10    10      -25.0  -39.7 -13.5    0.95 median qi       
## # … with 93 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex1.nc %&amp;gt;% 
spread_draws(`b_age_c`, r_id[id,]) %&amp;gt;% 
   median_qi(slope_mean = b_age_c + r_id) %&amp;gt;% 
    ggplot(aes(y = as.factor(id), x = slope_mean)) +
    geom_pointintervalh() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-80-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesplot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesplot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(bayesplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is bayesplot version 1.7.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - Online documentation and vignettes at mc-stan.org/bayesplot&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## - bayesplot theme set to bayesplot::theme_default()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    * Does _not_ affect other ggplot2 plots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    * See ?bayesplot_theme_set for details on theme setting&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;level-1-predictors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Level 1 predictors&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-explained&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance explained&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hypothesis function&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-checks&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Posterior-predictive checks&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Missing data&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;nonlinear&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Nonlinear&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;thanks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thanks&lt;/h2&gt;
&lt;p&gt;Many thanks to Solomon Kurz’s github for code&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
