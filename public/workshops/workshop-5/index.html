<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="intro to brms">

  
  <link rel="alternate" hreflang="en-us" href="/workshops/workshop-5/">

  


  

  
  
  
  <meta name="theme-color" content="rgb(0, 136, 204)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.301ab7eca2ca7d1311ed71a9afa2cc4f.css">

  

  
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/workshops/workshop-5/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@jackson_josh_">
  <meta property="twitter:creator" content="@jackson_josh_">
  
  <meta property="og:site_name" content="Applied Longitudinal Data Analysis">
  <meta property="og:url" content="/workshops/workshop-5/">
  <meta property="og:title" content="workshop#5 | Applied Longitudinal Data Analysis">
  <meta property="og:description" content="intro to brms"><meta property="og:image" content="/img/icon-192.png">
  <meta property="twitter:image" content="/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-09-26T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-09-26T13:03:35-05:00">
  

  


  





  <title>workshop#5 | Applied Longitudinal Data Analysis</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Applied Longitudinal Data Analysis</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/lectures"><span>Lectures</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/workshops"><span>Workshops</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/homeworks"><span>Homeworks</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/syllabus"><span>Syllabus</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article" itemscope itemtype="http://schema.org/Article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">workshop#5</h1>

  
  <p class="page-subtitle">intro to brms</p>
  

  
    



<meta content="2019-09-26 00:00:00 &#43;0000 UTC" itemprop="datePublished">
<meta content="2019-09-26 13:03:35 -0500 CDT" itemprop="dateModified">

<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    <time>Sep 26, 2019</time>
  </span>
  

  

  

  
  
  

  
  

  
    

  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<div id="TOC">
<ul>
<li><a href="#how-is-this-different">How is this different?</a></li>
<li><a href="#the-basic-parts">The basic parts</a><ul>
<li><a href="#posterior-predictive-distribution">Posterior predictive distribution</a></li>
</ul></li>
<li><a href="#how-to-think-about-these-models">How to think about these models</a></li>
<li><a href="#mcmc-estimation">MCMC Estimation</a></li>
<li><a href="#comparing-lmer-with-brms">Comparing lmer with brms</a><ul>
<li><a href="#what-priors-did-we-use">What priors did we use?</a></li>
<li><a href="#differenes-in-random-effects-with-brms">Differenes in random effects with brms</a></li>
</ul></li>
<li><a href="#more-models">More models</a><ul>
<li><a href="#ex-1">Ex 1</a></li>
<li><a href="#intercepts-are-different-because-of-priors">Intercepts are different because of priors</a></li>
<li><a href="#what-does-the-posterior-look-like">What does the posterior look like?</a></li>
<li><a href="#random-effects">Random effects</a></li>
<li><a href="#ex-2">Ex 2</a><ul>
<li><a href="#using-priors">Using priors</a></li>
<li><a href="#calculate-icc.">Calculate ICC.</a></li>
<li><a href="#adding-predictors">Adding predictors</a></li>
<li><a href="#marginal-effects">Marginal effects</a></li>
<li><a href="#comparing-models">Comparing models</a></li>
<li><a href="#random-effects-revisited">Random effects revisited</a></li>
<li><a href="#variance-explained">Variance explained</a></li>
<li><a href="#hypothesis-function">Hypothesis function</a></li>
<li><a href="#update-function-again">Update function again</a></li>
</ul></li>
<li><a href="#thanks">Thanks</a></li>
</ul></li>
</ul>
</div>

<p>#Why Bayesian?
The models we have been working with can easily be done in a Bayesian Framework. Why Bayes? For at least 3 reasons: 1. Better convergence. 2. More flexibility. 3. Fewer assumptions. We will go through each of these ideas throughout the course in more detail, so what I want to do is sell you not on the benefits but on the lack of difference.</p>
<p>Many places to read up on this. Try Kruschke’s Bayesian new statistics: <a href="https://rdcu.be/bRUvW" class="uri">https://rdcu.be/bRUvW</a></p>
<div id="how-is-this-different" class="section level1">
<h1>How is this different?</h1>
<p>Bayesian analysis differs in two major ways from our traditional MLMs that we have been working with. First, the end result is different in that it uses probability differently to derive estimates and to interpret the results. The lucky thing for us is that this will not be drastically different if we dont want it to be. In other words, we can keep with our focuses on estimates and precision around those estimates, same way as we would in standard stats world. Once you progress further, you can better understand the nuances, but right now lets not get bogged down by technical differences.</p>
<p>The second major difference is that prior are used. Priors are a way to incorporate your beliefs into the model. At first blush it feels as if this is wrong, and is the justification for many for why the standard approach is correct and Bayesian is wrong. I mean, most people are taught frequentist approaches, thus how can 10 million SPSS users be wrong? (The reason for the popularity of frequentist approaches is two fold: computers and history. Computation power is needed, which was lacking until recently and thus curtailed general use, the same way that computation power held back adoption of SEM and before that multivariate approaches like factor analysis, and before that multiple regression with continuous predictors. The second reason, history, is, like much of history, driven by disagreements between dead white guys. Fisher, you’ve heard of him, populated the p-value and disliked Bayes, so long story short, it went out of fashion)</p>
</div>
<div id="the-basic-parts" class="section level1">
<h1>The basic parts</h1>
<p>Bayes Theorm
<span class="math display">\[ P(\theta | y) =  \frac{P(y | \theta) P(\theta)}{P(y)}. \]</span></p>
<p>This is often rewritten for when we want to estimate some value, $ $. Note that the <span class="math inline">\(P(y)\)</span> drops out, as it does not vary across $ $. In our case y is will be the data, and the data will be collected only once, thus considered fixed. The probabiliy o y was included in the above equation to rescale the equation and create some nice properities eg integrating to 1. We don’t care about that as we are going to look at relative likelihoods of each theta conditioned on y.</p>
<p><span class="math display">\[ P(\theta | y) \propto P(y | \theta) P(\theta)\]</span></p>
<p>This reads as the conditional probability of theta, given y is proportional to the probability of y given theta, multiplied by the probability of theta. The this, likely, sounds like gobblygook unless you have taken a bunch of probability classes. Instead the more helpful way to think about this is:</p>
<p><span class="math display">\[ p(hypothesis|data) \propto p(data|hypothesis)p(hypothesis)\]</span></p>
<p>This is what we get: the probability our hypothesis. Not the standard p value interpretation of the probability of our data given some hypothesis like the null distribution.</p>
<p>Bayesian stats gives names to each of these terms:</p>
<p><span class="math display">\[ posterior \propto likelihood * prior \]</span></p>
<p>Prior - Allows us to provide a priori intuition about what the findings are. It is of the form of a probability distribution. E.g., “Extrodanary claims require extrodinary evidene.” Note we will note use this as “guesses” of what will happen. Instead we will use this as a regularization tool, similar to partial pooling in MLM.</p>
<p>Likelihood - Distribution of the likelihood of various hypothesis. Probability attaches to possible results; likelihood attaches to hypotheses. It is akin to flipping coins, something we did with binomials.</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ──────────────────────────────────────────── tidyverse 1.2.1.9000 ──</code></pre>
<pre><code>## ✔ ggplot2 3.2.1          ✔ purrr   0.3.2     
## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ readr   1.3.1          ✔ forcats 0.4.0</code></pre>
<pre><code>## ── Conflicts ──────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<p>For 7 successes out of 10 trials, what is the likelihood of different probabilities?</p>
<pre class="r"><code>tibble(prob = seq(from = 0, to = 1, by = .01)) %&gt;% 
  ggplot(aes(x = prob,
             y = dbinom(x = 7, size = 10, prob = prob))) +
  geom_line() +
  labs(x = &quot;probability&quot;,
       y = &quot;binomial likelihood&quot;) +
  theme(panel.grid = element_blank())</code></pre>
<p><img src="/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>tibble(prob = seq(from = 0, to = 1, by = .01)) %&gt;% 
  ggplot(aes(x = prob,
             y = dbinom(x = 4, size = 10, prob = prob))) +
  geom_line() +
  labs(x = &quot;probability&quot;,
       y = &quot;binomial likelihood&quot;) +
  theme(panel.grid = element_blank())</code></pre>
<p><img src="/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>It can be made similar to the maximum likelihood estimation (with flat priors), but think about this as nothing different than your standard estimation procedure.</p>
<p>Posterior – distribution of our belief about the parameter values after taking into account the likelihood and one’s priors. In regression terms, it is not a specific value of b that would make the data most likely, but a probability distribution for b that serves as a weighted combination of the likelihood and prior.</p>
<pre class="r"><code>sequence_length &lt;- 1e3

d &lt;-
  tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %&gt;% 
  expand(probability, row = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;)) %&gt;% 
  arrange(row, probability) %&gt;% 
  mutate(prior = ifelse(row == &quot;flat&quot;, 1,
                        ifelse(row == &quot;stepped&quot;, rep(0:1, each = sequence_length / 2),
                               exp(-abs(probability - .5) / .25) / ( 2 * .25))),
         likelihood = dbinom(x = 6, size = 9, prob = probability)) %&gt;% 
  group_by(row) %&gt;% 
  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %&gt;% 
  gather(key, value, -probability, -row) %&gt;% 
  ungroup() %&gt;% 
  mutate(key = factor(key, levels = c(&quot;prior&quot;, &quot;likelihood&quot;, &quot;posterior&quot;)),
         row = factor(row, levels = c(&quot;flat&quot;, &quot;stepped&quot;, &quot;Laplace&quot;)))
p1 &lt;-
  d %&gt;%
  filter(key == &quot;prior&quot;) %&gt;% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = &quot;prior&quot;) +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1)

p2 &lt;-
  d %&gt;%
  filter(key == &quot;likelihood&quot;) %&gt;% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = &quot;likelihood&quot;) +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1)

p3 &lt;-
  d %&gt;%
  filter(key == &quot;posterior&quot;) %&gt;% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = &quot;posterior&quot;) +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = &quot;free_y&quot;, ncol = 1)

library(gridExtra)</code></pre>
<pre><code>## 
## Attaching package: &#39;gridExtra&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre class="r"><code>grid.arrange(p1, p2, p3, ncol = 3)</code></pre>
<p><img src="/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>We can describe our</p>
<p>Another way to think about it:
updated belief = current evidence ∗ prior belief or evidence</p>
<div id="posterior-predictive-distribution" class="section level2">
<h2>Posterior predictive distribution</h2>
<p>Once we have the posterior distribution for <span class="math inline">\(\theta\)</span>, we can then feed new or unobserved data into the data generating process and get new distributions for any potential observation. We can use this to make predictions, check if the model is correct, and to evaluate model fit.</p>
</div>
</div>
<div id="how-to-think-about-these-models" class="section level1">
<h1>How to think about these models</h1>
<p>In standard regression, we can state some expectations up front:</p>
<p><span class="math display">\[Y_{i}  \sim \text{Normal}(\mu_i, \sigma) \]</span></p>
<p><span class="math display">\[ \mu_i  = \beta \times Predictor_i \]</span></p>
<p>The above two lines are what is implicitly implied in almost all regressions. Namely, we have some variable that is being generated from a normal distribution, with a mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. That <span class="math inline">\(\mu\)</span> is going to be described as the relationship between some (fixed) regression coefficient and a person specific predictor variable.</p>
<p>Belwo is where we get into some new stuff, where we describe what we think the estimated parameters will look like (ie format a prior).</p>
<p><span class="math inline">\(\beta \sim \text{Normal}(0, 10)\)</span></p>
<pre class="r"><code>ggplot() +
  aes(x = c(-40, 40)) +
  stat_function(fun = dnorm, n = 200, args = list(0, 10)) +
  labs(title = &quot;Normal (Gaussian) distribution&quot;)</code></pre>
<p><img src="/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>$(0, 1) $</p>
<pre class="r"><code>ggplot() +
  aes(x = c(0, 10)) +
  stat_function(fun = dcauchy, n = 200, args = list(0, 1)) +
  labs(title = &quot;Half Cauchy distribution&quot;)</code></pre>
<p><img src="/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Note that we do two important things here. First, we describe the data generating processes (DGP) we think our data are coming from. Often we will just assume Gaussian normal, but this does not need to be the case. For our residual we specify a half cauchy to make sure it is above zero and that lower terms are more likely than higher terms.</p>
<p>Second, we are specifying a prior. Here we are saying we expect our regression parameter to be centered around zero but that it could vary widely from that and we wouldn’t be suprised. The prior distribution for Beta is defined by a mean of 0 and an SD of 10. But we have control if we want to make this different. Same thing with the half cauchy for the variance. We expect that it cannot be negative, that it is likely closer to zero than not. Again, it is up to us in terms of how we want to specify it.</p>
</div>
<div id="mcmc-estimation" class="section level1">
<h1>MCMC Estimation</h1>
<p>Bayesian estimation, like maximum likelihood, starts with initial guesses as starting points and then runs in an iterative fashion, producing simulated draws from the posterior distribution. This occurs until some stopping value is reached. This part is key and different from frequentist statistics. We are aiming for creating a distribution as our end goal, not just a point estimate.</p>
<p>The simulation process is referred to as Markov Chain Monte Carlo, or MCMC for short. In MCMC, all of the simulated draws from the posterior are based on and correlated with previous draws. We will typically allow the process to “warm up”, as the initial random starting point may be way off of being plausible. As it runs, however, these estimates will get better and better, creating a distribution of plausible values. As a safety check, we will run the the process multiple times, what is known as having multiple chains. If multiple chains converge towards the same answer we are more confident in our results.</p>
</div>
<div id="comparing-lmer-with-brms" class="section level1">
<h1>Comparing lmer with brms</h1>
<p>Lets do a mixed effects model to test this out. We will use the sleepstudy dataset that is loaded with lme4</p>
<pre class="r"><code>library(lme4)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## 
## Attaching package: &#39;Matrix&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:tidyr&#39;:
## 
##     expand, pack, unpack</code></pre>
<pre class="r"><code>data(&quot;sleepstudy&quot;)
head(sleepstudy)</code></pre>
<pre><code>##   Reaction Days Subject
## 1 249.5600    0     308
## 2 258.7047    1     308
## 3 250.8006    2     308
## 4 321.4398    3     308
## 5 356.8519    4     308
## 6 414.6901    5     308</code></pre>
<pre class="r"><code>write.csv(sleepstudy, file = &quot;sleepstudy&quot;)</code></pre>
<pre class="r"><code>sleep_lmer &lt;- lmer(Reaction ~ Days + (1 + Days|Subject), data = sleepstudy)
summary(sleep_lmer)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: Reaction ~ Days + (1 + Days | Subject)
##    Data: sleepstudy
## 
## REML criterion at convergence: 1743.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.9536 -0.4634  0.0231  0.4633  5.1793 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  Subject  (Intercept) 611.90   24.737       
##           Days         35.08    5.923   0.07
##  Residual             654.94   25.592       
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  251.405      6.824  36.843
## Days          10.467      1.546   6.771
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.138</code></pre>
<pre class="r"><code>library(brms)

sleep_brm &lt;- brm(Reaction ~ Days + (1 + Days|Subject), data = sleepstudy)</code></pre>
<pre><code>## Compiling the C++ model</code></pre>
<pre><code>## Start sampling</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;3c408cb3dadd05772809281a09d7c178&#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.000159 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.59 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 3.01152 seconds (Warm-up)
## Chain 1:                1.20743 seconds (Sampling)
## Chain 1:                4.21894 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &#39;3c408cb3dadd05772809281a09d7c178&#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 3.8e-05 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 2.79123 seconds (Warm-up)
## Chain 2:                1.17906 seconds (Sampling)
## Chain 2:                3.97029 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &#39;3c408cb3dadd05772809281a09d7c178&#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 4e-05 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.4 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 3.7019 seconds (Warm-up)
## Chain 3:                1.41167 seconds (Sampling)
## Chain 3:                5.11357 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &#39;3c408cb3dadd05772809281a09d7c178&#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 5.3e-05 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.53 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 3.20541 seconds (Warm-up)
## Chain 4:                1.13799 seconds (Sampling)
## Chain 4:                4.3434 seconds (Total)
## Chain 4:</code></pre>
<pre class="r"><code>summary(sleep_brm)</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;xts&#39;:
##   method     from
##   as.zoo.xts zoo</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Reaction ~ Days + (1 + Days | Subject) 
##    Data: sleepstudy (Number of observations: 180) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~Subject (Number of levels: 18) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)          27.06      6.86    15.57    42.41 1.00     1853
## sd(Days)                6.60      1.48     4.18     9.96 1.00     1607
## cor(Intercept,Days)     0.09      0.30    -0.46     0.69 1.00      795
##                     Tail_ESS
## sd(Intercept)           2567
## sd(Days)                2437
## cor(Intercept,Days)     1448
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   251.51      7.34   237.03   265.88 1.00     1906     2490
## Days         10.44      1.73     6.95    13.82 1.00     1335     1850
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    25.91      1.61    22.96    29.27 1.00     3403     2492
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<div id="what-priors-did-we-use" class="section level2">
<h2>What priors did we use?</h2>
<pre class="r"><code>brms::get_prior(Reaction ~ Days + (1 + Days|Subject), data = sleepstudy)</code></pre>
<pre><code>##                    prior     class      coef   group resp dpar nlpar bound
## 1                                b                                        
## 2                                b      Days                              
## 3                 lkj(1)       cor                                        
## 4                              cor           Subject                      
## 5  student_t(3, 289, 59) Intercept                                        
## 6    student_t(3, 0, 59)        sd                                        
## 7                               sd           Subject                      
## 8                               sd      Days Subject                      
## 9                               sd Intercept Subject                      
## 10   student_t(3, 0, 59)     sigma</code></pre>
<p>The ts have three parameters: degress of freedom, mean, and then an SD.</p>
</div>
<div id="differenes-in-random-effects-with-brms" class="section level2">
<h2>Differenes in random effects with brms</h2>
<p>We often summarize the <span class="math inline">\(\gamma_{0j}\)</span> and <span class="math inline">\(\gamma_{1j}\)</span> deviations as <span class="math inline">\(\sigma_0^2\)</span> and <span class="math inline">\(\sigma_1^2\)</span>, respectively. And importantly, these variance parameters have a covariance <span class="math inline">\(\sigma_{01}\)</span>. However, <strong>brms</strong> parameterizes these in the standard-deviation metric. That is, in <strong>brms</strong>, these are expressed as <span class="math inline">\(\sigma_0\)</span> and <span class="math inline">\(\sigma_1\)</span>. Similarly, the <span class="math inline">\(\sigma_{01}\)</span> presented in <strong>brms</strong> output is in a correlation metric, rather than a covariance.</p>
<p><span class="math display">\[
\begin{align*}
\begin{bmatrix} U_{0i} \\ U_{1i} \end{bmatrix} &amp; 
\sim \text{N} 
\bigg ( \begin{bmatrix} 0 \\ 0 \end{bmatrix},  
\begin{bmatrix} \sigma_0^2 &amp; \sigma_{01}\\ \sigma_{01} &amp; \sigma_1^2 \end{bmatrix}
\bigg )
\end{align*}
\]</span></p>
<p>Note, that in many Bayesian texts, the Level 2 covariance matrix above is often rewritten as below</p>
<p><span class="math display">\[
U \sim \text{N} (\mathbf{0}, \mathbf{\Sigma})
\]</span></p>
<p>where <span class="math inline">\(\mathbf{0}\)</span> is the vector of 0 means and <span class="math inline">\(\mathbf{\Sigma}\)</span> is the variance/covariance matrix. In <strong>Stan</strong>, and thus <strong>brms</strong>, <span class="math inline">\(\mathbf{\Sigma}\)</span> is decomposed</p>
<p><span class="math display">\[
\begin{align*}
\mathbf{\Sigma} &amp; = \mathbf{D} \mathbf{\Omega} \mathbf{D}, \text{where} \\
\mathbf{D}      &amp; = \begin{bmatrix} \sigma_0 &amp; 0 \\ 0 &amp; \sigma_1 \end{bmatrix} \text{and} \\
\mathbf{\Omega} &amp; = \begin{bmatrix} 1 &amp; \rho \\ \rho &amp; 1 \end{bmatrix}
\end{align*}
\]</span></p>
<p>Thus <span class="math inline">\(\mathbf{D}\)</span> is the diagonal matrix of standard deviations and <span class="math inline">\(\mathbf{\Omega}\)</span> is the correlation matrix.</p>
</div>
</div>
<div id="more-models" class="section level1">
<h1>More models</h1>
<div id="ex-1" class="section level2">
<h2>Ex 1</h2>
<p>Here we’ll use a dataset from Singer &amp; Willet, chapter 3, lookng at cognitive performance across time for children who under went an intervention or not.</p>
<pre><code>##       zeta_0     zeta_1
## 1 10.7586672 -3.0908765
## 2  3.4258938 -0.4186497
## 3 -3.0770183  0.2140130
## 4 12.5303603 -4.9043416
## 5 -2.1114641  0.8936950
## 6 -0.5521597 -0.6310265</code></pre>
<pre><code>## # A tibble: 6 x 5
##      id gamma_00 gamma_01 gamma_10 gamma_11
##   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1     1     108.     6.85    -21.1     5.27
## 2     2     108.     6.85    -21.1     5.27
## 3     3     108.     6.85    -21.1     5.27
## 4     4     108.     6.85    -21.1     5.27
## 5     5     108.     6.85    -21.1     5.27
## 6     6     108.     6.85    -21.1     5.27</code></pre>
<pre><code>## # A tibble: 6 x 13
##      id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c
##   &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;
## 1     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0  
## 2     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0.5
## 3     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   1  
## 4     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0  
## 5     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0.5
## 6     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   1  
## # … with 4 more variables: epsilon &lt;dbl&gt;, pi_0 &lt;dbl&gt;, pi_1 &lt;dbl&gt;,
## #   cog &lt;dbl&gt;</code></pre>
<pre><code>## # A tibble: 6 x 14
##      id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c
##   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;int&gt; &lt;dbl&gt;
## 1     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0  
## 2     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0.5
## 3     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   1  
## 4     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0  
## 5     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0.5
## 6     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   1  
## # … with 5 more variables: epsilon &lt;dbl&gt;, pi_0 &lt;dbl&gt;, pi_1 &lt;dbl&gt;,
## #   cog &lt;dbl&gt;, age &lt;dbl&gt;</code></pre>
<pre><code>## Joining, by = c(&quot;id&quot;, &quot;age&quot;, &quot;cog&quot;, &quot;program&quot;, &quot;age_c&quot;)</code></pre>
<pre><code>## Observations: 309
## Variables: 5
## $ id      &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,…
## $ age     &lt;dbl&gt; 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2…
## $ cog     &lt;dbl&gt; 117, 113, 109, 108, 112, 102, 112, 113, 85, 138, 110, 97…
## $ program &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ age_c   &lt;dbl&gt; 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1…</code></pre>
<pre class="r"><code>load(&quot;early_int_sim.rda&quot;)</code></pre>
<pre class="r"><code>early_int_sim &lt;-
  early_int_sim %&gt;% 
  mutate(label = str_c(&quot;program = &quot;, program)) 

early_int_sim %&gt;% 
  ggplot(aes(x = age, y = cog, color = label)) +
  stat_smooth(aes(group = id),
              method = &quot;lm&quot;, se = F, size = 1/6) +
  stat_smooth(method = &quot;lm&quot;, se = F, size = 2) +
  scale_x_continuous(breaks = c(1, 1.5, 2)) +
  scale_color_viridis_d(option = &quot;B&quot;, begin = .33, end = .67) +
  ylim(50, 150) +
  theme(panel.grid = element_blank(),
        legend.position = &quot;none&quot;) +
  facet_wrap(~label)</code></pre>
<p><img src="/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-14-1.png" width="576" /></p>
<p>This is our assumed data generating model
<span class="math display">\[
\begin{align*}
\text{cog} &amp; \sim \text{Normal} (\mu_{ij}, \sigma_\epsilon^2) \\
\mu_{ij}   &amp; = \gamma_{0j} + \gamma_{1j} (\text{age}_{ij} - 1) + \gamma_{2j} (\text{Program})
\end{align*}
\]</span></p>
<pre class="r"><code>ex1 &lt;-
  brm(data = early_int_sim,
      family = gaussian,
      formula = cog ~ 0 + intercept + age_c + program + age_c:program + (1 + age_c | id),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.9),
      seed = 3)</code></pre>
</div>
<div id="intercepts-are-different-because-of-priors" class="section level2">
<h2>Intercepts are different because of priors</h2>
<p>Another important part of the syntax concerns the intercept. Normally we include a 1 (or lmer does it for us automatically) to reflect we want to fit an intercept. If we did that here, we would have made the assumption that our predictors are mean centered. The default priors set by <code>brms::brm()</code> are set based on this assumption. SO, if we want to use non mean centered predictors (eg setting time at initial wave or dummy variables), we need to respecify our model. Neither our variables are mean centered.</p>
<p>With a <code>0 + intercept</code>, we told <code>brm()</code> to suppress the default intercept and replace it with our smartly-named <code>intercept</code> parameter. This is our fixed effect for the population intercept and, importantly, <code>brms()</code> will assign default priors to it based on the data themselves without assumptions about centering. We will speak later about changing these default priors.</p>
<pre class="r"><code>print(ex1)</code></pre>
<pre><code>## Warning: There were 29 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help.
## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: cog ~ 0 + intercept + age_c + program + age_c:program + (1 + age_c | id) 
##    Data: early_int_sim (Number of observations: 309) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 103) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)            9.67      1.16     7.60    11.97 1.00     1043
## sd(age_c)                3.85      2.39     0.20     9.00 1.01      386
## cor(Intercept,age_c)    -0.48      0.36    -0.96     0.53 1.00     3234
##                      Tail_ESS
## sd(Intercept)            2084
## sd(age_c)                 487
## cor(Intercept,age_c)     1903
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## intercept       106.53      1.95   102.45   110.26 1.00     1176     1359
## age_c           -20.52      1.98   -24.42   -16.64 1.00     2217     2872
## program           9.21      2.54     4.14    14.09 1.00     1244     1938
## age_c:program     3.20      2.66    -2.03     8.38 1.00     1767     1347
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     8.59      0.52     7.54     9.59 1.01      729      606
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="what-does-the-posterior-look-like" class="section level2">
<h2>What does the posterior look like?</h2>
<pre class="r"><code>fixef(ex1)</code></pre>
<pre><code>##                 Estimate Est.Error       Q2.5      Q97.5
## intercept     106.529202  1.946755 102.448293 110.258352
## age_c         -20.516543  1.982355 -24.422934 -16.641683
## program         9.213920  2.536958   4.140112  14.087900
## age_c:program   3.201103  2.655993  -2.030914   8.378437</code></pre>
<pre class="r"><code>#post &lt;- posterior_samples(ex1)</code></pre>
<p>Here’s a look at the first 10 columns.</p>
<pre class="r"><code># post[, 1:10] %&gt;%
#   glimpse()</code></pre>
<p>We saved our results as <code>post</code>, which is a data frame with 4000 rows (i.e., 1000 post-warmup iterations times 4 chains) and 215 columns, each depicting one of the model parameters. With <strong>brms</strong>, the <span class="math inline">\(\gamma\)</span> parameters (i.e., the fixed effects or population parameters) get <code>b_</code> prefixes in the <code>posterior_samples()</code> output. So we can isolate them like so.</p>
<pre class="r"><code># post %&gt;% 
#   select(starts_with(&quot;b_&quot;)) %&gt;% 
#   head()</code></pre>
<pre class="r"><code># post %&gt;% 
#   select(starts_with(&quot;b_&quot;)) %&gt;% 
#   gather() %&gt;% 
#   
#   ggplot(aes(x = value)) +
#   geom_density(color = &quot;transparent&quot;, fill = &quot;grey&quot;) +
#   scale_y_continuous(NULL, breaks = NULL) +
#   theme(panel.grid = element_blank()) +
#   facet_wrap(~key, scales = &quot;free&quot;)</code></pre>
</div>
<div id="random-effects" class="section level2">
<h2>Random effects</h2>
<pre class="r"><code>VarCorr(ex1)</code></pre>
<pre><code>## $id
## $id$sd
##           Estimate Est.Error      Q2.5     Q97.5
## Intercept 9.667999  1.155713 7.6023029 11.965095
## age_c     3.845584  2.394552 0.1988863  8.997617
## 
## $id$cor
## , , Intercept
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept  1.0000000 0.0000000  1.0000000 1.0000000
## age_c     -0.4809751 0.3646166 -0.9641151 0.5332108
## 
## , , age_c
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept -0.4809751 0.3646166 -0.9641151 0.5332108
## age_c      1.0000000 0.0000000  1.0000000 1.0000000
## 
## 
## $id$cov
## , , Intercept
## 
##            Estimate Est.Error      Q2.5     Q97.5
## Intercept  94.80554  22.79074  57.79501 143.16349
## age_c     -21.32497  18.96069 -65.51948   4.40548
## 
## , , age_c
## 
##            Estimate Est.Error        Q2.5    Q97.5
## Intercept -21.32497  18.96069 -65.5194793  4.40548
## age_c      20.52097  22.16587   0.0395562 80.95712
## 
## 
## 
## $residual__
## $residual__$sd
##  Estimate Est.Error     Q2.5    Q97.5
##  8.585042 0.5183284 7.535189 9.592982</code></pre>
<p>In case that output is confusing, <code>VarCorr()</code> returned a 2-element list of lists.</p>
<p>If you just want the <span class="math inline">\(U_j\)</span>s, subset the first list of the first list. Note this is included in the standard summary.</p>
<pre class="r"><code>VarCorr(ex1)[[1]][[1]]</code></pre>
<pre><code>##           Estimate Est.Error      Q2.5     Q97.5
## Intercept 9.667999  1.155713 7.6023029 11.965095
## age_c     3.845584  2.394552 0.1988863  8.997617</code></pre>
<p>Here’s how to get their correlation matrix.</p>
<pre class="r"><code>VarCorr(ex1)[[1]][[2]]</code></pre>
<pre><code>## , , Intercept
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept  1.0000000 0.0000000  1.0000000 1.0000000
## age_c     -0.4809751 0.3646166 -0.9641151 0.5332108
## 
## , , age_c
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept -0.4809751 0.3646166 -0.9641151 0.5332108
## age_c      1.0000000 0.0000000  1.0000000 1.0000000</code></pre>
<p>variance/covariance matrix.</p>
<pre class="r"><code>VarCorr(ex1)[[1]][[3]]</code></pre>
<pre><code>## , , Intercept
## 
##            Estimate Est.Error      Q2.5     Q97.5
## Intercept  94.80554  22.79074  57.79501 143.16349
## age_c     -21.32497  18.96069 -65.51948   4.40548
## 
## , , age_c
## 
##            Estimate Est.Error        Q2.5    Q97.5
## Intercept -21.32497  18.96069 -65.5194793  4.40548
## age_c      20.52097  22.16587   0.0395562 80.95712</code></pre>
<pre class="r"><code># posterior_samples(ex1) %&gt;% 
#   transmute(`sigma[0]^2`  = sd_id__Intercept^2,
#             `sigma[1]^2`  = sd_id__age_c^2,
#             `sigma[0][1]` = sd_id__Intercept * cor_id__Intercept__age_c * sd_id__age_c,
#             `sigma[epsilon]^2` = sigma^2) %&gt;% 
#   gather(key, posterior) %&gt;% 
#   
#   ggplot(aes(x = posterior)) +
#   geom_density(color = &quot;transparent&quot;, fill = &quot;grey&quot;) +
#   scale_y_continuous(NULL, breaks = NULL) +
#   theme(panel.grid = element_blank(),
#         strip.text = element_text(size = 12)) +
#   facet_wrap(~key, scales = &quot;free&quot;, labeller = label_parsed) </code></pre>
</div>
<div id="ex-2" class="section level2">
<h2>Ex 2</h2>
<p>Load the data, here from chapter 4 of Singer and Willet</p>
<p>Data generating model we are fitting. You can also write this with L1 and L2 convention.</p>
<p><span class="math display">\[
\begin{align*}
\text{alcuse}_{ij} &amp; =  \gamma_{00} +  U_{0j} + \epsilon_{ij} \\
\epsilon_{ij} &amp; \sim \text{Normal} (0, \sigma_\epsilon^2) \\
U_{0i} &amp; \sim \text{Normal} (0, \sigma_0^2)
\end{align*}
\]</span></p>
<p>What are the default priors?</p>
<pre class="r"><code>#  get_prior(data = alcohol1_pp, 
#           family = gaussian,
#           alcuse ~ 1 + (1 | id))</code></pre>
<div id="using-priors" class="section level3">
<h3>Using priors</h3>
<p>How can we put that in directly to our code?</p>
<pre class="r"><code>ex2 &lt;-
  brm(data = alcohol1_pp, 
      family = gaussian,
      alcuse ~ 1 + (1 | id),
      prior = c(prior(student_t(3, 1, 10), class = Intercept),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)</code></pre>
<p>Visualizing priors.</p>
<pre class="r"><code>library(metRology)</code></pre>
<pre><code>## 
## Attaching package: &#39;metRology&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     cbind, rbind</code></pre>
<pre class="r"><code>tibble(x = seq(from = -100, to = 100, length.out = 1e3)) %&gt;%
  mutate(density = metRology::dt.scaled(x, df = 3, mean = 1, sd = 10)) %&gt;% 
  
  ggplot(aes(x = x, y = density)) +
  geom_vline(xintercept = 0, color = &quot;white&quot;) +
  geom_line() +
  labs(title = expression(paste(&quot;prior for &quot;, gamma[0][0])),
       x = &quot;parameter space&quot;) +
  theme(panel.grid = element_blank())</code></pre>
<p><img src="/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>Note a few things: First, this is a broad space for our intercept based on what we are looking at. This would be considering minimally informative.</p>
<p>Second, consider the variance priors – they go below zero. Does this make sense?</p>
<p>Here are the results.</p>
<pre class="r"><code>summary(ex2)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: alcuse ~ 1 + (1 | id) 
##    Data: alcohol1_pp (Number of observations: 246) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 82) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.77      0.08     0.62     0.94 1.00     1662     2274
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.92      0.10     0.72     1.12 1.00     2602     2734
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.76      0.04     0.68     0.84 1.00     3219     3327
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<pre class="r"><code># post2 &lt;- posterior_samples(ex2)</code></pre>
<p>Since all we’re interested in are the variance components, we’ll <code>select()</code> out the relevant columns from <code>post</code>, compute the squared versions, and save the results in a mini data frame, <code>v</code>.</p>
<pre class="r"><code># v &lt;-
#   post2 %&gt;% 
#   select(sigma, sd_id__Intercept)
# 
# head(v)</code></pre>
<p>Note these are in SD units</p>
<pre class="r"><code># v %&gt;% 
#   gather() %&gt;% 
#   
#   ggplot(aes(x = value)) +
#   geom_vline(xintercept = c(.25, .5, .75, 1), color = &quot;white&quot;) +
#   geom_density(size = 0, fill = &quot;grey&quot;) +
#   scale_x_continuous(NULL, limits = c(0, 1.25),
#                      breaks = seq(from = 0, to = 1.25, by = .25)) +
#   scale_y_continuous(NULL, breaks = NULL) +
#   theme(panel.grid = element_blank()) +
#   facet_wrap(~key, scales = &quot;free_y&quot;)</code></pre>
</div>
<div id="calculate-icc." class="section level3">
<h3>Calculate ICC.</h3>
<p>Note that the formula uses variances. ‘brms’ gives us SDs
<span class="math display">\[
ICC = \frac{\sigma_0^2}{\sigma_0^2 + \sigma_\epsilon^2}
\]</span></p>
<pre class="r"><code># v %&gt;%
#   transmute(ICC = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&gt;% 
#   ggplot(aes(x = ICC)) +
#   geom_density(size = 0, fill = &quot;grey&quot;) +
#   scale_x_continuous( limits = 0:1) +
#   scale_y_continuous(NULL, breaks = NULL) </code></pre>
<p>Note we get a distribution of ICCs, not just a singular score! Not all of our samples show as strong of between person association. Note that measuring this dispersion is a feature, not a problem. With standard MLM we are not taking into account potential sampling variance that may influence our estimates. Bayesian does.</p>
</div>
<div id="adding-predictors" class="section level3">
<h3>Adding predictors</h3>
<p>Using the composite formula, our next model, the unconditional growth model, follows the form</p>
<p><span class="math display">\[
\begin{align*}
\text{alcuse}_{ij} &amp; = \gamma_{00} + \gamma_{10} \text{age_14}_{ij} + U_{0j} + U_{1j} \text{age_14}_{ij} + \epsilon_{ij} \\
\epsilon_{ij} &amp; \sim \text{Normal} (0, \sigma_\epsilon^2) \\
\begin{bmatrix} U_{0j} \\ U_{1j} \end{bmatrix} &amp; \sim \text{Normal} 
\Bigg ( 
\begin{bmatrix} 0 \\ 0 \end{bmatrix}, 
\begin{bmatrix} \sigma_0^2 &amp; \sigma_{01} \\ \sigma_{01} &amp; \sigma_1^2 \end{bmatrix}
\Bigg )
\end{align*}
\]</span></p>
<pre class="r"><code># get_prior(data = alcohol1_pp, 
#           family = gaussian,
#           alcuse ~ 0 + intercept + age_14 + (1 + age_14 | id))</code></pre>
<p>Note that there are flat priors as the default for fixed effects. This essentially disregards the prior and spits back the likelihood, equivalent to the ML estimate.</p>
<pre class="r"><code>ex2.fit2 &lt;-
  brm(data = alcohol1_pp, 
      family = gaussian,
      alcuse ~ 0 + intercept + age_14 + (1 + age_14 | id),
      prior = c(prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(1), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)</code></pre>
</div>
<div id="marginal-effects" class="section level3">
<h3>Marginal effects</h3>
<pre class="r"><code># marginal_effects(ex2.fit2)</code></pre>
<pre class="r"><code>ex2.fit3 &lt;-
  brm(data = alcohol1_pp, 
      family = gaussian,
      alcuse ~ 0 + intercept + age_14 + coa + age_14:coa + (1 + age_14 | id),
      prior = c(prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(1), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)</code></pre>
<pre class="r"><code>summary(ex2.fit3)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: alcuse ~ 0 + intercept + age_14 + coa + age_14:coa + (1 + age_14 | id) 
##    Data: alcohol1_pp (Number of observations: 246) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 82) 
##                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)             0.70      0.10     0.50     0.90 1.00      763
## sd(age_14)                0.37      0.09     0.14     0.53 1.01      339
## cor(Intercept,age_14)    -0.10      0.28    -0.51     0.66 1.01      486
##                       Tail_ESS
## sd(Intercept)             1455
## sd(age_14)                 278
## cor(Intercept,age_14)      342
## 
## Population-Level Effects: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## intercept      0.32      0.13     0.06     0.57 1.00     2043     2563
## age_14         0.29      0.09     0.13     0.46 1.00     2902     2796
## coa            0.74      0.20     0.34     1.14 1.00     1992     2295
## age_14:coa    -0.05      0.13    -0.30     0.20 1.00     2931     3067
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.61      0.05     0.52     0.72 1.01      416      672
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<pre class="r"><code># marginal_effects(ex2.fit3)</code></pre>
<pre class="r"><code>fit3_f &lt;-
  update(ex2.fit3,
         newdata = alcohol1_pp %&gt;% mutate(coa = factor(coa)))</code></pre>
<pre class="r"><code># marginal_effects(fit3_f)</code></pre>
<pre class="r"><code># marginal_effects(fit3_f,
#                  effects = &quot;coa&quot;)
# ```
# 
# ```{r}
# marginal_effects(fit3_f,
#                  effects = &quot;coa:age_14&quot;)
# ```
# 
# 
# ```{r}
# marginal_effects(fit3_f,
#                  effects = &quot;age_14:coa&quot;)</code></pre>
<p>We can use fitted function to create “predicted” values, much like we did with lmer</p>
<pre class="r"><code>nd &lt;- 
  tibble(age_14 = seq(from = 0, to = 2, length.out = 30))

f &lt;- 
  fitted(ex2.fit2, 
         newdata = nd,
         re_formula = NA) %&gt;%
  data.frame() %&gt;%
  bind_cols(nd) %&gt;% 
  mutate(age = age_14 + 14)

head(f)</code></pre>
<pre><code>##    Estimate Est.Error      Q2.5     Q97.5     age_14      age
## 1 0.6464935 0.1110443 0.4285175 0.8709599 0.00000000 14.00000
## 2 0.6652664 0.1091008 0.4513743 0.8852507 0.06896552 14.06897
## 3 0.6840392 0.1073183 0.4766966 0.8986808 0.13793103 14.13793
## 4 0.7028121 0.1057051 0.4992086 0.9142175 0.20689655 14.20690
## 5 0.7215850 0.1042690 0.5198633 0.9306017 0.27586207 14.27586
## 6 0.7403579 0.1030174 0.5407352 0.9446840 0.34482759 14.34483</code></pre>
<pre class="r"><code>f %&gt;%
  ggplot(aes(x = age)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),
              fill = &quot;grey75&quot;, alpha = 3/4) +
  geom_line(aes(y = Estimate)) +
  scale_y_continuous(&quot;alcuse&quot;, breaks = 0:2, limits = c(0, 2)) +
  coord_cartesian(xlim = 13:17) +
  theme(panel.grid = element_blank())</code></pre>
<p><img src="/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
</div>
<div id="comparing-models" class="section level3">
<h3>Comparing models</h3>
<p>As it turns out, we Bayesians use the log-likelihood (LL), too. Recall how the numerator in the right-hand side of Bayes’ Theorem was <span class="math inline">\(p(\text{data} | \theta) p(\theta)\)</span>? That first part, <span class="math inline">\(p(\text{data} | \theta)\)</span>, is the likelihood. In words, the likelihood is the <em>probability of the data given the parameters</em>. And we take the log of the likelihood rather than the likelihood itself because it’s easier to work with statistically.</p>
<p>When you’re working with <strong>brms</strong>, you can extract the LL with the <code>log_lik()</code> function. Here’s an example with <code>fit1</code>, our unconditional means model.</p>
<pre class="r"><code># log_lik(ex2) %&gt;% 
#   str()</code></pre>
<p>You may have noticed we didn’t just get a single value back. Rather, we got an array of 4000 rows and 246 columns. The reason we got 4000 rows is because that’s how many post-warmup iterations we drew from the posterior. I.e., we set <code>brm(..., iter = 2000, warmup = 1000, chains = 4)</code>. With respect to the 246 columns, that’s how many rows there are in the <code>alcohol1_pp</code> data. So for each person in the data set, we get an entire posterior distribution of LL values.</p>
<pre class="r"><code># ll &lt;-
#   log_lik(ex2) %&gt;%
#   data.frame() %&gt;% 
#   mutate(sums     = rowSums(.)) %&gt;% 
#   mutate(deviance = -2 * sums) %&gt;% 
#   select(sums, deviance, everything())</code></pre>
<pre class="r"><code># ll %&gt;% 
#   ggplot(aes(x = deviance)) +
#   geom_density(fill = &quot;grey25&quot;, size = 0) +
#   scale_y_continuous(NULL, breaks = NULL) +
#   theme(panel.grid = element_blank())</code></pre>
<p>The AIC is frequentist and cannot handle models with priors. The BIC isis a misnomer as it is not Bayesian. The Widely Applicable Information Criterion (WAIC) is used instead.</p>
<p>The distinguishing feature of WAIC is that it is <em>pointwise</em>. This means that uncertainty in prediction is considered case-by-case, or point-by-point, in the data. This is useful, because some observations are much harder to predict than others and may also have different uncertainty. You can think of WAIC as handling uncertainty where it actually matters: for each independent observation.</p>
<pre class="r"><code># waic(ex2)</code></pre>
<p>For the statistic in each row, you get a point estimate and a standard error. The WAIC is on the bottom. The effective number of parameters, the <span class="math inline">\(p_\text{WAIC}\)</span>, is in the middle. Notice the <code>elpd_waic</code> on the top. That’s what you get without the <span class="math inline">\(-2 \times ...\)</span> in the formula. Remember how that part is just to put things in a metric amenable to <span class="math inline">\(\chi^2\)</span> difference testing? Well, not all Bayesians like that and within the <strong>Stan</strong> ecosystem you’ll also see the WAIC expressed instead as the <span class="math inline">\(\text{elpd}_\text{WAIC}\)</span>.</p>
<p>The current recommended workflow within <strong>brms</strong> is to attach the WAIC information to the model fit. You do it with the <code>add_criterion()</code> function.</p>
<pre class="r"><code># ex2 &lt;- add_criterion(ex2, &quot;waic&quot;)</code></pre>
<pre class="r"><code>ex2$waic</code></pre>
<pre><code>## NULL</code></pre>
<p>Leave-one-out cross-validation (LOO-CV).</p>
<p>Cross validation is quickly becoming the primary method to examine fit and utility of one’s model. The hope is our findings would generalize to other data we could have collected or may collect in the future. We’d like our findings to tell us something more general about the world at large. But we don’t have all the data and we typically don’t even know what all the relevant variables are. That is where validation comes in.</p>
<p>k-fold is a common type of CV. As <span class="math inline">\(k\)</span> increases, the number of cases with a fold get smaller. In the extreme, <span class="math inline">\(k = N\)</span>, the number of cases within the data. At that point, <span class="math inline">\(k\)</span>-fold cross-validation turns into leave-one-out cross-validation (LOO-CV).</p>
<p>But there’s a practical difficulty with LOO-CV: it’s costly. As you may have noticed, it takes some time to fit a Bayesian multilevel model. For large data and/or complicated models, sometimes it takes hours or days. Most of us just don’t have enough time or computational resources to fit that many models. Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO) as an efficient way to approximate true LOO-CV.</p>
<pre class="r"><code># l_fit1 &lt;- loo(ex2)
# 
# print(l_fit1)</code></pre>
<p>Comparing models with the WAIC and LOO.</p>
<pre class="r"><code># ex2 &lt;- add_criterion(ex2, c(&quot;loo&quot;, &quot;waic&quot;))
# ex2.fit2 &lt;- add_criterion(ex2.fit2, c(&quot;loo&quot;, &quot;waic&quot;))
# ex2.fit3 &lt;- add_criterion(ex2.fit3, c(&quot;loo&quot;, &quot;waic&quot;))</code></pre>
<p>The point to focus on, here, is we can use the <code>loo_compare()</code> function to compare fits by their WAIC or LOO. Let’s practice with the WAIC.</p>
<pre class="r"><code># ws &lt;- loo_compare(ex2, ex2.fit2, ex2.fit3, criterion = &quot;waic&quot;)
# 
# print(ws)</code></pre>
<p>And if you wanted a more focused comparison, say between <code>fit1</code> and <code>fit2</code>, you’d just simplify your input.</p>
<pre class="r"><code># loo_compare(ex2, ex2.fit2, criterion = &quot;loo&quot;) %&gt;% 
#   print(simplify = F)</code></pre>
</div>
<div id="random-effects-revisited" class="section level3">
<h3>Random effects revisited</h3>
<p>For one person</p>
<pre class="r"><code>alcohol1_pp %&gt;% 
  select(id:coa, cpeer, alcuse) %&gt;% 
  filter(id == 23)</code></pre>
<pre><code>## # A tibble: 3 x 5
##      id   age   coa cpeer alcuse
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1    23    14     1 -1.02   1   
## 2    23    15     1 -1.02   1   
## 3    23    16     1 -1.02   1.73</code></pre>
<pre class="r"><code># post_23 &lt;-
#   posterior_samples(ex2.fit3) %&gt;% 
#   select(starts_with(&quot;b_&quot;)) %&gt;% 
#   mutate(`gamma[0][&quot;,23&quot;]` = b_intercept + b_coa * 1 ,
#          `gamma[1][&quot;,23&quot;]` = b_age_14 + `b_age_14:coa`)
# 
# head(post_23)</code></pre>
<pre class="r"><code># post_23 %&gt;% 
#   select(starts_with(&quot;gamma&quot;)) %&gt;% 
#   gather() %&gt;% 
#   group_by(key) %&gt;% 
#   summarise(mean = mean(value),
#             ll = quantile(value, probs = .025),
#             ul = quantile(value, probs = .975)) %&gt;% 
#   mutate_if(is.double, round, digits = 3)</code></pre>
<pre class="r"><code># post_23 %&gt;% 
#   select(starts_with(&quot;gamma&quot;)) %&gt;% 
#   gather() %&gt;% 
#   
#   ggplot(aes(x = value)) +
#   geom_density(size = 0, fill = &quot;grey25&quot;) +
#   scale_y_continuous(NULL, breaks = NULL) +
#   xlab(&quot;participant-specific parameter estimates&quot;) +
#   theme(panel.grid = element_blank()) +
#   facet_wrap(~key, labeller = label_parsed, scales = &quot;free_y&quot;)</code></pre>
<p>Yet this approach neglects the <span class="math inline">\(U\)</span>s. W’ve been extracting the <span class="math inline">\(U\)</span>s with <code>ranef()</code>. We also get them when we use <code>posterior_samples()</code>. Here we’ll extract both the <span class="math inline">\(\gamma\)</span>s as well as the <span class="math inline">\(U\)</span>s for <code>id == 23</code>.</p>
<pre class="r"><code># post_23 &lt;-
#   posterior_samples(ex2.fit3) %&gt;% 
#   select(starts_with(&quot;b_&quot;), contains(&quot;23&quot;))
# 
# glimpse(post_23)</code></pre>
<p>With the <code>r_id</code> prefix, <strong>brms</strong> tells you these are residual estimates for the levels in the <code>id</code> grouping variable. Within the brackets, we learn these particular columns are for <code>id == 23</code>, the first with respect to the <code>Intercept</code> and second with respect to the <code>age_14</code> parameter. Let’s put them to use.</p>
<pre class="r"><code># post_23 &lt;-
#   post_23 %&gt;% 
#   mutate(`beta[0][&quot;,23&quot;]` = b_intercept + b_coa * 1  + `r_id[23,Intercept]`,
#          `beta[1][&quot;,23&quot;]` = b_age_14 + `r_id[23,age_14]`)
# 
# glimpse(post_23)</code></pre>
<pre class="r"><code># post_23 %&gt;% 
#   select(starts_with(&quot;beta&quot;)) %&gt;% 
#   gather() %&gt;% 
#   group_by(key) %&gt;% 
#   summarise(mean = mean(value),
#             ll = quantile(value, probs = .025),
#             ul = quantile(value, probs = .975)) %&gt;% 
#   mutate_if(is.double, round, digits = 3)</code></pre>
<pre class="r"><code># post_23 %&gt;% 
#   select(starts_with(&quot;beta&quot;)) %&gt;% 
#   gather() %&gt;% 
#   
#   ggplot(aes(x = value)) +
#   geom_density(size = 0, fill = &quot;grey25&quot;) +
#   scale_y_continuous(NULL, breaks = NULL) +
#   xlab(&quot;participant-specific parameter estimates&quot;) +
#   theme(panel.grid = element_blank()) +
#   facet_wrap(~key, labeller = label_parsed, scales = &quot;free_y&quot;)</code></pre>
</div>
<div id="variance-explained" class="section level3">
<h3>Variance explained</h3>
</div>
<div id="hypothesis-function" class="section level3">
<h3>Hypothesis function</h3>
</div>
<div id="update-function-again" class="section level3">
<h3>Update function again</h3>
</div>
</div>
<div id="thanks" class="section level2">
<h2>Thanks</h2>
<p>Many thanks to Solomon Kurz’s github for code</p>
</div>
</div>

    </div>

    


    



    
      








  
  
  







      
      
    

    
    <div class="article-widget">
      
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/workshops/workshop-4/" rel="prev">Workshop #4</a>
  </div>
  
</div>

    </div>
    

    


  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.0.0/mermaid.min.js" integrity="sha256-0w92bcB21IY5+rGI84MGj52jNfHNbXVeQLrZ0CGdjNY=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.16bbb3750feb7244c9bc409a5a4fe678.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms</a>
        
  </p>
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
