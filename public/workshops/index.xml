<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Workshops | Applied Longitudinal Data Analysis</title>
    <link>/workshops/</link>
      <atom:link href="/workshops/index.xml" rel="self" type="application/rss+xml" />
    <description>Workshops</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 26 Sep 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Workshops</title>
      <link>/workshops/</link>
    </image>
    
    <item>
      <title>workshop#5</title>
      <link>/workshops/workshop-5/</link>
      <pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-5/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#how-is-this-different&#34;&gt;How is this different?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-basic-parts&#34;&gt;The basic parts&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#posterior-predictive-distribution&#34;&gt;Posterior predictive distribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-think-about-these-models&#34;&gt;How to think about these models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mcmc-estimation&#34;&gt;MCMC Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-lmer-with-brms&#34;&gt;Comparing lmer with brms&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-priors-did-we-use&#34;&gt;What priors did we use?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#differenes-in-random-effects-with-brms&#34;&gt;Differenes in random effects with brms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#more-models&#34;&gt;More models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#ex-1&#34;&gt;Ex 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#intercepts-are-different-because-of-priors&#34;&gt;Intercepts are different because of priors&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#lets-fit-it-again-without-that-intercept.&#34;&gt;Lets fit it again without that intercept.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-does-the-posterior-look-like&#34;&gt;What does the posterior look like?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects&#34;&gt;Random effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ex-2&#34;&gt;Ex 2&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#using-priors&#34;&gt;Using priors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#calculate-icc.&#34;&gt;Calculate ICC.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-predictors&#34;&gt;Adding predictors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#marginal-effects&#34;&gt;Marginal effects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-models&#34;&gt;Comparing models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects-revisited&#34;&gt;Random effects revisited&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance-explained&#34;&gt;Variance explained&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hypothesis-function&#34;&gt;Hypothesis function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#update-function-again&#34;&gt;Update function again&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#thanks&#34;&gt;Thanks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;#Why Bayesian?
The models we have been working with can easily be done in a Bayesian Framework. Why Bayes? For at least 3 reasons: 1. Better convergence. 2. More flexibility. 3. Fewer assumptions. We will go through each of these ideas throughout the course in more detail, so what I want to do is sell you not on the benefits but on the lack of difference.&lt;/p&gt;
&lt;p&gt;Many places to read up on this. Try Kruschke’s Bayesian new statistics: &lt;a href=&#34;https://rdcu.be/bRUvW&#34; class=&#34;uri&#34;&gt;https://rdcu.be/bRUvW&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;how-is-this-different&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How is this different?&lt;/h1&gt;
&lt;p&gt;Bayesian analysis differs in two major ways from our traditional MLMs that we have been working with. First, the end result is different in that it uses probability differently to derive estimates and to interpret the results. The lucky thing for us is that this will not be drastically different if we dont want it to be. In other words, we can keep with our focuses on estimates and precision around those estimates, same way as we would in standard stats world. Once you progress further, you can better understand the nuances, but right now lets not get bogged down by technical differences.&lt;/p&gt;
&lt;p&gt;The second major difference is that prior are used. Priors are a way to incorporate your beliefs into the model. At first blush it feels as if this is wrong, and is the justification for many for why the standard approach is correct and Bayesian is wrong. I mean, most people are taught frequentist approaches, thus how can 10 million SPSS users be wrong? (The reason for the popularity of frequentist approaches is two fold: computers and history. Computation power is needed, which was lacking until recently and thus curtailed general use, the same way that computation power held back adoption of SEM and before that multivariate approaches like factor analysis, and before that multiple regression with continuous predictors. The second reason, history, is, like much of history, driven by disagreements between dead white guys. Fisher, you’ve heard of him, populated the p-value and disliked Bayes, so long story short, it went out of fashion)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-basic-parts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The basic parts&lt;/h1&gt;
&lt;p&gt;Bayes Theorm
&lt;span class=&#34;math display&#34;&gt;\[ P(\theta | y) =  \frac{P(y | \theta) P(\theta)}{P(y)}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is often rewritten for when we want to estimate some value, $ $. Note that the &lt;span class=&#34;math inline&#34;&gt;\(P(y)\)&lt;/span&gt; drops out, as it does not vary across $ $. In our case y is will be the data, and the data will be collected only once, thus considered fixed. The probabiliy o y was included in the above equation to rescale the equation and create some nice properities eg integrating to 1. We don’t care about that as we are going to look at relative likelihoods of each theta conditioned on y.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(\theta | y) \propto P(y | \theta) P(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This reads as the conditional probability of theta, given y is proportional to the probability of y given theta, multiplied by the probability of theta. The this, likely, sounds like gobblygook unless you have taken a bunch of probability classes. Instead the more helpful way to think about this is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ p(hypothesis|data) \propto p(data|hypothesis)p(hypothesis)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is what we get: the probability our hypothesis. Not the standard p value interpretation of the probability of our data given some hypothesis like the null distribution.&lt;/p&gt;
&lt;p&gt;Bayesian stats gives names to each of these terms:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ posterior \propto likelihood * prior \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Prior - Allows us to provide a priori intuition about what the findings are. It is of the form of a probability distribution. E.g., “Extrodanary claims require extrodinary evidene.” Note we will note use this as “guesses” of what will happen. Instead we will use this as a regularization tool, similar to partial pooling in MLM.&lt;/p&gt;
&lt;p&gt;Likelihood - Distribution of the likelihood of various hypothesis. Probability attaches to possible results; likelihood attaches to hypotheses. It is akin to flipping coins, something we did with binomials.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1          ✔ purrr   0.3.2     
## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   1.0.0.9000     ✔ stringr 1.4.0     
## ✔ readr   1.3.1          ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For 7 successes out of 10 trials, what is the likelihood of different probabilities?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(prob = seq(from = 0, to = 1, by = .01)) %&amp;gt;% 
  ggplot(aes(x = prob,
             y = dbinom(x = 7, size = 10, prob = prob))) +
  geom_line() +
  labs(x = &amp;quot;probability&amp;quot;,
       y = &amp;quot;binomial likelihood&amp;quot;) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(prob = seq(from = 0, to = 1, by = .01)) %&amp;gt;% 
  ggplot(aes(x = prob,
             y = dbinom(x = 4, size = 10, prob = prob))) +
  geom_line() +
  labs(x = &amp;quot;probability&amp;quot;,
       y = &amp;quot;binomial likelihood&amp;quot;) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It can be made similar to the maximum likelihood estimation (with flat priors), but think about this as nothing different than your standard estimation procedure.&lt;/p&gt;
&lt;p&gt;Posterior – distribution of our belief about the parameter values after taking into account the likelihood and one’s priors. In regression terms, it is not a specific value of b that would make the data most likely, but a probability distribution for b that serves as a weighted combination of the likelihood and prior.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sequence_length &amp;lt;- 1e3

d &amp;lt;-
  tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %&amp;gt;% 
  expand(probability, row = c(&amp;quot;flat&amp;quot;, &amp;quot;stepped&amp;quot;, &amp;quot;Laplace&amp;quot;)) %&amp;gt;% 
  arrange(row, probability) %&amp;gt;% 
  mutate(prior = ifelse(row == &amp;quot;flat&amp;quot;, 1,
                        ifelse(row == &amp;quot;stepped&amp;quot;, rep(0:1, each = sequence_length / 2),
                               exp(-abs(probability - .5) / .25) / ( 2 * .25))),
         likelihood = dbinom(x = 6, size = 9, prob = probability)) %&amp;gt;% 
  group_by(row) %&amp;gt;% 
  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %&amp;gt;% 
  gather(key, value, -probability, -row) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(key = factor(key, levels = c(&amp;quot;prior&amp;quot;, &amp;quot;likelihood&amp;quot;, &amp;quot;posterior&amp;quot;)),
         row = factor(row, levels = c(&amp;quot;flat&amp;quot;, &amp;quot;stepped&amp;quot;, &amp;quot;Laplace&amp;quot;)))
p1 &amp;lt;-
  d %&amp;gt;%
  filter(key == &amp;quot;prior&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = &amp;quot;prior&amp;quot;) +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = &amp;quot;free_y&amp;quot;, ncol = 1)

p2 &amp;lt;-
  d %&amp;gt;%
  filter(key == &amp;quot;likelihood&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = &amp;quot;likelihood&amp;quot;) +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = &amp;quot;free_y&amp;quot;, ncol = 1)

p3 &amp;lt;-
  d %&amp;gt;%
  filter(key == &amp;quot;posterior&amp;quot;) %&amp;gt;% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = &amp;quot;posterior&amp;quot;) +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = &amp;quot;free_y&amp;quot;, ncol = 1)

library(gridExtra)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;gridExtra&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     combine&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid.arrange(p1, p2, p3, ncol = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can describe our&lt;/p&gt;
&lt;p&gt;Another way to think about it:
updated belief = current evidence ∗ prior belief or evidence&lt;/p&gt;
&lt;div id=&#34;posterior-predictive-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior predictive distribution&lt;/h2&gt;
&lt;p&gt;Once we have the posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, we can then feed new or unobserved data into the data generating process and get new distributions for any potential observation. We can use this to make predictions, check if the model is correct, and to evaluate model fit.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-think-about-these-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How to think about these models&lt;/h1&gt;
&lt;p&gt;In standard regression, we can state some expectations up front:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
h_i  \sim \text{Normal}(\mu_i, \sigma) \\
\mu_i  = \alpha + \beta x_i \\
\alpha  \sim \text{Normal}(0, 10) \\
\beta  \sim \text{Normal}(0, 10) \\
\sigma  \sim \text{Half Cauchy}(0, 50)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Y_{i}  \sim \text{Normal}(\mu_i, \sigma) \]&lt;/span&gt;
This specifies the DGP and the family argument within &lt;code&gt;brms&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \mu_i  = \beta \times Predictor_i \]&lt;/span&gt;
This specifies themodel we are fitting. Here it is just a standard regression.&lt;/p&gt;
&lt;p&gt;The above two lines are what is implicitly implied in almost all regressions. Namely, we have some variable that is being generated from a normal distribution, with a mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. That &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is going to be described as the relationship between some (fixed) regression coefficient and a person specific predictor variable.&lt;/p&gt;
&lt;p&gt;Below is where we get into some new stuff, where we describe what we think the estimated parameters will look like (ie format a prior).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha \sim \text{Normal}(0, 10) \\\)&lt;/span&gt;
This says we have a prior idea bout the distribution of the intercept. Namely that it is centered around 0 with a possible range above and below that.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta \sim \text{Normal}(0, 10)\)&lt;/span&gt;
This says that we have a prior idea about the possible distribution of the regression coefficient. We think it is most likely zero, but we aren’t that confident. As it could be higher or lower.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  aes(x = c(-40, 40)) +
  stat_function(fun = dnorm, n = 200, args = list(0, 10)) +
  labs(title = &amp;quot;Normal (Gaussian) distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now for the priors for our residual.
$(0, 1) $
Same idea for the residual, the prior specifies what we expect is plausible.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot() +
  aes(x = c(0, 10)) +
  stat_function(fun = dcauchy, n = 200, args = list(0, 1)) +
  labs(title = &amp;quot;Half Cauchy distribution&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that we do two important things here. First, we describe the data generating processes (DGP) we think our data are coming from. Often we will just assume Gaussian normal, but this does not need to be the case. For our residual we specify a half cauchy to make sure it is above zero and that lower terms are more likely than higher terms.&lt;/p&gt;
&lt;p&gt;Second, we are specifying a prior. Here we are saying we expect our regression parameter to be centered around zero but that it could vary widely from that and we wouldn’t be suprised. The prior distribution for Beta is defined by a mean of 0 and an SD of 10. But we have control if we want to make this different. Same thing with the half cauchy for the variance. We expect that it cannot be negative, that it is likely closer to zero than not. Again, it is up to us in terms of how we want to specify it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mcmc-estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;MCMC Estimation&lt;/h1&gt;
&lt;p&gt;Bayesian estimation, like maximum likelihood, starts with initial guesses as starting points and then runs in an iterative fashion, producing simulated draws from the posterior distribution. Think of taking one of the posterior samples above and randomly selecting values from it, again and again. Lets take a sample the size of your study and call that sample 1. Then do that same procedure 4000 times. The result is that you have a lot of potential samples that can be combined in multiple ways. This part is key and different from frequentist statistics. We are aiming for creating a distribution of end goals as our end goal, not just a point estimate.&lt;/p&gt;
&lt;p&gt;The simulation process is referred to as Markov Chain Monte Carlo, or MCMC for short. In MCMC, all of the simulated draws from the posterior are based on and correlated with previous draws. We will typically allow the process to “warm up”, as the initial random starting point may be way off of being plausible. As it runs, however, these estimates will get better and better, creating a distribution of plausible values. As a safety check, we will run the the process multiple times, what is known as having multiple chains. If multiple chains converge towards the same answer we are more confident in our results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-lmer-with-brms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing lmer with brms&lt;/h1&gt;
&lt;p&gt;Lets do a mixed effects model to test this out. We will use the sleepstudy dataset that is loaded with lme4&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;Matrix&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;quot;sleepstudy&amp;quot;)
head(sleepstudy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   Reaction Days Subject
## 1 249.5600    0     308
## 2 258.7047    1     308
## 3 250.8006    2     308
## 4 321.4398    3     308
## 5 356.8519    4     308
## 6 414.6901    5     308&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write.csv(sleepstudy, file = &amp;quot;sleepstudy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sleep_lmer &amp;lt;- lmer(Reaction ~ Days + (1 + Days|Subject), data = sleepstudy)
summary(sleep_lmer)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: Reaction ~ Days + (1 + Days | Subject)
##    Data: sleepstudy
## 
## REML criterion at convergence: 1743.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.9536 -0.4634  0.0231  0.4633  5.1793 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  Subject  (Intercept) 611.90   24.737       
##           Days         35.08    5.923   0.07
##  Residual             654.94   25.592       
## Number of obs: 180, groups:  Subject, 18
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  251.405      6.824  36.843
## Days          10.467      1.546   6.771
## 
## Correlation of Fixed Effects:
##      (Intr)
## Days -0.138&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;xts&amp;#39;:
##   method     from
##   as.zoo.xts zoo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading &amp;#39;brms&amp;#39; package (version 2.10.0). Useful instructions
## can be found by typing help(&amp;#39;brms&amp;#39;). A more detailed introduction
## to the package is available through vignette(&amp;#39;brms_overview&amp;#39;).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;brms&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:lme4&amp;#39;:
## 
##     ngrps&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sleep_brm &amp;lt;- brm(Reaction ~ Days + (1 + Days|Subject), data = sleepstudy, file = &amp;quot;fit1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sleep_brm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;xts&amp;#39;:
##   method     from
##   as.zoo.xts zoo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: Reaction ~ Days + (1 + Days | Subject) 
##    Data: sleepstudy (Number of observations: 180) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~Subject (Number of levels: 18) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)          27.20      6.94    16.21    43.45 1.00     1745
## sd(Days)                6.65      1.57     4.14    10.36 1.00     1437
## cor(Intercept,Days)     0.08      0.30    -0.48     0.68 1.00     1096
##                     Tail_ESS
## sd(Intercept)           2348
## sd(Days)                1878
## cor(Intercept,Days)     1800
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept   251.16      7.62   235.92   266.63 1.00     1813     1960
## Days         10.40      1.68     7.19    13.65 1.00     1478     2246
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma    25.86      1.57    23.04    29.16 1.00     3680     3030
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;what-priors-did-we-use&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What priors did we use?&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brms::get_prior(Reaction ~ Days + (1 + Days|Subject), data = sleepstudy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    prior     class      coef   group resp dpar nlpar bound
## 1                                b                                        
## 2                                b      Days                              
## 3                 lkj(1)       cor                                        
## 4                              cor           Subject                      
## 5  student_t(3, 289, 59) Intercept                                        
## 6    student_t(3, 0, 59)        sd                                        
## 7                               sd           Subject                      
## 8                               sd      Days Subject                      
## 9                               sd Intercept Subject                      
## 10   student_t(3, 0, 59)     sigma&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;*Note that there are flat priors as the default for fixed effects. This essentially disregards the prior and spits back the likelihood, equivalent to the ML estimate.&lt;/p&gt;
&lt;p&gt;The ts have three parameters: degress of freedom, mean, and then an SD.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;differenes-in-random-effects-with-brms&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Differenes in random effects with brms&lt;/h2&gt;
&lt;p&gt;We often summarize the &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{0j}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\gamma_{1j}\)&lt;/span&gt; deviations as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2\)&lt;/span&gt;, respectively. And importantly, these variance parameters have a covariance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{01}\)&lt;/span&gt;. However, &lt;strong&gt;brms&lt;/strong&gt; parameterizes these in the standard-deviation metric. That is, in &lt;strong&gt;brms&lt;/strong&gt;, these are expressed as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1\)&lt;/span&gt;. Similarly, the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{01}\)&lt;/span&gt; presented in &lt;strong&gt;brms&lt;/strong&gt; output is in a correlation metric, rather than a covariance.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\begin{bmatrix} U_{0i} \\ U_{1i} \end{bmatrix} &amp;amp; 
\sim \text{N} 
\bigg ( \begin{bmatrix} 0 \\ 0 \end{bmatrix},  
\begin{bmatrix} \sigma_0^2 &amp;amp; \sigma_{01}\\ \sigma_{01} &amp;amp; \sigma_1^2 \end{bmatrix}
\bigg )
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note, that in many Bayesian texts, the Level 2 covariance matrix above is often rewritten as below.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
U \sim \text{N} (\mathbf{0}, \mathbf{\Sigma})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{0}\)&lt;/span&gt; is the vector of 0 means and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\Sigma}\)&lt;/span&gt; is the variance/covariance matrix. In &lt;strong&gt;Stan&lt;/strong&gt;, and thus &lt;strong&gt;brms&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\Sigma}\)&lt;/span&gt; is decomposed&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\mathbf{\Sigma} &amp;amp; = \mathbf{D} \mathbf{\Omega} \mathbf{D}, \text{where} \\
\mathbf{D}      &amp;amp; = \begin{bmatrix} \sigma_0 &amp;amp; 0 \\ 0 &amp;amp; \sigma_1 \end{bmatrix} \text{and} \\
\mathbf{\Omega} &amp;amp; = \begin{bmatrix} 1 &amp;amp; \rho \\ \rho &amp;amp; 1 \end{bmatrix}
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\)&lt;/span&gt; is the diagonal matrix of standard deviations and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\Omega}\)&lt;/span&gt; is the correlation matrix. As we will see later, we will need to specify priors for each of these components, the SDs of our random effects and the correlation between them. By splitting them into these two different matrixes we can handle that easier.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(sleep_brm)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-12-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;more-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;More models&lt;/h1&gt;
&lt;div id=&#34;ex-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ex 1&lt;/h2&gt;
&lt;p&gt;Here we’ll use a dataset from Singer &amp;amp; Willet, chapter 3, lookng at cognitive performance across time for children who under went an intervention or not.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##       zeta_0     zeta_1
## 1 10.7586672 -3.0908765
## 2  3.4258938 -0.4186497
## 3 -3.0770183  0.2140130
## 4 12.5303603 -4.9043416
## 5 -2.1114641  0.8936950
## 6 -0.5521597 -0.6310265&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##      id gamma_00 gamma_01 gamma_10 gamma_11
##   &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1     1     108.     6.85    -21.1     5.27
## 2     2     108.     6.85    -21.1     5.27
## 3     3     108.     6.85    -21.1     5.27
## 4     4     108.     6.85    -21.1     5.27
## 5     5     108.     6.85    -21.1     5.27
## 6     6     108.     6.85    -21.1     5.27&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 13
##      id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c
##   &amp;lt;int&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0  
## 2     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0.5
## 3     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   1  
## 4     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0  
## 5     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0.5
## 6     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   1  
## # … with 4 more variables: epsilon &amp;lt;dbl&amp;gt;, pi_0 &amp;lt;dbl&amp;gt;, pi_1 &amp;lt;dbl&amp;gt;,
## #   cog &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 14
##      id gamma_00 gamma_01 gamma_10 gamma_11 zeta_0 zeta_1 program age_c
##   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0  
## 2     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   0.5
## 3     1     108.     6.85    -21.1     5.27  10.8  -3.09        1   1  
## 4     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0  
## 5     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   0.5
## 6     2     108.     6.85    -21.1     5.27   3.43 -0.419       1   1  
## # … with 5 more variables: epsilon &amp;lt;dbl&amp;gt;, pi_0 &amp;lt;dbl&amp;gt;, pi_1 &amp;lt;dbl&amp;gt;,
## #   cog &amp;lt;dbl&amp;gt;, age &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;id&amp;quot;, &amp;quot;age&amp;quot;, &amp;quot;cog&amp;quot;, &amp;quot;program&amp;quot;, &amp;quot;age_c&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 309
## Variables: 5
## $ id      &amp;lt;dbl&amp;gt; 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 7,…
## $ age     &amp;lt;dbl&amp;gt; 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2.0, 1.0, 1.5, 2…
## $ cog     &amp;lt;dbl&amp;gt; 117, 113, 109, 108, 112, 102, 112, 113, 85, 138, 110, 97…
## $ program &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ age_c   &amp;lt;dbl&amp;gt; 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1.0, 0.0, 0.5, 1…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(&amp;quot;early_int_sim.rda&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;early_int_sim &amp;lt;-
  early_int_sim %&amp;gt;% 
  mutate(label = str_c(&amp;quot;program = &amp;quot;, program)) 

early_int_sim %&amp;gt;% 
  ggplot(aes(x = age, y = cog, color = label)) +
  stat_smooth(aes(group = id),
              method = &amp;quot;lm&amp;quot;, se = F, size = 1/6) +
  stat_smooth(method = &amp;quot;lm&amp;quot;, se = F, size = 2) +
  scale_x_continuous(breaks = c(1, 1.5, 2)) +
  scale_color_viridis_d(option = &amp;quot;B&amp;quot;, begin = .33, end = .67) +
  ylim(50, 150) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;) +
  facet_wrap(~label)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is our assumed data generating model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
cog  \sim \text{Normal} (\mu_{ij}, \sigma_\epsilon^2) \\
\mu_{ij}    = \gamma_{00j} + \gamma_{10j} ({age}_{ij} - 1) + \gamma_{11j} Program \\
\gamma_{00}      \sim \text{Normal}(1.335, 1) \\
\gamma_{10}      \sim \text{Normal}(0, 0.5) \\
\sigma_\epsilon  \sim \text{Student-t} (3, 0, 1)\\ 
\sigma_0         \sim \text{Student-t} (3, 0, 1) \\
\sigma_1         \sim \text{Student-t} (3, 0, 1) \\
\rho_{01}        \sim \text{LKJ} (4) \\
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex1 &amp;lt;-
  brm(data = early_int_sim,
      file = &amp;quot;fit2&amp;quot;,
      family = gaussian,
      formula = cog ~ 0 + intercept + age_c + program + age_c:program + (1 + age_c | id),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.9),
      seed = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;intercepts-are-different-because-of-priors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intercepts are different because of priors&lt;/h2&gt;
&lt;p&gt;Another important part of the syntax concerns the intercept. Normally we include a 1 (or lmer does it for us automatically) to reflect we want to fit an intercept. If we did that here, we would have made the assumption that our predictors are mean centered. The default priors set by &lt;code&gt;brms::brm()&lt;/code&gt; are set based on this assumption. SO, if we want to use non mean centered predictors (eg setting time at initial wave or dummy variables), we need to respecify our model. Neither our variables are mean centered.&lt;/p&gt;
&lt;p&gt;With a &lt;code&gt;0 + intercept&lt;/code&gt;, we told &lt;code&gt;brm()&lt;/code&gt; to suppress the default intercept and replace it with our smartly-named &lt;code&gt;intercept&lt;/code&gt; parameter. This is our fixed effect for the population intercept and, importantly, &lt;code&gt;brms()&lt;/code&gt; will assign default priors to it based on the data themselves without assumptions about centering. We will speak later about changing these default priors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(ex1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: There were 29 divergent transitions after warmup. Increasing adapt_delta above 0.9 may help.
## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: cog ~ 0 + intercept + age_c + program + age_c:program + (1 + age_c | id) 
##    Data: early_int_sim (Number of observations: 309) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 103) 
##                      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)            9.67      1.16     7.60    11.97 1.00     1043
## sd(age_c)                3.85      2.39     0.20     9.00 1.01      386
## cor(Intercept,age_c)    -0.48      0.36    -0.96     0.53 1.00     3234
##                      Tail_ESS
## sd(Intercept)            2084
## sd(age_c)                 487
## cor(Intercept,age_c)     1903
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## intercept       106.53      1.95   102.45   110.26 1.00     1176     1359
## age_c           -20.52      1.98   -24.42   -16.64 1.00     2217     2872
## program           9.21      2.54     4.14    14.09 1.00     1244     1938
## age_c:program     3.20      2.66    -2.03     8.38 1.00     1767     1347
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     8.59      0.52     7.54     9.59 1.01      729      606
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  brms::get_prior(cog ~ 0 + intercept + age_c + program + age_c:program + (1 + age_c | id), data = early_int_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  prior class          coef group resp dpar nlpar bound
## 1                          b                                          
## 2                          b         age_c                            
## 3                          b age_c:program                            
## 4                          b     intercept                            
## 5                          b       program                            
## 6               lkj(1)   cor                                          
## 7                        cor                  id                      
## 8  student_t(3, 0, 16)    sd                                          
## 9                         sd                  id                      
## 10                        sd         age_c    id                      
## 11                        sd     Intercept    id                      
## 12 student_t(3, 0, 16) sigma&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;lets-fit-it-again-without-that-intercept.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Lets fit it again without that intercept.&lt;/h3&gt;
&lt;p&gt;What is the difference?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex1.nc &amp;lt;-
  brm(data = early_int_sim,
      file = &amp;quot;fit2.nc&amp;quot;,
      family = gaussian,
      formula = cog ~ 1 + age.c + program + age.c:program + (1 + age | id),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.9),
      seed = 3)
print(ex1.nc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  brms::get_prior(cog ~  1 + age_c + program + age_c:program + (1 + age_c | id), data = early_int_sim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                    prior     class          coef group resp dpar nlpar
## 1                                b                                    
## 2                                b         age_c                      
## 3                                b age_c:program                      
## 4                                b       program                      
## 5                 lkj(1)       cor                                    
## 6                              cor                  id                
## 7  student_t(3, 102, 16) Intercept                                    
## 8    student_t(3, 0, 16)        sd                                    
## 9                               sd                  id                
## 10                              sd         age_c    id                
## 11                              sd     Intercept    id                
## 12   student_t(3, 0, 16)     sigma                                    
##    bound
## 1       
## 2       
## 3       
## 4       
## 5       
## 6       
## 7       
## 8       
## 9       
## 10      
## 11      
## 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A few take aways: 1. not much as changed. Why? Because our data overwelm the prior. Again, priors are not &lt;em&gt;that&lt;/em&gt; big of a deal.
2. We can forget about all of this if we do not use default priors. More on that later.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-the-posterior-look-like&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What does the posterior look like?&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(ex1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 Estimate Est.Error       Q2.5      Q97.5
## intercept     106.529202  1.946755 102.448293 110.258352
## age_c         -20.516543  1.982355 -24.422934 -16.641683
## program         9.213920  2.536958   4.140112  14.087900
## age_c:program   3.201103  2.655993  -2.030914   8.378437&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(brms)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Rcpp&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading &amp;#39;brms&amp;#39; package (version 2.10.0). Useful instructions
## can be found by typing help(&amp;#39;brms&amp;#39;). A more detailed introduction
## to the package is available through vignette(&amp;#39;brms_overview&amp;#39;).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;brms&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:lme4&amp;#39;:
## 
##     ngrps&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post &amp;lt;- brms::posterior_samples(ex1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a look at the first 10 columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; post[, 1:10] %&amp;gt;%
glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 4,000
## Variables: 10
## $ b_intercept              &amp;lt;dbl&amp;gt; 107.8432, 105.9106, 106.5840, 105.9220,…
## $ b_age_c                  &amp;lt;dbl&amp;gt; -22.15160, -18.79935, -20.28713, -21.92…
## $ b_program                &amp;lt;dbl&amp;gt; 8.058549, 7.426575, 7.172656, 7.401558,…
## $ `b_age_c:program`        &amp;lt;dbl&amp;gt; 6.71653288, 1.54733881, 2.46308672, 6.0…
## $ sd_id__Intercept         &amp;lt;dbl&amp;gt; 7.934120, 9.182119, 9.680560, 9.891759,…
## $ sd_id__age_c             &amp;lt;dbl&amp;gt; 0.07493328, 3.29208929, 0.46960055, 1.4…
## $ cor_id__Intercept__age_c &amp;lt;dbl&amp;gt; 0.11099642, -0.85142856, -0.62951652, -…
## $ sigma                    &amp;lt;dbl&amp;gt; 9.009290, 8.263876, 8.559613, 8.511837,…
## $ `r_id[1,Intercept]`      &amp;lt;dbl&amp;gt; 3.9047820, 6.2320897, 4.9022182, 8.3357…
## $ `r_id[2,Intercept]`      &amp;lt;dbl&amp;gt; -2.6429448, -3.2966948, 5.0002671, 2.64…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We saved our results as &lt;code&gt;post&lt;/code&gt;, which is a data frame with 4000 rows (i.e., 1000 post-warmup iterations times 4 chains) and 215 columns, each depicting one of the model parameters. With &lt;strong&gt;brms&lt;/strong&gt;, the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; parameters (i.e., the fixed effects or population parameters) get &lt;code&gt;b_&lt;/code&gt; prefixes in the &lt;code&gt;posterior_samples()&lt;/code&gt; output. So we can isolate them like so.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;% 
select(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;% 
head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_intercept   b_age_c b_program b_age_c:program
## 1    107.8432 -22.15160  8.058549        6.716533
## 2    105.9106 -18.79935  7.426575        1.547339
## 3    106.5840 -20.28713  7.172656        2.463087
## 4    105.9220 -21.92054  7.401558        6.062974
## 5    104.3962 -18.09924  9.998274        1.475840
## 6    108.5692 -22.13250  2.936269        7.693650&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post %&amp;gt;%
  select(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%

  ggplot(aes(x = value)) +
  geom_density(color = &amp;quot;transparent&amp;quot;, fill = &amp;quot;grey&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random effects&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(ex1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $id
## $id$sd
##           Estimate Est.Error      Q2.5     Q97.5
## Intercept 9.667999  1.155713 7.6023029 11.965095
## age_c     3.845584  2.394552 0.1988863  8.997617
## 
## $id$cor
## , , Intercept
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept  1.0000000 0.0000000  1.0000000 1.0000000
## age_c     -0.4809751 0.3646166 -0.9641151 0.5332108
## 
## , , age_c
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept -0.4809751 0.3646166 -0.9641151 0.5332108
## age_c      1.0000000 0.0000000  1.0000000 1.0000000
## 
## 
## $id$cov
## , , Intercept
## 
##            Estimate Est.Error      Q2.5     Q97.5
## Intercept  94.80554  22.79074  57.79501 143.16349
## age_c     -21.32497  18.96069 -65.51948   4.40548
## 
## , , age_c
## 
##            Estimate Est.Error        Q2.5    Q97.5
## Intercept -21.32497  18.96069 -65.5194793  4.40548
## age_c      20.52097  22.16587   0.0395562 80.95712
## 
## 
## 
## $residual__
## $residual__$sd
##  Estimate Est.Error     Q2.5    Q97.5
##  8.585042 0.5183284 7.535189 9.592982&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case that output is confusing, &lt;code&gt;VarCorr()&lt;/code&gt; returned a 2-element list of lists.&lt;/p&gt;
&lt;p&gt;If you just want the &lt;span class=&#34;math inline&#34;&gt;\(U_j\)&lt;/span&gt;s, subset the first list of the first list. Note this is included in the standard summary.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(ex1)[[1]][[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Estimate Est.Error      Q2.5     Q97.5
## Intercept 9.667999  1.155713 7.6023029 11.965095
## age_c     3.845584  2.394552 0.1988863  8.997617&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how to get their correlation matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(ex1)[[1]][[2]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , Intercept
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept  1.0000000 0.0000000  1.0000000 1.0000000
## age_c     -0.4809751 0.3646166 -0.9641151 0.5332108
## 
## , , age_c
## 
##             Estimate Est.Error       Q2.5     Q97.5
## Intercept -0.4809751 0.3646166 -0.9641151 0.5332108
## age_c      1.0000000 0.0000000  1.0000000 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;variance/covariance matrix.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;VarCorr(ex1)[[1]][[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , Intercept
## 
##            Estimate Est.Error      Q2.5     Q97.5
## Intercept  94.80554  22.79074  57.79501 143.16349
## age_c     -21.32497  18.96069 -65.51948   4.40548
## 
## , , age_c
## 
##            Estimate Est.Error        Q2.5    Q97.5
## Intercept -21.32497  18.96069 -65.5194793  4.40548
## age_c      20.52097  22.16587   0.0395562 80.95712&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_samples(ex1) %&amp;gt;%
  transmute(`sigma[0]^2`  = sd_id__Intercept^2,
            `sigma[1]^2`  = sd_id__age_c^2,
            `sigma[0][1]` = sd_id__Intercept * cor_id__Intercept__age_c * sd_id__age_c,
            `sigma[epsilon]^2` = sigma^2) %&amp;gt;%
  gather(key, posterior) %&amp;gt;%

  ggplot(aes(x = posterior)) +
  geom_density(color = &amp;quot;transparent&amp;quot;, fill = &amp;quot;grey&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank(),
        strip.text = element_text(size = 12)) +
  facet_wrap(~key, scales = &amp;quot;free&amp;quot;, labeller = label_parsed)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ex-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ex 2&lt;/h2&gt;
&lt;p&gt;Load the data, here from chapter 4 of Singer and Willet&lt;/p&gt;
&lt;p&gt;Data generating model we are fitting. You can also write this with L1 and L2 convention.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{alcuse}_{ij} &amp;amp; =  \gamma_{00} +  U_{0j} + \epsilon_{ij} \\
\epsilon_{ij} &amp;amp; \sim \text{Normal} (0, \sigma_\epsilon^2) \\
U_{0i} &amp;amp; \sim \text{Normal} (0, \sigma_0^2)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What are the default priors?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  get_prior(data = alcohol1_pp, 
           family = gaussian,
           alcuse ~ 1 + (1 | id))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 prior     class      coef group resp dpar nlpar bound
## 1 student_t(3, 1, 10) Intercept                                      
## 2 student_t(3, 0, 10)        sd                                      
## 3                            sd              id                      
## 4                            sd Intercept    id                      
## 5 student_t(3, 0, 10)     sigma&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;using-priors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Using priors&lt;/h3&gt;
&lt;p&gt;How can we put that in directly to our code?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2 &amp;lt;-
  brm(data = alcohol1_pp,
      file = &amp;quot;fit3&amp;quot;,
      family = gaussian,
      alcuse ~ 1 + (1 | id),
      prior = c(prior(student_t(3, 1, 10), class = Intercept),
                prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Visualizing priors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(metRology)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;metRology&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     cbind, rbind&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(x = seq(from = -100, to = 100, length.out = 1e3)) %&amp;gt;%
  mutate(density = metRology::dt.scaled(x, df = 3, mean = 1, sd = 10)) %&amp;gt;% 
  
  ggplot(aes(x = x, y = density)) +
  geom_vline(xintercept = 0, color = &amp;quot;white&amp;quot;) +
  geom_line() +
  labs(title = expression(paste(&amp;quot;prior for &amp;quot;, gamma[0][0])),
       x = &amp;quot;parameter space&amp;quot;) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note a few things: First, this is a broad space for our intercept based on what we are looking at. This would be considering minimally informative.&lt;/p&gt;
&lt;p&gt;Second, consider the variance priors – they go below zero. Does this make sense?&lt;/p&gt;
&lt;p&gt;Here are the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: alcuse ~ 1 + (1 | id) 
##    Data: alcohol1_pp (Number of observations: 246) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 82) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     0.77      0.08     0.62     0.94 1.00     1662     2274
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.92      0.10     0.72     1.12 1.00     2602     2734
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.76      0.04     0.68     0.84 1.00     3219     3327
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post2 &amp;lt;- posterior_samples(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since all we’re interested in are the variance components, we’ll &lt;code&gt;select()&lt;/code&gt; out the relevant columns from &lt;code&gt;post2&lt;/code&gt;, and save the results in a mini data frame, &lt;code&gt;v&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;v &amp;lt;-
  post2 %&amp;gt;%
  select(sigma, sd_id__Intercept)

head(v)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       sigma sd_id__Intercept
## 1 0.7657679        0.9317019
## 2 0.7630844        0.9259266
## 3 0.8073889        0.7670859
## 4 0.6964495        0.6670863
## 5 0.7029354        0.7075109
## 6 0.7410712        0.7324726&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(v)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 4000    2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note these are in SD units&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;v %&amp;gt;%
  gather() %&amp;gt;%
  ggplot(aes(x = value)) +
  geom_vline(xintercept = c(.25, .5, .75, 1), color = &amp;quot;white&amp;quot;) +
  geom_density(size = 0, fill = &amp;quot;grey&amp;quot;) +
  scale_x_continuous(NULL, limits = c(0, 1.25),
                     breaks = seq(from = 0, to = 1.25, by = .25)) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-38-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculate-icc.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculate ICC.&lt;/h3&gt;
&lt;p&gt;Note that the formula uses variances. ‘brms’ gives us SDs
&lt;span class=&#34;math display&#34;&gt;\[
ICC = \frac{\sigma_0^2}{\sigma_0^2 + \sigma_\epsilon^2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;v %&amp;gt;%
  transmute(ICC = sd_id__Intercept^2 / (sd_id__Intercept^2 + sigma^2)) %&amp;gt;%
  ggplot(aes(x = ICC)) +
  geom_density(size = 0, fill = &amp;quot;grey&amp;quot;) +
  scale_x_continuous( limits = 0:1) +
  scale_y_continuous(NULL, breaks = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-39-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note we get a distribution of ICCs, not just a singular score! Not all of our samples show as strong of between person association. Note that measuring this dispersion is a feature, not a problem. With standard MLM we are not taking into account potential sampling variance that may influence our estimates. Bayesian does.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-predictors&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding predictors&lt;/h3&gt;
&lt;p&gt;Using the composite formula, our next model, the unconditional growth model, follows the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{alcuse}_{ij} &amp;amp; = \gamma_{00} + \gamma_{10} \text{age_14}_{ij} + U_{0j} + U_{1j} \text{age_14}_{ij} + e_{ij} \\
\epsilon_{ij} &amp;amp; \sim \text{Normal} (0, \sigma_\epsilon^2) \\
\begin{bmatrix} U_{0j} \\ U_{1j} \end{bmatrix} &amp;amp; \sim \text{MVN} 
\Bigg ( 
\begin{bmatrix} 0 \\ 0 \end{bmatrix}, 
\begin{bmatrix} \sigma_0^2 &amp;amp; \sigma_{01} \\ \sigma_{01} &amp;amp; \sigma_1^2 \end{bmatrix}
\Bigg )
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_prior(data = alcohol1_pp,
          family = gaussian,
          alcuse ~ 0 + intercept + age_14 + (1 + age_14 | id))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  prior class      coef group resp dpar nlpar bound
## 1                          b                                      
## 2                          b    age_14                            
## 3                          b intercept                            
## 4               lkj(1)   cor                                      
## 5                        cor              id                      
## 6  student_t(3, 0, 10)    sd                                      
## 7                         sd              id                      
## 8                         sd    age_14    id                      
## 9                         sd Intercept    id                      
## 10 student_t(3, 0, 10) sigma&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;*Note that there are flat priors as the default for fixed effects. This essentially disregards the prior and spits back the likelihood, equivalent to the ML estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2.fit2 &amp;lt;-
  brm(data = alcohol1_pp, 
      file = &amp;quot;fit4&amp;quot;,
      family = gaussian,
      alcuse ~ 0 + intercept + age_14 + (1 + age_14 | id),
      prior = c(prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(1), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Marginal effects&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(ex2.fit2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-41-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2.fit3 &amp;lt;-
  brm(data = alcohol1_pp, 
      file = &amp;quot;fit5&amp;quot;,
      family = gaussian,
      alcuse ~ 0 + intercept + age_14 + coa + age_14:coa + (1 + age_14 | id),
      prior = c(prior(student_t(3, 0, 10), class = sd),
                prior(student_t(3, 0, 10), class = sigma),
                prior(lkj(1), class = cor)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(ex2.fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: alcuse ~ 0 + intercept + age_14 + coa + age_14:coa + (1 + age_14 | id) 
##    Data: alcohol1_pp (Number of observations: 246) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~id (Number of levels: 82) 
##                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)             0.70      0.10     0.50     0.90 1.00      763
## sd(age_14)                0.37      0.09     0.14     0.53 1.01      339
## cor(Intercept,age_14)    -0.10      0.28    -0.51     0.66 1.01      486
##                       Tail_ESS
## sd(Intercept)             1455
## sd(age_14)                 278
## cor(Intercept,age_14)      342
## 
## Population-Level Effects: 
##            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## intercept      0.32      0.13     0.06     0.57 1.00     2043     2563
## age_14         0.29      0.09     0.13     0.46 1.00     2902     2796
## coa            0.74      0.20     0.34     1.14 1.00     1992     2295
## age_14:coa    -0.05      0.13    -0.30     0.20 1.00     2931     3067
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     0.61      0.05     0.52     0.72 1.01      416      672
## 
## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
## is a crude measure of effective sample size, and Rhat is the potential 
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(ex2.fit3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/marg.ex2.fit3-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/marg.ex2.fit3-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/marg.ex2.fit3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit3_f &amp;lt;-
  update(ex2.fit3,
         newdata = alcohol1_pp %&amp;gt;% mutate(coa = factor(coa)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(fit3_f)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-43-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-43-2.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-43-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(fit3_f,
                 effects = &amp;quot;coa&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-44-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(fit3_f,
                 effects = &amp;quot;coa:age_14&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-45-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marginal_effects(fit3_f,
                 effects = &amp;quot;age_14:coa&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-46-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use fitted function to create “predicted” values, much like we did with lmer&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nd &amp;lt;- 
  tibble(age_14 = seq(from = 0, to = 2, length.out = 30))

f &amp;lt;- 
  fitted(ex2.fit2, 
         newdata = nd,
         re_formula = NA) %&amp;gt;%
  data.frame() %&amp;gt;%
  bind_cols(nd) %&amp;gt;% 
  mutate(age = age_14 + 14)

head(f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Estimate Est.Error      Q2.5     Q97.5     age_14      age
## 1 0.6464935 0.1110443 0.4285175 0.8709599 0.00000000 14.00000
## 2 0.6652664 0.1091008 0.4513743 0.8852507 0.06896552 14.06897
## 3 0.6840392 0.1073183 0.4766966 0.8986808 0.13793103 14.13793
## 4 0.7028121 0.1057051 0.4992086 0.9142175 0.20689655 14.20690
## 5 0.7215850 0.1042690 0.5198633 0.9306017 0.27586207 14.27586
## 6 0.7403579 0.1030174 0.5407352 0.9446840 0.34482759 14.34483&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;f %&amp;gt;%
  ggplot(aes(x = age)) +
  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),
              fill = &amp;quot;grey75&amp;quot;, alpha = 3/4) +
  geom_line(aes(y = Estimate)) +
  scale_y_continuous(&amp;quot;alcuse&amp;quot;, breaks = 0:2, limits = c(0, 2)) +
  coord_cartesian(xlim = 13:17) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-48-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing models&lt;/h3&gt;
&lt;p&gt;As it turns out, we Bayesians use the log-likelihood (LL), too. Recall how the numerator in the right-hand side of Bayes’ Theorem was &lt;span class=&#34;math inline&#34;&gt;\(p(\text{data} | \theta) p(\theta)\)&lt;/span&gt;? That first part, &lt;span class=&#34;math inline&#34;&gt;\(p(\text{data} | \theta)\)&lt;/span&gt;, is the likelihood. In words, the likelihood is the &lt;em&gt;probability of the data given the parameters&lt;/em&gt;. And we take the log of the likelihood rather than the likelihood itself because it’s easier to work with statistically.&lt;/p&gt;
&lt;p&gt;When you’re working with &lt;strong&gt;brms&lt;/strong&gt;, you can extract the LL with the &lt;code&gt;log_lik()&lt;/code&gt; function. Here’s an example with &lt;code&gt;fit1&lt;/code&gt;, our unconditional means model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_lik(ex2) %&amp;gt;%
  str()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  num [1:4000, 1:246] -0.654 -0.788 -0.795 -0.65 -0.587 ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   ..$ : NULL
##   ..$ : NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You may have noticed we didn’t just get a single value back. Rather, we got an array of 4000 rows and 246 columns. The reason we got 4000 rows is because that’s how many post-warmup iterations we drew from the posterior. I.e., we set &lt;code&gt;brm(..., iter = 2000, warmup = 1000, chains = 4)&lt;/code&gt;. With respect to the 246 columns, that’s how many rows there are in the &lt;code&gt;alcohol1_pp&lt;/code&gt; data. So for each person in the data set, we get an entire posterior distribution of LL values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll &amp;lt;-
  log_lik(ex2) %&amp;gt;%
  data.frame() %&amp;gt;%
  mutate(sums     = rowSums(.)) %&amp;gt;%
  mutate(deviance = -2 * sums) %&amp;gt;%
  select(sums, deviance, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ll %&amp;gt;%
  ggplot(aes(x = deviance)) +
  geom_density(fill = &amp;quot;grey25&amp;quot;, size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-51-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The AIC is frequentist and cannot handle models with priors. The BIC isis a misnomer as it is not Bayesian. The Widely Applicable Information Criterion (WAIC) is used instead.&lt;/p&gt;
&lt;p&gt;The distinguishing feature of WAIC is that it is &lt;em&gt;pointwise&lt;/em&gt;. This means that uncertainty in prediction is considered case-by-case, or point-by-point, in the data. This is useful, because some observations are much harder to predict than others and may also have different uncertainty. You can think of WAIC as handling uncertainty where it actually matters: for each independent observation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;waic(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 4000 by 246 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -312.2 12.0
## p_waic        54.8  4.7
## waic         624.5 24.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: 42 (17.1%) p_waic estimates greater than 0.4. We recommend trying
## loo instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the statistic in each row, you get a point estimate and a standard error. The WAIC is on the bottom. The effective number of parameters, the &lt;span class=&#34;math inline&#34;&gt;\(p_\text{WAIC}\)&lt;/span&gt;, is in the middle. Notice the &lt;code&gt;elpd_waic&lt;/code&gt; on the top. That’s what you get without the &lt;span class=&#34;math inline&#34;&gt;\(-2 \times ...\)&lt;/span&gt; in the formula. Remember how that part is just to put things in a metric amenable to &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; difference testing? Well, not all Bayesians like that and within the &lt;strong&gt;Stan&lt;/strong&gt; ecosystem you’ll also see the WAIC expressed instead as the &lt;span class=&#34;math inline&#34;&gt;\(\text{elpd}_\text{WAIC}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The current recommended workflow within &lt;strong&gt;brms&lt;/strong&gt; is to attach the WAIC information to the model fit. You do it with the &lt;code&gt;add_criterion()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2 &amp;lt;- add_criterion(ex2, &amp;quot;waic&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2$waic&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 4000 by 246 log-likelihood matrix
## 
##           Estimate   SE
## elpd_waic   -312.2 12.0
## p_waic        54.8  4.7
## waic         624.5 24.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: 42 (17.1%) p_waic estimates greater than 0.4. We recommend trying
## loo instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Leave-one-out cross-validation (LOO-CV).&lt;/p&gt;
&lt;p&gt;Cross validation is quickly becoming the primary method to examine fit and utility of one’s model. The hope is our findings would generalize to other data we could have collected or may collect in the future. We’d like our findings to tell us something more general about the world at large. But we don’t have all the data and we typically don’t even know what all the relevant variables are. That is where validation comes in.&lt;/p&gt;
&lt;p&gt;k-fold is a common type of CV. As &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; increases, the number of cases with a fold get smaller. In the extreme, &lt;span class=&#34;math inline&#34;&gt;\(k = N\)&lt;/span&gt;, the number of cases within the data. At that point, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;-fold cross-validation turns into leave-one-out cross-validation (LOO-CV).&lt;/p&gt;
&lt;p&gt;But there’s a practical difficulty with LOO-CV: it’s costly. As you may have noticed, it takes some time to fit a Bayesian multilevel model. For large data and/or complicated models, sometimes it takes hours or days. Most of us just don’t have enough time or computational resources to fit that many models. Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO) as an efficient way to approximate true LOO-CV.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;l_fit1 &amp;lt;- loo(ex2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Found 3 observations with a pareto_k &amp;gt; 0.7 in model &amp;#39;ex2&amp;#39;. It is
## recommended to set &amp;#39;reloo = TRUE&amp;#39; in order to calculate the ELPD without
## the assumption that these observations are negligible. This will refit
## the model 3 times to compute the ELPDs for the problematic observations
## directly.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(l_fit1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Computed from 4000 by 246 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo   -315.5 12.3
## p_loo        58.0  4.9
## looic       630.9 24.6
## ------
## Monte Carlo SE of elpd_loo is NA.
## 
## Pareto k diagnostic values:
##                          Count Pct.    Min. n_eff
## (-Inf, 0.5]   (good)     221   89.8%   715       
##  (0.5, 0.7]   (ok)        22    8.9%   395       
##    (0.7, 1]   (bad)        3    1.2%   99        
##    (1, Inf)   (very bad)   0    0.0%   &amp;lt;NA&amp;gt;      
## See help(&amp;#39;pareto-k-diagnostic&amp;#39;) for details.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparing models with the WAIC and LOO.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ex2 &amp;lt;- add_criterion(ex2, c(&amp;quot;loo&amp;quot;, &amp;quot;waic&amp;quot;))
ex2.fit2 &amp;lt;- add_criterion(ex2.fit2, c(&amp;quot;loo&amp;quot;, &amp;quot;waic&amp;quot;))
ex2.fit3 &amp;lt;- add_criterion(ex2.fit3, c(&amp;quot;loo&amp;quot;, &amp;quot;waic&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The point to focus on, here, is we can use the &lt;code&gt;loo_compare()&lt;/code&gt; function to compare fits by their WAIC or LOO. Let’s practice with the WAIC.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ws &amp;lt;- loo_compare(ex2, ex2.fit2, ex2.fit3, criterion = &amp;quot;waic&amp;quot;)

print(ws)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          elpd_diff se_diff
## ex2.fit3   0.0       0.0  
## ex2.fit2  -0.5       2.2  
## ex2      -35.7       7.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if you wanted a more focused comparison, say between &lt;code&gt;ex2&lt;/code&gt; and &lt;code&gt;ex2.fit2&lt;/code&gt;, you’d just simplify your input.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_compare(ex2, ex2.fit2, criterion = &amp;quot;loo&amp;quot;) %&amp;gt;%
  print(simplify = F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic 
## ex2.fit2    0.0       0.0  -290.3     12.8        95.8    7.8    580.6
## ex2       -25.1       7.6  -315.5     12.3        58.0    4.9    630.9
##          se_looic
## ex2.fit2   25.7  
## ex2        24.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects-revisited&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random effects revisited&lt;/h3&gt;
&lt;p&gt;For one person&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alcohol1_pp %&amp;gt;% 
  select(id:coa, cpeer, alcuse) %&amp;gt;% 
  filter(id == 23)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 5
##      id   age   coa cpeer alcuse
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1    23    14     1 -1.02   1   
## 2    23    15     1 -1.02   1   
## 3    23    16     1 -1.02   1.73&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23 &amp;lt;-
  posterior_samples(ex2.fit3) %&amp;gt;%
  select(starts_with(&amp;quot;b_&amp;quot;)) %&amp;gt;%
  mutate(`gamma[0][&amp;quot;,23&amp;quot;]` = b_intercept + b_coa * 1 ,
         `gamma[1][&amp;quot;,23&amp;quot;]` = b_age_14 + `b_age_14:coa`)

head(post_23)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   b_intercept  b_age_14     b_coa b_age_14:coa gamma[0][&amp;quot;,23&amp;quot;]
## 1   0.5346809 0.2726632 0.3404332  -0.10266363       0.8751141
## 2   0.5003572 0.2943097 0.4494779  -0.14723165       0.9498351
## 3   0.6032724 0.2168968 0.1844755   0.05751361       0.7877478
## 4   0.5964837 0.2549591 0.5012584  -0.11002381       1.0977422
## 5   0.1802073 0.3580173 0.8257751  -0.03640028       1.0059824
## 6   0.1855054 0.2323935 0.6888844  -0.02800347       0.8743898
##   gamma[1][&amp;quot;,23&amp;quot;]
## 1       0.1699996
## 2       0.1470781
## 3       0.2744105
## 4       0.1449353
## 5       0.3216170
## 6       0.2043900&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23 %&amp;gt;%
  select(starts_with(&amp;quot;gamma&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%
  group_by(key) %&amp;gt;%
  summarise(mean = mean(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %&amp;gt;%
  mutate_if(is.double, round, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
##   key                  mean    ll    ul
##   &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 &amp;quot;gamma[0][\&amp;quot;,23\&amp;quot;]&amp;quot; 1.05  0.765 1.34 
## 2 &amp;quot;gamma[1][\&amp;quot;,23\&amp;quot;]&amp;quot; 0.244 0.06  0.425&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23 %&amp;gt;%
  select(starts_with(&amp;quot;gamma&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%

  ggplot(aes(x = value)) +
  geom_density(size = 0, fill = &amp;quot;grey25&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;participant-specific parameter estimates&amp;quot;) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, labeller = label_parsed, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-62-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yet this approach neglects the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;s. That is, it would be the same for everyone with COA = 1. W’ve been extracting the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;s with &lt;code&gt;ranef()&lt;/code&gt;. We also get them when we use &lt;code&gt;posterior_samples()&lt;/code&gt;. Here we’ll extract both the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt;s as well as the &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;s for &lt;code&gt;id == 23&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23.r &amp;lt;-
  posterior_samples(ex2.fit3) %&amp;gt;%
  select(starts_with(&amp;quot;b_&amp;quot;), contains(&amp;quot;23&amp;quot;))

glimpse(post_23.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 4,000
## Variables: 6
## $ b_intercept          &amp;lt;dbl&amp;gt; 0.53468090, 0.50035715, 0.60327237, 0.59648…
## $ b_age_14             &amp;lt;dbl&amp;gt; 0.27266322, 0.29430973, 0.21689684, 0.25495…
## $ b_coa                &amp;lt;dbl&amp;gt; 0.3404332, 0.4494779, 0.1844755, 0.5012584,…
## $ `b_age_14:coa`       &amp;lt;dbl&amp;gt; -0.102663630, -0.147231646, 0.057513614, -0…
## $ `r_id[23,Intercept]` &amp;lt;dbl&amp;gt; -0.207329355, 0.396994028, -0.131291582, -0…
## $ `r_id[23,age_14]`    &amp;lt;dbl&amp;gt; 0.12645762, -0.04757970, 0.23943539, 0.5509…&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the &lt;code&gt;r_id&lt;/code&gt; prefix, &lt;strong&gt;brms&lt;/strong&gt; tells you these are residual estimates for the levels in the &lt;code&gt;id&lt;/code&gt; grouping variable. Within the brackets, we learn these particular columns are for &lt;code&gt;id == 23&lt;/code&gt;, the first with respect to the &lt;code&gt;Intercept&lt;/code&gt; and second with respect to the &lt;code&gt;age_14&lt;/code&gt; parameter. Let’s put them to use.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23.r &amp;lt;-
  post_23.r %&amp;gt;%
  mutate(`beta[0][&amp;quot;,23&amp;quot;]` = b_intercept + b_coa * 1  + `r_id[23,Intercept]`,
         `beta[1][&amp;quot;,23&amp;quot;]` = b_age_14 + `r_id[23,age_14]`)

glimpse(post_23.r)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 4,000
## Variables: 8
## $ b_intercept          &amp;lt;dbl&amp;gt; 0.53468090, 0.50035715, 0.60327237, 0.59648…
## $ b_age_14             &amp;lt;dbl&amp;gt; 0.27266322, 0.29430973, 0.21689684, 0.25495…
## $ b_coa                &amp;lt;dbl&amp;gt; 0.3404332, 0.4494779, 0.1844755, 0.5012584,…
## $ `b_age_14:coa`       &amp;lt;dbl&amp;gt; -0.102663630, -0.147231646, 0.057513614, -0…
## $ `r_id[23,Intercept]` &amp;lt;dbl&amp;gt; -0.207329355, 0.396994028, -0.131291582, -0…
## $ `r_id[23,age_14]`    &amp;lt;dbl&amp;gt; 0.12645762, -0.04757970, 0.23943539, 0.5509…
## $ `beta[0][&amp;quot;,23&amp;quot;]`     &amp;lt;dbl&amp;gt; 0.6677847, 1.3468291, 0.6564562, 0.5999346,…
## $ `beta[1][&amp;quot;,23&amp;quot;]`     &amp;lt;dbl&amp;gt; 0.39912085, 0.24673003, 0.45633223, 0.80588…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23.r %&amp;gt;%
  select(starts_with(&amp;quot;beta&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%
  group_by(key) %&amp;gt;%
  summarise(mean = mean(value),
            ll = quantile(value, probs = .025),
            ul = quantile(value, probs = .975)) %&amp;gt;%
  mutate_if(is.double, round, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
##   key                 mean     ll    ul
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 &amp;quot;beta[0][\&amp;quot;,23\&amp;quot;]&amp;quot; 0.979  0.247 1.73 
## 2 &amp;quot;beta[1][\&amp;quot;,23\&amp;quot;]&amp;quot; 0.332 -0.198 0.883&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;post_23.r %&amp;gt;%
  select(starts_with(&amp;quot;beta&amp;quot;)) %&amp;gt;%
  gather() %&amp;gt;%

  ggplot(aes(x = value)) +
  geom_density(size = 0, fill = &amp;quot;grey25&amp;quot;) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(&amp;quot;participant-specific parameter estimates&amp;quot;) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, labeller = label_parsed, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-26-workshop-5_files/figure-html/unnamed-chunk-66-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-explained&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Variance explained&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;hypothesis-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hypothesis function&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;update-function-again&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Update function again&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;thanks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thanks&lt;/h2&gt;
&lt;p&gt;Many thanks to Solomon Kurz’s github for code&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Workshop #4</title>
      <link>/workshops/workshop-4/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-4/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#workspace&#34;&gt;Workspace&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#packages&#34;&gt;Packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#read-in-data&#34;&gt;Read in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#restructure-data&#34;&gt;Restructure Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-basic-growth-model&#34;&gt;The Basic Growth Model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#predicted-values&#34;&gt;Predicted Values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#plot&#34;&gt;Plot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#two-level-categorical-moderator&#34;&gt;Two-Level Categorical Moderator&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#three-level-categorical-moderator&#34;&gt;Three-Level Categorical Moderator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#continuous-time-invariant-moderator&#34;&gt;Continuous Time-Invariant Moderator&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/emoriebeck/R-tutorials/master/ALDA/week_4_plotting/week_4_plotting.Rmd&#34; download&gt;Download .Rmd (won’t work in Safari or IE)&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://github.com/emoriebeck/R-tutorials/tree/master/ALDA/week_4_plotting&#34; target=&#34;_blank&#34;&gt;See GitHub Repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For a much more exhaustive tutorial, see &lt;a href=&#34;https://raw.githubusercontent.com/emoriebeck/R-tutorials/master/mlm/Conditional_Models_doc.Rmd&#34; download&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;workspace&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Workspace&lt;/h1&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;p&gt;First let’s load in our packages. We’re going to load in a couple of extras (&lt;code&gt;brms&lt;/code&gt; and &lt;code&gt;tidybayes&lt;/code&gt;) that we haven’t used before. These will let us make some pretty plots later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)
library(knitr)
library(kableExtra)
library(lme4)
library(broom.mixed)
library(brms)
library(tidybayes)
library(plyr)
library(tidyverse)

data_path &amp;lt;- &amp;quot;https://github.com/emoriebeck/R-tutorials/raw/master&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;read-in-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Read in Data&lt;/h2&gt;
&lt;p&gt;The National Longitudinal Study of Youths 1979 Child and Young Adult Sample (NLSYCYA) is a longitudinal study conducted by the National Bureau of Labor Statistics. The sample includes the children of the original 1979 sample, of which we will use a small subset. Here, we are going to use a subset of the more than 11,000 variables available that include the following:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Item Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Time-Varying?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;PROC_CID&lt;/td&gt;
&lt;td&gt;Participant ID&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Dem_DOB&lt;/td&gt;
&lt;td&gt;Year of Date of Birth&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;groups&lt;/td&gt;
&lt;td&gt;Jail, Community Service, None&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;DemPWeight&lt;/td&gt;
&lt;td&gt;Weight Percentile at age 10&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;age&lt;/td&gt;
&lt;td&gt;Age of participant&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Year&lt;/td&gt;
&lt;td&gt;Year of Survey&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;age0&lt;/td&gt;
&lt;td&gt;Age of participant (centered)&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;SensSeek&lt;/td&gt;
&lt;td&gt;Sensation-Seeking Composite&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;CESD&lt;/td&gt;
&lt;td&gt;CESD Depression Composite&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;load(url(sprintf(&amp;quot;%s/ALDA/week_4_plotting/data/sample.RData&amp;quot;, data_path)))

sample_dat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2,084 x 8
##    PROC_CID   age  year  age0 groups    CESD SensSeek DemPweight
##       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
##  1     1601    16  2006     2 CommServ 0.429     3.67     0.816 
##  2     1601    18  2008     4 CommServ 2         3        0.816 
##  3     9102    16  2012     2 None     0.182     3.33     0.671 
##  4     9501    14  2000     0 Jail     0.5       3        0.548 
##  5     9501    18  2004     4 Jail     0.429     3        0.548 
##  6     9501    22  2008     8 Jail     0.429     3        0.548 
##  7     9502    14  2002     0 Jail     0.143     3        0.421 
##  8     9502    16  2004     2 Jail     0.286     3        0.421 
##  9     9502    20  2008     6 Jail     0         3        0.421 
## 10     9503    16  2004     2 Jail     1.71      3        0.0314
## # … with 2,074 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;restructure-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Restructure Data&lt;/h2&gt;
&lt;p&gt;These data are already largely cleaned as that is not our focus today. We just need to do a little bit of restructuring.&lt;/p&gt;
&lt;p&gt;To run our models using &lt;code&gt;purrr&lt;/code&gt;, we need to restrucutre the data to long. While we’re at it, we need to create a time variable centered at zero, so we can interpret our moderators later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df1_long &amp;lt;- sample_dat %&amp;gt;%
  gather(key = trait, value = value, CESD, SensSeek, na.rm = T) %&amp;gt;%
   mutate(wave = year - 1996,
          age0 = age-16)) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4,168 x 9
##    PROC_CID   age  year  age0 groups   DemPweight trait value  wave
##       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1     1601    16  2006     0 CommServ     0.816  CESD  0.429    10
##  2     1601    18  2008     2 CommServ     0.816  CESD  2        12
##  3     9102    16  2012     0 None         0.671  CESD  0.182    16
##  4     9501    14  2000    -2 Jail         0.548  CESD  0.5       4
##  5     9501    18  2004     2 Jail         0.548  CESD  0.429     8
##  6     9501    22  2008     6 Jail         0.548  CESD  0.429    12
##  7     9502    14  2002    -2 Jail         0.421  CESD  0.143     6
##  8     9502    16  2004     0 Jail         0.421  CESD  0.286     8
##  9     9502    20  2008     4 Jail         0.421  CESD  0        12
## 10     9503    16  2004     0 Jail         0.0314 CESD  1.71      8
## # … with 4,158 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-basic-growth-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Basic Growth Model&lt;/h1&gt;
&lt;p&gt;We’ll start with the basic growth model (just looking at change in a single variable over time).&lt;/p&gt;
&lt;p&gt;we will use list columns to do it. We’ll start by using the &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;nest()&lt;/code&gt; functions from &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; to put the data for each trait into a cell of our data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df1_nested &amp;lt;- df1_long %&amp;gt;%
  group_by(trait) %&amp;gt;%
  nest())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
## # Groups:   trait [2]
##   trait              data
##   &amp;lt;chr&amp;gt;    &amp;lt;list&amp;lt;df[,8]&amp;gt;&amp;gt;
## 1 CESD        [2,084 × 8]
## 2 SensSeek    [2,084 × 8]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, our data frame is 2 x 2, with the elements in the second column each containing the data frame that corresponds to that trait. This makes it really easy to run our models using the &lt;code&gt;map()&lt;/code&gt; family of unctions from &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Before we fit the full growth model, we will first fit the unconditional model. Below, we will add a new column to our data frame that will contain the unconditional model for each trait.&lt;/p&gt;
In this case the model will be in the form of:
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fit0 = map(data, ~lmer(value ~ 1 + (1 | PROC_CID), data = .))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
## # Groups:   trait [2]
##   trait              data fit0     
##   &amp;lt;chr&amp;gt;    &amp;lt;list&amp;lt;df[,8]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;   
## 1 CESD        [2,084 × 8] &amp;lt;lmerMod&amp;gt;
## 2 SensSeek    [2,084 × 8] &amp;lt;lmerMod&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can see we have a new list column in our data frame called fit0 that contains an S4 class lmerMod, which simply means your growth model. To understand model, I personally find it easiest to visualize it. What this model is telling us is the mean across all observations as well as the between-person variability in that estimate.&lt;/p&gt;
&lt;p&gt;Now, moving on to the growth model:&lt;/p&gt;
In this case the model will be in the form of:
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \beta_{1j}*time_{ij} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1j} = \gamma_{10} + U_{1j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fit2 = map(data, ~lmer(value ~ 1 + age0 + (age0 | PROC_CID), data = .))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 4
## # Groups:   trait [2]
##   trait              data fit0      fit2     
##   &amp;lt;chr&amp;gt;    &amp;lt;list&amp;lt;df[,8]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;   
## 1 CESD        [2,084 × 8] &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 2 SensSeek    [2,084 × 8] &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we’ve run our model, we are ready to plot the results.&lt;/p&gt;
&lt;p&gt;To do so, we’ll write a short function that will get the predicted values both for group level effects, as well as for individual-level estimates. There’s also a hidden function for getting standard errors of our pointwise estimates that we’ll use to create confidence bands in our plots. Don’t worry about that.&lt;/p&gt;
&lt;div id=&#34;predicted-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predicted Values&lt;/h2&gt;
&lt;p&gt;Now we’ll use the predict function to get predicted values and the function I created to get pointwise standard errors for the fixed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# function to get fixed effects
fixed_pred_fun &amp;lt;- function(m){
  frame &amp;lt;- tibble(Intercept = 1, age0 = seq(0, 8, .1)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16)
  frame$SE &amp;lt;- pv_fun(frame %&amp;gt;% select(Intercept, age0), m)
  frame %&amp;gt;% select(-Intercept)
}

# function to get random effects predictions  
ran_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
         PROC_CID = m@frame$PROC_CID) %&amp;gt;%
    mutate(ran_pred = predict(m, newdata = .),
           age = age0 + 16)
}

# get fixed and random efects, and also combine them.
(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fixed_pred = map(fit2, fixed_pred_fun),
         ran_pred = map(fit2, ran_pred_fun),
         combined_pred = map2(fixed_pred, ran_pred, full_join)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 7
## # Groups:   trait [2]
##   trait          data fit0   fit2   fixed_pred   ran_pred    combined_pred 
##   &amp;lt;chr&amp;gt;  &amp;lt;list&amp;lt;df[,8&amp;gt; &amp;lt;list&amp;gt; &amp;lt;list&amp;gt; &amp;lt;list&amp;gt;       &amp;lt;list&amp;gt;      &amp;lt;list&amp;gt;        
## 1 CESD    [2,084 × 8] &amp;lt;lmer… &amp;lt;lmer… &amp;lt;tibble [81… &amp;lt;tibble [8… &amp;lt;tibble [8,38…
## 2 SensS…  [2,084 × 8] &amp;lt;lmer… &amp;lt;lmer… &amp;lt;tibble [81… &amp;lt;tibble [8… &amp;lt;tibble [8,38…&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot&lt;/h2&gt;
&lt;p&gt;Now let’s&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;%
  select(trait, combined_pred) %&amp;gt;%
  unnest(combined_pred) %&amp;gt;%
  ggplot(aes(x = age)) +
    geom_line(aes(y = ran_pred, color = trait, group = PROC_CID), size = .25, alpha = .5) +
    geom_line(aes(y = fixed_pred), size = 2) +
    lims(y = c(0,4)) +
    labs(x = &amp;quot;Age&amp;quot;, y = &amp;quot;Composite Rating&amp;quot;,
         title = &amp;quot;Simple Growth Models&amp;quot;) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And with confidence bands:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;% 
  mutate(df = map_dbl(fit2, df.residual)) %&amp;gt;%
  select(trait, fixed_pred, df) %&amp;gt;%
  unnest(fixed_pred) %&amp;gt;%
  ggplot(aes(x = age, y = fixed_pred, fill = trait)) +
    geom_ribbon(aes(ymin=fixed_pred-1.96*SE,ymax=fixed_pred+1.96*SE),alpha=0.2,fill=&amp;quot;blue&amp;quot;) +  
    geom_line(size = 2) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;% 
  mutate(df = map_dbl(fit2, df.residual)) %&amp;gt;%
  select(trait, fixed_pred, df) %&amp;gt;%
  unnest(fixed_pred) %&amp;gt;%
  ggplot(aes(x = age, y = fixed_pred, fill = trait)) +
    stat_dist_lineribbon(
      aes(dist = &amp;quot;student_t&amp;quot;, arg1 = unique(df), arg2 = fixed_pred, arg3 = SE),
      alpha = 1/4
    ) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;two-level-categorical-moderator&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Two-Level Categorical Moderator&lt;/h1&gt;
&lt;p&gt;Let’s start with the basic syntax:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \beta_{1j}*time_{1j} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + \gamma_{01}*X_{2j} + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1j} = \gamma_{10} + \gamma_{11}*X_{2j} + U_{1j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;p&gt;Now let’s swap that out for a 2 group sample from the present data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \beta_{1j}*age0_{ij} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + \gamma_{01}*groupsNone + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1j} = \gamma_{10} + \gamma_{11}*groupsNone + U_{1j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;D1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Jail&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(data_2g = map(data, function(x) x %&amp;gt;% filter(groups != &amp;quot;CommServ&amp;quot;)),
         fit3 = map(data_2g, ~lmer(value ~ 1 + age0*groups + (age0 | PROC_CID), data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we plot these, we are plotting the simple slopes. Subbing 1 and 0 into the equations above we end up with the following for the groups.&lt;/p&gt;
&lt;p&gt;None: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{10}*age\)&lt;/span&gt;&lt;br /&gt;
Jail: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{01} + (\gamma_{10} + \gamma_{11})*age\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixed_pred_fun &amp;lt;- function(m){
  frame &amp;lt;- crossing(Intercept = 1, age0 = seq(0, 8, .1),
           groups = c(&amp;quot;Jail&amp;quot;, &amp;quot;None&amp;quot;)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16,
           groupsn = as.numeric(mapvalues(groups, unique(groups), c(1,0))),
           Int = age0*groupsn)
  frame$SE &amp;lt;- pv_fun(frame %&amp;gt;% select(Intercept, age0, groupsn, Int), m)
  frame %&amp;gt;% select(-Intercept, -groupsn, -Int)
}

fixed_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
           groups = c(&amp;quot;Jail&amp;quot;, &amp;quot;None&amp;quot;)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16)
}

ran_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
         PROC_CID = m@frame$PROC_CID) %&amp;gt;%
    left_join(m@frame %&amp;gt;% tbl_df %&amp;gt;% select(PROC_CID, groups)) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(ran_pred = predict(m, newdata = .),
           age = age0 + 16)
}

(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fixed_pred3 = map(fit3, fixed_pred_fun),
         ran_pred3 = map(fit3, ran_pred_fun),
         combined_pred3 = map2(fixed_pred3, ran_pred3, full_join)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 12
## # Groups:   trait [2]
##   trait        data fit0  fit2  fixed_pred ran_pred combined_pred data_2g
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;   &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt; 
## 1 CESD  [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## 2 Sens… [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## # … with 4 more variables: fit3 &amp;lt;list&amp;gt;, fixed_pred3 &amp;lt;list&amp;gt;,
## #   ran_pred3 &amp;lt;list&amp;gt;, combined_pred3 &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;%
  select(trait, combined_pred3) %&amp;gt;%
  unnest(combined_pred3) %&amp;gt;%
  ggplot(aes(x = age)) +
    scale_color_manual(values = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;)) +
    geom_line(aes(y = ran_pred, color = groups, group = PROC_CID), size = .25, alpha = .1) +
    geom_line(aes(y = fixed_pred, color = groups), size = 2) +
    lims(y = c(0,4)) +
    labs(x = &amp;quot;Age&amp;quot;, y = &amp;quot;Composite Rating&amp;quot;,
         title = &amp;quot;Simple Growth Models&amp;quot;) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;three-level-categorical-moderator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Three-Level Categorical Moderator&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;strong&gt;Level 1:&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \beta_{0j} + \beta_{1j}*age0_{ij} + \varepsilon{ij}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;strong&gt;Level 2:&lt;/strong&gt;
&lt;/li&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0j} = \gamma_{00} + \gamma_{01}*D1 + \gamma_{02}*D2 + U_{0j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\beta_{1j} = \gamma_{10} + \gamma_{11}*D1 + \gamma_{12}*D2 + U_{1j}\)&lt;/span&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Variable&lt;/th&gt;
&lt;th&gt;D1&lt;/th&gt;
&lt;th&gt;D2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Jail&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;CommServ&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fit4 = map(data, ~lmer(value ~ 1 + age0*groups + (age0 | PROC_CID), data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we plot these, we are plotting the simple slopes. Subbing 1 and 0 into the equations above we end up with the following for the groups.&lt;/p&gt;
&lt;p&gt;None: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{10}*age\)&lt;/span&gt;&lt;br /&gt;
Jail: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{01} + (\gamma_{10} + \gamma_{11})*age\)&lt;/span&gt;&lt;br /&gt;
Community Service: &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij} = \gamma_{00} + \gamma_{02} + (\gamma_{10} + \gamma_{12})*age\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixed_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
           groups = c(&amp;quot;Jail&amp;quot;, &amp;quot;None&amp;quot;, &amp;quot;CommServ&amp;quot;)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16)
}

ran_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
         PROC_CID = m@frame$PROC_CID) %&amp;gt;%
    left_join(m@frame %&amp;gt;% tbl_df %&amp;gt;% select(PROC_CID, groups)) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(ran_pred = predict(m, newdata = .),
           age = age0 + 16)
}

(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fixed_pred4 = map(fit4, fixed_pred_fun),
         ran_pred4 = map(fit4, ran_pred_fun),
         combined_pred4 = map2(fixed_pred4, ran_pred4, full_join)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 16
## # Groups:   trait [2]
##   trait        data fit0  fit2  fixed_pred ran_pred combined_pred data_2g
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;   &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt; 
## 1 CESD  [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## 2 Sens… [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## # … with 8 more variables: fit3 &amp;lt;list&amp;gt;, fixed_pred3 &amp;lt;list&amp;gt;,
## #   ran_pred3 &amp;lt;list&amp;gt;, combined_pred3 &amp;lt;list&amp;gt;, fit4 &amp;lt;list&amp;gt;,
## #   fixed_pred4 &amp;lt;list&amp;gt;, ran_pred4 &amp;lt;list&amp;gt;, combined_pred4 &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;%
  select(trait, combined_pred4) %&amp;gt;%
  unnest(combined_pred4) %&amp;gt;%
  ggplot(aes(x = age)) +
    scale_color_manual(values = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;black&amp;quot;)) +
    geom_line(aes(y = ran_pred, color = groups, group = PROC_CID), size = .25, alpha = .1) +
    geom_line(aes(y = fixed_pred, color = groups), size = 2) +
    lims(y = c(0,4)) +
    labs(x = &amp;quot;Age&amp;quot;, y = &amp;quot;Composite Rating&amp;quot;,
         title = &amp;quot;Simple Growth Models&amp;quot;) +
    facet_grid(~trait) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-time-invariant-moderator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous Time-Invariant Moderator&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fit5 = map(data, ~lmer(value ~ 1 + age0*DemPweight + (age0 | PROC_CID), data = .)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixed_pred_fun &amp;lt;- function(m){
  desc &amp;lt;- Rmisc::summarySE(m@frame, measurevar = &amp;quot;DemPweight&amp;quot;)
    crossing(age0 = seq(0, 8, 1),
           DemPweight = c(desc$DemPweight - desc$sd, 
                          desc$DemPweight, 
                          desc$DemPweight + desc$sd)) %&amp;gt;%
    mutate(fixed_pred = predict(m, newdata = ., re.form = NA),
           age = age0 + 16,
           Weight = factor(DemPweight, levels = unique(DemPweight), labels = c(&amp;quot;-1SD&amp;quot;, &amp;quot;0SD&amp;quot;, &amp;quot;1SD&amp;quot;)))
}

ran_pred_fun &amp;lt;- function(m){
  crossing(age0 = seq(0, 8, 1),
         PROC_CID = m@frame$PROC_CID) %&amp;gt;%
    left_join(m@frame %&amp;gt;% tbl_df %&amp;gt;% select(PROC_CID, DemPweight)) %&amp;gt;%
    distinct() %&amp;gt;%
    mutate(ran_pred = predict(m, newdata = .),
           age = age0 + 16)
}

(df1_nested &amp;lt;- df1_nested %&amp;gt;%
  mutate(fixed_pred5 = map(fit5, fixed_pred_fun),
         ran_pred5 = map(fit5, ran_pred_fun),
         combined_pred5 = map2(fixed_pred5, ran_pred5, full_join)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 20
## # Groups:   trait [2]
##   trait        data fit0  fit2  fixed_pred ran_pred combined_pred data_2g
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;lis&amp;gt; &amp;lt;list&amp;gt;     &amp;lt;list&amp;gt;   &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt; 
## 1 CESD  [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## 2 Sens… [2,084 × 8] &amp;lt;lme… &amp;lt;lme… &amp;lt;tibble [… &amp;lt;tibble… &amp;lt;tibble [8,3… &amp;lt;tibbl…
## # … with 12 more variables: fit3 &amp;lt;list&amp;gt;, fixed_pred3 &amp;lt;list&amp;gt;,
## #   ran_pred3 &amp;lt;list&amp;gt;, combined_pred3 &amp;lt;list&amp;gt;, fit4 &amp;lt;list&amp;gt;,
## #   fixed_pred4 &amp;lt;list&amp;gt;, ran_pred4 &amp;lt;list&amp;gt;, combined_pred4 &amp;lt;list&amp;gt;,
## #   fit5 &amp;lt;list&amp;gt;, fixed_pred5 &amp;lt;list&amp;gt;, ran_pred5 &amp;lt;list&amp;gt;,
## #   combined_pred5 &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1_nested %&amp;gt;%
  select(trait, combined_pred5) %&amp;gt;%
  unnest(combined_pred5) %&amp;gt;%
  ggplot(aes(x = age)) +
    scale_color_manual(values = c(&amp;quot;blue&amp;quot;, &amp;quot;red&amp;quot;, &amp;quot;black&amp;quot;)) +
    # geom_line(aes(y = ran_pred, group = PROC_CID), size = .25, alpha = .1) +
    geom_line(aes(y = fixed_pred, color = Weight), size = 1) +
    lims(y = c(0,4)) +
    labs(x = &amp;quot;Age&amp;quot;, y = &amp;quot;Composite Rating&amp;quot;,
         title = &amp;quot;Simple Growth Models&amp;quot;) +
    facet_wrap(~trait, scales = &amp;quot;free_y&amp;quot;) +
    theme_classic() +
    theme(axis.text = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          axis.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2)),
          legend.position = &amp;quot;none&amp;quot;,
          plot.title = element_text(face = &amp;quot;bold&amp;quot;, size = rel(1.2), hjust = .5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-20-week-4-workshop_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Workshop #3</title>
      <link>/workshops/workshop-3/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-3/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#purrr&#34;&gt;purrr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#background-iteration&#34;&gt;Background: Iteration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-reading&#34;&gt;Data Reading&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#many-subjects-same-variables-example-1&#34;&gt;Many Subjects, Same Variables (Example 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#many-subjects-different-variables-example-2&#34;&gt;Many Subjects, Different Variables (Example 2)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-waves-same-variables-example-3&#34;&gt;Multiple Waves, Same Variables (Example 3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-waves-different-variables-example-4&#34;&gt;Multiple Waves, Different Variables (Example 4)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#multiple-waves-multiple-files-for-same-variables-example-5&#34;&gt;Multiple Waves, Multiple Files for Same Variables (Example 5)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#running-models&#34;&gt;Running Models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#read-in-data&#34;&gt;Read in Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clean-data&#34;&gt;Clean Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#descriptives&#34;&gt;Descriptives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coding-time&#34;&gt;Coding Time&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fit-unconditional-models&#34;&gt;Fit Unconditional Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#icc&#34;&gt;ICC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fit-growth-models&#34;&gt;Fit Growth Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#model-comparisons&#34;&gt;Model Comparisons&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#tabling-values&#34;&gt;Tabling Values&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;purrr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;purrr&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/emoriebeck/R-tutorials/master/ALDA/week_3_purrr/week_3_purrr.Rmd&#34; download&gt;Download .Rmd (won’t work in Safari or IE)&lt;/a&gt;&lt;br /&gt;
&lt;a href=&#34;https://github.com/emoriebeck/R-tutorials/tree/master/ALDA/week_3_purrr&#34; target=&#34;_blank&#34;&gt;See GitHub Repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;#&lt;code&gt;purrr&lt;/code&gt;&lt;br /&gt;
In my opinion, &lt;code&gt;purrr&lt;/code&gt; is one of the most underrated and under-utilized &lt;code&gt;R&lt;/code&gt; packages. It has completely revolutionized my own efficiency and workspace organization, particularly as someone who works with super messy data that comes in a variety of forms.&lt;/p&gt;
&lt;p&gt;In this tutorial, we are going to cover a number of what I believe are the most functional and important applications of &lt;code&gt;purrr&lt;/code&gt; in psychological research. Given the audience, in the first half of the tutorial, I will focus on working with the diverse forms of data that many of you work with, providing examples of how to load, clean, and merge data using &lt;code&gt;purrr&lt;/code&gt;. In the second half, I will focus on how we can use &lt;code&gt;purrr&lt;/code&gt; with longitudinal data analysis when we are working with multiple predictors and outcomes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;background-iteration&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background: Iteration&lt;/h1&gt;
&lt;p&gt;Before we get there, though, I think it’s useful to think about when and where we would use &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Iteration is everywhere. It underpins much of mathematics and statistics. If you’ve ever seen the &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; symbol, then you’ve seen (and probably used) iteration.&lt;/p&gt;
&lt;p&gt;It’s also incredibly useful. Anytime you have to repeat some sort of action many times, iteration is your best friend. In psychology, this often means reading in a bunch of individual data files from an experiment, repeating an analysis with a series of different predictors or outcomes, or creating a series of figures.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(psych)
library(knitr)
library(kableExtra)
library(lme4)
library(broom.mixed)
library(plyr)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Enter &lt;code&gt;for&lt;/code&gt; loops. &lt;code&gt;for&lt;/code&gt; loops are the “OG” form of iteration in computer science. The basic syntax is below. Basically, we can use a for loop to loop through and print a series of things.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in letters[1:5]){
  print(i)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot;
## [1] &amp;quot;b&amp;quot;
## [1] &amp;quot;c&amp;quot;
## [1] &amp;quot;d&amp;quot;
## [1] &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above “loops” through 5 times, printing the iteration letter.&lt;/p&gt;
&lt;p&gt;Essentially, like the &lt;code&gt;apply()&lt;/code&gt;, &lt;code&gt;lapply()&lt;/code&gt;, &lt;code&gt;sapply()&lt;/code&gt;, and &lt;code&gt;mapply()&lt;/code&gt; family of functions, &lt;code&gt;purrr&lt;/code&gt; is meant to be an alternative to iteration (i.e. &lt;code&gt;for&lt;/code&gt; loops) in &lt;code&gt;R&lt;/code&gt;. &lt;code&gt;for&lt;/code&gt; loops are great, but they aren’t as great in &lt;code&gt;R&lt;/code&gt; as they are in other programming languages. In &lt;code&gt;R&lt;/code&gt;, you’re better off vectorizing or building in C++ backends.&lt;/p&gt;
&lt;p&gt;There are a lot of functions in the &lt;code&gt;purrr&lt;/code&gt; package that I encourage you to check out. Today, though, we’ll focus on the &lt;code&gt;map()&lt;/code&gt; family of functions. The breakdown of map functions is pretty intuitive. The basic map function wants two things as input – a list or vector and a function. So the &lt;code&gt;purrr&lt;/code&gt; equivalent of the example above would be:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map(letters[1:5], print)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot;
## [1] &amp;quot;b&amp;quot;
## [1] &amp;quot;c&amp;quot;
## [1] &amp;quot;d&amp;quot;
## [1] &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;a&amp;quot;
## 
## [[2]]
## [1] &amp;quot;b&amp;quot;
## 
## [[3]]
## [1] &amp;quot;c&amp;quot;
## 
## [[4]]
## [1] &amp;quot;d&amp;quot;
## 
## [[5]]
## [1] &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this returns a list, which we may not always want. With &lt;code&gt;purrr&lt;/code&gt;, we can change the kind of output of &lt;code&gt;map()&lt;/code&gt; by adding a predicate, like &lt;code&gt;lgl&lt;/code&gt;, &lt;code&gt;dbl&lt;/code&gt;, &lt;code&gt;chr&lt;/code&gt;, and &lt;code&gt;df&lt;/code&gt;. So in the example above, we may have wanted just the characters to print. To do that we’d call &lt;code&gt;map_chr()&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_chr(letters[1:5], print)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot;
## [1] &amp;quot;b&amp;quot;
## [1] &amp;quot;c&amp;quot;
## [1] &amp;quot;d&amp;quot;
## [1] &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot; &amp;quot;d&amp;quot; &amp;quot;e&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that it also returns the concatenated character vector as well as printing each letter individually (i.e. iteratively).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;map()&lt;/code&gt; functions can also hand multiple inputs. Often we may need to input multiple pieces of information to a function, similarly to how we work with nested &lt;code&gt;for&lt;/code&gt; loops. In this case, we have &lt;code&gt;map2()&lt;/code&gt; and &lt;code&gt;pmap()&lt;/code&gt; that take additional arguments. &lt;code&gt;map2()&lt;/code&gt; shockingly takes two inputs and &lt;code&gt;pmap()&lt;/code&gt; takes p arguments that you feed in as list (e.g. &lt;code&gt;pmap(list(a, b, c, d), my_fun))&lt;/code&gt;. A simple printing example would be:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map2_chr(letters[1:5], 1:5, paste)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;a 1&amp;quot; &amp;quot;b 2&amp;quot; &amp;quot;c 3&amp;quot; &amp;quot;d 4&amp;quot; &amp;quot;e 5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note here that we can use &lt;code&gt;map2()&lt;/code&gt; and &lt;code&gt;pmap()&lt;/code&gt; with the predicates from above.&lt;/p&gt;
&lt;p&gt;This likely makes little sense at this point, and that’s fine. The examples in the rest of this tutorial should elucidate their usage. The last note I’ll make is that thinking about the structure of your data is going to be very important when using &lt;code&gt;purrr&lt;/code&gt;. To use it effectively, you’ll need your data in specific forms, which will often require data manipulations. It just takes practice.&lt;/p&gt;
&lt;p&gt;Regardless of the programmatic form, iteration is everywhere. It underpins much of mathematics and statistics. If you’ve ever seen the &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; symbol, then you’ve seen (and probably used) iteration.&lt;/p&gt;
&lt;p&gt;It’s also incredibly useful. Anytime you have to repeat some sort of action many times, iteration is your best friend. In psychology, this could mean reading in a bunch of separate data files (with separate files for different people, variables, waves, etc.) or performing a number of regressions or other statistical tests.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-reading&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data Reading&lt;/h1&gt;
&lt;p&gt;To demonstrate the first case in which I find &lt;code&gt;purrr&lt;/code&gt; useful, we are going to consider a five cases that, in my experience, capture many of the challenges we often face in working with psychological data. In each of these cases, we will use a codebook of the form we discussed in the previous tutorial on codebooks.&lt;/p&gt;
&lt;p&gt;All of these share a similar feature: multiple files. There are a variety of other techniques you could use to get your data into a usable form, such as those below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
Writing code to load in each file separately (not good).
&lt;/li&gt;
&lt;li&gt;
Copying each data file into one larger data set in Excel (worse)
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;But let’s not do that. Let’s use iteration to make our process efficient and transparent.&lt;/p&gt;
&lt;div id=&#34;many-subjects-same-variables-example-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Many Subjects, Same Variables (Example 1)&lt;/h2&gt;
&lt;p&gt;We will start with a data storage format that is very common in experimental studies in various fields of psychology as well as in observational studies of repeated assessments of individuals (i.e. ESM, EMA, etc.).&lt;/p&gt;
&lt;p&gt;For this first example, I’ll show you how this would look with a &lt;code&gt;for&lt;/code&gt; loop before I show you how it looks with &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Assuming you have all the data in a single folder and the format is reasonably similar, you have the following basic syntax:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_path &amp;lt;- &amp;quot;&amp;quot;
files &amp;lt;- list.files(data_path)
data &amp;lt;- list()
for(i in files){
  data[[i]] &amp;lt;- read.csv(i, stringsAsFactors = F)
}
data &amp;lt;- combine(data)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This works fine in this simple case, but where &lt;code&gt;purrr&lt;/code&gt; really shines in when you need to make modifications to your data before combining, whether this be recoding, removing missing cases, or renaming variables.&lt;/p&gt;
&lt;p&gt;But first, the simple case of reading data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_path &amp;lt;- &amp;quot;~/Documents/week_3_purrr&amp;quot;
df1 &amp;lt;- tibble(ID = list.files(sprintf(&amp;quot;%s/data/example_1&amp;quot;, data_path))) %&amp;gt;%
  mutate(path = sprintf(&amp;quot;%s/data/example_1/%s&amp;quot;, data_path, ID),
         data = map(path, read_csv),
         ID = str_remove(ID, &amp;quot;.csv&amp;quot;)) %&amp;gt;%
  unnest(data) %&amp;gt;%
  select(-path)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above creates a list of ID’s from the data path (files named for each person), reads the data in using the &lt;code&gt;map()&lt;/code&gt; function from &lt;code&gt;purrr&lt;/code&gt;, removes the “.csv” from the ID variable, then unnests the data, resulting in a data frame for each person.&lt;/p&gt;
&lt;p&gt;But often, we have variable names that aren’t super informative, so we want to rename them. In this case, we need to use our codebook to give them more informative variable names.&lt;/p&gt;
&lt;p&gt;In this case, where all people have the same variables, it’s easiest to just rename them after unnesting, so the full code would look like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_path &amp;lt;- &amp;quot;https://github.com/emoriebeck/R-tutorials/raw/master&amp;quot;
(codebook &amp;lt;- sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/codebook_ex1.csv&amp;quot;, data_path) %&amp;gt;% read_csv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 2
##    new_name   old_name
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;   
##  1 O_AesSens  O_1     
##  2 E_Assert   E_1     
##  3 N_Depr     N_1     
##  4 N_EmoVol   N_2     
##  5 O_IntCur   O_2     
##  6 C_Org      C_1     
##  7 A_Rspct    A_1     
##  8 C_Rspnbl   C_2     
##  9 A_Cmpn     A_2     
## 10 O_CrtvImag O_3     
## 11 E_EnerLev  E_2     
## 12 C_Prdctv   C_3     
## 13 A_Trust    A_3     
## 14 N_Anxty    N_3     
## 15 E_Scblty   E_3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;old.names &amp;lt;- codebook$old_name
new.names &amp;lt;- codebook$new_name
df1 &amp;lt;- tibble(ID = list.files(sprintf(&amp;quot;%s/data/example_1&amp;quot;, data_path))) %&amp;gt;%
  mutate(path = sprintf(&amp;quot;%s/data/example_1/%s&amp;quot;, data_path, ID),
         data = map(path, read_csv),
         ID = str_remove(ID, &amp;quot;.csv&amp;quot;))%&amp;gt;%
  unnest(data) %&amp;gt;%
  select(ID, old.names) %&amp;gt;%
  setNames(c(&amp;quot;ID&amp;quot;, new.names))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;many-subjects-different-variables-example-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Many Subjects, Different Variables (Example 2)&lt;/h2&gt;
&lt;p&gt;In some cases, participants may have different variables. This could be do to a skip rule in a study or intentionally different variable collection (e.g. in between-person experiments or idiographic work like I do). In this case, we might need to filter or rename variables within our iterative loop.&lt;/p&gt;
&lt;p&gt;In this case, all participants have the same set of core variables but were randomly assigned to complete one additional scale.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df2 &amp;lt;- tibble(ID = list.files(sprintf(&amp;quot;%s/data/example_2&amp;quot;, data_path))) %&amp;gt;%
  mutate(path = sprintf(&amp;quot;%s/data/example_2/%s&amp;quot;, data_path, ID),
         data = map(path, read_csv),
         ID = str_remove(ID, &amp;quot;.csv&amp;quot;))%&amp;gt;%
  unnest(data) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-waves-same-variables-example-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Waves, Same Variables (Example 3)&lt;/h2&gt;
&lt;p&gt;In some cases, instead of multiple files for each participant, we collect a single file for all participants across different waves (e.g. using Qualtrics). In this case, we need to index the files a little differently. Instead of reading in files for participants, we need to read in files for waves, which may be named in a variety of ways.&lt;/p&gt;
&lt;p&gt;Here, I’ll start with a simple example of data that were well-managed and nicely named the same except for wave content. This is a good practice to do. I’m in general against modifying data, but I am a fan of changing file &lt;em&gt;names&lt;/em&gt; because I think this actually helps with data management and prevents the need to actually go in and modify information within files.&lt;/p&gt;
&lt;p&gt;These data come from a longitudinal study of personality. We have seven waves, and the variable names for all items are consistent across waves. In this case, our code is almost identical to reading in multiple files for each participant, except that now we have wave info and will need to toss out part of the file names at the end.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codebook &amp;lt;- sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/codebook_ex3.csv&amp;quot;, data_path) %&amp;gt;% read_csv
old.names &amp;lt;- str_remove_all(codebook$old_name, &amp;quot;[ ]&amp;quot;)
new.names &amp;lt;- codebook$new_name

df3 &amp;lt;- tibble(wave = paste(&amp;quot;T&amp;quot;, 1:7, sep = &amp;quot;&amp;quot;),
              path = sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/example_3/%s.csv&amp;quot;, data_path, wave)) %&amp;gt;%
  mutate(data = map(path, read_csv),
         wave = as.numeric(str_extract_all(wave, &amp;quot;[0-9]&amp;quot;))) %&amp;gt;%
  select(-path) %&amp;gt;%
  unnest(data) %&amp;gt;%
  select(old.names) %&amp;gt;%
  setNames(new.names)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The only change from the code for reading in multiple files for participants is that we have “wave” as a variable instead of “ID” and we use the &lt;code&gt;str_extract_all()&lt;/code&gt; function from the &lt;code&gt;stringr&lt;/code&gt; package (part of &lt;code&gt;tidyverse&lt;/code&gt;) to get rid of everything except the numeric wave value.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-waves-different-variables-example-4&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Waves, Different Variables (Example 4)&lt;/h2&gt;
&lt;p&gt;Oftentimes, however, we do not have the same variables across waves or they do have the same names across waves. In those cases, we’ll have to do a little extra work to get our data into a form where we can &lt;code&gt;unnest()&lt;/code&gt; them – that is where shared column names will actually be shared.&lt;/p&gt;
&lt;p&gt;We’ll start with the case where we have some additional information (e.g. demographics) in the first wave.&lt;/p&gt;
&lt;p&gt;These data are the same as we used in the previous example except that I changed the names and added demographic information for this example. This means that we have slightly different information in wave one and need a way to match the same variables across waves. We’ll use our codebook to achieve this with little issue!&lt;/p&gt;
&lt;p&gt;However, because of this, we’ll need to use a function that take the year as input, so that we pull the correct variables from the codebook.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;read_fun &amp;lt;- function(Wave){
  old.names &amp;lt;- str_remove_all((codebook %&amp;gt;% filter(wave == &amp;quot;All&amp;quot; | wave == Wave))$old_name, &amp;quot;[ ]&amp;quot;)
  new.names &amp;lt;- (codebook %&amp;gt;% filter(wave == &amp;quot;All&amp;quot; | wave == Wave))$new_name
  
  sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/example_4/T%s.csv&amp;quot;, data_path, Wave) %&amp;gt;%
    read_csv() %&amp;gt;%
    select(old.names) %&amp;gt;%
    setNames(new.names) %&amp;gt;%
    gather(key = item, value = value, -SID)
}

codebook &amp;lt;- sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/codebook_ex4.csv&amp;quot;, data_path) %&amp;gt;% read_csv

df4 &amp;lt;- tibble(wave = 1:7) %&amp;gt;%
  mutate(data = map(wave, read_fun)) %&amp;gt;%
  unnest(data) %&amp;gt;%
  unite(tmp, item, wave, sep = &amp;quot;.&amp;quot;) %&amp;gt;%
  spread(tmp, value) %&amp;gt;%
  gather(key = item, value = value, -SID, -contains(&amp;quot;Dem&amp;quot;)) %&amp;gt;%
  separate(item, c(&amp;quot;item&amp;quot;, &amp;quot;wave&amp;quot;), sep = &amp;quot;[.]&amp;quot;) %&amp;gt;%
  spread(item, value) &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-waves-multiple-files-for-same-variables-example-5&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Waves, Multiple Files for Same Variables (Example 5)&lt;/h2&gt;
&lt;p&gt;In other cases, we may have multiple types of files for different waves. Across waves, those variables may be the same or different, but we’ll focus on the case when we largely want the same variables.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Running Models&lt;/h1&gt;
&lt;p&gt;Another really powerful feature of &lt;code&gt;purrr&lt;/code&gt; is keeping your data, models, tables, plots, etc all conveniently indexed together. Often we need to do this for multiple DV’s or predictors, and you may end up with an environment that looks something like &lt;code&gt;E_fit1&lt;/code&gt;, &lt;code&gt;A_fit1&lt;/code&gt;, &lt;code&gt;E_fit2&lt;/code&gt;, &lt;code&gt;A_fit2&lt;/code&gt; and so on. There’s nothing wrong with this. But eventually you’ll want to pull out coefficients, plot results, etc., and it’s easy to make a copy and paste error or name different types of objects inconsistently, which can be difficult both for future you or someone else using your code.&lt;/p&gt;
&lt;p&gt;Before we can learn how to use &lt;code&gt;purrr&lt;/code&gt; for this, we need to understand what a nested data frame is. If you’ve ever worked with a list in R, you are halfway there. Basically a nested data frame takes the normal data frame you are probably familiar with and adds some new features. It still has columns, rows, and cells, but what makes up those cells isn’t restrictred to numbers, strings, or logicals. Instead, you can put essentially anything you want: lists, models, data frames, plots, etc!&lt;/p&gt;
&lt;p&gt;If this sounds scary, it will hopefully become clearer if we use our read in data from above to run, table, and plot some basic longitudinal models of our data.&lt;/p&gt;
&lt;div id=&#34;read-in-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Read in Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;codebook &amp;lt;- sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/codebook_ex6.csv&amp;quot;, data_path) %&amp;gt;%
  read_csv %&amp;gt;%
  mutate(old_name = str_to_lower(old_name))

read_fun &amp;lt;- function(Year){
  old.names &amp;lt;- (codebook %&amp;gt;% filter(year == Year | year == 0))$old_name
  new.names &amp;lt;- (codebook %&amp;gt;% filter(year == Year | year == 0))$new_name
  set &amp;lt;- (codebook %&amp;gt;% filter(year == Year))$dataset[1]
  sprintf(&amp;quot;%s/ALDA/week_3_purrr/data/example_6/%s.csv&amp;quot;, data_path, set) %&amp;gt;%
    read_csv %&amp;gt;%
    select(old.names) %&amp;gt;%
    setNames(new.names)
}

(df6 &amp;lt;- tibble(year = 2005:2015) %&amp;gt;%
  mutate(data = map(year, read_fun)) %&amp;gt;%
  select(-year) %&amp;gt;%
  unnest(data) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 256,663 x 151
##    Procedural__SID Procedural__hou… `Big 5__C_thoro… `Big 5__E_commu…
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1             901               94                6                4
##  2            1202              124                7                6
##  3            2301              230                7                6
##  4            2304              230                5                7
##  5            2302              230                6                5
##  6            4601              469                6                6
##  7            4701              477                6                6
##  8            4901              493                5                7
##  9            5201              523                7                5
## 10            5202              523                6                5
## # … with 256,653 more rows, and 147 more variables: `Big
## #   5__A_coarse.2005` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2005` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2005` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2005` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2005` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2005` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__DadDied.2005` &amp;lt;dbl&amp;gt;, `Life Event__Married.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2005` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__PartDied.2005` &amp;lt;dbl&amp;gt;, `Life Event__SepPart.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2006` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2006` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2006` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2006` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Married.2006` &amp;lt;dbl&amp;gt;, `Life Event__MomDied.2006` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MoveIn.2006` &amp;lt;dbl&amp;gt;, `Life Event__PartDied.2006` &amp;lt;dbl&amp;gt;, `Life
## #   Event__SepPart.2006` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2007` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__DadDied.2007` &amp;lt;dbl&amp;gt;, `Life Event__Married.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2007` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__PartDied.2007` &amp;lt;dbl&amp;gt;, `Life Event__SepPart.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2008` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2008` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2008` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Married.2008` &amp;lt;dbl&amp;gt;, `Life Event__MomDied.2008` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MoveIn.2008` &amp;lt;dbl&amp;gt;, `Life Event__PartDied.2008` &amp;lt;dbl&amp;gt;, `Life
## #   Event__SepPart.2008` &amp;lt;dbl&amp;gt;, `Big 5__C_thorough.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_communic.2009` &amp;lt;dbl&amp;gt;, `Big 5__A_coarse.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_original.2009` &amp;lt;dbl&amp;gt;, `Big 5__N_worry.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_forgive.2009` &amp;lt;dbl&amp;gt;, `Big 5__C_lazy.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_sociable.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_artistic.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_nervous.2009` &amp;lt;dbl&amp;gt;, `Big 5__C_efficient.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_reserved.2009` &amp;lt;dbl&amp;gt;, `Big 5__A_friendly.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_imagin.2009` &amp;lt;dbl&amp;gt;, `Big 5__N_dealStress.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2009` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2009` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Married.2009` &amp;lt;dbl&amp;gt;, `Life Event__MomDied.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MoveIn.2009` &amp;lt;dbl&amp;gt;, `Life Event__PartDied.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__SepPart.2009` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2010` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__DadDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__Married.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__PartDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__SepPart.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2011` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2011` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__NewPart.2011` &amp;lt;dbl&amp;gt;, `Life Event__Married.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2011` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__PartDied.2011` &amp;lt;dbl&amp;gt;, `Life Event__SepPart.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2012` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2012` &amp;lt;dbl&amp;gt;, `Life
## #   Event__Divorce.2012` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2012` &amp;lt;dbl&amp;gt;, `Life
## #   Event__NewPart.2012` &amp;lt;dbl&amp;gt;, `Life Event__Married.2012` &amp;lt;dbl&amp;gt;, `Life
## #   Event__MomDied.2012` &amp;lt;dbl&amp;gt;, `Life Event__MoveIn.2012` &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Clean Data&lt;/h2&gt;
&lt;p&gt;Now the data are all loaded in and have been given informative variable names, but we still need to do some data cleaning for the personality data.&lt;/p&gt;
&lt;p&gt;We’ll start by selecting only the personality variables and reverse-scoring them. Then we’ll create composites. To do so, we’ll again use our codebook.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# reverse code
(df6_long &amp;lt;- df6 %&amp;gt;%
  select(Procedural__SID, contains(&amp;quot;Big 5&amp;quot;)) %&amp;gt;%
  gather(key = item, value = value, -Procedural__SID, na.rm = T) %&amp;gt;%
  left_join(codebook %&amp;gt;% select(item = new_name, reverse, mini, maxi)) %&amp;gt;%
  mutate(value = ifelse(reverse == 1, value, 
                        reverse.code(-1, value, mini = mini, maxi = maxi))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 544,830 x 6
##    Procedural__SID item                   value reverse  mini  maxi
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 Big 5__C_thorough.2005     6       1     1     8
##  2            1202 Big 5__C_thorough.2005     7       1     1     8
##  3            2301 Big 5__C_thorough.2005     7       1     1     8
##  4            2304 Big 5__C_thorough.2005     5       1     1     8
##  5            2302 Big 5__C_thorough.2005     6       1     1     8
##  6            4601 Big 5__C_thorough.2005     6       1     1     8
##  7            4701 Big 5__C_thorough.2005     6       1     1     8
##  8            4901 Big 5__C_thorough.2005     5       1     1     8
##  9            5201 Big 5__C_thorough.2005     7       1     1     8
## 10            5202 Big 5__C_thorough.2005     6       1     1     8
## # … with 544,820 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create compoistes  
(df6_long &amp;lt;- df6_long %&amp;gt;%
  mutate(item = str_remove(item, &amp;quot;Big 5__&amp;quot;)) %&amp;gt;%
  separate(item, c(&amp;quot;trait&amp;quot;, &amp;quot;item&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  separate(item, c(&amp;quot;item&amp;quot;, &amp;quot;year&amp;quot;), sep = &amp;quot;[.]&amp;quot;) %&amp;gt;%
  group_by(Procedural__SID, trait, year) %&amp;gt;%
  summarize(value = mean(value, na.rm = T)) %&amp;gt;%
  ungroup())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 181,610 x 4
##    Procedural__SID trait year  value
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 A     2005   5   
##  2             901 A     2009   5.33
##  3             901 A     2013   5   
##  4             901 C     2005   5.33
##  5             901 C     2009   3.33
##  6             901 C     2013   6   
##  7             901 E     2005   4   
##  8             901 E     2009   4   
##  9             901 E     2013   4   
## 10             901 N     2005   4   
## # … with 181,600 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;descriptives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Descriptives&lt;/h2&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-2&#34;&gt;Table 1: &lt;/span&gt;Descriptive Statistics of Study Variables
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Extraversion
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Agreeablness
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Conscientiousness
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Neuroticism
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;3&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Openness
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
Year
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
M
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
SD
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
N
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2005
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.14
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.78
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.21
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.72
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10451
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2009
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.09
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.17
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.66
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.01
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.13
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.00
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.23
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.36
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.28
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10327
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
2013
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.08
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.04
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.27
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.30
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.98
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.64
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.88
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.75
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15544
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;coding-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Coding Time&lt;/h2&gt;
&lt;p&gt;It’s important to remember how we code time. There are several ways we can do it. For now, for simplicity, we will create a new wave variable where 2005 = 0, 2009 = 1, and 2013 = 3, but we could make a lot of other choices depending on our goals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_long &amp;lt;- df6_long %&amp;gt;%
  mutate(wave = as.numeric(mapvalues(year, from = seq(2005, 2013, 4), to = seq(0, 2, 1)))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 181,610 x 5
##    Procedural__SID trait year  value  wave
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 A     2005   5        0
##  2             901 A     2009   5.33     1
##  3             901 A     2013   5        2
##  4             901 C     2005   5.33     0
##  5             901 C     2009   3.33     1
##  6             901 C     2013   6        2
##  7             901 E     2005   4        0
##  8             901 E     2009   4        1
##  9             901 E     2013   4        2
## 10             901 N     2005   4        0
## # … with 181,600 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s going to get mad later when I run growth models if I keep people with only one wave, so we’re going to remove them now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_long &amp;lt;- df6_long %&amp;gt;%
  group_by(trait, Procedural__SID) %&amp;gt;%
  filter(n() &amp;gt; 1) %&amp;gt;%
  ungroup())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 110,935 x 5
##    Procedural__SID trait year  value  wave
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 A     2005   5        0
##  2             901 A     2009   5.33     1
##  3             901 A     2013   5        2
##  4             901 C     2005   5.33     0
##  5             901 C     2009   3.33     1
##  6             901 C     2013   6        2
##  7             901 E     2005   4        0
##  8             901 E     2009   4        1
##  9             901 E     2013   4        2
## 10             901 N     2005   4        0
## # … with 110,925 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-unconditional-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit Unconditional Models&lt;/h2&gt;
&lt;p&gt;Now, here we could run separate unconditional growth models for each of the Big 5 like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fit0_E &amp;lt;- lmer(value ~ 1 + (1 | Procedural__SID), data = df6_long %&amp;gt;% filter(trait == &amp;quot;E&amp;quot;))
summary(fit0_E)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: value ~ 1 + (1 | Procedural__SID)
##    Data: df6_long %&amp;gt;% filter(trait == &amp;quot;E&amp;quot;)
## 
## REML criterion at convergence: 62339.1
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -5.8959 -0.4997  0.0144  0.5498  3.3758 
## 
## Random effects:
##  Groups          Name        Variance Std.Dev.
##  Procedural__SID (Intercept) 0.7865   0.8868  
##  Residual                    0.5308   0.7286  
## Number of obs: 22187, groups:  Procedural__SID, 8592
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  5.11572    0.01078   474.6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But this would be tedious and prone to error. So instead we will use list columns to do it. We’ll start by using the &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;nest()&lt;/code&gt; functions from &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; to put the data for each trait into a cell of our data frame:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_long %&amp;gt;%
  group_by(trait) %&amp;gt;%
  nest())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 2
## # Groups:   trait [5]
##   trait           data
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt;
## 1 A       [22,187 × 4]
## 2 C       [22,187 × 4]
## 3 E       [22,187 × 4]
## 4 N       [22,187 × 4]
## 5 O       [22,187 × 4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, our data frame is 5 x 2, with the elements in the second column each containing the data frame that corresponds to that trait. This makes it really easy to run our models using the &lt;code&gt;map()&lt;/code&gt; family of unctions from &lt;code&gt;purrr&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Below, we will add a new column to our data frame that will contain the unconditional model for each trait.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(fit0 = map(data, ~lmer(value ~ 1 + (1 | Procedural__SID), data = .))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 3
## # Groups:   trait [5]
##   trait           data fit0     
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;   
## 1 A       [22,187 × 4] &amp;lt;lmerMod&amp;gt;
## 2 C       [22,187 × 4] &amp;lt;lmerMod&amp;gt;
## 3 E       [22,187 × 4] &amp;lt;lmerMod&amp;gt;
## 4 N       [22,187 × 4] &amp;lt;lmerMod&amp;gt;
## 5 O       [22,187 × 4] &amp;lt;lmerMod&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can see we have a new list column in our data frame called fit0 that contains an S4 class lmerMod, which simply means your growth model. To understand model, I personally find it easiest to visualize it. What this model is telling us is the mean across all observations as well as the between-person variability in that estimate. I find it easiest to plot this. We’ll go over the code for it next week.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/week_3_purrr_files/figure-html/unc%20plot%20ex6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;icc&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ICC&lt;/h2&gt;
&lt;p&gt;If you remember, what we’re often intersted in with the unconditional model is the ICC (relative between v. within variance), so let’s extract that from the models using the &lt;code&gt;ICC()&lt;/code&gt; function from the &lt;code&gt;reghelper&lt;/code&gt; package. In this case, we will use a version of &lt;code&gt;map()&lt;/code&gt; called &lt;code&gt;map_dbl&lt;/code&gt; because we want our result to be a regular numeric column, not a list column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(ICC = map_dbl(fit0, reghelper::ICC)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 4
## # Groups:   trait [5]
##   trait           data fit0        ICC
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 A       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.484
## 2 C       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.490
## 3 E       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.597
## 4 N       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.543
## 5 O       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.545&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-growth-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit Growth Models&lt;/h2&gt;
&lt;p&gt;What we’re starting to see is that we still have a tidy working environment, but we’re still holding onto a lot of info that we can access with relative ease.&lt;/p&gt;
&lt;p&gt;But before we get to things like pulling info from our models, let’s go ahead and run our basic growth model with and without a random slope.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(fit1 = map(data, ~lmer(value ~ 1 + wave + (1 | Procedural__SID), data = .)),
         fit2 = map(data, ~lmer(value ~ 1 + wave + (wave | Procedural__SID), data = .))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 6
## # Groups:   trait [5]
##   trait           data fit0        ICC fit1      fit2     
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;   
## 1 A       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.484 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 2 C       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.490 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 3 E       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.597 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 4 N       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.543 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;
## 5 O       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.545 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our data frame has expanded to have two more columns.&lt;/p&gt;
&lt;p&gt;Let’s visualize the difference between these two models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_fun &amp;lt;- function(m){
  crossing(wave = seq(0, 2, .5), 
           Procedural__SID = m@frame$Procedural__SID) %&amp;gt;%
    mutate(pred = predict(m, newdata = .))
}

subs &amp;lt;- sample(df6_nested$fit1[[1]]@frame$Procedural__SID, 50)
df6_nested %&amp;gt;%
  select(trait, fit1, fit2) %&amp;gt;%
  gather(model, fit, fit1, fit2) %&amp;gt;%
  mutate(model = mapvalues(model, c(&amp;quot;fit1&amp;quot;, &amp;quot;fit2&amp;quot;), c(&amp;quot;Random Intercept&amp;quot;, &amp;quot;Random Intercept + Slope&amp;quot;)),
         pred = map(fit, pred_fun)) %&amp;gt;%
  select(trait, model, pred) %&amp;gt;%
  unnest(pred) %&amp;gt;%
  mutate(Procedural__SID = as.character(Procedural__SID)) %&amp;gt;%
  filter(Procedural__SID %in% subs) %&amp;gt;%
  ggplot(aes(x = wave, y = pred, color = Procedural__SID, group = Procedural__SID)) + 
    geom_line(alpha = .5, size = .25) +
    facet_grid(model ~ trait) +
    theme_classic() +
    theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/week_3_purrr_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparisons&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Comparisons&lt;/h2&gt;
&lt;p&gt;To decide if we should have a random slope, we typically do nested model comparisons. We can do that here, too. Here, we need to use both fit1 and fit2, so we’ll use the &lt;code&gt;map2()&lt;/code&gt; function from &lt;code&gt;purrr&lt;/code&gt; to take 2 inputs and use the &lt;code&gt;anova()&lt;/code&gt; function to compare them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(anova1 = map2(fit1, fit2, anova)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 7
## # Groups:   trait [5]
##   trait           data fit0        ICC fit1      fit2      anova1          
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;          
## 1 A       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.484 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;
## 2 C       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.490 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;
## 3 E       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.597 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;
## 4 N       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.543 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;
## 5 O       [22,187 × 4] &amp;lt;lmerMod&amp;gt; 0.545 &amp;lt;lmerMod&amp;gt; &amp;lt;lmerMod&amp;gt; &amp;lt;df[,8] [2 × 8]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To see the results, we can do the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df6_nested$anova1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)
## .x[[1L]]  4 58533 58565 -29263    58525                         
## .y[[1L]]  6 58535 58583 -29261    58523 2.8338      2     0.2425
## 
## [[2]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)   
## .x[[1L]]  4 57394 57426 -28693    57386                            
## .y[[1L]]  6 57389 57437 -28688    57377 9.3688      2   0.009238 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## [[3]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)   
## .x[[1L]]  4 62305 62337 -31149    62297                            
## .y[[1L]]  6 62298 62346 -31143    62286 11.531      2   0.003134 **
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## [[4]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)    
## .x[[1L]]  4 66171 66203 -33081    66163                             
## .y[[1L]]  6 66146 66194 -33067    66134 28.585      2  6.207e-07 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## [[5]]
## Data: .
## Models:
## .x[[1L]]: value ~ 1 + wave + (1 | Procedural__SID)
## .y[[1L]]: value ~ 1 + wave + (wave | Procedural__SID)
##          Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)    
## .x[[1L]]  4 67780 67812 -33886    67772                             
## .y[[1L]]  6 67766 67814 -33877    67754 18.507      2  9.579e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tabling-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tabling Values&lt;/h2&gt;
&lt;p&gt;Looks like we have enough slope variance for all traits but Agreeableness to proceed with the random slope models. We’re going to proceed with the random slope models for all traits for consistency.&lt;/p&gt;
&lt;p&gt;The next thing we want to do is actually examine the model coefficients. To do that, I prefer to use the &lt;code&gt;tidy()&lt;/code&gt; function from the &lt;code&gt;broom.mixed&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(df6_nested &amp;lt;- df6_nested %&amp;gt;%
  mutate(tidy = map(fit2, ~tidy(., conf.int = T))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 8
## # Groups:   trait [5]
##   trait           data fit0      ICC fit1    fit2    anova1      tidy      
##   &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,4]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;list&amp;gt;  &amp;lt;list&amp;gt;  &amp;lt;list&amp;gt;      &amp;lt;list&amp;gt;    
## 1 A       [22,187 × 4] &amp;lt;lmerM… 0.484 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […
## 2 C       [22,187 × 4] &amp;lt;lmerM… 0.490 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […
## 3 E       [22,187 × 4] &amp;lt;lmerM… 0.597 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […
## 4 N       [22,187 × 4] &amp;lt;lmerM… 0.543 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […
## 5 O       [22,187 × 4] &amp;lt;lmerM… 0.545 &amp;lt;lmerM… &amp;lt;lmerM… &amp;lt;df[,8] [2… &amp;lt;tibble […&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a new column called tidy that contains a data frame. But we want to be able to see those values. This is where &lt;code&gt;purrr&lt;/code&gt; will really shine once again, especially when coupled with the &lt;code&gt;unnest()&lt;/code&gt; from &lt;code&gt;tidyr&lt;/code&gt;. Watch:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df6_nested %&amp;gt;%
  select(trait, tidy) %&amp;gt;%
  unnest(tidy)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 x 9
## # Groups:   trait [5]
##    trait effect group term  estimate std.error statistic conf.low conf.high
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 A     fixed  &amp;lt;NA&amp;gt;  (Int…   5.77     0.0107     539.     5.75      5.79  
##  2 A     fixed  &amp;lt;NA&amp;gt;  wave   -0.0576   0.00646     -8.91  -0.0702   -0.0449
##  3 A     ran_p… Proc… sd__…   0.701   NA           NA     NA        NA     
##  4 A     ran_p… Proc… sd__…   0.107   NA           NA     NA        NA     
##  5 A     ran_p… Proc… cor_…  -0.121   NA           NA     NA        NA     
##  6 A     ran_p… Resi… sd__…   0.707   NA           NA     NA        NA     
##  7 C     fixed  &amp;lt;NA&amp;gt;  (Int…   6.22     0.0105     592.     6.20      6.24  
##  8 C     fixed  &amp;lt;NA&amp;gt;  wave   -0.0461   0.00633     -7.29  -0.0586   -0.0337
##  9 C     ran_p… Proc… sd__…   0.699   NA           NA     NA        NA     
## 10 C     ran_p… Proc… sd__…   0.143   NA           NA     NA        NA     
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is fine, but kind of ugly (I can’t publish this table, and it should be clear right now that I do not like copying and pasting).&lt;/p&gt;
&lt;p&gt;The code below is going to clean this up a bit. See if you can figure out what’s going on:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;terms &amp;lt;- tibble(old = c(&amp;quot;(Intercept)&amp;quot;, &amp;quot;wave&amp;quot;, &amp;quot;sd__(Intercept)&amp;quot;, &amp;quot;sd__wave&amp;quot;,
                        &amp;quot;cor__(Intercept).wave&amp;quot;, &amp;quot;sd__Observation&amp;quot;),
                new = c(&amp;quot;Intercept&amp;quot;, &amp;quot;Slope&amp;quot;, &amp;quot;SD Intercept&amp;quot;, &amp;quot;SD Slope&amp;quot;, &amp;quot;Intercept-Slope Correlation&amp;quot;, &amp;quot;SD Residual&amp;quot;))

(tab &amp;lt;- df6_nested %&amp;gt;%
  select(trait, tidy) %&amp;gt;%
  unnest(tidy) %&amp;gt;% 
  mutate(term = mapvalues(term, from = terms$old, to = terms$new)) %&amp;gt;%
  select(trait, effect, term, estimate, conf.low, conf.high))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 30 x 6
## # Groups:   trait [5]
##    trait effect   term                        estimate conf.low conf.high
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;                          &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 A     fixed    Intercept                     5.77     5.75      5.79  
##  2 A     fixed    Slope                        -0.0576  -0.0702   -0.0449
##  3 A     ran_pars SD Intercept                  0.701   NA        NA     
##  4 A     ran_pars SD Slope                      0.107   NA        NA     
##  5 A     ran_pars Intercept-Slope Correlation  -0.121   NA        NA     
##  6 A     ran_pars SD Residual                   0.707   NA        NA     
##  7 C     fixed    Intercept                     6.22     6.20      6.24  
##  8 C     fixed    Slope                        -0.0461  -0.0586   -0.0337
##  9 C     ran_pars SD Intercept                  0.699   NA        NA     
## 10 C     ran_pars SD Slope                      0.143   NA        NA     
## # … with 20 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We extracted some elements, but we still aren’t quite ready for publication. Let’s do some reshaping so that we have different rows for terms and different columns for traits.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;levs &amp;lt;- paste(rep(c(&amp;quot;E&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;C&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;O&amp;quot;), each = 2), rep(c(&amp;quot;b&amp;quot;, &amp;quot;CI&amp;quot;), 5), sep = &amp;quot;.&amp;quot;)
(tab &amp;lt;- tab %&amp;gt;%
  mutate(sig = ifelse(sign(conf.low) == sign(conf.high), &amp;quot;sig&amp;quot;, &amp;quot;ns&amp;quot;)) %&amp;gt;%
  mutate_at(vars(estimate:conf.high), ~sprintf(&amp;quot;%.2f&amp;quot;, .)) %&amp;gt;%
  mutate_at(vars(conf.low, conf.high), ~ifelse(. == &amp;quot;NA&amp;quot;, &amp;quot;&amp;quot;, .)) %&amp;gt;%
  mutate(CI = ifelse(effect == &amp;quot;fixed&amp;quot;, sprintf(&amp;quot;[%s, %s]&amp;quot;, conf.low, conf.high), &amp;quot;&amp;quot;)) %&amp;gt;%
  mutate_at(vars(estimate, CI), ~ifelse(is.na(sig), .,
              ifelse(sig == &amp;quot;sig&amp;quot;, sprintf(&amp;quot;&amp;lt;strong&amp;gt;%s&amp;lt;/strong&amp;gt;&amp;quot;, .), .))) %&amp;gt;%
  select(trait:term, b = estimate, CI) %&amp;gt;%
  gather(key = est, value = value, b, CI) %&amp;gt;%
  unite(tmp, trait, est, sep = &amp;quot;.&amp;quot;) %&amp;gt;%
  mutate(tmp = factor(tmp, levels = levs)) %&amp;gt;%
  spread(tmp, value))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 12
##   effect  term  E.b   E.CI  A.b   A.CI  C.b   C.CI  N.b   N.CI  O.b   O.CI 
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
## 1 fixed   Inte… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str…
## 2 fixed   Slope &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str… &amp;lt;str…
## 3 ran_pa… Inte… -0.20 &amp;quot;&amp;quot;    -0.12 &amp;quot;&amp;quot;    -0.19 &amp;quot;&amp;quot;    -0.28 &amp;quot;&amp;quot;    -0.16 &amp;quot;&amp;quot;   
## 4 ran_pa… SD I… 0.91  &amp;quot;&amp;quot;    0.70  &amp;quot;&amp;quot;    0.70  &amp;quot;&amp;quot;    0.94  &amp;quot;&amp;quot;    0.95  &amp;quot;&amp;quot;   
## 5 ran_pa… SD R… 0.71  &amp;quot;&amp;quot;    0.71  &amp;quot;&amp;quot;    0.68  &amp;quot;&amp;quot;    0.79  &amp;quot;&amp;quot;    0.83  &amp;quot;&amp;quot;   
## 6 ran_pa… SD S… 0.16  &amp;quot;&amp;quot;    0.11  &amp;quot;&amp;quot;    0.14  &amp;quot;&amp;quot;    0.22  &amp;quot;&amp;quot;    0.20  &amp;quot;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have it formatted, but let’s make it pretty using the &lt;code&gt;kable()&lt;/code&gt; function from the &lt;code&gt;knitr&lt;/code&gt; package and the &lt;code&gt;kableExtra&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tab %&amp;gt;%
  mutate(effect = mapvalues(effect, c(&amp;quot;fixed&amp;quot;, &amp;quot;ran_pars&amp;quot;), c(&amp;quot;Fixed&amp;quot;, &amp;quot;Random&amp;quot;))) %&amp;gt;%
  kable(., &amp;quot;html&amp;quot;, escape = F, booktabs = T, 
        col.names = c(&amp;quot;Effect&amp;quot;, &amp;quot;Term&amp;quot;, rep(c(&amp;quot;b&amp;quot;, &amp;quot;CI&amp;quot;), times = 5)),
        align = c(&amp;quot;r&amp;quot;, &amp;quot;r&amp;quot;, rep(&amp;quot;c&amp;quot;,10)),
        caption = &amp;quot;Growth Model Terms for the Big 5&amp;quot;) %&amp;gt;%
  kable_styling(full_width = F) %&amp;gt;%
  collapse_rows(1, valign = &amp;quot;top&amp;quot;) %&amp;gt;%
  add_header_above(c(&amp;quot; &amp;quot; = 2, &amp;quot;Extraversion&amp;quot; = 2, &amp;quot;Agreeablness&amp;quot; = 2, 
                     &amp;quot;Conscientiousness&amp;quot; = 2, &amp;quot;Neuroticism&amp;quot; = 2, &amp;quot;Openness&amp;quot; = 2))&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
(#tab:kable ex6)Growth Model Terms for the Big 5
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;border-bottom:hidden&#34; colspan=&#34;2&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Extraversion
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Agreeablness
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Conscientiousness
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Neuroticism
&lt;/div&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Openness
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Effect
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Term
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
b
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
CI
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;vertical-align: top !important;&#34; rowspan=&#34;2&#34;&gt;
Fixed
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
Intercept
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;5.15&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[5.13, 5.18]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;5.77&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[5.75, 5.79]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;6.22&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[6.20, 6.24]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;4.74&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[4.72, 4.77]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;4.46&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[4.44, 4.49]&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
Slope
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;-0.04&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[-0.05, -0.03]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;-0.06&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[-0.07, -0.04]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;-0.05&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[-0.06, -0.03]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;0.08&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[0.07, 0.10]&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;-0.04&lt;/strong&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;strong&gt;[-0.06, -0.03]&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;vertical-align: top !important;&#34; rowspan=&#34;4&#34;&gt;
Random
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
Intercept-Slope Correlation
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.12
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.19
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.28
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.16
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
SD Intercept
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.91
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.70
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.70
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.94
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.95
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
SD Residual
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.71
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.68
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.79
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.83
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
SD Slope
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.16
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.11
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.14
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.22
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.20
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Workshop #2</title>
      <link>/workshops/workshop-2/</link>
      <pubDate>Wed, 04 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-2/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#tidyr&#34;&gt;tidyr&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#wide-and-long-form&#34;&gt;Wide and Long form&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lme4&#34;&gt;lme4&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-calculate-icc&#34;&gt;How to calculate ICC?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#exploring-beyond-the-summary&#34;&gt;Exploring beyond the summary&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-do-the-random-effects-look-like&#34;&gt;what do the random effects look like?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#adding-time-to-the-mlm&#34;&gt;Adding time to the MLM&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fixed-slope&#34;&gt;Fixed slope&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-slope&#34;&gt;Random slope&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#testing-models&#34;&gt;Testing models&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#why-treating-time-is-so-important&#34;&gt;Why treating time is so important&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#random-effects&#34;&gt;Random effects&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#calculation-of-random-effect-confidence-interval&#34;&gt;Calculation of random effect confidence interval&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#using-simulations-to-get-better-estimates-of-confidence-around-our-estimates&#34;&gt;Using simulations to get better estimates of confidence around our estimates&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#caterpillar-plots&#34;&gt;Caterpillar plots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#density-of-individual-random-effects&#34;&gt;Density of individual random effects&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-to-a-standard-linear-model&#34;&gt;Comparing to a standard linear model&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#comparing-models&#34;&gt;Comparing models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#predictions-and-prediction-intervals&#34;&gt;Predictions and prediction intervals&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#predictions-and-prediction-intervals-1&#34;&gt;Predictions and prediction intervals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;tidyr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;tidyr&lt;/h1&gt;
&lt;div id=&#34;wide-and-long-form&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wide and Long form&lt;/h2&gt;
&lt;p&gt;Depending on what type of analysis you want to perform you may need to restructure your data. I recommend the combination of tidyr and dplyr (among others) to restructure and manage your dataframes. The first decision you need to make is whether you want your data structured in a long or a wide format. There are multiple names to refer to these two types: multivariate vs univariate, person-level vs person-period, etc but they all refer to the same idea. How to structure your data depends on both what level of analysis (individual, dyad, household) and what type of analyses (MLM/SEM). Typically our focus is on individuals.&lt;/p&gt;
&lt;p&gt;Wide form is common among non-longitudinal data. It has one line per individual with all of their repeated measures in the same row, each with some name to distinguish which assessment wave the data came from. In general, this format is used for SEM.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##      ID ext_1 ext_2 ext_3
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     4     4     4
## 2     2     6     5     4
## 3     3     4     5     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In contrast, long format has a row per observation. Thus, participants likely have many rows, each one referring to a different assessment wave. There are fewer variables in this format which makes organization somewhat easier. Thus this has been referred to as “Tidy” data. Graphing with ggplot is facilitated when using tidy data such as being in the long format.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##      ID  time   ext
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     1     4
## 2     1     2     4
## 3     1     3     4
## 4     2     1     6
## 5     2     2     5
## 6     2     3     4
## 7     3     1     4
## 8     3     2     5
## 9     3     3     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do you go back and forth? We use the tidyr package! As of a few months ago, you would use the gather and the spread functions. Now these are being phased out and are being replaced by pivot_longer and pivot_wider. The functions work similar but the newer ones are a little more intuitive both in terms of remembering the correct function name as well as well adding more bells and whistles.&lt;/p&gt;
&lt;p&gt;Gather and pivot_longer goes from wide to long.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)

wide_to_long &amp;lt;- wide %&amp;gt;% 
  gather(ext_1:ext_3,key = &amp;quot;time&amp;quot;, value = &amp;quot;ext&amp;quot;) 
wide_to_long&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##      ID time    ext
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 ext_1     4
## 2     2 ext_1     6
## 3     3 ext_1     4
## 4     1 ext_2     4
## 5     2 ext_2     5
## 6     3 ext_2     5
## 7     1 ext_3     4
## 8     2 ext_3     4
## 9     3 ext_3     6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)

wide_to_long.p &amp;lt;- wide %&amp;gt;% 
  pivot_longer(-ID, names_to = &amp;quot;time&amp;quot;, values_to = &amp;quot;ext&amp;quot;) 
wide_to_long.p&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##      ID time    ext
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1 ext_1     4
## 2     1 ext_2     4
## 3     1 ext_3     4
## 4     2 ext_1     6
## 5     2 ext_2     5
## 6     2 ext_3     4
## 7     3 ext_1     4
## 8     3 ext_2     5
## 9     3 ext_3     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the similarities. The key in all three is to 1. identify which columns need to be reshaped. Here is it all of them besides ID. 2. we need to name the newly created variable that consists of the old column names (here time). 3. we need to name what those values represent (here levels of extraversion)&lt;/p&gt;
&lt;p&gt;The separate function could be used to get only the assessment wave number. This might be useful when combining data together or for creating a common time metric for everyone.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wide_to_long2 &amp;lt;- wide_to_long %&amp;gt;% 
  separate(time, into = c(&amp;quot;omit&amp;quot;, &amp;quot;wave&amp;quot;), sep = &amp;quot;_&amp;quot;, convert = TRUE) %&amp;gt;%
  dplyr::select(-omit) %&amp;gt;% 
  arrange(ID)
wide_to_long2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##      ID  wave   ext
##   &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     1     4
## 2     1     2     4
## 3     1     3     4
## 4     2     1     6
## 5     2     2     5
## 6     2     3     4
## 7     3     1     4
## 8     3     2     5
## 9     3     3     6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Note that the seperate function will identify non numeric characters and use that to seperate the values. You can omit the sep = function to check yourself. &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One issue that comes up here is that we have differing dates for each assessment. Ideally we would like to utilize that extra information.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 7
##      ID ext_1 ext_2 ext_3 date_1 date_2  date_3 
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;  
## 1     1     4     4     4 1/1/10 5/1/10  8/1/10 
## 2     2     6     5     4 1/6/10 4/10/10 9/1/10 
## 3     3     4     5     6 1/8/10 4/25/10 9/13/10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do we fix it? The same way we would with multiple variables we want to convert. Wave, along with ID helps us keep track of what variables go with which person at which time. Together, the two serve as a unique identifier. To better understand the code go through each line to see what the intervening data frame looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long.date &amp;lt;- wide.date %&amp;gt;% 
  gather(-ID, key = &amp;quot;time&amp;quot;, value = &amp;quot;value&amp;quot;) %&amp;gt;% 
  separate(time, into = c(&amp;quot;variable&amp;quot;, &amp;quot;wave&amp;quot;)) %&amp;gt;% 
  spread(variable,value) 
long.date&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 4
##      ID wave  date    ext  
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;
## 1     1 1     1/1/10  4    
## 2     1 2     5/1/10  4    
## 3     1 3     8/1/10  4    
## 4     2 1     1/6/10  6    
## 5     2 2     4/10/10 5    
## 6     2 3     9/1/10  4    
## 7     3 1     1/8/10  4    
## 8     3 2     4/25/10 5    
## 9     3 3     9/13/10 6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One difficulty of creating a wave variable is whether or not the variables are named in a manner such that 1) assessment wave is easily identifiable (e.g. does _a always refer to the first wave whereas _b always refer to the second?) and 2) if that is consistent across variables. Having a wave identifier for your variables is important/necessary. Having an easily selected one (ie at the end of the variable name, hopefully separated by an underscore or a period). If assessment wave separators are embedded within the variable name it will be harder to covert your data. Often, variable data is attached at the end of a name such as SWB_4 to refer to the fourth item in a scale. This may obscure wave identification as in SWB_A_4. A similar naming problem can occur with multiple reports e.g,. SWB_4_parent. I recommend putting wave identification last. The difficulties become partly moot when working in long format (read: entering data in) as opposed to wide. This also becomes moot when stored in a separate dataset. This is another reason why you should use codebooks!&lt;/p&gt;
&lt;p&gt;In the above code we used spread to go from long to wide as a means of creating a long dataset where there were multiple variables. Technically this is not a tidy dataset in that it comprises of both long and wide information, but it is the typical format used for MLM analyses.&lt;/p&gt;
&lt;p&gt;Going from long to wide uses spread or pivot_wider function. We will utilize this when converting our MLM models to SEM models, but try the code below to see what happens.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long_to_wide &amp;lt;- long %&amp;gt;% 
  spread(time, ext)
long_to_wide&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 4
##      ID   `1`   `2`   `3`
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1     1     4     4     4
## 2     2     6     5     4
## 3     3     4     5     6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this is technically the dataframe format that we want. The problem is that our variable names are numeric. This often causes problems. When working with tibbles use backticks ’ to refer to the column e.g., select(‘1’). I’d recode into a more usable variable name.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lme4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;lme4&lt;/h1&gt;
&lt;p&gt;The basic function we will work with is lmer from the lme4 package&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lme4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: Matrix&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;Matrix&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:tidyr&amp;#39;:
## 
##     expand, pack, unpack&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The package was developed to be similar to the lm function. The code will be similar to the formula for the combined model&lt;/p&gt;
&lt;p&gt;Code for empty/null/intercept only model&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmer(Y ~ 1 + (1 | subjects), data=example)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Level 1
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \beta_{0j}  +\varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Level 2
&lt;span class=&#34;math display&#34;&gt;\[ {\beta}_{0j} = \gamma_{00} + U_{0j} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Combined
&lt;span class=&#34;math display&#34;&gt;\[ {Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;1 is the way to reference the intercept. All additional fixed effects go outside the parentheses. Inside the parentheses are the random effects and residual terms. To the right of the vertical line is our level 1 residual term, which references the grouping variable. In this case, as with almost all longitudinal work, is the subject ID. To the left of the vertical line is the random effects we want to estimate. Right now this estimates only one random effect, one for the intercept.&lt;/p&gt;
&lt;p&gt;It is possible to suppress a random intercept by putting a zero instead of a 1. If you do not put anything there the 1 is implied.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lmer(y ~ 1 + time + (1 + time | subjects), data=data)

lmer(y ~ time + (time | subjects), data=data)
# both are equivalent&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Example&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.1 &amp;lt;- lmer(SMN7 ~ 1 + (1 | ID), data=example)
summary(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: SMN7 ~ 1 + (1 | ID)
##    Data: example
## 
## REML criterion at convergence: -714.1
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.1575 -0.4728 -0.0232  0.4512  3.2750 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  ID       (Intercept) 0.001823 0.04270 
##  Residual             0.001302 0.03608 
## Number of obs: 225, groups:  ID, 91
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.106972   0.005106   20.95&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-calculate-icc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to calculate ICC?&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.001823/(0.001823 + 0.001302)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.58336&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploring-beyond-the-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploring beyond the summary&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;lmerMod&amp;quot;
## attr(,&amp;quot;package&amp;quot;)
## [1] &amp;quot;lme4&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;what-do-the-random-effects-look-like&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;what do the random effects look like?&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(sjPlot)
plot_model(mod.1, type = &amp;quot;re&amp;quot;, sort.est = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(ranef(mod.1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ID
##       (Intercept)
## 6   -0.0597240676
## 29  -0.0101119688
## 34  -0.0103698893
## 36  -0.0035902640
## 37  -0.0082433829
## 48   0.0455797808
## 53  -0.0222710793
## 54  -0.0066548052
## 58  -0.0060624543
## 61  -0.0271347235
## 66  -0.0123359896
## 67  -0.0026491341
## 69   0.0348398944
## 71  -0.0486040243
## 74   0.0484338355
## 75   0.0224228634
## 76  -0.0021583228
## 78   0.0224780927
## 79  -0.0054325535
## 80  -0.0194707993
## 81   0.0712662731
## 82   0.0053695094
## 85  -0.0532215425
## 86  -0.0388885304
## 87  -0.0387411472
## 89  -0.0208712287
## 91   0.0123812011
## 92  -0.0078125821
## 93   0.0430219016
## 94  -0.0543390588
## 96   0.0233440081
## 97  -0.0497003277
## 98  -0.0432302582
## 99   0.0104394983
## 101  0.0508032394
## 102 -0.0104344307
## 103 -0.0206130188
## 104 -0.0482473609
## 105 -0.0478231980
## 106 -0.0028045239
## 110  0.0418641247
## 112 -0.0109089622
## 114 -0.0549314098
## 115 -0.0013505715
## 116  0.0062422910
## 120  0.0300499418
## 122  0.0793976365
## 125  0.0532803435
## 127 -0.0105050866
## 129 -0.0448207025
## 135  0.0406255726
## 136 -0.0364069792
## 137 -0.0444890904
## 140 -0.0153440709
## 141  0.0770651692
## 142  0.0817077387
## 143  0.0072423981
## 144  0.0001680065
## 146 -0.0551006778
## 149 -0.0137965477
## 150  0.0091583792
## 152 -0.0187707293
## 153  0.0490992150
## 155 -0.0233396072
## 156 -0.0218943803
## 159 -0.0488368935
## 160  0.0024524455
## 162  0.0911638809
## 163  0.0155327007
## 165  0.0320764602
## 167 -0.0025217361
## 169  0.0647586755
## 171 -0.0397728293
## 174  0.0259232134
## 182 -0.0154177625
## 187 -0.0581588783
## 189  0.0348767402
## 190 -0.0030744230
## 193  0.0636533019
## 194  0.0099321407
## 201  0.0104848276
## 204  0.0414352908
## 205  0.0353188897
## 208  0.0033367444
## 209 -0.0346144188
## 211  0.0168223034
## 214 -0.0374146988
## 219  0.0564683729
## 222 -0.0262135788
## 223 -0.0228606119
## 229 -0.0484315899&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(coef(mod.1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ID
##     (Intercept)
## 6    0.04724795
## 29   0.09686005
## 34   0.09660212
## 36   0.10338175
## 37   0.09872863
## 48   0.15255179
## 53   0.08470093
## 54   0.10031721
## 58   0.10090956
## 61   0.07983729
## 66   0.09463602
## 67   0.10432288
## 69   0.14181191
## 71   0.05836799
## 74   0.15540585
## 75   0.12939488
## 76   0.10481369
## 78   0.12945011
## 79   0.10153946
## 80   0.08750121
## 81   0.17823829
## 82   0.11234152
## 85   0.05375047
## 86   0.06808348
## 87   0.06823087
## 89   0.08610079
## 91   0.11935322
## 92   0.09915943
## 93   0.14999392
## 94   0.05263296
## 96   0.13031602
## 97   0.05727169
## 98   0.06374176
## 99   0.11741151
## 101  0.15777525
## 102  0.09653758
## 103  0.08635900
## 104  0.05872465
## 105  0.05914882
## 106  0.10416749
## 110  0.14883614
## 112  0.09606305
## 114  0.05204060
## 115  0.10562144
## 116  0.11321430
## 120  0.13702196
## 122  0.18636965
## 125  0.16025236
## 127  0.09646693
## 129  0.06215131
## 135  0.14759759
## 136  0.07056503
## 137  0.06248292
## 140  0.09162794
## 141  0.18403718
## 142  0.18867975
## 143  0.11421441
## 144  0.10714002
## 146  0.05187134
## 149  0.09317547
## 150  0.11613039
## 152  0.08820128
## 153  0.15607123
## 155  0.08363241
## 156  0.08507763
## 159  0.05813512
## 160  0.10942446
## 162  0.19813589
## 163  0.12250471
## 165  0.13904847
## 167  0.10445028
## 169  0.17173069
## 171  0.06719918
## 174  0.13289523
## 182  0.09155425
## 187  0.04881314
## 189  0.14184875
## 190  0.10389759
## 193  0.17062532
## 194  0.11690415
## 201  0.11745684
## 204  0.14840730
## 205  0.14229090
## 208  0.11030876
## 209  0.07235760
## 211  0.12379432
## 214  0.06955732
## 219  0.16344039
## 222  0.08075844
## 223  0.08411140
## 229  0.05854042&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fixef(mod.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) 
##    0.106972&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do these relate? Lets calculate ID 6 intercept random effect&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#coef = fixef + raneff

# coef for ID = 6 is 0.04724795  
0.106972 -0.0597240676 &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04724793&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get residuals and fitted scores&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom)
example.aug&amp;lt;- augment(mod.1, data = example)


# .fitted    = predicted values
# .resid    = residuals/errors
# .fixed     = predicted values with no random effects&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-time-to-the-mlm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding time to the MLM&lt;/h2&gt;
&lt;div id=&#34;fixed-slope&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fixed slope&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2f &amp;lt;- lmer(SMN7 ~ 1 + year + (1  | ID), data=example)
summary(mod.2f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: SMN7 ~ 1 + year + (1 | ID)
##    Data: example
## 
## REML criterion at convergence: -675.4
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.2308 -0.4868 -0.0377  0.4542  3.2337 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  ID       (Intercept) 0.001815 0.04261 
##  Residual             0.001300 0.03606 
## Number of obs: 216, groups:  ID, 88
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.104041   0.005733  18.147
## year        0.001331   0.001755   0.758
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.426&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What does this look like graphically?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-slope&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Random slope&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod.2 &amp;lt;- lmer(SMN7 ~ 1 + year + (year  | ID), data=example)
summary(mod.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: SMN7 ~ 1 + year + (year | ID)
##    Data: example
## 
## REML criterion at convergence: -678.1
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -1.93345 -0.47015 -0.00405  0.46985  2.67965 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev. Corr
##  ID       (Intercept) 1.688e-03 0.041085     
##           year        5.999e-05 0.007745 0.11
##  Residual             1.114e-03 0.033378     
## Number of obs: 216, groups:  ID, 88
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.104719   0.005466  19.157
## year        0.000489   0.001913   0.256
## 
## Correlation of Fixed Effects:
##      (Intr)
## year -0.339&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does the intercept change from the random intercept only model? It may change because the intercept is now conditional on time ie after accounting for time. It is not the predicted outcome when time = 0. You can think of the previous intercept as the grand mean of person means. If our year variable here changed across time then there would be a larger change in the intercept.&lt;/p&gt;
&lt;p&gt;How do you interpret year?&lt;/p&gt;
&lt;p&gt;How did the random effects change?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing models&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anova(mod.2f, mod.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## refitting model(s) with ML (instead of REML)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data: example
## Models:
## mod.2f: SMN7 ~ 1 + year + (1 | ID)
## mod.2: SMN7 ~ 1 + year + (year | ID)
##        Df     AIC     BIC logLik deviance  Chisq Chi Df Pr(&amp;gt;Chisq)
## mod.2f  4 -686.93 -673.43 347.46  -694.93                         
## mod.2   6 -685.45 -665.20 348.73  -697.45 2.5248      2      0.283&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Why is there a 2df difference?&lt;/p&gt;
&lt;p&gt;also you can see the non-REML fit info here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glance(mod.2f)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 6
##    sigma logLik   AIC   BIC deviance df.residual
##    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;int&amp;gt;
## 1 0.0361   338. -667. -654.    -695.         212&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;why-treating-time-is-so-important&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Why treating time is so important&lt;/h3&gt;
&lt;p&gt;Time with a different scale. How do we interpret? And what changes?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;example$year.n &amp;lt;- (example$year - 30)
  
mod.2n &amp;lt;- lmer(SMN7 ~ 1 + year.n + (year.n  | ID), data=example)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in checkConv(attr(opt, &amp;quot;derivs&amp;quot;), opt$par, ctrl =
## control$checkConv, : Model failed to converge with max|grad| = 1.99825 (tol
## = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(mod.2n)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear mixed model fit by REML [&amp;#39;lmerMod&amp;#39;]
## Formula: SMN7 ~ 1 + year.n + (year.n | ID)
##    Data: example
## 
## REML criterion at convergence: -674.8
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.2053 -0.4841 -0.0423  0.4505  3.2646 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev.  Corr 
##  ID       (Intercept) 1.001e-03 0.0316327      
##           year.n      1.333e-07 0.0003652 -1.00
##  Residual             1.320e-03 0.0363343      
## Number of obs: 216, groups:  ID, 88
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.146910   0.050598   2.904
## year.n      0.001432   0.001763   0.812
## 
## Correlation of Fixed Effects:
##        (Intr)
## year.n 0.995 
## convergence code: 0
## Model failed to converge with max|grad| = 1.99825 (tol = 0.002, component 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What happened?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-effects&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random effects&lt;/h2&gt;
&lt;div id=&#34;calculation-of-random-effect-confidence-interval&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Calculation of random effect confidence interval&lt;/h3&gt;
&lt;p&gt;Conveys the predicted range around each fixed effect in which 95% of the sample individuals are predicted to fall.&lt;/p&gt;
&lt;p&gt;95% random effect = fixed effect plus minus 1.96 * random standard deviation&lt;/p&gt;
&lt;p&gt;How to calculate?
1. Intercept &lt;span class=&#34;math display&#34;&gt;\[ \gamma_{00} \pm  1.96  *  \tau_{U_{0j}}  \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.1193933 + (1.96 * 0.240217) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5902186&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.1193933 - (1.96 * 0.240217) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.351432&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Slope &lt;span class=&#34;math display&#34;&gt;\[ \gamma_{10} \pm  1.96  *  \tau_{U_{1j}}  \]&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.0004891 + (1.96 * 0.007745) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0156693&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;0.0004891 - (1.96 * 0.007745) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -0.0146911&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;###Individual level random effects&lt;/p&gt;
&lt;p&gt;Are the intercept random effects the same as the model with only the intercept? Why or why not?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(ranef(mod.1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ID
##       (Intercept)
## 6   -0.0597240676
## 29  -0.0101119688
## 34  -0.0103698893
## 36  -0.0035902640
## 37  -0.0082433829
## 48   0.0455797808
## 53  -0.0222710793
## 54  -0.0066548052
## 58  -0.0060624543
## 61  -0.0271347235
## 66  -0.0123359896
## 67  -0.0026491341
## 69   0.0348398944
## 71  -0.0486040243
## 74   0.0484338355
## 75   0.0224228634
## 76  -0.0021583228
## 78   0.0224780927
## 79  -0.0054325535
## 80  -0.0194707993
## 81   0.0712662731
## 82   0.0053695094
## 85  -0.0532215425
## 86  -0.0388885304
## 87  -0.0387411472
## 89  -0.0208712287
## 91   0.0123812011
## 92  -0.0078125821
## 93   0.0430219016
## 94  -0.0543390588
## 96   0.0233440081
## 97  -0.0497003277
## 98  -0.0432302582
## 99   0.0104394983
## 101  0.0508032394
## 102 -0.0104344307
## 103 -0.0206130188
## 104 -0.0482473609
## 105 -0.0478231980
## 106 -0.0028045239
## 110  0.0418641247
## 112 -0.0109089622
## 114 -0.0549314098
## 115 -0.0013505715
## 116  0.0062422910
## 120  0.0300499418
## 122  0.0793976365
## 125  0.0532803435
## 127 -0.0105050866
## 129 -0.0448207025
## 135  0.0406255726
## 136 -0.0364069792
## 137 -0.0444890904
## 140 -0.0153440709
## 141  0.0770651692
## 142  0.0817077387
## 143  0.0072423981
## 144  0.0001680065
## 146 -0.0551006778
## 149 -0.0137965477
## 150  0.0091583792
## 152 -0.0187707293
## 153  0.0490992150
## 155 -0.0233396072
## 156 -0.0218943803
## 159 -0.0488368935
## 160  0.0024524455
## 162  0.0911638809
## 163  0.0155327007
## 165  0.0320764602
## 167 -0.0025217361
## 169  0.0647586755
## 171 -0.0397728293
## 174  0.0259232134
## 182 -0.0154177625
## 187 -0.0581588783
## 189  0.0348767402
## 190 -0.0030744230
## 193  0.0636533019
## 194  0.0099321407
## 201  0.0104848276
## 204  0.0414352908
## 205  0.0353188897
## 208  0.0033367444
## 209 -0.0346144188
## 211  0.0168223034
## 214 -0.0374146988
## 219  0.0564683729
## 222 -0.0262135788
## 223 -0.0228606119
## 229 -0.0484315899&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(ranef(mod.2))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ID
##       (Intercept)          year
## 6   -5.657190e-02 -0.0036945459
## 29  -8.481756e-03 -0.0012401120
## 34  -6.559720e-03 -0.0036272361
## 36  -7.107377e-03  0.0047529824
## 37  -7.584156e-03  0.0003779790
## 48   3.760800e-02  0.0079905136
## 53  -1.989445e-02 -0.0015608099
## 54   1.298061e-03 -0.0046572183
## 58   3.763976e-04 -0.0054304028
## 61  -2.523030e-02 -0.0013740463
## 66  -1.692834e-02  0.0022291440
## 67  -4.435808e-03  0.0020525694
## 71  -4.790346e-02  0.0002829010
## 75   2.147451e-02  0.0025670702
## 76  -1.178132e-03  0.0005880005
## 78   1.590422e-02  0.0031440504
## 79  -4.554999e-03  0.0005646937
## 80  -1.789820e-02 -0.0007154790
## 81   5.310404e-02  0.0082155118
## 82   4.163353e-03  0.0008114128
## 85  -5.229368e-02 -0.0013883325
## 86  -3.747004e-02 -0.0020088462
## 87  -3.530494e-02 -0.0042629307
## 89  -1.906497e-02 -0.0008530205
## 91   1.547560e-02 -0.0021933791
## 92  -5.127067e-03 -0.0012825084
## 93   3.779074e-02  0.0055466427
## 94  -5.318192e-02 -0.0005568113
## 96   2.535874e-02 -0.0004379578
## 97  -4.654850e-02 -0.0019515143
## 98  -3.875698e-02 -0.0027541673
## 99   9.785177e-03  0.0016155143
## 101  4.073120e-02  0.0116039230
## 103 -1.910831e-02 -0.0013254022
## 104 -4.656380e-02 -0.0034349757
## 105 -4.532176e-02 -0.0021913974
## 106  4.024883e-05 -0.0015454111
## 110  5.266477e-02 -0.0065630548
## 112 -5.482604e-03 -0.0041413140
## 114 -5.187791e-02 -0.0023737578
## 115 -6.018565e-04  0.0004676209
## 116  7.916722e-03 -0.0002939168
## 120  3.156554e-02  0.0021828094
## 122  7.845607e-02  0.0030220350
## 125  5.565560e-02 -0.0004030572
## 127 -1.035814e-02  0.0009947753
## 129 -4.383714e-02 -0.0011901610
## 135  4.517320e-02 -0.0027851051
## 136 -3.257406e-02 -0.0027536989
## 137 -4.307457e-02 -0.0026649722
## 140 -1.389973e-02 -0.0009470411
## 141  7.828180e-02  0.0011246004
## 142  7.656429e-02  0.0071700560
## 143  8.802167e-03  0.0001782843
## 144  5.023920e-04  0.0007247389
## 146 -5.190868e-02 -0.0030652978
## 149 -1.315714e-02  0.0001913378
## 150  1.329264e-02 -0.0028911814
## 152 -1.719482e-02 -0.0013669192
## 153  4.865193e-02  0.0022929034
## 155 -2.212352e-02 -0.0004931690
## 156 -2.041537e-02 -0.0004522343
## 159 -4.783022e-02 -0.0015435325
## 160  3.459146e-03  0.0001563020
## 162  8.771544e-02  0.0058967120
## 163  1.575853e-02  0.0011486036
## 165  3.244233e-02  0.0013196820
## 167 -5.436807e-04 -0.0009083619
## 169  6.576873e-02  0.0011926808
## 171 -3.475852e-02 -0.0047511867
## 174  2.798144e-02 -0.0004759192
## 182 -5.231973e-03 -0.0081378967
## 187 -5.609959e-02 -0.0019680493
## 189  3.456648e-02  0.0020405469
## 190 -1.620723e-03 -0.0003743333
## 193  5.854085e-02  0.0071786529
## 194  4.241261e-03  0.0066159420
## 201  1.187752e-02 -0.0001340229
## 204  3.799808e-02  0.0055309494
## 205  4.004323e-02 -0.0029413522
## 208  3.040083e-04  0.0043109412
## 209 -3.367017e-02 -0.0004225192
## 211  1.856239e-02 -0.0003192506
## 214 -3.676234e-02 -0.0001666149
## 219  4.541593e-02  0.0143735740
## 222 -1.947934e-02 -0.0063364890
## 223 -1.719018e-02 -0.0051038253
## 229 -4.254994e-02 -0.0060019177&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-simulations-to-get-better-estimates-of-confidence-around-our-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using simulations to get better estimates of confidence around our estimates&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(broom.mixed)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 methods overwritten by &amp;#39;broom.mixed&amp;#39;:
##   method         from 
##   augment.lme    broom
##   augment.merMod broom
##   glance.lme     broom
##   glance.merMod  broom
##   glance.stanreg broom
##   tidy.brmsfit   broom
##   tidy.gamlss    broom
##   tidy.lme       broom
##   tidy.merMod    broom
##   tidy.rjags     broom
##   tidy.stanfit   broom
##   tidy.stanreg   broom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;broom.mixed&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:broom&amp;#39;:
## 
##     tidyMCMC&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;random_params &amp;lt;- tidy(mod.2,  effects = &amp;quot;ran_vals&amp;quot;, conf.int=TRUE)
head(random_params)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##   effect   group level term        estimate std.error   conf.low conf.high
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 ran_vals ID    6     (Intercept) -0.0566     0.0181 -0.0920      -0.0212
## 2 ran_vals ID    29    (Intercept) -0.00848    0.0205 -0.0487       0.0317
## 3 ran_vals ID    34    (Intercept) -0.00656    0.0211 -0.0479       0.0348
## 4 ran_vals ID    36    (Intercept) -0.00711    0.0214 -0.0491       0.0349
## 5 ran_vals ID    37    (Intercept) -0.00758    0.0191 -0.0451       0.0299
## 6 ran_vals ID    48    (Intercept)  0.0376     0.0192 -0.0000131    0.0752&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(merTools)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: arm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: MASS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;MASS&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## arm (Version 1.10-1, built: 2018-4-12)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Working directory is /Users/jackson/Box/5165 Applied Longitudinal Data Analysis/ALDA/content/Workshops&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FEsim(mod.2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          term         mean       median          sd
## 1 (Intercept) 0.1049025608 0.1051183677 0.005295477
## 2        year 0.0007514429 0.0008749235 0.002054281&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;re.sim &amp;lt;- REsim(mod.2)
head(re.sim)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   groupFctr groupID        term         mean       median         sd
## 1        ID       6 (Intercept) -0.054039874 -0.054006147 0.01849033
## 2        ID      29 (Intercept) -0.005934701 -0.004010852 0.02089182
## 3        ID      34 (Intercept) -0.006448982 -0.007406987 0.01989969
## 4        ID      36 (Intercept) -0.006238323 -0.006724604 0.02380377
## 5        ID      37 (Intercept) -0.008978693 -0.008864176 0.02005051
## 6        ID      48 (Intercept)  0.038832930  0.037810554 0.01781832&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This can be used to create CIs for each individual random effect (and fixed effect). What is the confidence interval around person 6’s intercept estimate compared to person 2000 who has 25 repeated measurements?&lt;/p&gt;
&lt;div id=&#34;caterpillar-plots&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Caterpillar plots&lt;/h3&gt;
&lt;p&gt;Look through these different methods of getting random effects. Note that they are not all exactly the same.&lt;/p&gt;
&lt;p&gt;caterpillar plots&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- plotREsim(re.sim)
p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-of-individual-random-effects&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density of individual random effects&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1.gg1 &amp;lt;- re.sim %&amp;gt;% 
  filter(term == &amp;quot;(Intercept)&amp;quot;) 

ggplot(p1.gg1, aes(mean)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1.gg2 &amp;lt;- re.sim %&amp;gt;% 
  filter(term == &amp;quot;year&amp;quot;) 


ggplot(p1.gg2, aes(mean)) +
  geom_density()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-37-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-to-a-standard-linear-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing to a standard linear model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm.1 &amp;lt;- lm(SMN7 ~ 1 + year, data = example)
summary(lm.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = SMN7 ~ 1 + year, data = example)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.099294 -0.039929 -0.005938  0.032715  0.169885 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 0.100161   0.005261  19.039   &amp;lt;2e-16 ***
## year        0.004087   0.002563   1.595    0.112    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.05562 on 214 degrees of freedom
##   (9 observations deleted due to missingness)
## Multiple R-squared:  0.01174,    Adjusted R-squared:  0.007124 
## F-statistic: 2.543 on 1 and 214 DF,  p-value: 0.1123&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;comparing-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Comparing models&lt;/h3&gt;
&lt;p&gt;LRT&lt;/p&gt;
&lt;p&gt;Parametric bootstrap for CIs&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;confint(mod.1, method=&amp;quot;boot&amp;quot;, nsim=1000)
summary(mod.1)

# uses SDs of random effects
# sigma = residual standard error&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparing two models. fit the reduced model, then repeatedly simulate from it and compute the differences between the deviance of the reduced and the full model for each simulated data set. Compare this null distribution to the observed deviance difference.&lt;/p&gt;
&lt;p&gt;This procedure is implemented in the pbkrtest package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pbkrtest)
#pb &amp;lt;- PBmodcomp(mod.2,mod.2r)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;predictions-and-prediction-intervals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Predictions and prediction intervals&lt;/h2&gt;
&lt;p&gt;Predict function is deterministic and uses only the fixed effects (i.e. does not include random effects in the predictions). It does not do prediction in the typical sense where you are predicting &lt;em&gt;new&lt;/em&gt; individual’s scores.&lt;/p&gt;
&lt;p&gt;Simulate is non-deterministic because it samples random effect values for all subjects and then samples from the conditional distribution. Simulation is needed to create true predictions.&lt;/p&gt;
&lt;div id=&#34;predictions-and-prediction-intervals-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Predictions and prediction intervals&lt;/h3&gt;
&lt;p&gt;Predict function is deterministic and uses only the fixed effects (i.e. does not include random effects in the predictions). It does not do prediction in the typical sense where you are predicting &lt;em&gt;new&lt;/em&gt; individual’s scores.&lt;/p&gt;
&lt;p&gt;Simulate is non-deterministic because it samples random effect values for all subjects and then samples from the conditional distribution. Simulation is needed to create true predictions.&lt;/p&gt;
&lt;p&gt;Short of a fully Bayesian analysis, bootstrapping is the gold-standard for deriving prediction intervals/bands (ie where would a new person score given X), but the time required is typically high.&lt;/p&gt;
&lt;p&gt;In order to generate a proper prediction (for either a new person or a new observation within a person), a prediction must account for three sources of uncertainty in mixed models:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the residual (observation-level) variance,&lt;/li&gt;
&lt;li&gt;the uncertainty in the fixed coefficients, and&lt;/li&gt;
&lt;li&gt;the uncertainty in the variance parameters for the random effects&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Does so by:
1. extracting the fixed and random coefficients
2. takes n draws from the multivariate normal distribution of the fixed and random coefficients (separately)
3. calculates the linear predictor for each row in newdata based on these draws, and
4. incorporates the residual variation&lt;br /&gt;
then:
5. returns newdata with the lower and upper limits of the prediction interval and the mean or median of the simulated predictions&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(merTools)
# see also their shiny app: shinyMer(mod.1)

PI &amp;lt;- predictInterval(merMod = mod.2, newdata = example, level = 0.9, n.sims = 100, stat = &amp;quot;median&amp;quot;, include.resid.var = TRUE)
head(PI)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          fit       upr         lwr
## 1 0.05637403 0.1097137 -0.01587163
## 2 0.04308280 0.1112285 -0.01217230
## 3 0.02906342 0.1003347 -0.02107285
## 4 0.08963131 0.1538768  0.02283071
## 5 0.09504809 0.1510902  0.03936437
## 6 0.10191796 0.1657385  0.02358774&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nice for bringing in confidence bands around your prediction (And we might use this later)&lt;/p&gt;
&lt;p&gt;Broom offers the fitted (predicted) values already if you just want to plot your trajectory. But note that these are not typical prediction intervals (what happens if you get a new participant with a certain value of X). The bands fit in ggplot are for predicted &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;|X&lt;/p&gt;
&lt;p&gt;Broom offers the fitted (predicted) values already if you just want to plot your trajectory. But note that these are not typical prediction intervals (what happens if you get a new participant with a certain value of X). The bands fit in ggplot are for predicted &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;|X&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;P.gg &amp;lt;- ggplot(example.aug, aes(x= year, y = .fitted)) + geom_point() + stat_smooth(method = &amp;quot;lm&amp;quot;)   

P.gg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 9 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/Workshops/2019-09-04-workshop-2_files/figure-html/unnamed-chunk-42-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Can also explicitly simulate new data (rather than rely on another function to do so), which will be useful for power calculations later. In the simulated data, the subject means are different from the means in the original data because simulate samples by-subject random effect values using the variance components in the fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim.1&amp;lt;- simulate(mod.2)
head(sim.1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Workshop Week 1</title>
      <link>/workshops/workshop-1/</link>
      <pubDate>Wed, 14 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/workshops/workshop-1/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-are-data&#34;&gt;What Are Data?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#workspace&#34;&gt;Workspace&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#packages&#34;&gt;Packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#codebook&#34;&gt;Codebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data&#34;&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clean-data&#34;&gt;Clean Data&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#recode-variables&#34;&gt;Recode Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reverse-scoring&#34;&gt;Reverse-Scoring&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#create-composites&#34;&gt;Create Composites&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bfi-s&#34;&gt;BFI-S&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#life-events&#34;&gt;Life Events&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#descriptives&#34;&gt;Descriptives&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#scale-reliability&#34;&gt;Scale Reliability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#zero-order-correlations&#34;&gt;Zero-Order Correlations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;what-are-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What Are Data?&lt;/h1&gt;
&lt;p&gt;Data are the core of everything that we do in statistical analysis. Data come in many forms, and I don’t just mean &lt;code&gt;.csv&lt;/code&gt;, &lt;code&gt;.xls&lt;/code&gt;, &lt;code&gt;.sav&lt;/code&gt;, etc. Data can be wide, long, documented, fragmented, messy, and about anything else that you can imagine.&lt;/p&gt;
&lt;p&gt;Although data could arguably be more means than end in psychology, the importance of understanding the structure and format of your data cannot overstated. Failure to understand your data could end in improper techniques and flagrantly wrong inferences at worst. This is especially important for longitudinal data.&lt;/p&gt;
&lt;p&gt;In this workshop, we are going to talk data management and basic data cleaning. Other tutorials will go more in depth into data cleaning and reshaping. This tutorial is meant to prepare you to think about those in more nuanced ways and to help you develop a functional workflow for conducting your own research.&lt;/p&gt;
&lt;p&gt;The workshop applies to ALL of your data/projects/analysis, not just longitudinal data. These are practices that will accomplish three goals: 1) efficiently load and leave your data in the right form to be analyzed, 2) have the organization so as to follow what you did and so others can understand you did, and 3. share the data/code/plots/analyses easily and effectively. None of the following are the absolute necessary way to accomplish your data analytic goals. However, we feel that people mostly don’t think through these steps. Hammstringing them later. In other words, if you know an alternative that means you already know what we are trying to convey.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;workspace&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Workspace&lt;/h1&gt;
&lt;p&gt;When I create an &lt;code&gt;rmarkdown&lt;/code&gt; document for my own research projects, I always start by setting up my my workspace. This involves 3 steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Packages&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Codebook(s)&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below, we will step through each of these separately, setting ourselves up to (hopefully) flawlessly communicate with &lt;code&gt;R&lt;/code&gt; and our data.&lt;/p&gt;
&lt;div id=&#34;packages&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packages&lt;/h2&gt;
&lt;p&gt;Packages seems like the most basic step, but it is actually very important. &lt;strong&gt;ALWAYS LOAD YOUR PACKAGES IN A VERY INTENTIONAL ORDER AT THE BEGINNING OF YOUR SCRIPT.&lt;/strong&gt; Package conflicts suck, so it needs to be shouted. (Note: Josh will often reload or not follow this advice for didactic reasons, choosing to put library calls above the code. )&lt;/p&gt;
&lt;p&gt;For this tutorial, we are going to quite simple. We will load the &lt;code&gt;psych&lt;/code&gt; package for data descriptives, some options for cleaning and reverse coding, and some evaluations of our scales. The &lt;code&gt;plyr&lt;/code&gt; package is the predecessor of the &lt;code&gt;dplyr&lt;/code&gt; package, which is a core package of the &lt;code&gt;tidyverse&lt;/code&gt;, which you will become quite familiar with in these tutorials. I like the plyr package because it contains a couple of functions (e.g. &lt;code&gt;mapvalues()&lt;/code&gt;) that I find quite useful. Finally, we load the &lt;code&gt;tidyverse&lt;/code&gt; package, which is actually a complilation of 8 packages. Some of these we will use today and some we will use in later tutorials. All are very useful and are arguably some of the most powerful tools R offers.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load packages
library(psych)
library(plyr)
library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1.9000 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.2.1           ✔ purrr   0.3.2      
## ✔ tibble  2.1.3           ✔ dplyr   0.8.3      
## ✔ tidyr   0.8.99.9000     ✔ stringr 1.4.0      
## ✔ readr   1.3.1           ✔ forcats 0.4.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ ggplot2::%+%()     masks psych::%+%()
## ✖ ggplot2::alpha()   masks psych::alpha()
## ✖ dplyr::arrange()   masks plyr::arrange()
## ✖ purrr::compact()   masks plyr::compact()
## ✖ dplyr::count()     masks plyr::count()
## ✖ dplyr::failwith()  masks plyr::failwith()
## ✖ dplyr::filter()    masks stats::filter()
## ✖ dplyr::id()        masks plyr::id()
## ✖ dplyr::lag()       masks stats::lag()
## ✖ dplyr::mutate()    masks plyr::mutate()
## ✖ dplyr::rename()    masks plyr::rename()
## ✖ dplyr::summarise() masks plyr::summarise()
## ✖ dplyr::summarize() masks plyr::summarize()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;codebook&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Codebook&lt;/h2&gt;
&lt;p&gt;The second step is a codebook. Arguably, this is the first step because you should &lt;em&gt;create&lt;/em&gt; the codebook long before you open &lt;code&gt;R&lt;/code&gt; and load your data.&lt;/p&gt;
&lt;p&gt;In this case, we are going to using some data from the &lt;a href=&#34;https://www.diw.de/en/soep/&#34;&gt;German Socioeconomic Panel Study (GSOEP)&lt;/a&gt;, which is an ongoing Panel Study in Germany. Note that these data are for teaching purposes only, shared under the license for the Comprehensive SOEP teaching dataset, which I, as a contracted SOEP user, can use for teaching purposes. These data represent select cases from the full data set and should not be used for the purpose of publication. The full data are available for free at &lt;a href=&#34;https://www.diw.de/en/diw_02.c.222829.en/access_and_ordering.html&#34; class=&#34;uri&#34;&gt;https://www.diw.de/en/diw_02.c.222829.en/access_and_ordering.html&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For this tutorial, I created the codebook for you, and included what I believe are the core columns you may need. Some of these columns will not be particularly helpful for this dataset. For example, many of you likely work with datasets that have only a single file while others work with datasetsspread across many files. As a result, the “dataset” column of the codebook may only have a single value whereas for others it may have multiple. With longitudinal data it is likely you will have multiple.&lt;/p&gt;
&lt;p&gt;Here are my core columns that are based on the original data:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;dataset&lt;/strong&gt;: this column indexes the &lt;strong&gt;name&lt;/strong&gt; of the dataset that you will be pulling the data from. This is important because we will use this info later on (see &lt;code&gt;purrr&lt;/code&gt; tutorial) to load and clean specific data files. Even if you don’t have multiple data sets, I believe consistency is more important and suggest using this.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;old_name&lt;/strong&gt;: this column is the name of the variable in the data you are pulling it from. This should be exact. The goal of this column is that it will allow us to &lt;code&gt;select()&lt;/code&gt; variables from the original data file and rename them something that is more useful to us. If you have worked with qualtrics (really any data) you know why this is important.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;item_text&lt;/strong&gt;: this column is the original text that participants saw or a description of the item.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;scale&lt;/strong&gt;: this column tells you what the scale of the variable is. Is it a numeric variable, a text variable, etc. This is helpful for knowing the plausible range.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;reverse&lt;/strong&gt;: this column tells you whether items in a scale need to be reverse coded. I recommend coding this as 1 (leave alone) and -1 (reverse) for reasons that will become clear later.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;mini&lt;/strong&gt;: this column represents the minimum value of scales that are numeric. Leave blank otherwise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;maxi&lt;/strong&gt;: this column represents the maximumv alue of scales that are numeric. Leave blank otherwise.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;recode&lt;/strong&gt;: sometimes, we want to recode variables for analyses (e.g. for categorical variables with many levels where sample sizes for some levels are too small to actually do anything with it). I use this column to note the kind of recoding I’ll do to a variable for transparency.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here are additional columns that will make our lives easier or are applicable to some but not all data sets:&lt;/p&gt;
&lt;ol start=&#34;9&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;category&lt;/strong&gt;: broad categories that different variables can be put into. I’m a fan of naming them things like “outcome”, “predictor”, “moderator”, “demographic”, “procedural”, etc. but sometimes use more descriptive labels like “Big 5” to indicate the model from which the measures are derived.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;label&lt;/strong&gt;: label is basically one level lower than category. So if the category is Big 5, the label would be, or example, “A” for Agreeableness, “SWB” for subjective well-being, etc. This column is most important and useful when you have multiple items in a scales, so I’ll typically leave this blank when something is a standalone variable (e.g. sex, single-item scales, etc.).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;item_name&lt;/strong&gt;: This is the lowest level and most descriptive variable. It indicates which item in scale something is. So it may be “kind” for Agreebleness or “sex” for the demographic biological sex variable.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;year&lt;/strong&gt;: for longitudinal data, we have several waves of data and the name of the same item across waves is often different, so it’s important to note to which wave an item belongs. You can do this by noting the wave (e.g. 1, 2, 3), but I prefer the actual year the data were collected (e.g. 2005, 2009, etc.) if that is appropriate. See Lecture #1 on discussion of meaningful time metrics. Note that this differs from that discussion in codebook describes how you collected the data, not necessarily how you want to analyze the data.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;new_name&lt;/strong&gt;: This is a column that brings together much of the information we’ve already collected. It’s purpose is to be the new name that we will give to the variable that is more useful and descriptive to us. This is a constructed variable that brings together others. I like to make it a combination of “category”, “label”, “item_name”, and year using varying combos of &#34;_&#34; and “.” that we can use later with tidyverse functions. I typically construct this variable in Excel using the &lt;code&gt;CONCATENATE()&lt;/code&gt; function, but it could also be done in &lt;code&gt;R&lt;/code&gt;. The reason I do it in Excel is that it makes it easier for someone who may be reviewing my codebook.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is a seperate discussion to be had on naming conventions for your variables, but the important idea to remember is that names convey important information and we want to use this information later on to make our life easier. By coding these variables using this information AND systematically using different seperators we can accomplish this goal.&lt;/p&gt;
&lt;ol start=&#34;14&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;meta&lt;/strong&gt;: Some datasets have a meta name, which essentially means a name that variable has across all waves to make it clear which variables are the same. They are not always useful as some data sets have meta names but no great way of extracting variables using them. But they’re still typically useful to include in your codebook regardless.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below, I’ll load in the codebook we will use for this study, which will include all of the above columns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set the path
wd &amp;lt;- &amp;quot;https://github.com/emoriebeck/R-tutorials/blob/master/ALDA/week_1_descriptives&amp;quot;

# load the codebook
(codebook &amp;lt;- url(sprintf(&amp;quot;%s/codebook.csv?raw=true&amp;quot;, wd)) %&amp;gt;% 
    read_csv(.) %&amp;gt;%
    mutate(old_name = str_to_lower(old_name)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   dataset = col_character(),
##   old_name = col_character(),
##   item_text = col_character(),
##   scale = col_character(),
##   category = col_character(),
##   label = col_character(),
##   item_name = col_character(),
##   year = col_double(),
##   new_name = col_character(),
##   reverse = col_double(),
##   mini = col_double(),
##   maxi = col_double(),
##   recode = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 153 x 13
##    dataset old_name item_text scale category label item_name  year new_name
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;     &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   
##  1 &amp;lt;NA&amp;gt;    persnr   Never Ch… &amp;lt;NA&amp;gt;  Procedu… &amp;lt;NA&amp;gt;  SID           0 Procedu…
##  2 &amp;lt;NA&amp;gt;    hhnr     househol… &amp;lt;NA&amp;gt;  Procedu… &amp;lt;NA&amp;gt;  household     0 Procedu…
##  3 ppfad   gebjahr  Year of … nume… Demogra… &amp;lt;NA&amp;gt;  DOB           0 Demogra…
##  4 ppfad   sex      Sex       &amp;quot;\n1… Demogra… &amp;lt;NA&amp;gt;  Sex           0 Demogra…
##  5 vp      vp12501  Thorough… &amp;lt;NA&amp;gt;  Big 5    C     thorough   2005 Big 5__…
##  6 zp      zp12001  Thorough… &amp;lt;NA&amp;gt;  Big 5    C     thorough   2009 Big 5__…
##  7 bdp     bdp15101 Thorough… &amp;lt;NA&amp;gt;  Big 5    C     thorough   2013 Big 5__…
##  8 vp      vp12502  Am commu… &amp;lt;NA&amp;gt;  Big 5    E     communic   2005 Big 5__…
##  9 zp      zp12002  Am commu… &amp;lt;NA&amp;gt;  Big 5    E     communic   2009 Big 5__…
## 10 bdp     bdp15102 Am commu… &amp;lt;NA&amp;gt;  Big 5    E     communic   2013 Big 5__…
## # … with 143 more rows, and 4 more variables: reverse &amp;lt;dbl&amp;gt;, mini &amp;lt;dbl&amp;gt;,
## #   maxi &amp;lt;dbl&amp;gt;, recode &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;First, we need to load in the data. We’re going to use three waves of data from the German Socioeconomic Panel Study, which is a longitudinal study of German households that has been conducted since 1984. We’re going to use more recent data from three waves of personality data collected between 2005 and 2013.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: we will be using the teaching set of the GSOEP data set. I will not be pulling from the raw files as a result of this. I will also not be mirroring the format that you would usually load the GSOEP from because that is slightly more complicated and somethng we will return to in a later tutorial after we have more skills. I’ve left that code for now, but it won’t make a lot of sense right now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;path &amp;lt;- &amp;quot;~/Box/network/other projects/PCLE Replication/data/sav_files&amp;quot;
ref &amp;lt;- sprintf(&amp;quot;%s/cirdef.sav&amp;quot;, path) %&amp;gt;% haven::read_sav(.) %&amp;gt;% select(hhnr, rgroup20)
read_fun &amp;lt;- function(Year){
  vars &amp;lt;- (codebook %&amp;gt;% filter(year == Year | year == 0))$old_name
  set &amp;lt;- (codebook %&amp;gt;% filter(year == Year))$dataset[1]
  sprintf(&amp;quot;%s/%s.sav&amp;quot;, path, set) %&amp;gt;% haven::read_sav(.) %&amp;gt;%
    full_join(ref) %&amp;gt;%
    filter(rgroup20 &amp;gt; 10) %&amp;gt;%
    select(one_of(vars)) %&amp;gt;%
    gather(key = item, value = value, -persnr, -hhnr, na.rm = T)
}

vars &amp;lt;- (codebook %&amp;gt;% filter(year == 0))$old_name
dem &amp;lt;- sprintf(&amp;quot;%s/ppfad.sav&amp;quot;, path) %&amp;gt;% 
  haven::read_sav(.) %&amp;gt;%
  select(vars)
  
tibble(year = c(2005:2015)) %&amp;gt;%
  mutate(data = map(year, read_fun)) %&amp;gt;%
  select(-year) %&amp;gt;% 
  unnest(data) %&amp;gt;%
  distinct() %&amp;gt;% 
  filter(!is.na(value)) %&amp;gt;%
  spread(key = item, value = value) %&amp;gt;%
  left_join(dem) %&amp;gt;%
  write.csv(., file = &amp;quot;~/Documents/Github/R-tutorials/ALDA/week_1_descriptives/data/wdeek_1_data.csv&amp;quot;, row.names = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code below shows how I would read in and rename a wide-format data set using the codebook I created.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;old.names &amp;lt;- codebook$old_name # get old column names
new.names &amp;lt;- codebook$new_name # get new column names

(soep &amp;lt;- url(sprintf(&amp;quot;%s/data/week_1_data.csv?raw=true&amp;quot;, wd)) %&amp;gt;% # path to data
  read_csv(.) %&amp;gt;% # read in data
  select(old.names) %&amp;gt;% # select the columns from our codebook
  setNames(new.names)) # rename columns with our new names&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   .default = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## See spec(...) for full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 28,290 x 153
##    Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1             901               94             1951                2
##  2            1202              124             1913                2
##  3            2301              230             1946                1
##  4            2302              230             1946                2
##  5            2304              230             1978                1
##  6            2305              230             1946                2
##  7            4601              469             1933                2
##  8            4701              477             1919                2
##  9            4901              493             1925                2
## 10            5201              523             1955                1
## # … with 28,280 more rows, and 149 more variables: `Big
## #   5__C_thorough.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_thorough.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_thorough.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_communic.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_communic.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_communic.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_coarse.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_coarse.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_coarse.2013` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_original.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_worry.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2013` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_forgive.2009` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_lazy.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_sociable.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2005` &amp;lt;dbl&amp;gt;, `Big 5__O_artistic.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2013` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_nervous.2009` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_efficient.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_reserved.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_friendly.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2013` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_imagin.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_dealStress.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2013` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2006` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2010` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2012` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2013` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2014` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2015` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2005` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2006` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2011` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2013` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2014` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2015` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2005` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2006` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2007` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2008` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2009` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2010` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2011` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2013` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2014` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2015` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2005` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2006` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2007` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2008` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2011` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2012` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2013` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2014` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2015` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2011` &amp;lt;dbl&amp;gt;, `Life Event__NewPart.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2013` &amp;lt;dbl&amp;gt;, `Life Event__NewPart.2014` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2015` &amp;lt;dbl&amp;gt;, `Life Event__Married.2005` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2006` &amp;lt;dbl&amp;gt;, `Life Event__Married.2007` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2008` &amp;lt;dbl&amp;gt;, `Life Event__Married.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2010` &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;clean-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Clean Data&lt;/h1&gt;
&lt;div id=&#34;recode-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recode Variables&lt;/h2&gt;
&lt;p&gt;Many of the data we work with have observations that are missing for a variety of reasons. In &lt;code&gt;R&lt;/code&gt;, we treat missing values as &lt;code&gt;NA&lt;/code&gt;, but many other programs from which you may be importing your data may use other codes (e.g. 999, -999, etc.). Large panel studies tend to use small negative values to indicate different types of missingness. This is why it is important to note down the scale in your codebook. That way you can check which values may need to be recoded to explicit &lt;code&gt;NA&lt;/code&gt; values.&lt;/p&gt;
&lt;p&gt;In the GSOEP, &lt;code&gt;-1&lt;/code&gt; to &lt;code&gt;-7&lt;/code&gt; indicate various types of missing values, so we will recode these to &lt;code&gt;NA&lt;/code&gt;. To do this, we will use one of my favorite functions, &lt;code&gt;mapvalues()&lt;/code&gt;, from the &lt;code&gt;plyr&lt;/code&gt; package. In later tutorials where we read in and manipulate more complex data sets, we will use &lt;code&gt;mapvalues()&lt;/code&gt; a lot. Basically, mapvalues takes 4 key arguments: (1) the variable you are recoding, (2) a vector of initial values &lt;code&gt;from&lt;/code&gt; which you want to (3) recode your variable &lt;code&gt;to&lt;/code&gt; using a vector of new values in the same order as the old values, and (4) a way to turn off warnings if some levels are not in your data (&lt;code&gt;warn_missing = F&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(soep &amp;lt;- soep %&amp;gt;%
  mutate_all(~as.numeric(mapvalues(., from = seq(-1,-7, -1), # recode negative 
                to = rep(NA, 7), warn_missing = F)))) # values to NA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 28,290 x 153
##    Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
##  1             901               94             1951                2
##  2            1202              124             1913                2
##  3            2301              230             1946                1
##  4            2302              230             1946                2
##  5            2304              230             1978                1
##  6            2305              230             1946                2
##  7            4601              469             1933                2
##  8            4701              477             1919                2
##  9            4901              493             1925                2
## 10            5201              523             1955                1
## # … with 28,280 more rows, and 149 more variables: `Big
## #   5__C_thorough.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_thorough.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_thorough.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_communic.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_communic.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_communic.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_coarse.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_coarse.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_coarse.2013` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_original.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_original.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_worry.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_worry.2013` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_forgive.2009` &amp;lt;dbl&amp;gt;, `Big 5__A_forgive.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_lazy.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_lazy.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_sociable.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_sociable.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2005` &amp;lt;dbl&amp;gt;, `Big 5__O_artistic.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_artistic.2013` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_nervous.2009` &amp;lt;dbl&amp;gt;, `Big 5__N_nervous.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2005` &amp;lt;dbl&amp;gt;, `Big 5__C_efficient.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__C_efficient.2013` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__E_reserved.2009` &amp;lt;dbl&amp;gt;, `Big 5__E_reserved.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2005` &amp;lt;dbl&amp;gt;, `Big 5__A_friendly.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__A_friendly.2013` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2005` &amp;lt;dbl&amp;gt;, `Big
## #   5__O_imagin.2009` &amp;lt;dbl&amp;gt;, `Big 5__O_imagin.2013` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2005` &amp;lt;dbl&amp;gt;, `Big 5__N_dealStress.2009` &amp;lt;dbl&amp;gt;, `Big
## #   5__N_dealStress.2013` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2005` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2006` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2009` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2010` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2011` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2012` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2013` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldBrth.2014` &amp;lt;dbl&amp;gt;, `Life Event__ChldBrth.2015` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2005` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2006` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2007` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2008` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2010` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2011` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__ChldMvOut.2013` &amp;lt;dbl&amp;gt;, `Life
## #   Event__ChldMvOut.2014` &amp;lt;dbl&amp;gt;, `Life Event__ChldMvOut.2015` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2005` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2006` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2007` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2008` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2009` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2010` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2011` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2013` &amp;lt;dbl&amp;gt;, `Life Event__Divorce.2014` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Divorce.2015` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2005` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2006` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2007` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2008` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2010` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2011` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2012` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2013` &amp;lt;dbl&amp;gt;,
## #   `Life Event__DadDied.2014` &amp;lt;dbl&amp;gt;, `Life Event__DadDied.2015` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2011` &amp;lt;dbl&amp;gt;, `Life Event__NewPart.2012` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2013` &amp;lt;dbl&amp;gt;, `Life Event__NewPart.2014` &amp;lt;dbl&amp;gt;,
## #   `Life Event__NewPart.2015` &amp;lt;dbl&amp;gt;, `Life Event__Married.2005` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2006` &amp;lt;dbl&amp;gt;, `Life Event__Married.2007` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2008` &amp;lt;dbl&amp;gt;, `Life Event__Married.2009` &amp;lt;dbl&amp;gt;,
## #   `Life Event__Married.2010` &amp;lt;dbl&amp;gt;, …&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reverse-scoring&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reverse-Scoring&lt;/h2&gt;
&lt;p&gt;Many scales we use have items that are positively or negatively keyed. High ratings on positively keyed items are indicative of being high on a construct. In contrast, high ratings on negatively keyed items are indicative of being low on a construct. Thus, to create the composite scores of constructs we often use, we must first “reverse” the negatively keyed items so that high scores indicate being higher on the construct.&lt;/p&gt;
&lt;p&gt;There are a few ways to do this in &lt;code&gt;R&lt;/code&gt;. Below, I’ll demonstrate how to do so using the &lt;code&gt;reverse.code()&lt;/code&gt; function in the &lt;code&gt;psych&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt;. This function was built to make reverse coding more efficient (i.e. please don’t run every item that needs to be recoded with separate lines of code!!).&lt;/p&gt;
&lt;p&gt;Before we can do that, though, we need to restructure the data a bit in order to bring in the reverse coding information from our codebook. We will talk more about what’s happening here in later tutorials on &lt;code&gt;tidyr&lt;/code&gt;, so for now, just bear with me.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(soep_long &amp;lt;- soep %&amp;gt;%
  gather(key = item, value = value, -contains(&amp;quot;Procedural&amp;quot;), # change to long format
         -contains(&amp;quot;Demographic&amp;quot;), na.rm = T) %&amp;gt;%
  left_join(codebook %&amp;gt;% select(item = new_name, reverse, mini, maxi)) %&amp;gt;% # bring in codebook
  separate(item, c(&amp;quot;type&amp;quot;, &amp;quot;item&amp;quot;), sep = &amp;quot;__&amp;quot;) %&amp;gt;% # separate category
  separate(item, c(&amp;quot;item&amp;quot;, &amp;quot;year&amp;quot;), sep = &amp;quot;[.]&amp;quot;) %&amp;gt;% # seprate year
  separate(item, c(&amp;quot;item&amp;quot;, &amp;quot;scrap&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;% # separate scale and item
  mutate(value = as.numeric(value), # change to numeric
         value = ifelse(reverse == -1, 
            reverse.code(-1, value, mini = mini, maxi = maxi), value)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;item&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 19618 rows
## [452105, 452106, 452107, 452108, 452109, 452110, 452111, 452112, 452113,
## 452114, 452115, 452116, 452117, 452118, 452119, 452120, 452121, 452122,
## 452123, 452124, ...].&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 471,722 x 12
##    Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex type 
##              &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
##  1             901               94             1951                2 Big 5
##  2            1202              124             1913                2 Big 5
##  3            2301              230             1946                1 Big 5
##  4            2302              230             1946                2 Big 5
##  5            2304              230             1978                1 Big 5
##  6            4601              469             1933                2 Big 5
##  7            4701              477             1919                2 Big 5
##  8            4901              493             1925                2 Big 5
##  9            5201              523             1955                1 Big 5
## 10            5202              523             1956                2 Big 5
## # … with 471,712 more rows, and 7 more variables: item &amp;lt;chr&amp;gt;, scrap &amp;lt;chr&amp;gt;,
## #   year &amp;lt;chr&amp;gt;, value &amp;lt;dbl&amp;gt;, reverse &amp;lt;dbl&amp;gt;, mini &amp;lt;dbl&amp;gt;, maxi &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-composites&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Create Composites&lt;/h2&gt;
&lt;p&gt;Now that we have reverse coded our items, we can create composites.&lt;/p&gt;
&lt;div id=&#34;bfi-s&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;BFI-S&lt;/h3&gt;
&lt;p&gt;We’ll start with our scale – in this case, the Big 5 from the German translation of the BFI-S.&lt;/p&gt;
&lt;p&gt;Here’s the simplest way, which is also the long way because you’d have to do it for each scale in each year, which I don’t recommend.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;soep$C.2005 &amp;lt;- with(soep, rowMeans(cbind(`Big 5__C_thorough.2005`, `Big 5__C_lazy.2005`, `Big 5__C_efficient.2005`), na.rm = T)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But personally, I don’t have a desire to do that 15 times (5 traits times 3 waves), so we can use our codebook and &lt;code&gt;dplyr&lt;/code&gt; to make our lives a whole lot easier. In general, trying to run everything simultanously saves from copy-paste errors, makes your code more readable, and reduces the total amount of code. So while the below code may not make intuiative sense immediately, it is nonetheless what we are working towards.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;soep &amp;lt;- soep %&amp;gt;% select(-C.2005) # get rid of added column

(b5_soep_long &amp;lt;- soep_long %&amp;gt;%
  filter(type == &amp;quot;Big 5&amp;quot;) %&amp;gt;% # keep Big 5 variables
  group_by(Procedural__SID, item, year) %&amp;gt;% # group by person, construct, &amp;amp; year
  summarize(value = mean(value, na.rm = T)) %&amp;gt;% # calculate means
  ungroup() %&amp;gt;% # ungroup
  left_join(soep_long %&amp;gt;% # bring demographic info back in 
    select(Procedural__SID, DOB = Demographic__DOB, Sex = Demographic__Sex) %&amp;gt;%
    distinct()))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;Procedural__SID&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 151,186 x 6
##    Procedural__SID item  year  value   DOB   Sex
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1             901 A     2005   5     1951     2
##  2             901 A     2009   5.33  1951     2
##  3             901 A     2013   5     1951     2
##  4             901 C     2005   5.33  1951     2
##  5             901 C     2009   5.5   1951     2
##  6             901 C     2013   6     1951     2
##  7             901 E     2005   4     1951     2
##  8             901 E     2009   4     1951     2
##  9             901 E     2013   4     1951     2
## 10             901 N     2005   4     1951     2
## # … with 151,176 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;life-events&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Life Events&lt;/h3&gt;
&lt;p&gt;We also want to create a variable that indexes whether our participants experienced any of the life events during the years of interest (2005-2015).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(events_long &amp;lt;- soep_long %&amp;gt;%
  filter(type == &amp;quot;Life Event&amp;quot;) %&amp;gt;% # keep only life events
  group_by(Procedural__SID, item) %&amp;gt;% # group by person and event
  summarize(value = sum(value, na.rm = T), # sum up whether they experiened the event at all
            value = ifelse(value &amp;gt; 1, 1, 0))) # if more than once 1, otherwise 0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15,061 x 3
## # Groups:   Procedural__SID [10,019]
##    Procedural__SID item      value
##              &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
##  1             901 MomDied       1
##  2            2301 MoveIn        0
##  3            2301 PartDied      1
##  4            2305 MoveIn        0
##  5            4601 PartDied      0
##  6            5201 ChldMvOut     1
##  7            5201 DadDied       0
##  8            5202 ChldMvOut     1
##  9            5203 MoveIn        1
## 10            5303 MomDied       0
## # … with 15,051 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;descriptives&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Descriptives&lt;/h1&gt;
&lt;p&gt;Descriptives of your data are incredibly important. They are your first line of defense against things that could go wrong later on when you run inferential stats. They help you check the distribution of your variables (e.g. non-normally distributed), look for implausible values made through coding or participant error, and allow you to anticipate what your findings will look like.&lt;/p&gt;
&lt;p&gt;There are lots of ways to create great tables of descriptives. My favorite way is using &lt;code&gt;dplyr&lt;/code&gt;, but we will save that for a later lesson on creating great APA style tables in &lt;code&gt;R&lt;/code&gt;. For now, we’ll use a wonderfully helpful function from the &lt;code&gt;psych&lt;/code&gt; package called &lt;code&gt;describe()&lt;/code&gt; in conjunction with a small amount of &lt;code&gt;tidyr&lt;/code&gt; to reshape the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b5_soep_long  %&amp;gt;%
  unite(tmp, item, year, sep = &amp;quot;_&amp;quot;) %&amp;gt;% # make new column that joins item and year
  spread(tmp, value) %&amp;gt;% # make wide because that helps describe
  describe(.) # call describe&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                 vars     n       mean          sd     median    trimmed
## Procedural__SID    1 16719 8321022.91 10677731.19 3105002.00 6487615.30
## DOB                2 16719    1960.03       18.48    1960.00    1960.22
## Sex                3 16719       1.52        0.50       2.00       1.53
## A_2005             4 10419       5.79        0.98       6.00       5.83
## A_2009             5 10294       5.68        0.99       5.67       5.72
## A_2013             6  9535       5.74        0.96       5.67       5.78
## C_2005             7 10412       6.23        0.96       6.33       6.34
## C_2009             8 10290       6.16        0.95       6.33       6.25
## C_2013             9  9530       6.17        0.91       6.33       6.25
## E_2005            10 10416       5.15        1.15       5.33       5.18
## E_2009            11 10291       5.11        1.15       5.00       5.13
## E_2013            12  9533       5.20        1.11       5.33       5.24
## N_2005            13 10413       4.71        1.23       4.67       4.72
## N_2009            14 10294       4.84        1.22       5.00       4.87
## N_2013            15  9534       4.92        1.21       5.00       4.93
## O_2005            16 10408       4.51        1.22       4.67       4.53
## O_2009            17 10287       4.40        1.22       4.33       4.41
## O_2013            18  9530       4.60        1.18       4.67       4.62
##                        mad     min        max       range  skew kurtosis
## Procedural__SID 3735541.17  901.00 3.5022e+07 35021101.00  1.50     0.56
## DOB                  20.76 1909.00 1.9950e+03       86.00 -0.08    -0.84
## Sex                   0.00    1.00 2.0000e+00        1.00 -0.10    -1.99
## A_2005                0.99    1.00 7.5000e+00        6.50 -0.40    -0.14
## A_2009                0.99    1.33 8.0000e+00        6.67 -0.36    -0.19
## A_2013                0.99    1.33 8.0000e+00        6.67 -0.43     0.05
## C_2005                0.99    1.00 8.0000e+00        7.00 -0.95     0.84
## C_2009                0.99    1.00 8.0000e+00        7.00 -0.82     0.49
## C_2013                0.99    1.33 8.0000e+00        6.67 -0.72     0.17
## E_2005                0.99    1.00 7.5000e+00        6.50 -0.27    -0.16
## E_2009                0.99    1.00 7.3300e+00        6.33 -0.23    -0.18
## E_2013                0.99    1.33 7.3300e+00        6.00 -0.29    -0.18
## N_2005                1.48    1.50 8.0000e+00        6.50 -0.07    -0.32
## N_2009                0.99    1.50 7.6700e+00        6.17 -0.17    -0.30
## N_2013                1.48    1.50 8.0000e+00        6.50 -0.15    -0.30
## O_2005                1.48    1.00 7.0000e+00        6.00 -0.23    -0.18
## O_2009                1.48    1.00 7.0000e+00        6.00 -0.10    -0.30
## O_2013                0.99    1.00 7.0000e+00        6.00 -0.21    -0.20
##                       se
## Procedural__SID 82579.80
## DOB                 0.14
## Sex                 0.00
## A_2005              0.01
## A_2009              0.01
## A_2013              0.01
## C_2005              0.01
## C_2009              0.01
## C_2013              0.01
## E_2005              0.01
## E_2009              0.01
## E_2013              0.01
## N_2005              0.01
## N_2009              0.01
## N_2013              0.01
## O_2005              0.01
## O_2009              0.01
## O_2013              0.01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For count variables, like life events, we need to use something slightly different. We’re typically more interested in counts – in this case, how many people experienced each life event in the 10 years we’re considering?&lt;/p&gt;
&lt;p&gt;To do this, we’ll use a little bit of &lt;code&gt;dplyr&lt;/code&gt; rather than the base &lt;code&gt;R&lt;/code&gt; function &lt;code&gt;table()&lt;/code&gt; that is often used for count data. Instead, we’ll use a combination of &lt;code&gt;group_by()&lt;/code&gt; and &lt;code&gt;n()&lt;/code&gt; to get the counts by group. In the end, we’re left with a nice little table of counts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;events_long %&amp;gt;%
  group_by(item, value) %&amp;gt;% 
  summarize(N = n()) %&amp;gt;%
  ungroup() %&amp;gt;%
  spread(value, N)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##    item        `0`   `1`
##    &amp;lt;chr&amp;gt;     &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1 ChldBrth   1600   735
##  2 ChldMvOut  1555   830
##  3 DadDied     953   213
##  4 Divorce     414   122
##  5 Married    1646   331
##  6 MomDied     929   219
##  7 MoveIn     1403   419
##  8 NewPart    1207   420
##  9 PartDied    402    76
## 10 SepPart    1172   415&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;scale-reliability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Scale Reliability&lt;/h2&gt;
&lt;p&gt;When we work with scales, it’s often a good idea to check the internal consistency of your scale. If the scale isn’t performing how it should be, that could critically impact the inferences you make from your data.&lt;/p&gt;
&lt;p&gt;To check the internal consistency of our Big 5 scales, we will use the &lt;code&gt;alpha()&lt;/code&gt; function from the &lt;code&gt;psych&lt;/code&gt; package, which will give us Cronbach’s as well as a number of other indicators of internal consistency.&lt;/p&gt;
&lt;p&gt;Here’s the way you may have seen / done this in the past.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;alpha.C.2005 &amp;lt;- with(soep, psych::alpha(x = cbind(`Big 5__C_thorough.2005`, 
                                  `Big 5__C_lazy.2005`, `Big 5__C_efficient.2005`)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in psych::alpha(x = cbind(`Big 5__C_thorough.2005`, `Big 5__C_lazy.2005`, : Some items were negatively correlated with the total scale and probably 
## should be reversed.  
## To do this, run the function again with the &amp;#39;check.keys=TRUE&amp;#39; option&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Some items ( Big 5__C_lazy.2005 ) were negatively correlated with the total scale and 
## probably should be reversed.  
## To do this, run the function again with the &amp;#39;check.keys=TRUE&amp;#39; option&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But again, doing this 15 times would be quite a pain and would open you up to the possibility of a lot of copy and paste errors.&lt;/p&gt;
&lt;p&gt;So instead, to do this, I’m going to use a mix of the tidyverse. At first glance, it may seem complex but as you move through other tutorials (particularly the &lt;code&gt;purrr&lt;/code&gt; tutorial), it will begin to make much more sense.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# short function to reshape data and run alpha
alpha_fun &amp;lt;- function(df){
  df %&amp;gt;% spread(scrap,value) %&amp;gt;% psych::alpha(.)
}

(alphas &amp;lt;- soep_long %&amp;gt;%
  filter(type == &amp;quot;Big 5&amp;quot;) %&amp;gt;% # filter out Big 5
  select(Procedural__SID, item:value) %&amp;gt;% # get rid of extra columns
  group_by(item, year) %&amp;gt;% # group by construct and year
  nest() %&amp;gt;% # nest the data
  mutate(alpha_res = map(data, alpha_fun), # run alpha
         alpha = map(alpha_res, ~.$total[2])) %&amp;gt;% # get the alpha value
  unnest(alpha)) # pull it out of the list column&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 15 x 5
## # Groups:   item, year [15]
##    item  year            data alpha_res std.alpha
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;list&amp;lt;df[,3]&amp;gt;&amp;gt; &amp;lt;list&amp;gt;        &amp;lt;dbl&amp;gt;
##  1 C     2005    [31,117 × 3] &amp;lt;psych&amp;gt;       0.515
##  2 C     2009    [30,728 × 3] &amp;lt;psych&amp;gt;       0.494
##  3 C     2013    [28,496 × 3] &amp;lt;psych&amp;gt;       0.482
##  4 E     2005    [31,188 × 3] &amp;lt;psych&amp;gt;       0.540
##  5 E     2009    [30,770 × 3] &amp;lt;psych&amp;gt;       0.530
##  6 E     2013    [28,532 × 3] &amp;lt;psych&amp;gt;       0.544
##  7 A     2005    [31,184 × 3] &amp;lt;psych&amp;gt;       0.418
##  8 A     2009    [30,796 × 3] &amp;lt;psych&amp;gt;       0.410
##  9 A     2013    [28,529 × 3] &amp;lt;psych&amp;gt;       0.401
## 10 O     2005    [31,091 × 3] &amp;lt;psych&amp;gt;       0.527
## 11 O     2009    [30,722 × 3] &amp;lt;psych&amp;gt;       0.510
## 12 O     2013    [28,451 × 3] &amp;lt;psych&amp;gt;       0.497
## 13 N     2005    [31,162 × 3] &amp;lt;psych&amp;gt;       0.473
## 14 N     2009    [30,802 × 3] &amp;lt;psych&amp;gt;       0.490
## 15 N     2013    [28,536 × 3] &amp;lt;psych&amp;gt;       0.480&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;zero-order-correlations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Zero-Order Correlations&lt;/h2&gt;
&lt;p&gt;Finally, we often want to look at the zero-order correlation among study variables to make sure they are performing as we think they should.&lt;/p&gt;
&lt;p&gt;To run the correlations, we will need to have our data in wide format, so we’re going to do a little bit of reshaping before we do.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b5_soep_long %&amp;gt;%
  unite(tmp, item, year, sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  spread(key = tmp, value = value) %&amp;gt;% 
  select(-Procedural__SID) %&amp;gt;%
  cor(., use = &amp;quot;pairwise&amp;quot;) %&amp;gt;%
  round(., 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          DOB   Sex A_2005 A_2009 A_2013 C_2005 C_2009 C_2013 E_2005 E_2009
## DOB     1.00  0.00  -0.08  -0.07  -0.06  -0.13  -0.12  -0.14   0.10   0.12
## Sex     0.00  1.00   0.18   0.17   0.18   0.05   0.07   0.09   0.08   0.08
## A_2005 -0.08  0.18   1.00   0.50   0.50   0.32   0.20   0.19   0.10   0.06
## A_2009 -0.07  0.17   0.50   1.00   0.55   0.19   0.28   0.18   0.05   0.08
## A_2013 -0.06  0.18   0.50   0.55   1.00   0.18   0.19   0.29   0.04   0.06
## C_2005 -0.13  0.05   0.32   0.19   0.18   1.00   0.52   0.48   0.19   0.10
## C_2009 -0.12  0.07   0.20   0.28   0.19   0.52   1.00   0.55   0.12   0.16
## C_2013 -0.14  0.09   0.19   0.18   0.29   0.48   0.55   1.00   0.13   0.14
## E_2005  0.10  0.08   0.10   0.05   0.04   0.19   0.12   0.13   1.00   0.61
## E_2009  0.12  0.08   0.06   0.08   0.06   0.10   0.16   0.14   0.61   1.00
## E_2013  0.10  0.11   0.04   0.04   0.07   0.10   0.10   0.18   0.59   0.65
## N_2005  0.06 -0.18   0.10   0.06   0.02   0.09   0.06   0.03   0.18   0.10
## N_2009  0.03 -0.22   0.07   0.09   0.03   0.06   0.08   0.05   0.13   0.16
## N_2013  0.02 -0.21   0.06   0.06   0.10   0.04   0.06   0.08   0.10   0.10
## O_2005  0.11  0.06   0.12   0.09   0.07   0.17   0.12   0.08   0.40   0.29
## O_2009  0.10  0.05   0.05   0.11   0.07   0.06   0.13   0.08   0.26   0.36
## O_2013  0.05  0.07   0.08   0.09   0.13   0.07   0.08   0.15   0.24   0.28
##        E_2013 N_2005 N_2009 N_2013 O_2005 O_2009 O_2013
## DOB      0.10   0.06   0.03   0.02   0.11   0.10   0.05
## Sex      0.11  -0.18  -0.22  -0.21   0.06   0.05   0.07
## A_2005   0.04   0.10   0.07   0.06   0.12   0.05   0.08
## A_2009   0.04   0.06   0.09   0.06   0.09   0.11   0.09
## A_2013   0.07   0.02   0.03   0.10   0.07   0.07   0.13
## C_2005   0.10   0.09   0.06   0.04   0.17   0.06   0.07
## C_2009   0.10   0.06   0.08   0.06   0.12   0.13   0.08
## C_2013   0.18   0.03   0.05   0.08   0.08   0.08   0.15
## E_2005   0.59   0.18   0.13   0.10   0.40   0.26   0.24
## E_2009   0.65   0.10   0.16   0.10   0.29   0.36   0.28
## E_2013   1.00   0.11   0.13   0.15   0.26   0.28   0.35
## N_2005   0.11   1.00   0.55   0.53   0.09   0.08   0.06
## N_2009   0.13   0.55   1.00   0.60   0.06   0.07   0.07
## N_2013   0.15   0.53   0.60   1.00   0.05   0.05   0.05
## O_2005   0.26   0.09   0.06   0.05   1.00   0.58   0.55
## O_2009   0.28   0.08   0.07   0.05   0.58   1.00   0.61
## O_2013   0.35   0.06   0.07   0.05   0.55   0.61   1.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a lot of values and a little hard to make sense of, so as a bonus, I’m going to give you a little bit of more complex code that makes this more readable (and publishable ).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- b5_soep_long %&amp;gt;%
  unite(tmp, item, year, sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  spread(key = tmp, value = value) %&amp;gt;% 
  select(-Procedural__SID, -DOB, -Sex) %&amp;gt;%
  cor(., use = &amp;quot;pairwise&amp;quot;) 

r[upper.tri(r, diag = T)] &amp;lt;- NA
diag(r) &amp;lt;- (alphas %&amp;gt;% arrange(item, year))$std.alpha

r %&amp;gt;% data.frame %&amp;gt;%
  rownames_to_column(&amp;quot;V1&amp;quot;) %&amp;gt;%
  gather(key = V2, value = r, na.rm = T, -V1) %&amp;gt;%
  separate(V1, c(&amp;quot;T1&amp;quot;, &amp;quot;Year1&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  separate(V2, c(&amp;quot;T2&amp;quot;, &amp;quot;Year2&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  mutate_at(vars(Year1), ~factor(., levels = c(2013, 2009, 2005))) %&amp;gt;%
  ggplot(aes(x = Year2, y = Year1, fill = r)) +
    geom_raster() + 
  scale_fill_gradient2(low = &amp;quot;blue&amp;quot;, high = &amp;quot;red&amp;quot;, mid = &amp;quot;white&amp;quot;, 
   midpoint = 0, limit = c(-1,1), space = &amp;quot;Lab&amp;quot;, 
   name=&amp;quot;Correlations&amp;quot;) +  
  geom_text(aes(label = round(r,2))) +
    facet_grid(T1 ~ T2) +
    theme_classic()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/Workshops/2019-08-14-workshop-1_files/figure-html/unnamed-chunk-5-1.png&#34; alt=&#34;Correlations among Personality Indicators. Values on the diagonal represent Chronbach&#39;s alpha for each scale in each year. Within-trait correlations represent test-retest correlations.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Correlations among Personality Indicators. Values on the diagonal represent Chronbach’s alpha for each scale in each year. Within-trait correlations represent test-retest correlations.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
