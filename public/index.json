[{"authors":[],"categories":[],"content":" Please take a look at the syllabus for important class details.\nAny updates or announcements to the class will be found here on the main page. Otherwise, the lectures and workshop pages will where you will spend most of your time.\n","date":1567036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565821345,"objectID":"2817befdacfe088fecf6e7544e7a690e","permalink":"/post/welcome-to-longitudinal-data-analysis/","publishdate":"2019-08-29T00:00:00Z","relpermalink":"/post/welcome-to-longitudinal-data-analysis/","section":"post","summary":"Please take a look at the syllabus for important class details.\nAny updates or announcements to the class will be found here on the main page. Otherwise, the lectures and workshop pages will where you will spend most of your time.","tags":[],"title":"Welcome to longitudinal data analysis! ","type":"post"},{"authors":null,"categories":null,"content":"  LDA basics Procedural Goals Motivation, terms, concepts Why longitudinal? What can we ask? Types of change (most common) Between person versus within person variables Modeling frameworks: MLM \u0026amp; SEM  Meaningful time metric Thinking through longitudinal data example Person level Doing this with MLM (and SEM)  Design considerations 1. Number of assessment waves 2. Measurement  Threats to validity 1. Missing data 2. Attrition/Mortality 3. History/cohort 4. Maturation 5. Testing 6. Selection  Why not RM ANOVA?    LDA basics Procedural By virtue of being on this page, you know the class website. You may access all of the code and datasets and everything that is used to create the lectures through my github: https://github.com/josh-jackson/ALDA. The provided code should be enough but if you want to search farther go for it.\n Goals This first class is set to orientate you to the world of longitudinal data and MLM models.\n Motivation, terms, concepts Why longitudinal? What can we ask? At least 7 reasons:\nIdentification of intraindividual change (and stability). Do people increase or decrease with time or age. Is this pattern monotonic? Should this best be conceptualized as a stable process or something that is more dynamic? On average how do people change? Ex: people decline in cognitive ability across time.\n Inter-individual differences in intraindividual change. Does everyone change the same? Do some people start higher but change less? Do some increase while some decrease? Ex: not all people people decline in cognitive ability across time, but some do.\n Examine joint relationship among intraindividual change for two or more constructs. If variable X goes up does variable Y also go up across time? Does this always happen or only during certain times? Is this association due to a third variable or does it mean that change occurs for similar reasons? Ex: changes in cognitive ability are associated with changes in health across time.\n Determinants of intraindividual change. What are the repeated experiences that can push construct X around. Do these have similar effects at all times? Ex: people declinein cognitive ability across time. Ex: I have better memory compared to other times when I engage in cognitive activities vs times that I do not.\n Determinants of interindividual differences in intraindividual change. Do events, background characteristics, interventions or other between person characteristic shape why certain people change while others don’t? Ex: people decline less in cognitive ability across time if tend to do cognitively engaging activities.\n Inter-individual differences in intraindividual fluctuation and determinants of intraindividual fluctuation. Does everyone vary the same? Why are some more variable than others? Ex: Someone who is depressed fluctuates more in happiness than someone who is not depressed\n Are there different classes/populations/mixtures of intraindividual change? Ex: do people who decrease vs don’t in cognitive ability across time exist as different groups? (Vs construing differences as on a continuum).\n   Types of change (most common) There are many ways to think of change and stability. We will only have time to go into a few of these types, but it is helpful to think about what type you are interested in when you plan a project or sit down to analyze data. “Change” can mean different things. The above questions you can ask mostly map onto #3 definition of change below.\nDifferential / rank order consistency/ rank order stability. Goes by many names but in the end it is just a correlation. This is a group/sample/population level variable and indexes the relative standing of a person with regard to the rest of the members in the sample. Does not take into account mean structure. Best used with heterotypic continuity where the construct may be the same but the measurement of the construct changes e.g., childhood IQ or acting out in school versus when you are an adult.  A specialized case of this is ipsative change, which looks at the rank order of constructs within a person. This is not done on a single variable (depression) but on a broad number of them (all PD symptoms). Often uses profiles.\nMean level/ absolute change. Takes into account mean structure and indexes absolute levels of a construct. A strong assumption is that the construct means (not a pun) the same thing across time. That is, my measure of depression is interpreted the same for a 40 year old and a 90 year old if I want to look at absolute differences between the two ages.  Mean level change is not dependent at all on rank order consistency. Can have no mean level change and high rank order consistency and vice versa.\nIndividual differences in change. Rank order and mean level provide an index of change and or stability for the sample. Here this provides an assessment of change for an individual. For example, if it is typical to decline in cognitive ability do some people buck the trend and stay at their past level? Individual differences in change get at both mean level changes as well as the tendency of the sample to show stability. It is the type of change that we will focus on the most.\n Structural. Does the construct (or measure) change across time? The assumption for mean level change assumes that the measurement properties stays the same. But maybe it is theoretically interesting to ask whether what you are measuring changes. Examples include practice effects, age effects (cog ability in kids vs adults), differences due to health and life events.\n Variance. Does your experiment lead to an increas in variability in response? You may show no mean levels (which is what is looked at in typical t-tests and ANOVAs) but you could see people increase or decrease in their expected range of response.\n  So how do we refer to ‘change’? Usually it is easier to refer to pictorially or in terms of an equation. Putting a word onto it usually causes some confusion, which is why there are a lot of redundant terms in the literature. All of these might refer to the same thing when used within a model. However, the names of some models use these terms differently and thus can refer to different models or conditions that you are working with. In this class I will try to point out the important differences but you will be fine if you supplement your terms with graphs or equations. Math is the great equalizer.\n Between person versus within person variables Between-person versus within-person are the shortened version of interindividaul differences in change versus intraindividaul differences in change. Refers to across people versus within a particular person. Do you care about how people differ from their previous and future self or do you care about how people differ from other people? (See examples in the 7 types of questions we can ask)\nOften we are interested in modeling both between person and within person variables simultaneously. This is related to Level 1 and Level 2 (for those of you familiar with this terminology). It is helpful to start thinking about what variables you are working with and whether they are within or between person variables. For predictors, it is typically the case that between person effects are constant (between person) variables (e.g., gender) that do not change from assessment to assessment or are only assessed once. In contrast, within person questions are best understood by time varying predictors (within person variables e.g., daily mood) that are assessed more than once.\nWe will incorporate both time invariant (between person) and time varying (within person) predictors into our eventual model. In addition to thinking about the types of questions you want to ask it is important to think about what “type” or variables you are working with. Your choice of questions you can ask depends on how often you assess variables or how you conceptualize them.\n Modeling frameworks: MLM \u0026amp; SEM In this class (and in the field) two primary techniques are used with longitudinal models: MLM and SEM. At some levels they are completely equivalent. At others, one is better than the other and vice versa.\nMLM/HLM is a simple extension of standard regression models. As a result it is easy to interpret and implement. In terms of longitudinal data it is best suited to run models when the time of measurement differs from person to person (compared to equal intervals). For this class we will use lme4 and brms as our MLM program but there are many others we could use e.g., nlme.\nSEM is related to regression in that regression is a subset of SEM techniques. In other words, an SEM program could run a simple regression analysis.\nThe primary advantage of MLM is that you may have assessment waves that vary in length between participants. An assumption of SEM models is that everyone has the same amount of time between assessment waves (though this assumption can be relaxed). MLM is also better suited for complex error structures and complex nesting above and beyond assessments within person. It is also easier to model interactions. Currently, it is easier to do MLM within a Bayesian framework too.\nSEM primary advantage is the ability to account for measurement error via latent assessment of the repeated measures. Other advantages include the ability to model multiple DVs at once, and do so in a flexible manner to look at, for example, the associations between change in one construct and change in the another (though these are also possible with MLMs). Another major advantage is the ability to look at latent groups via latent class or mixture models.\nBottom line: MLM is probably best suited for “basic” or “standard” growth models. More complex analyses of change with multiple variables would benefit from an SEM approach. This is also an oversimplification.\n  Meaningful time metric Time is the most important part of a longitudinal analyses. Without some sort of explicit operationalization of time or thought into how you handle time in your analyses you are not conducting longitudinal analyses. The key to interpreting your output is to know how you handled your time variable. What units is it in? Does everyone have the same differences between assessments? Is time something you are explicitly interested in or merely there as a means to collect repeated measures? We will discuss more of these as the semester progresses. Right now however an important distinction is what should the scale of our x-axis variable, time, be in?\nAt one level, the distinction is relevant to what is the process that is changing someone? Is it a naturally occurring developmental process? Then maybe age is the best metric. What about tracking child’s cognitive ability, something that might be influenced by level of schooling? Here grade may be more important than age. Another common metric is time in study. This may be useful if you are running an intervention or if you want to put everyone on the same starting metric and then control for nuisance variables like age or schooling level. Similarly, year of study as a prime time candidate may be useful if you are working from panel studies and interested in historical events and or cohort effects. A wave variable (ie study measurement occasion) may be good enough to use as a time metric (though this makes some assumptions about the regularity of assessments both within and across people).\nDepending on your choice of time metric you may see different rates of change and variability in change. For psychological applications the most common would be age and time in study (followed by grades for assessments of kids). Age is nice because it captures a number of developmental processes thought to drive change (maturation, history, time-in-study) but does not identify a single reason. Time in study is the opposite in that it does not index any other type of change but that simplicity aides in testing different reasons for change (e.g, age moderation). Thus choosing one type of time metric will naturally guide the types of questions you are able to address. E.g. if you use age as your time metric you won’t be able to control for age or examine the effects of age as simply as if you used time in study.\n Thinking through longitudinal data example library(ggplot2) library(tidyverse) ## ── Attaching packages ─────────────────────────── tidyverse 1.2.1.9000 ── ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 0.8.99.9000 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ✔ purrr 0.3.2 ## ── Conflicts ─────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() Using some resting state imaging data, lets think about how we can model and think about this data using our current skills (ie standard regression and plotting)\nWe defined time as year in study. How would this look if we used age?\ngg1 \u0026lt;- ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_point() print(gg1) ## Warning: Removed 9 rows containing missing values (geom_point). The above graph just plots datapoints. Do we have repeated assessments per person? Lets find out.\nPerson level gg2 \u0026lt;- ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_line() gg2 ## Warning: Removed 9 rows containing missing values (geom_path). Note that some people start at different levels. Some people have more data in terms of assessment points and years. Note that the shape of change isn’t necessarily a straight line.\nWe often want to look at this at a per person level to get more info.\ngg3 \u0026lt;- ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_line() + geom_point() + facet_wrap( ~ ID) gg3 ## Warning: Removed 9 rows containing missing values (geom_path). ## Warning: Removed 9 rows containing missing values (geom_point). As part of our dataset we have different groups. A question we may have is if they change differently across time. Lets take a look at this.\ngg4 \u0026lt;- ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_line() + facet_grid(. ~ group) gg4 ## Warning: Removed 9 rows containing missing values (geom_path). We’re not in Kansas anymore - look at the technocolor.\ngg5 \u0026lt;- gg2 + aes(colour = factor(ID)) + guides(colour=FALSE) gg5 ## Warning: Removed 9 rows containing missing values (geom_path). Okay, beside the occular technique, we’re going to need to do something more to address our theoretical questions. Lets look at some random people in the sample and run some regressions.\nset.seed(11) ex.random \u0026lt;- example %\u0026gt;% dplyr::select(ID) %\u0026gt;% distinct %\u0026gt;% sample_n(10) example2 \u0026lt;- left_join(ex.random, example)  ## Joining, by = \u0026quot;ID\u0026quot; gg6 \u0026lt;- ggplot(example2, aes(x = week, y = SMN7, group = ID)) + geom_point() + stat_smooth(method=\u0026quot;lm\u0026quot;) + facet_wrap( ~ID) gg6 ## Warning in qt((1 - level)/2, df): NaNs produced ## Warning in qt((1 - level)/2, df): NaNs produced ## Warning in qt((1 - level)/2, df): NaNs produced ## Warning in qt((1 - level)/2, df): NaNs produced ## Warning in qt((1 - level)/2, df): NaNs produced ## Warning in qt((1 - level)/2, df): NaNs produced Lets look at individual level regressions\nlibrary(tidyverse) library(broom) regressions \u0026lt;- example2 %\u0026gt;% group_by(ID) %\u0026gt;% do(tidy(lm(SMN7 ~ week, data=.))) regressions ## # A tibble: 20 x 6 ## # Groups: ID [10] ## ID term estimate std.error statistic p.value ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 67 (Intercept) 0.0921 0.0161 5.72 0.0292 ## 2 67 week 0.00662 0.00657 1.01 0.420 ## 3 75 (Intercept) 0.126 NaN NaN NaN ## 4 75 week 0.00771 NaN NaN NaN ## 5 87 (Intercept) 0.0787 NaN NaN NaN ## 6 87 week -0.0227 NaN NaN NaN ## 7 99 (Intercept) 0.111 0.0236 4.69 0.0426 ## 8 99 week 0.00545 0.0122 0.446 0.699 ## 9 101 (Intercept) 0.111 0.0424 2.62 0.232 ## 10 101 week 0.0421 0.0217 1.94 0.303 ## 11 103 (Intercept) 0.0887 NaN NaN NaN ## 12 103 week -0.0168 NaN NaN NaN ## 13 105 (Intercept) 0.0465 0.00658 7.06 0.0896 ## 14 105 week 0.00122 0.00467 0.261 0.838 ## 15 142 (Intercept) 0.197 NaN NaN NaN ## 16 142 week 0.0130 NaN NaN NaN ## 17 149 (Intercept) 0.0801 NaN NaN NaN ## 18 149 week 0.00497 NaN NaN NaN ## 19 152 (Intercept) 0.0921 NaN NaN NaN ## 20 152 week -0.0172 NaN NaN NaN What can we see? Estimates give us an intercept and regression coefficient for each person. Some people increase across time, some decrease. Some we cannot do statistical tests on – why?\nWell that is per person. Lets get the average starting value and change per week\nregressions %\u0026gt;% group_by(term) %\u0026gt;% summarise(avg.reg = mean(estimate)) ## # A tibble: 2 x 2 ## term avg.reg ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 0.102 ## 2 week 0.00244 Lets plot the average trend across everyone. Start with a best fit line not taking into account that people have repeated measures.\ngg7 \u0026lt;- gg1 \u0026lt;- ggplot(example, aes(x = year, y = SMN7)) + geom_point() + stat_smooth() gg7 ## `geom_smooth()` using method = \u0026#39;loess\u0026#39; and formula \u0026#39;y ~ x\u0026#39; ## Warning: Removed 9 rows containing non-finite values (stat_smooth). ## Warning: Removed 9 rows containing missing values (geom_point). That lowess line is a little strange. How about a linear estimate.\nSplit up by group\ngg8 \u0026lt;- ggplot(example, aes(x = year, y = SMN7)) + geom_point() + stat_smooth(method = \u0026quot;lm\u0026quot;) + facet_grid(. ~ group) gg8 ## Warning: Removed 9 rows containing non-finite values (stat_smooth). ## Warning: Removed 9 rows containing missing values (geom_point). But I also want to see the individual slopes, not just the average.\ngg9 \u0026lt;- ggplot(example, aes(x = year, y = SMN7, group = ID)) + geom_point(alpha = 0.05) + stat_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) gg10 \u0026lt;- gg9 + stat_smooth(data = example, aes(x = year, y = SMN7, group=1, color = \u0026quot;black\u0026quot;), method = \u0026quot;lm\u0026quot;, size = 2) + guides(fill=FALSE) gg11 \u0026lt;- gg10 + facet_grid(.~ group) + theme(legend.position=\u0026quot;none\u0026quot;) gg11 ## Warning: Removed 9 rows containing non-finite values (stat_smooth). ## Warning: Removed 9 rows containing non-finite values (stat_smooth). ## Warning: Removed 9 rows containing missing values (geom_point).  Doing this with MLM (and SEM) These regressions and plots are how you should begin to think about longitudinal data analysis. These growth models (the simplest form of longitudinal data analysis) are just a bunch of regressions for each person plus a little extra stuff. MLM is just a fancy regression equation. We want to create a line for everyone. Does someone go up? Does someone go down? On average, do people’s lines indciate that this construct increases or decreases? That is it. Seriously.\nWhat is different from normal regression? Extra error terms, mostly. For regression, we think of error as existing in one big bucket. For MLMs (and other longitudinal models) we will be breaking up unexplained variance (error) into multiple buckets.\nThis is where fixed effects and random effects come into play. We will discuss this more next class, but the gist is that fixed effects are the regression coefficients you are used to. Fixed effects index group level change. Do people decline in memory across time, on average? Random effects vary among individuals (in the longitudinal models we are talking about) and index variation from the group. While most people decline in their memory, does everyone? In other words, an average trajectory will be termed a fixed effect and the random effect indexes how much variability there is around that group level effect. This extra variability we measure through random effects means that we are explaining more variance and thus there is less unexplained variance.\n  Design considerations 1. Number of assessment waves Remember high school algebra: two points define a line. But, that assumes we can measure constructs without error. Three assessment points will better define changes in psychological variables. As a default, you need three waves of data to use MLM models. However, some simplifications can be made with MLM. Two wave assessments are mostly better with SEM approaches.\n 2. Measurement Scale of measurement Measurement is always the basis for good quantiative analysis. Without good measurement you are just spitting into the wind. Standard measurement concerns remain (reliability, dimensionality) but extra concerns exist with longitudinal data.\nWhat does it mean for categorical variables to change over time? Can you imagine a trajectory for what this is measuring? How would dichotomous responses impact ability to measure change?\nWhat about ranks, such as in preference for school subjects? What if the class composition changes – what is this assessing? Given that ranks are related such that if I increase someone has to decrease, how does that impact change assessments?\nCan I analyze childhood and adult variables simultaneously if assess the same construct, even though they may be measured differently? How can you measure change in the same construct but with different measures? To assess math ability in 5 year olds you can ask them about addition, can you do that in a sample of 20 year olds? Does that measure continue to assess math ability?\nOften the answer to these is to use a different form of the longitudinal model. In general, the better measured the construct (continuous, not dependent on others, using the same scale) the more complex/sophisticated analysis you can run. Worse measurment leads to simplification both in terms of the models and the types of conclusions you can make.\nAs with all of science, everything rests on measurement. Poor measurement results in poor conclusions.\n Standardizing It is standard practice to z-score to get standardized responses. However, it is not straight forward to do so when using longitudinal data. Why would z-scoring your variables be problematic?\nFirst, if you scale for age, for example, this takes out a potential explanatory variable.\nSecond, more worriesome, it also can add error if not everyone is standardized consistently (say if standardization is across age groups and someone just misses a cut). Or if the sample changes due to attrition.\nThird, is that you take away the mean for each assessment such that the expected change across time is zero. We will talk more about solutions to this problem as the semester progresses but the short answer is to avoid or to use SEM.\nWhile helpful for cross sectional analyses, z-scoring adds in layers of computational and interpretational problems.\n Reliability The goal of longitudinal analyses is to understand why some construct changes or stays the same across time. A major difficulty in addressing this goal is whether you are able to accurately assess the construct of interest. One of the key characteristics (but not the only characteristic) is whether or not your assessment would be consistent if you gave an alternative measure or if you retook it immediately after your first assessment. This is known as reliability of measurement. To the extent that your measure is reliable it assesses true score variance as opposed to error variance. The amount of error score variance assessed is important given that error variance will masquerade as change across time given that error can correlate with anything else. The more error in your measurement the more change you will find. Of course this is unreliable change – change that is not true change, just stochastic noise.\nWe can think of reliability two ways. First, reliability of the change estimate. This depends on how much error there is in the assessment and the number of waves. These two components are similar to inter item correlation and number of items being the two main components that effect reliability in cross sectional analyses. Increase the number of items (waves) you increase your alpha. Increase the average correlation among items, you increase your alpha. The parallel to average correlation among items is our ability to accurately assess the construct. When comparing a construct across time we examine this with measurement invariance.\n Measurement invariance The second way to think of reliability is in terms of how consistently the measure is assessed across time. Or, do you assess the same construct at each time? What would happen if we looked at change in IQ from 1st grade to 12 grade and used the first grade IQ test at each time? The construct that you assessed at the first wave is likely not the same assessed later. To test this formally is called measurement invariance and is typically done through SEM. We will talk more about this later in the semester. Until we get there we make a large assumption that what we are measuring now is the same at each wave of assessment.\n   Threats to validity 1. Missing data Types of missing data On a scale from 1 to you’re completely screwed, how confident are you that the missingness is not related to your study variables?\nMissing completely at random (MCAR) means that the missingness pattern is due entirely to randomness\nMissing at random (MAR) means that there is conditional randomness. Missingness may be due to other variables in the dataset. Pretty standard for longitudinal data.\nNot missing at random (NMAR) means that the missingness is systematic based on the missing values and not associated with measured variables. For example, in a study of reading ability, kids with low reading ability drop out, due to not liking to take tests on reading ability. However, if reading ability is associated with other variables in the model, then this missingness becomes closer in kind to MAR, and thus somewhat less problematic.\nTypically, we make the assumption we are working under MAR and thus we will have unbiased estimates when predictors of missingness are incorporated into the model.\nThere are tests to distinguish MAR from NMAR but you cannot distinguish MCAR from MAR because it is based entirely on knowing something that you dont have.\n How to handle missing data Listwise? Nah\nFull information maximum likelihood and other ML approaches? Sure. Multiple imputation? Cannot hurt. More on these approaches later.\n  2. Attrition/Mortality Major contributor to missing data\n 3. History/cohort Know that the processes driving change can be due to a specific event or cohort.\n 4. Maturation Change may occur because of natural processes. Thus if you just follow someone across time they will likely change irregardless of say, if they are in the control group.\n 5. Testing Having people take the same survey, test or interview multiple times may lead them to respond differently. Does that change result from development or does it result from them being familiar with the test?\n 6. Selection If you are looking at life events, know that life events are not distributed randomly. Moreover, people who stay in studies and even sign up for studies are different from those that do not. As a result, it is often hard to make internally valid inferences with longitudinal data.\n  Why not RM ANOVA? Cannot handle missing data\n Assumes rate of change is the same for all individuals.\n Time is usually done with orthogonal polynomials, making it difficult to interpret or to model non-linear. In other words, you have flexibility on how you want to model time.\n Accounting for correlation across time uses up many parameters, MLM is more efficient.\n Can accommodate differences in time between assessment waves across participants\n Handles various types of predictors - continuous vs nominal \u0026amp; static vs dynamic\n  Bottom line: this is an old way of doing these analyses with no upside. Don’t do them.\n  ","date":1565827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565827200,"objectID":"b6a3cdd8068a432e668db64269b4f199","permalink":"/lectures/02-lda-basics/","publishdate":"2019-08-15T00:00:00Z","relpermalink":"/lectures/02-lda-basics/","section":"Lectures","summary":"LDA basics","tags":null,"title":"Week 1","type":"post"},{"authors":null,"categories":null,"content":"  Growth curves Between person models and cross sectional data Within person models e.g., 2-level models Thinking about random effects Random effects Empty model equation Putting it together Visualize what you are doing ICC  Adding time What does this look like graphically? Adding a random slope?  Individaul level random effects Calculation of individaul level random effects How are these random effects calculated? random effects and residual (standard) assumptions Random effect decomposition  working with models in R Data analysis and data structures Wide and Long form basic lmer code Example How to calculate ICC?  Exploring beyond the summary what do the random effects look like?  Adding time to the MLM fixed slope Random slope  Random effects Calcualtion of random effect confidence interval adding time caterpillar plots Density of individaul random effects  comparing to a standard linear model Other types of models Matrix notation (as a way to help understand what is going on) Estimation Testing significance (adapted from Ben Bolker) P values are not included Likelhiood ratio test Likelihood tests for random effects AIC and BIC MCMC Bootstraps  Predictions and prediction intervals Predictions and prediction intervals  Coefficient of determination equivalents batch analyses     Growth curves Between person models and cross sectional data You already know this, but it gives us a chance to review regression\n\\[ {Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... +\\epsilon_{i} \\]\n\\[ \\hat{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... \\]\nParameters are considered fixed where one regression value corresponds to everyone. I.e., that association between X1 and Y is the same for everyone.\nEach person has a Y, denoted by the subscript i, and each has a residual associated with them, also designated by i.\nlibrary(readr) example \u0026lt;- read_csv(\u0026quot;~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/example.csv\u0026quot;) example$ID \u0026lt;- as.factor(example$ID) Lets look at some data. These data examine older adults who came into a study up to six times over a six year period. Multiple cognitive, psychiatric and imaging assessments were done. Let’s look at functional connectivity network called SMN7.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ── ## ✔ ggplot2 3.2.1 ✔ dplyr 0.8.3 ## ✔ tibble 2.1.3 ✔ stringr 1.4.0 ## ✔ tidyr 0.8.99.9000 ✔ forcats 0.4.0 ## ✔ purrr 0.3.2 ## ── Conflicts ─────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(ggplot2) gg1 \u0026lt;- ggplot(example, aes(x = week, y = SMN7)) + geom_point() + stat_smooth(method = \u0026quot;lm\u0026quot;) print(gg1) What happens if we run a regression?\nregression \u0026lt;- lm(SMN7 ~ week, data = example) summary(regression) ## ## Call: ## lm(formula = SMN7 ~ week, data = example) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.099294 -0.039929 -0.005938 0.032715 0.169885 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.100161 0.005261 19.039 \u0026lt;2e-16 *** ## week 0.004087 0.002563 1.595 0.112 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.05562 on 214 degrees of freedom ## (9 observations deleted due to missingness) ## Multiple R-squared: 0.01174, Adjusted R-squared: 0.007124 ## F-statistic: 2.543 on 1 and 214 DF, p-value: 0.1123  Within person models e.g., 2-level models We saw this last time where we can think of everyone being run in a seperate regression model. Here the lines connect the dots of the same people across time.\nlibrary(tidyverse) gg2 \u0026lt;- ggplot(example, aes(x = week, y = SMN7, group = ID)) + geom_point() + stat_smooth(method = \u0026quot;lm\u0026quot;, se = FALSE) gg3 \u0026lt;- gg2 + stat_smooth(data = example, aes(x = week, y = SMN7, group=1, colour=\u0026quot;#990000\u0026quot;), method = \u0026quot;lm\u0026quot;, size = 3, se=FALSE) + theme(legend.position = \u0026quot;none\u0026quot;) print(gg3) Each person has multiple assessments, so we need to distinguish between people and their assessments. In normal regression we wouldn’t think about this as everyone datapoint is assumed to be independent. However, this is not the case here. Failing to distinguish would lead to violation of independence, an important assumption of the standard regression model.\nAs seen in the graph above, what we have now is both individual level slopes as well as an average level slope. The average level slope is going to be the average of the individual level slopes, which will look like our average slope ignoring all dependencies. Same for the intercept.\nOne way to do this is to run seperate regressions for each person. Then we could just pool (or average) together where people start and how much they change to get the average intercept (starting value) and trajectory (how much people change). We will see later that this is a somewhat poor approach.\n## Joining, by = \u0026quot;ID\u0026quot; regressions \u0026lt;- example2 %\u0026gt;% group_by(ID) %\u0026gt;% do(tidy(lm(SMN7 ~ week, data = .))) head(regressions) ## # A tibble: 6 x 6 ## # Groups: ID [3] ## ID term estimate std.error statistic p.value ## \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 67 (Intercept) 0.0921 0.0161 5.72 0.0292 ## 2 67 week 0.00662 0.00657 1.01 0.420 ## 3 75 (Intercept) 0.126 NaN NaN NaN ## 4 75 week 0.00771 NaN NaN NaN ## 5 87 (Intercept) 0.0787 NaN NaN NaN ## 6 87 week -0.0227 NaN NaN NaN In addition to the average intercept and the average trajectory there is also the amount of variation around each of these estimates. Do people tend to change the same? Are there individual differences in the initial assessment?\nThis type of meaningful variation is lost when we have a between subjects only model that ignores the individual level. This variation will be called Random Effects (or variance estimates in SEM).\n[Side note: note how some people do not have se estimates for their regression coefficients. The reason for this will impact our ability to fit longitudinal models later on.]\nThere is another important source of variation different from standard regression models. The within-subjects error that can be seen in the below graph. If we did not take people into account and just collapsed across people to get a between subjects assessment of change, this error would be confounded with individual differences in change. We will discuss this error more in depth later, but one way to think about our goal is to utilize our repeated assessments to make better predictions. A way to do that is to create additional buckets of explained variance, resulting in a smaller bucket of unexplained variance.\nexample3 \u0026lt;- example2 %\u0026gt;% filter(ID == \u0026quot;67\u0026quot;)  gg4 \u0026lt;- ggplot(example3, aes(x = week, y = SMN7, group = ID)) + geom_point() + stat_smooth(method = \u0026quot;lm\u0026quot;) gg4 Thinking about random effects  Random effects Within subjects variability in either starting value or slope/trajectory is referenced in terms of random effects. How do we represent this in our equation? Easy, we just say that the typical regression parameters we have are not the same for everyone – that they are random (in contrast to fixed).\nIn general, when would we want to use random effects? If there is some sort of selection (random or not) of many possible values of the predictor (e.g., stimuli are 3 random depression drugs, three semi random chosen levels of a drug). With longitudinal data this is (random) people.\nSide bar: Even in situations where these levels are not random (eg working with states) it is still useful to use MLM and we still call them random effects. To be consistent with language, random here can refer to as random from the population average, not randomly selected. When talking about “random effects” you can mean either of these defintions (and a few more). Luckily we can mostly ignore these semantic issues.\nWhat is necessary for modeling random effects? For longitudinal models, there needs to be multiple assessments per your grouping category (people, schools, neighborhoods, trials).\nWe are assuming these random effects are sampled from some population and thus vary from group to group (or person to person). This means that your coeffients (like traditional regression coefficients) are estimates of some population parameter and thus have error associated with them. This error is not like like a standard residual, which represents error for your overall model. Nor is it like the standard error for a point estimate. Random effects can best be thought of as deviation of individual regression lines from the group regression line (though it technically is not this).\nTo facilitate the multiple assessments per person we will now use both i and j subscripts. We will see that the random effects are part of the overall error term in the model. Counterintutitively, the main focus of these types of models will be the fixed effects, with less attention paid to the random effects. That said, the random effects are necessary to account for dependency in the data. One can think about these models as normal fixed effects regressions, with the random effects there to account for the longitudinal nature of the data. They are made up of a number of standard regression equations, each for a single individual. Doing so side steps the trouble of having correlated errors, and thus allows us to interpret our findings without concern.\nTo faciliate adding random effects to our model it is helpful to think about two “levels” to our regression equation. We are going to put a regression equation within our requestion equation. (Que Xzhibit joke). The first level will be the within-person model, in that it described how people differ across time. The second level will be the between person level. Note that these do not correspond to fixed or random effects. Instead they can be thought to model either within person differences or between person differences. Mastering thinking at these two levels will help make sense of these MLM models.\n Empty model equation Let’s start with the most basic model and then expand from there.\nLevel 1 - within person \\[ {Y}_{ij} = \\beta_{0j} +\\varepsilon_{ij} \\]\nNote that we have multiple responses per individual j, noted with an i to refer to specific times.\nAlso note that the intercept has a subscript. In typical regression it does not. This suggests that not everyone has the same intercept.\nThe residuals at this level are thought of as measurment error OR as something that can be explained by time varying predictors.\nLevel 2 - between person \\[ {\\beta}_{0j} = \\gamma_{00} + U_{0j} \\]\nLevel 2 takes the intercept (or other parameter) at level 1 and breaks it down into an equation for each individual, j. An overall group average (the gamma) and a residual term specific to deviation around the intercept (see below).\nAnd two variance components: 1. a random effect of the intercept \\[ {U}_{0j} \\sim \\mathcal{N}(0, \\tau_{00}^{2}) \\] The subscript of the \\(U_{0j}\\) refers to the number of the parameter where 0 is the intercept, 1 is the first regression coefficient, and so on. The second refers to the individual, j. So \\(U_{0j}\\) refers to the intercept whereas \\(U_{1j}\\) would refer to the random effect of the first regression coefficient.\nThe \\(U_{0j}\\) random effect is said to be normally distributed with a mean of zero and a variance of \\(\\tau\\)\nthe residual error term \\[ {R}_{ij} \\sim \\mathcal{N}(0, \\sigma^{2}) \\] Much like in normal regression there is an error term for all of the variation we cannot account for. What is unique here is that we took that normal variation and split it into two components. One that is attributable to variation around the intercept \\({U}_{0j}\\) and a catch all residual.  Technically this is not a growth model, nor one that is inherently longitudinal. However, it does serve as a nice starting point to identify random effects.\n Putting it together \\[ {Y}_{ij} = \\gamma_{00} + U_{0j} + \\varepsilon_{ij} \\]\n Visualize what you are doing Imagine the raw data plotted without knowing person j, how would \\(\\varepsilon_{i}\\) be calculated?\nNow think about the data plotted again but with knowing each person has their own intercept. How would \\(\\varepsilon_{ij}\\) be calculated?\nFinally, how is U_{0j} calculated?\n ICC ICC:\nIf the ICC is greater than zero, we are breaking standard regression assumptions.\n\\[\\frac{U_{0j}}{U_{0j}+ \\varepsilon_{ij}}\\]\nIs defined as % variation between over total variance.\nICC can also be interpreted as the average (or expected) correlation within a nested group, in this case a person. On other words, the ICC is the correlation between any person’s repeated measures (technically residuals).\n  Adding time Here is the basic growth model where our predictor is a time variable\nLevel 1:\n\\[ {Y}_{ij} = \\beta_{0j} + \\beta_{1j}X_{ij} + \\varepsilon_{ij} \\]\nNote how similar this looks like to a normal regression equation. Again, the differences are due to those pesky subscripts. Like before, think of this as a normal regression equation at the level of a person. Each person would have one of these equations with, in addition to a unique Y, X and residual, a unique \\(\\beta_{0}\\) and \\(\\beta_{1}\\). Look above to those individaul regressions we did at the start of this section.\nLevel 2:\n\\[ {\\beta}_{0j} = \\gamma_{00} + U_{0j}\\]\nLevel 2 takes the parameters at level 1 and decomposes them into a fixed component that reflects that average and then the individual deviations around that fixed effect. \\(U_{0j}\\) is not error in the traditional sense. It describes how much variation there is around that parameter. Do some people start higher while some start lower, for example.\n\\[ {\\beta}_{1j} = \\gamma_{10} \\]\nThe new level 2 term refers to the first predictor in the level 1 regression equation ie the slope. This slope is fixed in that the level 2 equation only has a gamma term and no U residual term.\nPutting it together: \\[ {Y}_{ij} = \\gamma_{00} + \\gamma_{10} (X_{1j})+ U_{0j} + \\varepsilon_{ij} \\]\nNote that in computing a single individuals Y, it depends on the two fixed effects, the Xj, and the random effect for the intercept.\nWhat does this look like graphically? And how does this differ from the random intercept model?\nWhat does this look like graphically?\nCan you draw out the sources of error? The random effects for each participant? The fixed effects?\n Adding a random slope? What happens when we add a random slope? Level 1:\n\\[ {Y}_{ij} = \\beta_{0j} + \\beta_{1j}X_{1j} + \\varepsilon_{ij} \\] Level 2:\n\\[ {\\beta}_{0j} = \\gamma_{00} + U_{0j}\\]\n\\[ {\\beta}_{1j} = \\gamma_{10} + U_{1j} \\]\n\\[ {Y}_{ij} = \\gamma_{00} + \\gamma_{10}(X_{ij})+ U_{0j} + U_{1j}(X_{ij}) + \\varepsilon_{ij} \\]\nCan think of a persons score divided up into a fixed component as well as the random component.\nThese random effects are likely related to one another. For example, if someone starts high on a construct they are then less likely to increase across time. This negative correlation can be seen in the residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance.\n\\[ \\begin{pmatrix} {U}_{0j} \\\\ {U}_{1j} \\end{pmatrix} \\sim \\mathcal{N} \\begin{pmatrix} 0, \u0026amp; \\tau_{00}^{2} \u0026amp; \\tau_{01}\\\\ 0, \u0026amp; \\tau_{01} \u0026amp; \\tau_{10}^{2} \\end{pmatrix} \\]\nNote that it is possible to have a different error structure, one where there is no relationship between the intercept and the slope. We will discuss this more later in the semester. Right now just know that the default is to have correlated random effects.\nWe also have the within subject variance term that accounts for deviations that are not accounted for by time variable and other level 1 predictors.\n\\[ {R}_{ij} \\sim \\mathcal{N}(0, \\sigma^{2}) \\]\nNote that it is possible to model these level 1 residuals with different structures. This specification implies that there is no correlation across an individuals residuals, once you account for level 1 predictors (ie growth trajectories). Having a specific level 1 autoregressive or other type of pattern is common in other treatments of longitudinal models (panel models) but is not necessary with growth models (but possible).\nThis is the basic format of the growth model. It will be expanded later on by adding variables to the level 1 model and to the level 2 model. Adding to the level 1 model is only possible with repeated variables.\nLevel 1 regression coefficients are added to the level 2 model. These coefficients are decomposed into a fixed effect, a random effect (possibly), and between person predictors. As with any regression model, each of these only have a single error term.\n  Individaul level random effects Calculation of individaul level random effects Random effects are often thought in terms of variance components. We can see this if we think of individual level regressions for each person where we then have a mean and a variance for both the intercept or the slope. The greater the variance around the intercept and the slope means that not everyone starts at the same position and not everyone changes at the same rate.\nIf you want to look at a specific person’s random effect you can think of it as a deviation from the fixed effect where subject 6’s intercept can be thought of as\n\\[ {\\beta}_{06} = \\gamma_{00} \\pm U_{06}\\] e.g 2.2 = 3 - .8\n How are these random effects calculated? It isn’t as straightforward as calculating a slope for each person and then using the difference between that slope and the average slope. Instead, the estimates are partially pooled towards the overall mean of the sample, the fixed effect. We do this to get a better estimate of the parameters, the same way that using regression to predict y-hat given an X is better than binning X and calculating y-hat. More information = better.\nWhy not full pooling? Because it ignores individaul differences in change.\nThe result is that the variance of the change trajectories (using MLM) will be smaller than the variance of the fitted linear models.\n random effects and residual (standard) assumptions Joint normal distribution of random effects\nNormally distributed residual\nConstant variance over time\nRandom effects and residual are uncorrelated\nBoth have a mean of zero\nRandom effects and residual size will depend on predictors in the model.\n Random effect decomposition Think of the original total variance in a scatter plot of our DVs. Adding random effects takes that variance and trims it down.\nThe intercept only MLM seperates it into a level 1 variance (which at this stage is treated as error) and a level 2 random intercept variance.\nCreating a random slopes model takes the Level 1 residual variance and creates a new “pile” of explained variance.\n  working with models in R  Data analysis and data structures Wide and Long form Depending on what type of analysis you want to perform you may need to restructure your data. I recommend the combination of tidyr and dplyr (among others) to restructure and manage your dataframes. The first decision you need to make is whether you want your data structured in a long or a wide format. There are multiple names to refer to these two types: multivariate vs univariate, person-level vs person-period, etc but they all refer to the same idea. How to structure your data depends on both what level of analysis (individual, dyad, household) and what type of analyses (MLM/SEM). Typically our focus is on individuals.\nWide form is common among non-longitudinal data. It has one line per individual with all of their repeated measures in the same row, each with some name to distinguish which assessment wave the data came from. In general, this format is used for SEM.\n## # A tibble: 3 x 4 ## ID ext_1 ext_2 ext_3 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 4 4 4 ## 2 2 6 5 4 ## 3 3 4 5 6 In contrast, long format has a row per observation. Thus, participants likely have many rows, each one referring to a different assessment wave. There are fewer variables in this format which makes organization somewhat easier. Thus this has been referred to as “Tidy” data. Graphing with ggplot is facilitated when using tidy data such as being in the long format.\n## # A tibble: 9 x 3 ## ID time ext ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 4 ## 2 1 2 4 ## 3 1 3 4 ## 4 2 1 6 ## 5 2 2 5 ## 6 2 3 4 ## 7 3 1 4 ## 8 3 2 5 ## 9 3 3 6 How do you go back and forth? We use the tidyr package! As of a few months ago, you would use the gather and the spread functions. Now these are being phased out and are being replaced by pivot_longer and pivot_wider. The functions work similar but the newer ones are a little more intuitive both in terms of remembering the correct function name as well as well adding more bells and whistles.\nGather and pivot_longer goes from wide to long.\nlibrary(tidyr) wide_to_long \u0026lt;- wide %\u0026gt;% gather(ext_1:ext_3,key = \u0026quot;time\u0026quot;, value = \u0026quot;ext\u0026quot;) wide_to_long ## # A tibble: 9 x 3 ## ID time ext ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 ext_1 4 ## 2 2 ext_1 6 ## 3 3 ext_1 4 ## 4 1 ext_2 4 ## 5 2 ext_2 5 ## 6 3 ext_2 5 ## 7 1 ext_3 4 ## 8 2 ext_3 4 ## 9 3 ext_3 6 library(tidyr) wide_to_long.p \u0026lt;- wide %\u0026gt;% pivot_longer(-ID, names_to = \u0026quot;time\u0026quot;, values_to = \u0026quot;ext\u0026quot;) wide_to_long.p ## # A tibble: 9 x 3 ## ID time ext ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 ext_1 4 ## 2 1 ext_2 4 ## 3 1 ext_3 4 ## 4 2 ext_1 6 ## 5 2 ext_2 5 ## 6 2 ext_3 4 ## 7 3 ext_1 4 ## 8 3 ext_2 5 ## 9 3 ext_3 6 Note the similarities. The key in all three is to 1. identify which columns need to be reshaped. Here is it all of them besides ID. 2. we need to name the newly created variable that consists of the old column names (here time). 3. we need to name what those values represent (here levels of extraversion)\nThe separate function could be used to get only the assessment wave number. This might be useful when combining data together or for creating a common time metric for everyone.\nwide_to_long2 \u0026lt;- wide_to_long %\u0026gt;% separate(time, into = c(\u0026quot;omit\u0026quot;, \u0026quot;wave\u0026quot;), sep = \u0026quot;_\u0026quot;, convert = TRUE) %\u0026gt;% dplyr::select(-omit) %\u0026gt;% arrange(ID) wide_to_long2 ## # A tibble: 9 x 3 ## ID wave ext ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 1 4 ## 2 1 2 4 ## 3 1 3 4 ## 4 2 1 6 ## 5 2 2 5 ## 6 2 3 4 ## 7 3 1 4 ## 8 3 2 5 ## 9 3 3 6 # Note that the seperate function will identify non numeric characters and use that to seperate the values. You can omit the sep = function to check yourself.  One issue that comes up here is that we have differing dates for each assessment. Ideally we would like to utilize that extra information.\n## # A tibble: 3 x 7 ## ID ext_1 ext_2 ext_3 date_1 date_2 date_3 ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 4 4 4 1/1/10 5/1/10 8/1/10 ## 2 2 6 5 4 1/6/10 4/10/10 9/1/10 ## 3 3 4 5 6 1/8/10 4/25/10 9/13/10 How do we fix it? The same way we would with multiple variables we want to convert. Wave, along with ID helps us keep track of what variables go with which person at which time. Together, the two serve as a unique identifier. To better understand the code go through each line to see what the intervening data frame looks like.\nlong.date \u0026lt;- wide.date %\u0026gt;% gather(-ID, key = \u0026quot;time\u0026quot;, value = \u0026quot;value\u0026quot;) %\u0026gt;% separate(time, into = c(\u0026quot;variable\u0026quot;, \u0026quot;wave\u0026quot;)) %\u0026gt;% spread(variable,value) long.date ## # A tibble: 9 x 4 ## ID wave date ext ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 1 1/1/10 4 ## 2 1 2 5/1/10 4 ## 3 1 3 8/1/10 4 ## 4 2 1 1/6/10 6 ## 5 2 2 4/10/10 5 ## 6 2 3 9/1/10 4 ## 7 3 1 1/8/10 4 ## 8 3 2 4/25/10 5 ## 9 3 3 9/13/10 6 One difficulty of creating a wave variable is whether or not the variables are named in a manner such that 1) assessment wave is easily identifiable (e.g. does _a always refer to the first wave whereas _b always refer to the second?) and 2) if that is consistent across variables. Having a wave identifier for your variables is important/necessary. Having an easily selected one (ie at the end of the variable name, hopefully separated by an underscore or a period). If assessment wave separators are embedded within the variable name it will be harder to covert your data. Often, variable data is attached at the end of a name such as SWB_4 to refer to the fourth item in a scale. This may obscure wave identification as in SWB_A_4. A similar naming problem can occur with multiple reports e.g,. SWB_4_parent. I recommend putting wave identification last. The difficulties become partly moot when working in long format (read: entering data in) as opposed to wide.\nIn the above code we used spread to go from long to wide as a means of creating a long dataset where there were multiple variables. Technically this is not a tidy dataset in that it comprises of both long and wide information, but it is the typical format used for MLM analyses.\nGoing from long to wide uses spread or pivot_wider function. We will utilize this when converting our MLM models to SEM models, but try the code below to see what happens.\nlong_to_wide \u0026lt;- long %\u0026gt;% spread(time, ext) long_to_wide ## # A tibble: 3 x 4 ## ID `1` `2` `3` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 4 4 4 ## 2 2 6 5 4 ## 3 3 4 5 6 Note that this is technically the dataframe format that we want. The problem is that our variable names are numeric. This often causes problems. When working with tibbles use backticks ’ to refer to the column e.g., select(‘1’)\n basic lmer code The basic function we will work with is lmer from the lme4 package\nlibrary(lme4) ## Loading required package: Matrix ## ## Attaching package: \u0026#39;Matrix\u0026#39; ## The following objects are masked from \u0026#39;package:tidyr\u0026#39;: ## ## expand, pack, unpack The package was developed to be similar to the lm function. The code will be similar to the formula for the combined model\nCode for empty model\nlmer(Y ~ 1 + (1 | subjects), data=example) Level 1 \\[ {Y}_{ij} = \\beta_{0j} +\\varepsilon_{ij} \\] Level 2 \\[ {\\beta}_{0j} = \\gamma_{00} + U_{0j} \\]\nCombined \\[ {Y}_{ij} = \\gamma_{00} + U_{0j} + \\varepsilon_{ij} \\]\n1 is the way to reference the intercept. All additional fixed effects go outside the parentheses. Inside the parentheses are the random effects and residual terms. To the right of the vertical line is our level 1 residual term, which references the grouping variable. In this case, as with almost all longitudinal work, is the subject ID. To the left of the vertical line is the random effects we want to estimate. Right now this estimates only one random effect, one for the intercept.\nIt is possible to suppress a random intercept by putting a zero instead of a 1. If you do not put anything there the 1 is implied.\nlmer(y ~ 1 + time + (1 + time | subjects), data=data) lmer(y ~ time + (time | subjects), data=data) # both are equivalent  Example mod.1 \u0026lt;- lmer(SMN7 ~ 1 + (1 | ID), data=example) summary(mod.1) ## Linear mixed model fit by REML [\u0026#39;lmerMod\u0026#39;] ## Formula: SMN7 ~ 1 + (1 | ID) ## Data: example ## ## REML criterion at convergence: -714.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.1575 -0.4728 -0.0232 0.4512 3.2750 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 0.001823 0.04270 ## Residual 0.001302 0.03608 ## Number of obs: 225, groups: ID, 91 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.106972 0.005106 20.95  How to calculate ICC? 0.001823/(0.001823 + 0.001302) ## [1] 0.58336   Exploring beyond the summary class(mod.1) ## [1] \u0026quot;lmerMod\u0026quot; ## attr(,\u0026quot;package\u0026quot;) ## [1] \u0026quot;lme4\u0026quot; what do the random effects look like? library(sjPlot) plot_model(mod.1, facet.grid = FALSE, sort = \u0026quot;sort.all\u0026quot;) ## Warning in min(new_value, na.rm = T): no non-missing arguments to min; ## returning Inf ## Warning in min(dat$conf.low): no non-missing arguments to min; returning ## Inf ## Warning in min(dat$estimate): no non-missing arguments to min; returning ## Inf ## Warning in max(dat$conf.high): no non-missing arguments to max; returning - ## Inf ## Warning in max(dat$estimate): no non-missing arguments to max; returning - ## Inf ## Warning in min(ticks): no non-missing arguments to min; returning Inf ## Warning in max(ticks): no non-missing arguments to max; returning -Inf Best Linear Unbiased Predictor = BLUP. More on this later.\nhead(ranef(mod.1)) ## $ID ## (Intercept) ## 6 -0.0597240676 ## 29 -0.0101119688 ## 34 -0.0103698893 ## 36 -0.0035902640 ## 37 -0.0082433829 ## 48 0.0455797808 ## 53 -0.0222710793 ## 54 -0.0066548052 ## 58 -0.0060624543 ## 61 -0.0271347235 ## 66 -0.0123359896 ## 67 -0.0026491341 ## 69 0.0348398944 ## 71 -0.0486040243 ## 74 0.0484338355 ## 75 0.0224228634 ## 76 -0.0021583228 ## 78 0.0224780927 ## 79 -0.0054325535 ## 80 -0.0194707993 ## 81 0.0712662731 ## 82 0.0053695094 ## 85 -0.0532215425 ## 86 -0.0388885304 ## 87 -0.0387411472 ## 89 -0.0208712287 ## 91 0.0123812011 ## 92 -0.0078125821 ## 93 0.0430219016 ## 94 -0.0543390588 ## 96 0.0233440081 ## 97 -0.0497003277 ## 98 -0.0432302582 ## 99 0.0104394983 ## 101 0.0508032394 ## 102 -0.0104344307 ## 103 -0.0206130188 ## 104 -0.0482473609 ## 105 -0.0478231980 ## 106 -0.0028045239 ## 110 0.0418641247 ## 112 -0.0109089622 ## 114 -0.0549314098 ## 115 -0.0013505715 ## 116 0.0062422910 ## 120 0.0300499418 ## 122 0.0793976365 ## 125 0.0532803435 ## 127 -0.0105050866 ## 129 -0.0448207025 ## 135 0.0406255726 ## 136 -0.0364069792 ## 137 -0.0444890904 ## 140 -0.0153440709 ## 141 0.0770651692 ## 142 0.0817077387 ## 143 0.0072423981 ## 144 0.0001680065 ## 146 -0.0551006778 ## 149 -0.0137965477 ## 150 0.0091583792 ## 152 -0.0187707293 ## 153 0.0490992150 ## 155 -0.0233396072 ## 156 -0.0218943803 ## 159 -0.0488368935 ## 160 0.0024524455 ## 162 0.0911638809 ## 163 0.0155327007 ## 165 0.0320764602 ## 167 -0.0025217361 ## 169 0.0647586755 ## 171 -0.0397728293 ## 174 0.0259232134 ## 182 -0.0154177625 ## 187 -0.0581588783 ## 189 0.0348767402 ## 190 -0.0030744230 ## 193 0.0636533019 ## 194 0.0099321407 ## 201 0.0104848276 ## 204 0.0414352908 ## 205 0.0353188897 ## 208 0.0033367444 ## 209 -0.0346144188 ## 211 0.0168223034 ## 214 -0.0374146988 ## 219 0.0564683729 ## 222 -0.0262135788 ## 223 -0.0228606119 ## 229 -0.0484315899 head(coef(mod.1)) ## $ID ## (Intercept) ## 6 0.04724795 ## 29 0.09686005 ## 34 0.09660212 ## 36 0.10338175 ## 37 0.09872863 ## 48 0.15255179 ## 53 0.08470093 ## 54 0.10031721 ## 58 0.10090956 ## 61 0.07983729 ## 66 0.09463602 ## 67 0.10432288 ## 69 0.14181191 ## 71 0.05836799 ## 74 0.15540585 ## 75 0.12939488 ## 76 0.10481369 ## 78 0.12945011 ## 79 0.10153946 ## 80 0.08750121 ## 81 0.17823829 ## 82 0.11234152 ## 85 0.05375047 ## 86 0.06808348 ## 87 0.06823087 ## 89 0.08610079 ## 91 0.11935322 ## 92 0.09915943 ## 93 0.14999392 ## 94 0.05263296 ## 96 0.13031602 ## 97 0.05727169 ## 98 0.06374176 ## 99 0.11741151 ## 101 0.15777525 ## 102 0.09653758 ## 103 0.08635900 ## 104 0.05872465 ## 105 0.05914882 ## 106 0.10416749 ## 110 0.14883614 ## 112 0.09606305 ## 114 0.05204060 ## 115 0.10562144 ## 116 0.11321430 ## 120 0.13702196 ## 122 0.18636965 ## 125 0.16025236 ## 127 0.09646693 ## 129 0.06215131 ## 135 0.14759759 ## 136 0.07056503 ## 137 0.06248292 ## 140 0.09162794 ## 141 0.18403718 ## 142 0.18867975 ## 143 0.11421441 ## 144 0.10714002 ## 146 0.05187134 ## 149 0.09317547 ## 150 0.11613039 ## 152 0.08820128 ## 153 0.15607123 ## 155 0.08363241 ## 156 0.08507763 ## 159 0.05813512 ## 160 0.10942446 ## 162 0.19813589 ## 163 0.12250471 ## 165 0.13904847 ## 167 0.10445028 ## 169 0.17173069 ## 171 0.06719918 ## 174 0.13289523 ## 182 0.09155425 ## 187 0.04881314 ## 189 0.14184875 ## 190 0.10389759 ## 193 0.17062532 ## 194 0.11690415 ## 201 0.11745684 ## 204 0.14840730 ## 205 0.14229090 ## 208 0.11030876 ## 209 0.07235760 ## 211 0.12379432 ## 214 0.06955732 ## 219 0.16344039 ## 222 0.08075844 ## 223 0.08411140 ## 229 0.05854042 fixef(mod.1) ## (Intercept) ## 0.106972 How do these relate? Lets calculate ID 6 intercept random effect\n#coef = fixef + raneff # coef for ID = 6 is 0.04724795 0.106972 -0.0597240676  ## [1] 0.04724793 To get residuals and fitted scores\nlibrary(broom) example.aug\u0026lt;- augment(mod.1, data = example) # .fitted = predicted values # .resid = residuals/errors # .fixed = predicted values with no random effects   Adding time to the MLM fixed slope mod.2f \u0026lt;- lmer(SMN7 ~ 1 + week + (1 | ID), data=example) summary(mod.2f) ## Linear mixed model fit by REML [\u0026#39;lmerMod\u0026#39;] ## Formula: SMN7 ~ 1 + week + (1 | ID) ## Data: example ## ## REML criterion at convergence: -675.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2308 -0.4868 -0.0377 0.4542 3.2337 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 0.001815 0.04261 ## Residual 0.001300 0.03606 ## Number of obs: 216, groups: ID, 88 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.104041 0.005733 18.147 ## week 0.001331 0.001755 0.758 ## ## Correlation of Fixed Effects: ## (Intr) ## week -0.426 What does this look like graphically?\n Random slope mod.2 \u0026lt;- lmer(SMN7 ~ 1 + week + (week | ID), data=example) summary(mod.2) ## Linear mixed model fit by REML [\u0026#39;lmerMod\u0026#39;] ## Formula: SMN7 ~ 1 + week + (week | ID) ## Data: example ## ## REML criterion at convergence: -678.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.93345 -0.47015 -0.00405 0.46985 2.67965 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## ID (Intercept) 1.688e-03 0.041085 ## week 5.999e-05 0.007745 0.11 ## Residual 1.114e-03 0.033378 ## Number of obs: 216, groups: ID, 88 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.104719 0.005466 19.157 ## week 0.000489 0.001913 0.256 ## ## Correlation of Fixed Effects: ## (Intr) ## week -0.339 How does the intercept change from the random intercpet only model? It may change because the intercept is now conditional on time ie after accounting for time. It is not the predicted outcome when time = 0. You can think of the previous intercept as the grand mean of person means. If our week variable here changed across time then there would be a larger change in the intercept.\nHow do you interpret week?\nHow did the random effects change?\nWhy treating time is so important Time with a different scale. How do we interpret? And what changes?\nexample$week.n \u0026lt;- (example$week - 30) mod.2n \u0026lt;- lmer(SMN7 ~ 1 + week.n + (week.n | ID), data=example) ## Warning in checkConv(attr(opt, \u0026quot;derivs\u0026quot;), opt$par, ctrl = ## control$checkConv, : Model failed to converge with max|grad| = 1.99825 (tol ## = 0.002, component 1) summary(mod.2n) ## Linear mixed model fit by REML [\u0026#39;lmerMod\u0026#39;] ## Formula: SMN7 ~ 1 + week.n + (week.n | ID) ## Data: example ## ## REML criterion at convergence: -674.8 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2053 -0.4841 -0.0423 0.4505 3.2646 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## ID (Intercept) 1.001e-03 0.0316327 ## week.n 1.333e-07 0.0003652 -1.00 ## Residual 1.320e-03 0.0363343 ## Number of obs: 216, groups: ID, 88 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.146910 0.050598 2.904 ## week.n 0.001432 0.001763 0.812 ## ## Correlation of Fixed Effects: ## (Intr) ## week.n 0.995 ## convergence code: 0 ## Model failed to converge with max|grad| = 1.99825 (tol = 0.002, component 1)    Random effects Calcualtion of random effect confidence interval Conveys the predicted range around each fixed effect in which 95% of the sample individauls are predicted to fall.\n95% random effect = fixed effect plus minus 1.96 * random standard deviation\nHow to calcualte? 1. Intercept \\[ \\gamma_{00} \\pm 1.96 * \\tau_{U_{0j}} \\]\n0.1193933 + (1.96 * 0.240217)  ## [1] 0.5902186 0.1193933 - (1.96 * 0.240217)  ## [1] -0.351432 Slope \\[ \\gamma_{10} \\pm 1.96 * \\tau_{U_{1j}} \\]  0.0004891 + (1.96 * 0.007745)  ## [1] 0.0156693 0.0004891 - (1.96 * 0.007745)  ## [1] -0.0146911 ###Individaul level random effects\nAre the intercept random effects the same as the model with only the intercept? Why or why not?\nhead(ranef(mod.1)) ## $ID ## (Intercept) ## 6 -0.0597240676 ## 29 -0.0101119688 ## 34 -0.0103698893 ## 36 -0.0035902640 ## 37 -0.0082433829 ## 48 0.0455797808 ## 53 -0.0222710793 ## 54 -0.0066548052 ## 58 -0.0060624543 ## 61 -0.0271347235 ## 66 -0.0123359896 ## 67 -0.0026491341 ## 69 0.0348398944 ## 71 -0.0486040243 ## 74 0.0484338355 ## 75 0.0224228634 ## 76 -0.0021583228 ## 78 0.0224780927 ## 79 -0.0054325535 ## 80 -0.0194707993 ## 81 0.0712662731 ## 82 0.0053695094 ## 85 -0.0532215425 ## 86 -0.0388885304 ## 87 -0.0387411472 ## 89 -0.0208712287 ## 91 0.0123812011 ## 92 -0.0078125821 ## 93 0.0430219016 ## 94 -0.0543390588 ## 96 0.0233440081 ## 97 -0.0497003277 ## 98 -0.0432302582 ## 99 0.0104394983 ## 101 0.0508032394 ## 102 -0.0104344307 ## 103 -0.0206130188 ## 104 -0.0482473609 ## 105 -0.0478231980 ## 106 -0.0028045239 ## 110 0.0418641247 ## 112 -0.0109089622 ## 114 -0.0549314098 ## 115 -0.0013505715 ## 116 0.0062422910 ## 120 0.0300499418 ## 122 0.0793976365 ## 125 0.0532803435 ## 127 -0.0105050866 ## 129 -0.0448207025 ## 135 0.0406255726 ## 136 -0.0364069792 ## 137 -0.0444890904 ## 140 -0.0153440709 ## 141 0.0770651692 ## 142 0.0817077387 ## 143 0.0072423981 ## 144 0.0001680065 ## 146 -0.0551006778 ## 149 -0.0137965477 ## 150 0.0091583792 ## 152 -0.0187707293 ## 153 0.0490992150 ## 155 -0.0233396072 ## 156 -0.0218943803 ## 159 -0.0488368935 ## 160 0.0024524455 ## 162 0.0911638809 ## 163 0.0155327007 ## 165 0.0320764602 ## 167 -0.0025217361 ## 169 0.0647586755 ## 171 -0.0397728293 ## 174 0.0259232134 ## 182 -0.0154177625 ## 187 -0.0581588783 ## 189 0.0348767402 ## 190 -0.0030744230 ## 193 0.0636533019 ## 194 0.0099321407 ## 201 0.0104848276 ## 204 0.0414352908 ## 205 0.0353188897 ## 208 0.0033367444 ## 209 -0.0346144188 ## 211 0.0168223034 ## 214 -0.0374146988 ## 219 0.0564683729 ## 222 -0.0262135788 ## 223 -0.0228606119 ## 229 -0.0484315899 head(coef(mod.1)) ## $ID ## (Intercept) ## 6 0.04724795 ## 29 0.09686005 ## 34 0.09660212 ## 36 0.10338175 ## 37 0.09872863 ## 48 0.15255179 ## 53 0.08470093 ## 54 0.10031721 ## 58 0.10090956 ## 61 0.07983729 ## 66 0.09463602 ## 67 0.10432288 ## 69 0.14181191 ## 71 0.05836799 ## 74 0.15540585 ## 75 0.12939488 ## 76 0.10481369 ## 78 0.12945011 ## 79 0.10153946 ## 80 0.08750121 ## 81 0.17823829 ## 82 0.11234152 ## 85 0.05375047 ## 86 0.06808348 ## 87 0.06823087 ## 89 0.08610079 ## 91 0.11935322 ## 92 0.09915943 ## 93 0.14999392 ## 94 0.05263296 ## 96 0.13031602 ## 97 0.05727169 ## 98 0.06374176 ## 99 0.11741151 ## 101 0.15777525 ## 102 0.09653758 ## 103 0.08635900 ## 104 0.05872465 ## 105 0.05914882 ## 106 0.10416749 ## 110 0.14883614 ## 112 0.09606305 ## 114 0.05204060 ## 115 0.10562144 ## 116 0.11321430 ## 120 0.13702196 ## 122 0.18636965 ## 125 0.16025236 ## 127 0.09646693 ## 129 0.06215131 ## 135 0.14759759 ## 136 0.07056503 ## 137 0.06248292 ## 140 0.09162794 ## 141 0.18403718 ## 142 0.18867975 ## 143 0.11421441 ## 144 0.10714002 ## 146 0.05187134 ## 149 0.09317547 ## 150 0.11613039 ## 152 0.08820128 ## 153 0.15607123 ## 155 0.08363241 ## 156 0.08507763 ## 159 0.05813512 ## 160 0.10942446 ## 162 0.19813589 ## 163 0.12250471 ## 165 0.13904847 ## 167 0.10445028 ## 169 0.17173069 ## 171 0.06719918 ## 174 0.13289523 ## 182 0.09155425 ## 187 0.04881314 ## 189 0.14184875 ## 190 0.10389759 ## 193 0.17062532 ## 194 0.11690415 ## 201 0.11745684 ## 204 0.14840730 ## 205 0.14229090 ## 208 0.11030876 ## 209 0.07235760 ## 211 0.12379432 ## 214 0.06955732 ## 219 0.16344039 ## 222 0.08075844 ## 223 0.08411140 ## 229 0.05854042 fixef(mod.1) ## (Intercept) ## 0.106972 How do these relate? Lets calculate ID 6 intercept random effect\n#coef = fixef + raneff # 0.04724795 0.106972 -0.0597240676  ## [1] 0.04724793 To get residuals and fitted scores\nlibrary(broom) example.aug\u0026lt;- augment(mod.1, data = example) # .fitted = predicted values # .resid = residuals/errors # .fixed = predicted values with no random effects  adding time mod.2 \u0026lt;- lmer(SMN7 ~ 1 + week + (week | ID), data=example) summary(mod.2) ## Linear mixed model fit by REML [\u0026#39;lmerMod\u0026#39;] ## Formula: SMN7 ~ 1 + week + (week | ID) ## Data: example ## ## REML criterion at convergence: -678.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.93345 -0.47015 -0.00405 0.46985 2.67965 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## ID (Intercept) 1.688e-03 0.041085 ## week 5.999e-05 0.007745 0.11 ## Residual 1.114e-03 0.033378 ## Number of obs: 216, groups: ID, 88 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.104719 0.005466 19.157 ## week 0.000489 0.001913 0.256 ## ## Correlation of Fixed Effects: ## (Intr) ## week -0.339 Random effects another way\nre2 \u0026lt;- ranef(mod.2) head(re2) ## $ID ## (Intercept) week ## 6 -5.657190e-02 -0.0036945459 ## 29 -8.481756e-03 -0.0012401120 ## 34 -6.559720e-03 -0.0036272361 ## 36 -7.107377e-03 0.0047529824 ## 37 -7.584156e-03 0.0003779790 ## 48 3.760800e-02 0.0079905136 ## 53 -1.989445e-02 -0.0015608099 ## 54 1.298061e-03 -0.0046572183 ## 58 3.763976e-04 -0.0054304028 ## 61 -2.523030e-02 -0.0013740463 ## 66 -1.692834e-02 0.0022291440 ## 67 -4.435808e-03 0.0020525694 ## 71 -4.790346e-02 0.0002829010 ## 75 2.147451e-02 0.0025670702 ## 76 -1.178132e-03 0.0005880005 ## 78 1.590422e-02 0.0031440504 ## 79 -4.554999e-03 0.0005646937 ## 80 -1.789820e-02 -0.0007154790 ## 81 5.310404e-02 0.0082155118 ## 82 4.163353e-03 0.0008114128 ## 85 -5.229368e-02 -0.0013883325 ## 86 -3.747004e-02 -0.0020088462 ## 87 -3.530494e-02 -0.0042629307 ## 89 -1.906497e-02 -0.0008530205 ## 91 1.547560e-02 -0.0021933791 ## 92 -5.127067e-03 -0.0012825084 ## 93 3.779074e-02 0.0055466427 ## 94 -5.318192e-02 -0.0005568113 ## 96 2.535874e-02 -0.0004379578 ## 97 -4.654850e-02 -0.0019515143 ## 98 -3.875698e-02 -0.0027541673 ## 99 9.785177e-03 0.0016155143 ## 101 4.073120e-02 0.0116039230 ## 103 -1.910831e-02 -0.0013254022 ## 104 -4.656380e-02 -0.0034349757 ## 105 -4.532176e-02 -0.0021913974 ## 106 4.024883e-05 -0.0015454111 ## 110 5.266477e-02 -0.0065630548 ## 112 -5.482604e-03 -0.0041413140 ## 114 -5.187791e-02 -0.0023737578 ## 115 -6.018565e-04 0.0004676209 ## 116 7.916722e-03 -0.0002939168 ## 120 3.156554e-02 0.0021828094 ## 122 7.845607e-02 0.0030220350 ## 125 5.565560e-02 -0.0004030572 ## 127 -1.035814e-02 0.0009947753 ## 129 -4.383714e-02 -0.0011901610 ## 135 4.517320e-02 -0.0027851051 ## 136 -3.257406e-02 -0.0027536989 ## 137 -4.307457e-02 -0.0026649722 ## 140 -1.389973e-02 -0.0009470411 ## 141 7.828180e-02 0.0011246004 ## 142 7.656429e-02 0.0071700560 ## 143 8.802167e-03 0.0001782843 ## 144 5.023920e-04 0.0007247389 ## 146 -5.190868e-02 -0.0030652978 ## 149 -1.315714e-02 0.0001913378 ## 150 1.329264e-02 -0.0028911814 ## 152 -1.719482e-02 -0.0013669192 ## 153 4.865193e-02 0.0022929034 ## 155 -2.212352e-02 -0.0004931690 ## 156 -2.041537e-02 -0.0004522343 ## 159 -4.783022e-02 -0.0015435325 ## 160 3.459146e-03 0.0001563020 ## 162 8.771544e-02 0.0058967120 ## 163 1.575853e-02 0.0011486036 ## 165 3.244233e-02 0.0013196820 ## 167 -5.436807e-04 -0.0009083619 ## 169 6.576873e-02 0.0011926808 ## 171 -3.475852e-02 -0.0047511867 ## 174 2.798144e-02 -0.0004759192 ## 182 -5.231973e-03 -0.0081378967 ## 187 -5.609959e-02 -0.0019680493 ## 189 3.456648e-02 0.0020405469 ## 190 -1.620723e-03 -0.0003743333 ## 193 5.854085e-02 0.0071786529 ## 194 4.241261e-03 0.0066159420 ## 201 1.187752e-02 -0.0001340229 ## 204 3.799808e-02 0.0055309494 ## 205 4.004323e-02 -0.0029413522 ## 208 3.040083e-04 0.0043109412 ## 209 -3.367017e-02 -0.0004225192 ## 211 1.856239e-02 -0.0003192506 ## 214 -3.676234e-02 -0.0001666149 ## 219 4.541593e-02 0.0143735740 ## 222 -1.947934e-02 -0.0063364890 ## 223 -1.719018e-02 -0.0051038253 ## 229 -4.254994e-02 -0.0060019177 random_params \u0026lt;- tidy(mod.2, effect = \u0026quot;ran_modes\u0026quot;) head(random_params) ## level group term estimate std.error ## 1 101 ID (Intercept) 0.1454506216 0.018681911 ## 2 101 ID week 0.0120929048 0.006503270 ## 3 103 ID (Intercept) 0.0856111110 0.020552331 ## 4 103 ID week -0.0008364205 0.007484617 ## 5 104 ID (Intercept) 0.0581556164 0.020564418 ## 6 104 ID week -0.0029459940 0.007471121 Using simulations to get better estimates of confidence around our estimates\nlibrary(merTools) ## Loading required package: arm ## Loading required package: MASS ## ## Attaching package: \u0026#39;MASS\u0026#39; ## The following object is masked from \u0026#39;package:dplyr\u0026#39;: ## ## select ## ## arm (Version 1.10-1, built: 2018-4-12) ## Working directory is /Users/jackson/Box/5165 Applied Longitudinal Data Analysis/ALDA/content/Lectures FEsim(mod.2) ## term mean median sd ## 1 (Intercept) 0.1044074017 0.1039119381 0.005272623 ## 2 week 0.0007611029 0.0006878781 0.001868060 re.sim \u0026lt;- REsim(mod.2) head(re.sim) ## groupFctr groupID term mean median sd ## 1 ID 6 (Intercept) -0.056526527 -0.056522578 0.01853652 ## 2 ID 29 (Intercept) -0.009124553 -0.008835317 0.02100211 ## 3 ID 34 (Intercept) -0.008828907 -0.006406779 0.01971484 ## 4 ID 36 (Intercept) -0.007870006 -0.006666001 0.02263088 ## 5 ID 37 (Intercept) -0.008727056 -0.008822997 0.01890044 ## 6 ID 48 (Intercept) 0.039487607 0.040242941 0.02003504 This can be used to create CIs for each individaul random effect (and fixed effect). What is the confidence interval around person 6’s intercept estimate compared to person 2000 who has 25 repeated measurements?\n caterpillar plots Look through these different methods of getting random effects. Note that they are not all exactly the same.\ncaterpillar plots\np1 \u0026lt;- plotREsim(re.sim) p1  Density of individaul random effects p1.gg1 \u0026lt;- re.sim %\u0026gt;% filter(term == \u0026quot;(Intercept)\u0026quot;) ggplot(p1.gg1, aes(mean)) + geom_density() p1.gg2 \u0026lt;- re.sim %\u0026gt;% filter(term == \u0026quot;week\u0026quot;) ggplot(p1.gg2, aes(mean)) + geom_density()   comparing to a standard linear model lm.1 \u0026lt;- lm(SMN7 ~ 1 + week, data = example) summary(lm.1) ## ## Call: ## lm(formula = SMN7 ~ 1 + week, data = example) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.099294 -0.039929 -0.005938 0.032715 0.169885 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.100161 0.005261 19.039 \u0026lt;2e-16 *** ## week 0.004087 0.002563 1.595 0.112 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.05562 on 214 degrees of freedom ## (9 observations deleted due to missingness) ## Multiple R-squared: 0.01174, Adjusted R-squared: 0.007124 ## F-statistic: 2.543 on 1 and 214 DF, p-value: 0.1123  Other types of models Depending on your DV, you might not want to have a Gaussian sampling distribution. Instead you may want something like a Poisson or a negative binomial if you are using some sort of count data. You can do this somewhat with lme4. However, the BRMS package – which uses Bayesian estimation – has many more possibilities: geometric, log normal, weibull, exponential, gamma, Beta, hurdle Poisson/gamma/negative binomial, zero inflated beta/Poisson/negative binomial, cumulative. Maybe we will fit some of these later in the semester.\n Matrix notation (as a way to help understand what is going on) \\[ y_{i} = X_{i}\\beta + Z_{i}b_{i} + \\varepsilon_{i} \\]\nLets assume we have four time points. This formula is equivalent to:\n$$ \\begin{bmatrix} y_{1j} \\ y_{2j} \\ y_{3j} \\ y_{4j}\n\\end{bmatrix} +\n\\begin{bmatrix} 1 \u0026amp; time_{1j} \\ 1 \u0026amp; time_{2j} \\ 1 \u0026amp; time_{3j} \\ 1 \u0026amp; time_{4j}\n\\end{bmatrix}\n\\begin{bmatrix} {0} \\ {1} \\ {2} \\ {3}\n\\end{bmatrix}\n \\begin{bmatrix} 1 \u0026amp; time_{1j} \\ 1 \u0026amp; time_{2j} \\ 1 \u0026amp; time_{3j} \\ 1 \u0026amp; time_{4j}\n\\end{bmatrix} \\begin{bmatrix} b_{0j} \\ b_{1j} \\ \\end{bmatrix} \\begin{bmatrix} {1j} \\ {2j} \\ {3j} \\ {4j}\n\\end{bmatrix} $$  X is the design matrix for fixed effects Z is the design matrix for random effects. \\(\\beta\\) is a vector of fixed effects b is a vector of random effects \\(\\varepsilon\\) is a vector of residual error\nNote that all are subject specific (j) besides the vector of fixed effects\nThe design matrix can be increased with the inclusion of other predictors (See next week)\nmodel.matrix(mod.2) ## (Intercept) week ## 1 1 0.0000000 ## 2 1 0.9479452 ## 3 1 1.8493151 ## 4 1 0.0000000 ## 5 1 1.0383562 ## 6 1 0.0000000 ## 7 1 2.3917808 ## 8 1 0.0000000 ## 9 1 2.9589041 ## 10 1 0.0000000 ## 11 1 1.9369863 ## 12 1 2.7917808 ## 13 1 0.0000000 ## 14 1 1.9068493 ## 15 1 2.9424658 ## 16 1 0.0000000 ## 17 1 3.2246575 ## 18 1 0.0000000 ## 19 1 1.8602740 ## 20 1 4.0109589 ## 21 1 0.0000000 ## 22 1 1.4082192 ## 23 1 2.4739726 ## 24 1 0.0000000 ## 25 1 2.6575342 ## 26 1 0.0000000 ## 27 1 2.9698630 ## 28 1 5.8109589 ## 29 1 0.0000000 ## 30 1 0.8931507 ## 31 1 1.6657534 ## 32 1 4.5232877 ## 35 1 0.0000000 ## 36 1 0.8657534 ## 37 1 1.7753425 ## 41 1 0.0000000 ## 42 1 2.9589041 ## 43 1 0.0000000 ## 44 1 1.0164384 ## 45 1 2.0328767 ## 46 1 0.0000000 ## 47 1 3.1232877 ## 48 1 5.9863014 ## 49 1 0.0000000 ## 50 1 2.0164384 ## 51 1 0.0000000 ## 52 1 3.8109589 ## 53 1 0.0000000 ## 54 1 3.1150685 ## 55 1 6.0493151 ## 56 1 0.0000000 ## 57 1 2.1369863 ## 58 1 3.2301370 ## 59 1 6.3178082 ## 60 1 0.0000000 ## 61 1 1.0712329 ## 62 1 0.0000000 ## 63 1 1.2109589 ## 64 1 0.0000000 ## 65 1 2.1369863 ## 66 1 0.0000000 ## 67 1 1.1397260 ## 68 1 2.0383562 ## 69 1 0.0000000 ## 70 1 0.9753425 ## 71 1 1.8657534 ## 72 1 0.0000000 ## 73 1 1.9260274 ## 74 1 3.0356164 ## 75 1 0.0000000 ## 76 1 1.9315068 ## 77 1 3.0438356 ## 78 1 0.0000000 ## 79 1 1.0054795 ## 80 1 2.2136986 ## 81 1 0.0000000 ## 82 1 0.9698630 ## 83 1 0.0000000 ## 84 1 1.0054795 ## 85 1 2.0575342 ## 86 1 2.9150685 ## 87 1 0.0000000 ## 88 1 0.9616438 ## 89 1 2.2986301 ## 90 1 3.3232877 ## 91 1 0.0000000 ## 92 1 1.0520548 ## 93 1 2.1835616 ## 94 1 3.0164384 ## 95 1 0.0000000 ## 96 1 0.9041096 ## 97 1 3.2657534 ## 102 1 0.0000000 ## 103 1 1.1534247 ## 104 1 0.0000000 ## 105 1 1.1945205 ## 106 1 0.0000000 ## 107 1 0.9671233 ## 108 1 2.2410959 ## 109 1 0.0000000 ## 110 1 1.0410959 ## 111 1 3.1123288 ## 112 1 0.0000000 ## 113 1 1.6164384 ## 114 1 3.8794521 ## 115 1 0.0000000 ## 116 1 0.9369863 ## 117 1 3.2273973 ## 118 1 0.0000000 ## 119 1 1.0602740 ## 120 1 3.1232877 ## 121 1 0.0000000 ## 122 1 1.0136986 ## 123 1 3.2986301 ## 124 1 0.0000000 ## 125 1 0.9178082 ## 126 1 3.1616438 ## 127 1 0.0000000 ## 128 1 1.0027397 ## 129 1 0.0000000 ## 130 1 0.9780822 ## 131 1 3.3041096 ## 132 1 0.0000000 ## 133 1 1.2054795 ## 134 1 3.0767123 ## 135 1 0.0000000 ## 136 1 1.0410959 ## 137 1 3.0739726 ## 138 1 0.0000000 ## 139 1 0.9232877 ## 140 1 0.0000000 ## 141 1 0.9589041 ## 142 1 2.9972603 ## 143 1 0.0000000 ## 144 1 1.0383562 ## 145 1 3.3890411 ## 146 1 0.0000000 ## 147 1 1.0739726 ## 148 1 0.0000000 ## 149 1 0.9095890 ## 150 1 0.0000000 ## 151 1 3.2602740 ## 152 1 0.0000000 ## 153 1 3.2273973 ## 154 1 0.0000000 ## 155 1 1.1205479 ## 156 1 0.0000000 ## 157 1 3.2082192 ## 158 1 0.0000000 ## 159 1 2.9972603 ## 160 1 0.0000000 ## 161 1 3.2794521 ## 162 1 0.0000000 ## 163 1 2.9917808 ## 164 1 0.0000000 ## 165 1 1.2356164 ## 166 1 0.0000000 ## 167 1 3.1780822 ## 168 1 0.0000000 ## 169 1 1.9123288 ## 170 1 0.0000000 ## 171 1 1.0520548 ## 172 1 2.9397260 ## 173 1 0.0000000 ## 174 1 0.9753425 ## 175 1 0.0000000 ## 176 1 3.0876712 ## 177 1 0.0000000 ## 178 1 1.0383562 ## 179 1 3.0109589 ## 180 1 0.0000000 ## 181 1 3.0876712 ## 182 1 0.0000000 ## 183 1 3.0493151 ## 184 1 0.0000000 ## 185 1 2.9342466 ## 186 1 0.0000000 ## 187 1 3.1315068 ## 188 1 0.0000000 ## 189 1 2.9342466 ## 190 1 0.0000000 ## 191 1 2.9780822 ## 192 1 0.0000000 ## 193 1 3.4575342 ## 194 1 0.0000000 ## 195 1 2.9945205 ## 196 1 0.0000000 ## 197 1 3.0520548 ## 198 1 0.0000000 ## 199 1 2.9972603 ## 200 1 0.0000000 ## 201 1 3.1041096 ## 202 1 0.0000000 ## 203 1 3.1863014 ## 204 1 0.0000000 ## 205 1 3.4547945 ## 206 1 0.0000000 ## 207 1 2.9369863 ## 208 1 0.0000000 ## 209 1 3.0849315 ## 210 1 0.0000000 ## 211 1 2.9945205 ## 212 1 0.0000000 ## 213 1 3.3150685 ## 214 1 0.0000000 ## 215 1 3.0876712 ## 216 1 0.0000000 ## 217 1 2.9945205 ## 218 1 0.0000000 ## 219 1 2.8273973 ## 220 1 0.0000000 ## 221 1 2.9178082 ## 222 1 0.0000000 ## 223 1 2.9479452 ## 224 1 0.0000000 ## 225 1 2.8465753 ## attr(,\u0026quot;assign\u0026quot;) ## [1] 0 1 ## attr(,\u0026quot;msgScaleX\u0026quot;) ## character(0)  Estimation Maximum likelihood estimation. Uses a likelihood function that describes the probability of observing the sample data as a function of the parameters. Attempts to maximize the function.\nREML vs ML\nDifferences account for the fact that fixed effects are being estimated simultaously with th variance parameters. REML accounts for uncertainty in the fixed effects before estimating residual variance. REML attempts to maximize the likelihood of the residuals whereas ML the sample data.\nIf you use REML you should be careful in testing fixed effects against eachother (more down below)\n Testing significance (adapted from Ben Bolker) Methods for testing single parameters From worst to best:\nWald Z-tests. Easy to compute. However, they are asymptotic approximations, assuming both that (1) the sampling distributions of the parameters are multivariate normal and that (2) the sampling distribution of the log-likelihood is (proportional to) χ2. Wald t-tests Likelihood ratio test.\n Markov chain Monte Carlo (MCMC) or parametric bootstrap confidence intervals  P values are not included Authors are not convinced of the utility of the general approach of testing with reference to an approximate null distribution. In general, it is not clear that the null distribution of the computed ratio of sums of squares is really an F distribution, for any choice of denominator degrees of freedom. While this is true for special cases that correspond to classical experimental designs (nested, split-plot, randomized block, etc.), it is apparently not true for more complex designs (unbalanced, GLMMs, temporal or spatial correlation, etc.).\ntl;dr: it gets messy with more complex models\nIf you really want p values\n# library(lmerTest)  Likelhiood ratio test How much more likely the data is under a more complex model than under the simpler model (these models need to be nested to compare this).\nLog Likelihood (LL) is derived from ML estimation. Larger the LL the better the fit.\nDeviance compares two LLs. Current model and a saturated model (that fits data perfectly).\nDeviance = -2[LL current - LL saturated]\nLL saturated = 1 for MLMs (probability it will perfectly recapture data). log of 1 is 0. So this term drops out.\nDeviance = -2LL current model.\nComparing 2 models is called a likelihood ration test. Need to have: 1. same data 2. nested models (think of constraining a parameter to zero)\nDistributed as chi-square with df equal to constraint differences between models.\nmod.2r \u0026lt;- lmer(SMN7 ~ 1 + week + ( 1 | ID), data=example) summary(mod.2r) ## Linear mixed model fit by REML [\u0026#39;lmerMod\u0026#39;] ## Formula: SMN7 ~ 1 + week + (1 | ID) ## Data: example ## ## REML criterion at convergence: -675.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2308 -0.4868 -0.0377 0.4542 3.2337 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 0.001815 0.04261 ## Residual 0.001300 0.03606 ## Number of obs: 216, groups: ID, 88 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.104041 0.005733 18.147 ## week 0.001331 0.001755 0.758 ## ## Correlation of Fixed Effects: ## (Intr) ## week -0.426 anova(mod.2, mod.2r) ## refitting model(s) with ML (instead of REML) ## Data: example ## Models: ## mod.2r: SMN7 ~ 1 + week + (1 | ID) ## mod.2: SMN7 ~ 1 + week + (week | ID) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(\u0026gt;Chisq) ## mod.2r 4 -686.93 -673.43 347.46 -694.93 ## mod.2 6 -685.45 -665.20 348.73 -697.45 2.5248 2 0.283  Likelihood tests for random effects Not listed in the output because it is hard to do this with variances. Remember variances do not have values below zero and thus the distributions get a wonky quickly. Needs mixture distributions (Cannot be easily done with chi square, for example)\nCan Do anova comparisons, though that falls to many similar problems as trying to do a Wald test.\nThe sampling distribution of variance estimates is in general strongly asymmetric: the standard error may be a poor characterization of the uncertainty. Thus the best way to handle is to do bootstrapped estimates.\n AIC and BIC AIC (Akaike’s Information Criterion) and the BIC (Bayesian Information Criterion) where “smaller is better.” This is the opposite of LL/Deviance As with the other types, these may give you wonky findings depending on some factors as they are related to LLs.\nAIC = 2(number of parameters) + (−2LL) BIC = ln(n)(number of parameters) + (−2LL)\nBIC penalizes models with more parameters more than AIC does.\n MCMC More on this later.\nlibrary(rstanarm) library(brms) library(mcmcsamp) #library(mcmcsamp)  Bootstraps Parametric bootstrap:\nconfint(mod.1, method=\u0026quot;boot\u0026quot;, nsim=1000) ## Computing bootstrap confidence intervals ... ## ## 7 warning(s): Model failed to converge with max|grad| = 0.00326574 (tol = 0.002, component 1) (and others) ## 2.5 % 97.5 % ## .sig01 0.03437911 0.05119232 ## .sigma 0.03201764 0.04051648 ## (Intercept) 0.09665970 0.11679542 summary(mod.1) ## Linear mixed model fit by REML [\u0026#39;lmerMod\u0026#39;] ## Formula: SMN7 ~ 1 + (1 | ID) ## Data: example ## ## REML criterion at convergence: -714.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.1575 -0.4728 -0.0232 0.4512 3.2750 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 0.001823 0.04270 ## Residual 0.001302 0.03608 ## Number of obs: 225, groups: ID, 91 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 0.106972 0.005106 20.95 # uses SDs of random effects # sigma = residual standard error Comparing two models. fit the reduced model, then repeatedly simulate from it and compute the differences between the deviance of the reduced and the full model for each simulated data set. Compare this null distribution to the observed deviance difference. This procedure is implemented in the pbkrtest package.\nlibrary(pbkrtest) #pb \u0026lt;- PBmodcomp(mod.2,mod.2r)   Predictions and prediction intervals Predict function is deterministic and uses only the fixed effects (i.e. does not include random effects in the predictions). It does not do prediction in the typical sense where you are predicting new individual’s scores.\nSimulate is non-deterministic because it samples random effect values for all subjects and then samples from the conditional distribution. Simulation is needed to create true predictions.\nPredictions and prediction intervals Predict function is deterministic and uses only the fixed effects (i.e. does not include random effects in the predictions). It does not do prediction in the typical sense where you are predicting new individuals’s scores.\nSimulate is non-deterministic because it samples random effect values for all subjects and then samples from the conditional distribution. Simulation is needed to create true preditions.\nShort of a fully Bayesian analysis, bootstrapping is the gold-standard for deriving prediction intervals/bands (ie where would a new person score given X), but the time required is typically high.\nIn order to generate a proper prediction (for either a new person or a new observation within a person), a prediction must account for three sources of uncertainty in mixed models:\nthe residual (observation-level) variance, the uncertainty in the fixed coefficients, and the uncertainty in the variance parameters for the random effects  Does so by: 1. extracting the fixed and random coefficients 2. takes n draws from the multivariate normal distribution of the fixed and random coefficients (separately) 3. calculates the linear predictor for each row in newdata based on these draws, and 4. incorporates the residual variation\nthen: 5. returns newdata with the lower and upper limits of the prediction interval and the mean or median of the simulated predictions\nlibrary(merTools) # see also their shiny app: shinyMer(mod.1) PI \u0026lt;- predictInterval(merMod = mod.2, newdata = example, level = 0.9, n.sims = 100, stat = \u0026quot;median\u0026quot;, include.resid.var = TRUE) head(PI) ## fit upr lwr ## 1 0.05308144 0.1183545 -0.002075082 ## 2 0.03924756 0.1016895 -0.024710060 ## 3 0.04589155 0.1067230 -0.027015400 ## 4 0.09898652 0.1629507 0.022304816 ## 5 0.09481132 0.1548010 0.028729007 ## 6 0.09965443 0.1668782 0.041778680 Nice for bringing in confidence bands around your prediction (And we might use this later)\nBroom offers the fitted (predicted) values already if you just want to plot your trajectory. But note that these are not typical prediction intervals (what happens if you get a new participant with a certain value of X). The bands fit in ggplot are for predicted \\(\\mu\\)|X\nBroom offers the fitted (predicted) values already if you just want to plot your trajectory. But note that these are not typical prediction intervals (what happens if you get a new particpant with a certain value of X). The bands fit in ggplot are for predicted \\(\\mu\\)|X\nP.gg \u0026lt;- ggplot(example.aug, aes(x= week, y = .fitted)) + geom_point() + stat_smooth(method = \u0026quot;lm\u0026quot;) P.gg ## Warning: Removed 9 rows containing non-finite values (stat_smooth). ## Warning: Removed 9 rows containing missing values (geom_point). Can also explicitly simulate new data (rather than rely on another function to do so), which will be useful for power calculations later. In the simulated data, the subject means are different from the means in the original data because simulate samples by-subject random effect values using the variance components in the fitted model.\nsim.1\u0026lt;- simulate(mod.2) head(sim.1)   Coefficient of determination equivalents Issue is: should you include or exclude variation of different random-effects terms?\nCan do a more Psuedo R2 by taking the difference in variance between model 1 and model 2 and deviding it by model 1.\nE.g,. residual variance in varying intercept model subtracted from growth model devided by intercept only model.\n(sigma(mod.1) - sigma(mod.2)) / sigma(mod.1) ## [1] 0.07484984 Proportion of variance explained by time.\nbatch analyses Can easily do a lot of models simulataniously. You do not need to use for loops.\nCheck out dplyr::do as well as purrr::map.\n##Now you try:\nGraph by hand what an empty model, showing at least two people’s raw data points and describign different sources or error.  1.Run linear models on all of your subjects (a basic regression). What is the average intercept, the average slope?\nNow run a mlm/lmer model with only a random intercept. What is the ICC? What does residual variance look like compared to linear model? Create a graph to show this effect.\n Introduce a fixed slope term. What is the difference in terms of the fixed effects estimates between this estimate and the previous? Of the residual standard error? Create a graph to show both fixed effects estimates and the CIs around them.\n Run an additional model with a random slope. How does this change compare to the previous model? Should you keep the random slope or not?\n Interpret the correlation between the slope and the intercept.\n Create a density plot of the random effects from your final model.\n Create a catepilar plot of the random effects. Is there any person that seems odd in terms of a large standard errors around intercept and slope estimates?\n Create a plot of the trajectory, along with a spaghetti plot of each person’s individual slope. Set the alpha level (transparency) on the individual slopes to make them easier to see.\n Create a plot of the trajectory, along with a spagehtti plot of each person’s individual slope. Set the alpha level (transperancy) on the individual slopes to make them easier to see.\n     ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"8addbc10cd70b885f5e400aa208175ab","permalink":"/lectures/03-growth-curves/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/lectures/03-growth-curves/","section":"Lectures","summary":"Growth Curves","tags":null,"title":"Week 2","type":"post"},{"authors":null,"categories":null,"content":"\n\n\n\n","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565740800,"objectID":"753e1444f35b18a6aa22906136df3ae1","permalink":"/lectures/2019-08-14-lecture-3-test/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/lectures/2019-08-14-lecture-3-test/","section":"Lectures","summary":"Conditional Models","tags":null,"title":"Week 3","type":"post"},{"authors":null,"categories":["R"],"content":"  What Are Data? Workspace Packages Codebook Data  Clean Data Recode Variables Reverse-Scoring Create Composites BFI-S Life Events   Descriptives Scale Reliability Zero-Order Correlations    What Are Data? Data are the core of everything that we do in statistical analysis. Data come in many forms, and I don’t just mean .csv, .xls, .sav, etc. Data can be wide, long, documented, fragmented, messy, and about anything else that you can imagine.\nAlthough data could arguably be more means than end in psychology, the importance of understanding the structure and format of your data cannot overstated. Failure to understand your data could end in improper techniques and flagrantly wrong inferences at worst. This is especially important for longitudinal data.\nIn this workshop, we are going to talk data management and basic data cleaning. Other tutorials will go more in depth into data cleaning and reshaping. This tutorial is meant to prepare you to think about those in more nuanced ways and to help you develop a functional workflow for conducting your own research.\nThe workshop applies to ALL of your data/projects/analysis, not just longitudinal data. These are practices that will accomplish three goals: 1) efficiently load and leave your data in the right form to be analyzed, 2) have the organization so as to follow what you did and so others can understand you did, and 3. share the data/code/plots/analyses easily and effectively. None of the following are the absolute necessary way to accomplish your data analytic goals. However, we feel that people mostly don’t think through these steps. Hammstringing them later. In other words, if you know an alternative that means you already know what we are trying to convey.\n Workspace When I create an rmarkdown document for my own research projects, I always start by setting up my my workspace. This involves 3 steps:\nPackages\n Codebook(s)\n Data  Below, we will step through each of these separately, setting ourselves up to (hopefully) flawlessly communicate with R and our data.\nPackages Packages seems like the most basic step, but it is actually very important. ALWAYS LOAD YOUR PACKAGES IN A VERY INTENTIONAL ORDER AT THE BEGINNING OF YOUR SCRIPT. Package conflicts suck, so it needs to be shouted. (Note: Josh will often reload or not follow this advice for didactic reasons, choosing to put library calls above the code. )\nFor this tutorial, we are going to quite simple. We will load the psych package for data descriptives, some options for cleaning and reverse coding, and some evaluations of our scales. The plyr package is the predecessor of the dplyr package, which is a core package of the tidyverse, which you will become quite familiar with in these tutorials. I like the plyr package because it contains a couple of functions (e.g. mapvalues()) that I find quite useful. Finally, we load the tidyverse package, which is actually a complilation of 8 packages. Some of these we will use today and some we will use in later tutorials. All are very useful and are arguably some of the most powerful tools R offers.\n# load packages library(psych) library(plyr) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1.9000 ── ## ✔ ggplot2 3.2.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 0.8.99.9000 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ─────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ ggplot2::%+%() masks psych::%+%() ## ✖ ggplot2::alpha() masks psych::alpha() ## ✖ dplyr::arrange() masks plyr::arrange() ## ✖ purrr::compact() masks plyr::compact() ## ✖ dplyr::count() masks plyr::count() ## ✖ dplyr::failwith() masks plyr::failwith() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::id() masks plyr::id() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::mutate() masks plyr::mutate() ## ✖ dplyr::rename() masks plyr::rename() ## ✖ dplyr::summarise() masks plyr::summarise() ## ✖ dplyr::summarize() masks plyr::summarize()  Codebook The second step is a codebook. Arguably, this is the first step because you should create the codebook long before you open R and load your data.\nIn this case, we are going to using some data from the German Socioeconomic Panel Study (GSOEP), which is an ongoing Panel Study in Germany. Note that these data are for teaching purposes only, shared under the license for the Comprehensive SOEP teaching dataset, which I, as a contracted SOEP user, can use for teaching purposes. These data represent select cases from the full data set and should not be used for the purpose of publication. The full data are available for free at https://www.diw.de/en/diw_02.c.222829.en/access_and_ordering.html.\nFor this tutorial, I created the codebook for you, and included what I believe are the core columns you may need. Some of these columns will not be particularly helpful for this dataset. For example, many of you likely work with datasets that have only a single file while others work with datasetsspread across many files. As a result, the “dataset” column of the codebook may only have a single value whereas for others it may have multiple. With longitudinal data it is likely you will have multiple.\nHere are my core columns that are based on the original data:\ndataset: this column indexes the name of the dataset that you will be pulling the data from. This is important because we will use this info later on (see purrr tutorial) to load and clean specific data files. Even if you don’t have multiple data sets, I believe consistency is more important and suggest using this.\n old_name: this column is the name of the variable in the data you are pulling it from. This should be exact. The goal of this column is that it will allow us to select() variables from the original data file and rename them something that is more useful to us. If you have worked with qualtrics (really any data) you know why this is important.\n item_text: this column is the original text that participants saw or a description of the item.\n scale: this column tells you what the scale of the variable is. Is it a numeric variable, a text variable, etc. This is helpful for knowing the plausible range.\n reverse: this column tells you whether items in a scale need to be reverse coded. I recommend coding this as 1 (leave alone) and -1 (reverse) for reasons that will become clear later.\n mini: this column represents the minimum value of scales that are numeric. Leave blank otherwise.\n maxi: this column represents the maximumv alue of scales that are numeric. Leave blank otherwise.\n recode: sometimes, we want to recode variables for analyses (e.g. for categorical variables with many levels where sample sizes for some levels are too small to actually do anything with it). I use this column to note the kind of recoding I’ll do to a variable for transparency.\n  Here are additional columns that will make our lives easier or are applicable to some but not all data sets:\ncategory: broad categories that different variables can be put into. I’m a fan of naming them things like “outcome”, “predictor”, “moderator”, “demographic”, “procedural”, etc. but sometimes use more descriptive labels like “Big 5” to indicate the model from which the measures are derived.\n label: label is basically one level lower than category. So if the category is Big 5, the label would be, or example, “A” for Agreeableness, “SWB” for subjective well-being, etc. This column is most important and useful when you have multiple items in a scales, so I’ll typically leave this blank when something is a standalone variable (e.g. sex, single-item scales, etc.).\n item_name: This is the lowest level and most descriptive variable. It indicates which item in scale something is. So it may be “kind” for Agreebleness or “sex” for the demographic biological sex variable.\n year: for longitudinal data, we have several waves of data and the name of the same item across waves is often different, so it’s important to note to which wave an item belongs. You can do this by noting the wave (e.g. 1, 2, 3), but I prefer the actual year the data were collected (e.g. 2005, 2009, etc.) if that is appropriate. See Lecture #1 on discussion of meaningful time metrics. Note that this differs from that discussion in codebook describes how you collected the data, not necessarily how you want to analyze the data.\n new_name: This is a column that brings together much of the information we’ve already collected. It’s purpose is to be the new name that we will give to the variable that is more useful and descriptive to us. This is a constructed variable that brings together others. I like to make it a combination of “category”, “label”, “item_name”, and year using varying combos of \"_\" and “.” that we can use later with tidyverse functions. I typically construct this variable in Excel using the CONCATENATE() function, but it could also be done in R. The reason I do it in Excel is that it makes it easier for someone who may be reviewing my codebook.\n  There is a seperate discussion to be had on naming conventions for your variables, but the important idea to remember is that names convey important information and we want to use this information later on to make our life easier. By coding these variables using this information AND systematically using different seperators we can accomplish this goal.\nmeta: Some datasets have a meta name, which essentially means a name that variable has across all waves to make it clear which variables are the same. They are not always useful as some data sets have meta names but no great way of extracting variables using them. But they’re still typically useful to include in your codebook regardless.  Below, I’ll load in the codebook we will use for this study, which will include all of the above columns.\n# set the path wd \u0026lt;- \u0026quot;https://github.com/emoriebeck/R-tutorials/blob/master/ALDA/week_1_descriptives\u0026quot; # load the codebook (codebook \u0026lt;- url(sprintf(\u0026quot;%s/codebook.csv?raw=true\u0026quot;, wd)) %\u0026gt;% read_csv(.) %\u0026gt;% mutate(old_name = str_to_lower(old_name))) ## Parsed with column specification: ## cols( ## dataset = col_character(), ## old_name = col_character(), ## item_text = col_character(), ## scale = col_character(), ## category = col_character(), ## label = col_character(), ## item_name = col_character(), ## year = col_double(), ## new_name = col_character(), ## reverse = col_double(), ## mini = col_double(), ## maxi = col_double(), ## recode = col_character() ## ) ## # A tibble: 153 x 13 ## dataset old_name item_text scale category label item_name year new_name ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026lt;NA\u0026gt; persnr Never Ch… \u0026lt;NA\u0026gt; Procedu… \u0026lt;NA\u0026gt; SID 0 Procedu… ## 2 \u0026lt;NA\u0026gt; hhnr househol… \u0026lt;NA\u0026gt; Procedu… \u0026lt;NA\u0026gt; household 0 Procedu… ## 3 ppfad gebjahr Year of … nume… Demogra… \u0026lt;NA\u0026gt; DOB 0 Demogra… ## 4 ppfad sex Sex \u0026quot;\\n1… Demogra… \u0026lt;NA\u0026gt; Sex 0 Demogra… ## 5 vp vp12501 Thorough… \u0026lt;NA\u0026gt; Big 5 C thorough 2005 Big 5__… ## 6 zp zp12001 Thorough… \u0026lt;NA\u0026gt; Big 5 C thorough 2009 Big 5__… ## 7 bdp bdp15101 Thorough… \u0026lt;NA\u0026gt; Big 5 C thorough 2013 Big 5__… ## 8 vp vp12502 Am commu… \u0026lt;NA\u0026gt; Big 5 E communic 2005 Big 5__… ## 9 zp zp12002 Am commu… \u0026lt;NA\u0026gt; Big 5 E communic 2009 Big 5__… ## 10 bdp bdp15102 Am commu… \u0026lt;NA\u0026gt; Big 5 E communic 2013 Big 5__… ## # … with 143 more rows, and 4 more variables: reverse \u0026lt;dbl\u0026gt;, mini \u0026lt;dbl\u0026gt;, ## # maxi \u0026lt;dbl\u0026gt;, recode \u0026lt;chr\u0026gt;  Data First, we need to load in the data. We’re going to use three waves of data from the German Socioeconomic Panel Study, which is a longitudinal study of German households that has been conducted since 1984. We’re going to use more recent data from three waves of personality data collected between 2005 and 2013.\nNote: we will be using the teaching set of the GSOEP data set. I will not be pulling from the raw files as a result of this. I will also not be mirroring the format that you would usually load the GSOEP from because that is slightly more complicated and somethng we will return to in a later tutorial after we have more skills. I’ve left that code for now, but it won’t make a lot of sense right now.\npath \u0026lt;- \u0026quot;~/Box/network/other projects/PCLE Replication/data/sav_files\u0026quot; ref \u0026lt;- sprintf(\u0026quot;%s/cirdef.sav\u0026quot;, path) %\u0026gt;% haven::read_sav(.) %\u0026gt;% select(hhnr, rgroup20) read_fun \u0026lt;- function(Year){ vars \u0026lt;- (codebook %\u0026gt;% filter(year == Year | year == 0))$old_name set \u0026lt;- (codebook %\u0026gt;% filter(year == Year))$dataset[1] sprintf(\u0026quot;%s/%s.sav\u0026quot;, path, set) %\u0026gt;% haven::read_sav(.) %\u0026gt;% full_join(ref) %\u0026gt;% filter(rgroup20 \u0026gt; 10) %\u0026gt;% select(one_of(vars)) %\u0026gt;% gather(key = item, value = value, -persnr, -hhnr, na.rm = T) } vars \u0026lt;- (codebook %\u0026gt;% filter(year == 0))$old_name dem \u0026lt;- sprintf(\u0026quot;%s/ppfad.sav\u0026quot;, path) %\u0026gt;% haven::read_sav(.) %\u0026gt;% select(vars) tibble(year = c(2005:2015)) %\u0026gt;% mutate(data = map(year, read_fun)) %\u0026gt;% select(-year) %\u0026gt;% unnest(data) %\u0026gt;% distinct() %\u0026gt;% filter(!is.na(value)) %\u0026gt;% spread(key = item, value = value) %\u0026gt;% left_join(dem) %\u0026gt;% write.csv(., file = \u0026quot;~/Documents/Github/R-tutorials/ALDA/week_1_descriptives/data/wdeek_1_data.csv\u0026quot;, row.names = F) This code below shows how I would read in and rename a wide-format data set using the codebook I created.\nold.names \u0026lt;- codebook$old_name # get old column names new.names \u0026lt;- codebook$new_name # get new column names (soep \u0026lt;- url(sprintf(\u0026quot;%s/data/week_1_data.csv?raw=true\u0026quot;, wd)) %\u0026gt;% # path to data read_csv(.) %\u0026gt;% # read in data select(old.names) %\u0026gt;% # select the columns from our codebook setNames(new.names)) # rename columns with our new names ## Parsed with column specification: ## cols( ## .default = col_double() ## ) ## See spec(...) for full column specifications. ## # A tibble: 28,290 x 153 ## Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 901 94 1951 2 ## 2 1202 124 1913 2 ## 3 2301 230 1946 1 ## 4 2302 230 1946 2 ## 5 2304 230 1978 1 ## 6 2305 230 1946 2 ## 7 4601 469 1933 2 ## 8 4701 477 1919 2 ## 9 4901 493 1925 2 ## 10 5201 523 1955 1 ## # … with 28,280 more rows, and 149 more variables: `Big ## # 5__C_thorough.2005` \u0026lt;dbl\u0026gt;, `Big 5__C_thorough.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_thorough.2013` \u0026lt;dbl\u0026gt;, `Big 5__E_communic.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__E_communic.2009` \u0026lt;dbl\u0026gt;, `Big 5__E_communic.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_coarse.2005` \u0026lt;dbl\u0026gt;, `Big 5__A_coarse.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_coarse.2013` \u0026lt;dbl\u0026gt;, `Big 5__O_original.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__O_original.2009` \u0026lt;dbl\u0026gt;, `Big 5__O_original.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_worry.2005` \u0026lt;dbl\u0026gt;, `Big 5__N_worry.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_worry.2013` \u0026lt;dbl\u0026gt;, `Big 5__A_forgive.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_forgive.2009` \u0026lt;dbl\u0026gt;, `Big 5__A_forgive.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_lazy.2005` \u0026lt;dbl\u0026gt;, `Big 5__C_lazy.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_lazy.2013` \u0026lt;dbl\u0026gt;, `Big 5__E_sociable.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__E_sociable.2009` \u0026lt;dbl\u0026gt;, `Big 5__E_sociable.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__O_artistic.2005` \u0026lt;dbl\u0026gt;, `Big 5__O_artistic.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__O_artistic.2013` \u0026lt;dbl\u0026gt;, `Big 5__N_nervous.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_nervous.2009` \u0026lt;dbl\u0026gt;, `Big 5__N_nervous.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_efficient.2005` \u0026lt;dbl\u0026gt;, `Big 5__C_efficient.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_efficient.2013` \u0026lt;dbl\u0026gt;, `Big 5__E_reserved.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__E_reserved.2009` \u0026lt;dbl\u0026gt;, `Big 5__E_reserved.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_friendly.2005` \u0026lt;dbl\u0026gt;, `Big 5__A_friendly.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_friendly.2013` \u0026lt;dbl\u0026gt;, `Big 5__O_imagin.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__O_imagin.2009` \u0026lt;dbl\u0026gt;, `Big 5__O_imagin.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_dealStress.2005` \u0026lt;dbl\u0026gt;, `Big 5__N_dealStress.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_dealStress.2013` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2005` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2006` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2007` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2008` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2009` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2010` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2011` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2012` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2013` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2014` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2015` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldMvOut.2005` \u0026lt;dbl\u0026gt;, `Life Event__ChldMvOut.2006` \u0026lt;dbl\u0026gt;, ## # `Life Event__ChldMvOut.2007` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldMvOut.2008` \u0026lt;dbl\u0026gt;, `Life Event__ChldMvOut.2009` \u0026lt;dbl\u0026gt;, ## # `Life Event__ChldMvOut.2010` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldMvOut.2011` \u0026lt;dbl\u0026gt;, `Life Event__ChldMvOut.2012` \u0026lt;dbl\u0026gt;, ## # `Life Event__ChldMvOut.2013` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldMvOut.2014` \u0026lt;dbl\u0026gt;, `Life Event__ChldMvOut.2015` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2005` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2006` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2007` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2008` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2009` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2010` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2011` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2012` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2013` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2014` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2015` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2005` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2006` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2007` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2008` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2009` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2010` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2011` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2012` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2013` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2014` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2015` \u0026lt;dbl\u0026gt;, ## # `Life Event__NewPart.2011` \u0026lt;dbl\u0026gt;, `Life Event__NewPart.2012` \u0026lt;dbl\u0026gt;, ## # `Life Event__NewPart.2013` \u0026lt;dbl\u0026gt;, `Life Event__NewPart.2014` \u0026lt;dbl\u0026gt;, ## # `Life Event__NewPart.2015` \u0026lt;dbl\u0026gt;, `Life Event__Married.2005` \u0026lt;dbl\u0026gt;, ## # `Life Event__Married.2006` \u0026lt;dbl\u0026gt;, `Life Event__Married.2007` \u0026lt;dbl\u0026gt;, ## # `Life Event__Married.2008` \u0026lt;dbl\u0026gt;, `Life Event__Married.2009` \u0026lt;dbl\u0026gt;, ## # `Life Event__Married.2010` \u0026lt;dbl\u0026gt;, …   Clean Data Recode Variables Many of the data we work with have observations that are missing for a variety of reasons. In R, we treat missing values as NA, but many other programs from which you may be importing your data may use other codes (e.g. 999, -999, etc.). Large panel studies tend to use small negative values to indicate different types of missingness. This is why it is important to note down the scale in your codebook. That way you can check which values may need to be recoded to explicit NA values.\nIn the GSOEP, -1 to -7 indicate various types of missing values, so we will recode these to NA. To do this, we will use one of my favorite functions, mapvalues(), from the plyr package. In later tutorials where we read in and manipulate more complex data sets, we will use mapvalues() a lot. Basically, mapvalues takes 4 key arguments: (1) the variable you are recoding, (2) a vector of initial values from which you want to (3) recode your variable to using a vector of new values in the same order as the old values, and (4) a way to turn off warnings if some levels are not in your data (warn_missing = F).\n(soep \u0026lt;- soep %\u0026gt;% mutate_all(~as.numeric(mapvalues(., from = seq(-1,-7, -1), # recode negative to = rep(NA, 7), warn_missing = F)))) # values to NA ## # A tibble: 28,290 x 153 ## Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 901 94 1951 2 ## 2 1202 124 1913 2 ## 3 2301 230 1946 1 ## 4 2302 230 1946 2 ## 5 2304 230 1978 1 ## 6 2305 230 1946 2 ## 7 4601 469 1933 2 ## 8 4701 477 1919 2 ## 9 4901 493 1925 2 ## 10 5201 523 1955 1 ## # … with 28,280 more rows, and 149 more variables: `Big ## # 5__C_thorough.2005` \u0026lt;dbl\u0026gt;, `Big 5__C_thorough.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_thorough.2013` \u0026lt;dbl\u0026gt;, `Big 5__E_communic.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__E_communic.2009` \u0026lt;dbl\u0026gt;, `Big 5__E_communic.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_coarse.2005` \u0026lt;dbl\u0026gt;, `Big 5__A_coarse.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_coarse.2013` \u0026lt;dbl\u0026gt;, `Big 5__O_original.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__O_original.2009` \u0026lt;dbl\u0026gt;, `Big 5__O_original.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_worry.2005` \u0026lt;dbl\u0026gt;, `Big 5__N_worry.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_worry.2013` \u0026lt;dbl\u0026gt;, `Big 5__A_forgive.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_forgive.2009` \u0026lt;dbl\u0026gt;, `Big 5__A_forgive.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_lazy.2005` \u0026lt;dbl\u0026gt;, `Big 5__C_lazy.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_lazy.2013` \u0026lt;dbl\u0026gt;, `Big 5__E_sociable.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__E_sociable.2009` \u0026lt;dbl\u0026gt;, `Big 5__E_sociable.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__O_artistic.2005` \u0026lt;dbl\u0026gt;, `Big 5__O_artistic.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__O_artistic.2013` \u0026lt;dbl\u0026gt;, `Big 5__N_nervous.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_nervous.2009` \u0026lt;dbl\u0026gt;, `Big 5__N_nervous.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_efficient.2005` \u0026lt;dbl\u0026gt;, `Big 5__C_efficient.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__C_efficient.2013` \u0026lt;dbl\u0026gt;, `Big 5__E_reserved.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__E_reserved.2009` \u0026lt;dbl\u0026gt;, `Big 5__E_reserved.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_friendly.2005` \u0026lt;dbl\u0026gt;, `Big 5__A_friendly.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__A_friendly.2013` \u0026lt;dbl\u0026gt;, `Big 5__O_imagin.2005` \u0026lt;dbl\u0026gt;, `Big ## # 5__O_imagin.2009` \u0026lt;dbl\u0026gt;, `Big 5__O_imagin.2013` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_dealStress.2005` \u0026lt;dbl\u0026gt;, `Big 5__N_dealStress.2009` \u0026lt;dbl\u0026gt;, `Big ## # 5__N_dealStress.2013` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2005` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2006` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2007` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2008` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2009` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2010` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2011` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2012` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2013` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldBrth.2014` \u0026lt;dbl\u0026gt;, `Life Event__ChldBrth.2015` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldMvOut.2005` \u0026lt;dbl\u0026gt;, `Life Event__ChldMvOut.2006` \u0026lt;dbl\u0026gt;, ## # `Life Event__ChldMvOut.2007` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldMvOut.2008` \u0026lt;dbl\u0026gt;, `Life Event__ChldMvOut.2009` \u0026lt;dbl\u0026gt;, ## # `Life Event__ChldMvOut.2010` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldMvOut.2011` \u0026lt;dbl\u0026gt;, `Life Event__ChldMvOut.2012` \u0026lt;dbl\u0026gt;, ## # `Life Event__ChldMvOut.2013` \u0026lt;dbl\u0026gt;, `Life ## # Event__ChldMvOut.2014` \u0026lt;dbl\u0026gt;, `Life Event__ChldMvOut.2015` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2005` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2006` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2007` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2008` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2009` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2010` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2011` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2012` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2013` \u0026lt;dbl\u0026gt;, `Life Event__Divorce.2014` \u0026lt;dbl\u0026gt;, ## # `Life Event__Divorce.2015` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2005` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2006` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2007` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2008` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2009` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2010` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2011` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2012` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2013` \u0026lt;dbl\u0026gt;, ## # `Life Event__DadDied.2014` \u0026lt;dbl\u0026gt;, `Life Event__DadDied.2015` \u0026lt;dbl\u0026gt;, ## # `Life Event__NewPart.2011` \u0026lt;dbl\u0026gt;, `Life Event__NewPart.2012` \u0026lt;dbl\u0026gt;, ## # `Life Event__NewPart.2013` \u0026lt;dbl\u0026gt;, `Life Event__NewPart.2014` \u0026lt;dbl\u0026gt;, ## # `Life Event__NewPart.2015` \u0026lt;dbl\u0026gt;, `Life Event__Married.2005` \u0026lt;dbl\u0026gt;, ## # `Life Event__Married.2006` \u0026lt;dbl\u0026gt;, `Life Event__Married.2007` \u0026lt;dbl\u0026gt;, ## # `Life Event__Married.2008` \u0026lt;dbl\u0026gt;, `Life Event__Married.2009` \u0026lt;dbl\u0026gt;, ## # `Life Event__Married.2010` \u0026lt;dbl\u0026gt;, …  Reverse-Scoring Many scales we use have items that are positively or negatively keyed. High ratings on positively keyed items are indicative of being high on a construct. In contrast, high ratings on negatively keyed items are indicative of being low on a construct. Thus, to create the composite scores of constructs we often use, we must first “reverse” the negatively keyed items so that high scores indicate being higher on the construct.\nThere are a few ways to do this in R. Below, I’ll demonstrate how to do so using the reverse.code() function in the psych package in R. This function was built to make reverse coding more efficient (i.e. please don’t run every item that needs to be recoded with separate lines of code!!).\nBefore we can do that, though, we need to restructure the data a bit in order to bring in the reverse coding information from our codebook. We will talk more about what’s happening here in later tutorials on tidyr, so for now, just bear with me.\n(soep_long \u0026lt;- soep %\u0026gt;% gather(key = item, value = value, -contains(\u0026quot;Procedural\u0026quot;), # change to long format -contains(\u0026quot;Demographic\u0026quot;), na.rm = T) %\u0026gt;% left_join(codebook %\u0026gt;% select(item = new_name, reverse, mini, maxi)) %\u0026gt;% # bring in codebook separate(item, c(\u0026quot;type\u0026quot;, \u0026quot;item\u0026quot;), sep = \u0026quot;__\u0026quot;) %\u0026gt;% # separate category separate(item, c(\u0026quot;item\u0026quot;, \u0026quot;year\u0026quot;), sep = \u0026quot;[.]\u0026quot;) %\u0026gt;% # seprate year separate(item, c(\u0026quot;item\u0026quot;, \u0026quot;scrap\u0026quot;), sep = \u0026quot;_\u0026quot;) %\u0026gt;% # separate scale and item mutate(value = as.numeric(value), # change to numeric value = ifelse(reverse == -1, reverse.code(-1, value, mini = mini, maxi = maxi), value))) ## Joining, by = \u0026quot;item\u0026quot; ## Warning: Expected 2 pieces. Missing pieces filled with `NA` in 19618 rows ## [452105, 452106, 452107, 452108, 452109, 452110, 452111, 452112, 452113, ## 452114, 452115, 452116, 452117, 452118, 452119, 452120, 452121, 452122, ## 452123, 452124, ...]. ## # A tibble: 471,722 x 12 ## Procedural__SID Procedural__hou… Demographic__DOB Demographic__Sex type ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 901 94 1951 2 Big 5 ## 2 1202 124 1913 2 Big 5 ## 3 2301 230 1946 1 Big 5 ## 4 2302 230 1946 2 Big 5 ## 5 2304 230 1978 1 Big 5 ## 6 4601 469 1933 2 Big 5 ## 7 4701 477 1919 2 Big 5 ## 8 4901 493 1925 2 Big 5 ## 9 5201 523 1955 1 Big 5 ## 10 5202 523 1956 2 Big 5 ## # … with 471,712 more rows, and 7 more variables: item \u0026lt;chr\u0026gt;, scrap \u0026lt;chr\u0026gt;, ## # year \u0026lt;chr\u0026gt;, value \u0026lt;dbl\u0026gt;, reverse \u0026lt;dbl\u0026gt;, mini \u0026lt;dbl\u0026gt;, maxi \u0026lt;dbl\u0026gt;  Create Composites Now that we have reverse coded our items, we can create composites.\nBFI-S We’ll start with our scale – in this case, the Big 5 from the German translation of the BFI-S.\nHere’s the simplest way, which is also the long way because you’d have to do it for each scale in each year, which I don’t recommend.\nsoep$C.2005 \u0026lt;- with(soep, rowMeans(cbind(`Big 5__C_thorough.2005`, `Big 5__C_lazy.2005`, `Big 5__C_efficient.2005`), na.rm = T))  But personally, I don’t have a desire to do that 15 times (5 traits times 3 waves), so we can use our codebook and dplyr to make our lives a whole lot easier. In general, trying to run everything simultanously saves from copy-paste errors, makes your code more readable, and reduces the total amount of code. So while the below code may not make intuiative sense immediately, it is nonetheless what we are working towards.\nsoep \u0026lt;- soep %\u0026gt;% select(-C.2005) # get rid of added column (b5_soep_long \u0026lt;- soep_long %\u0026gt;% filter(type == \u0026quot;Big 5\u0026quot;) %\u0026gt;% # keep Big 5 variables group_by(Procedural__SID, item, year) %\u0026gt;% # group by person, construct, \u0026amp; year summarize(value = mean(value, na.rm = T)) %\u0026gt;% # calculate means ungroup() %\u0026gt;% # ungroup left_join(soep_long %\u0026gt;% # bring demographic info back in select(Procedural__SID, DOB = Demographic__DOB, Sex = Demographic__Sex) %\u0026gt;% distinct())) ## Joining, by = \u0026quot;Procedural__SID\u0026quot; ## # A tibble: 151,186 x 6 ## Procedural__SID item year value DOB Sex ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 901 A 2005 5 1951 2 ## 2 901 A 2009 5.33 1951 2 ## 3 901 A 2013 5 1951 2 ## 4 901 C 2005 5.33 1951 2 ## 5 901 C 2009 5.5 1951 2 ## 6 901 C 2013 6 1951 2 ## 7 901 E 2005 4 1951 2 ## 8 901 E 2009 4 1951 2 ## 9 901 E 2013 4 1951 2 ## 10 901 N 2005 4 1951 2 ## # … with 151,176 more rows  Life Events We also want to create a variable that indexes whether our participants experienced any of the life events during the years of interest (2005-2015).\n(events_long \u0026lt;- soep_long %\u0026gt;% filter(type == \u0026quot;Life Event\u0026quot;) %\u0026gt;% # keep only life events group_by(Procedural__SID, item) %\u0026gt;% # group by person and event summarize(value = sum(value, na.rm = T), # sum up whether they experiened the event at all value = ifelse(value \u0026gt; 1, 1, 0))) # if more than once 1, otherwise 0 ## # A tibble: 15,061 x 3 ## # Groups: Procedural__SID [10,019] ## Procedural__SID item value ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 901 MomDied 1 ## 2 2301 MoveIn 0 ## 3 2301 PartDied 1 ## 4 2305 MoveIn 0 ## 5 4601 PartDied 0 ## 6 5201 ChldMvOut 1 ## 7 5201 DadDied 0 ## 8 5202 ChldMvOut 1 ## 9 5203 MoveIn 1 ## 10 5303 MomDied 0 ## # … with 15,051 more rows    Descriptives Descriptives of your data are incredibly important. They are your first line of defense against things that could go wrong later on when you run inferential stats. They help you check the distribution of your variables (e.g. non-normally distributed), look for implausible values made through coding or participant error, and allow you to anticipate what your findings will look like.\nThere are lots of ways to create great tables of descriptives. My favorite way is using dplyr, but we will save that for a later lesson on creating great APA style tables in R. For now, we’ll use a wonderfully helpful function from the psych package called describe() in conjunction with a small amount of tidyr to reshape the data.\nb5_soep_long %\u0026gt;% unite(tmp, item, year, sep = \u0026quot;_\u0026quot;) %\u0026gt;% # make new column that joins item and year spread(tmp, value) %\u0026gt;% # make wide because that helps describe describe(.) # call describe ## vars n mean sd median trimmed ## Procedural__SID 1 16719 8321022.91 10677731.19 3105002.00 6487615.30 ## DOB 2 16719 1960.03 18.48 1960.00 1960.22 ## Sex 3 16719 1.52 0.50 2.00 1.53 ## A_2005 4 10419 5.79 0.98 6.00 5.83 ## A_2009 5 10294 5.68 0.99 5.67 5.72 ## A_2013 6 9535 5.74 0.96 5.67 5.78 ## C_2005 7 10412 6.23 0.96 6.33 6.34 ## C_2009 8 10290 6.16 0.95 6.33 6.25 ## C_2013 9 9530 6.17 0.91 6.33 6.25 ## E_2005 10 10416 5.15 1.15 5.33 5.18 ## E_2009 11 10291 5.11 1.15 5.00 5.13 ## E_2013 12 9533 5.20 1.11 5.33 5.24 ## N_2005 13 10413 4.71 1.23 4.67 4.72 ## N_2009 14 10294 4.84 1.22 5.00 4.87 ## N_2013 15 9534 4.92 1.21 5.00 4.93 ## O_2005 16 10408 4.51 1.22 4.67 4.53 ## O_2009 17 10287 4.40 1.22 4.33 4.41 ## O_2013 18 9530 4.60 1.18 4.67 4.62 ## mad min max range skew kurtosis ## Procedural__SID 3735541.17 901.00 3.5022e+07 35021101.00 1.50 0.56 ## DOB 20.76 1909.00 1.9950e+03 86.00 -0.08 -0.84 ## Sex 0.00 1.00 2.0000e+00 1.00 -0.10 -1.99 ## A_2005 0.99 1.00 7.5000e+00 6.50 -0.40 -0.14 ## A_2009 0.99 1.33 8.0000e+00 6.67 -0.36 -0.19 ## A_2013 0.99 1.33 8.0000e+00 6.67 -0.43 0.05 ## C_2005 0.99 1.00 8.0000e+00 7.00 -0.95 0.84 ## C_2009 0.99 1.00 8.0000e+00 7.00 -0.82 0.49 ## C_2013 0.99 1.33 8.0000e+00 6.67 -0.72 0.17 ## E_2005 0.99 1.00 7.5000e+00 6.50 -0.27 -0.16 ## E_2009 0.99 1.00 7.3300e+00 6.33 -0.23 -0.18 ## E_2013 0.99 1.33 7.3300e+00 6.00 -0.29 -0.18 ## N_2005 1.48 1.50 8.0000e+00 6.50 -0.07 -0.32 ## N_2009 0.99 1.50 7.6700e+00 6.17 -0.17 -0.30 ## N_2013 1.48 1.50 8.0000e+00 6.50 -0.15 -0.30 ## O_2005 1.48 1.00 7.0000e+00 6.00 -0.23 -0.18 ## O_2009 1.48 1.00 7.0000e+00 6.00 -0.10 -0.30 ## O_2013 0.99 1.00 7.0000e+00 6.00 -0.21 -0.20 ## se ## Procedural__SID 82579.80 ## DOB 0.14 ## Sex 0.00 ## A_2005 0.01 ## A_2009 0.01 ## A_2013 0.01 ## C_2005 0.01 ## C_2009 0.01 ## C_2013 0.01 ## E_2005 0.01 ## E_2009 0.01 ## E_2013 0.01 ## N_2005 0.01 ## N_2009 0.01 ## N_2013 0.01 ## O_2005 0.01 ## O_2009 0.01 ## O_2013 0.01 For count variables, like life events, we need to use something slightly different. We’re typically more interested in counts – in this case, how many people experienced each life event in the 10 years we’re considering?\nTo do this, we’ll use a little bit of dplyr rather than the base R function table() that is often used for count data. Instead, we’ll use a combination of group_by() and n() to get the counts by group. In the end, we’re left with a nice little table of counts.\nevents_long %\u0026gt;% group_by(item, value) %\u0026gt;% summarize(N = n()) %\u0026gt;% ungroup() %\u0026gt;% spread(value, N) ## # A tibble: 10 x 3 ## item `0` `1` ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; ## 1 ChldBrth 1600 735 ## 2 ChldMvOut 1555 830 ## 3 DadDied 953 213 ## 4 Divorce 414 122 ## 5 Married 1646 331 ## 6 MomDied 929 219 ## 7 MoveIn 1403 419 ## 8 NewPart 1207 420 ## 9 PartDied 402 76 ## 10 SepPart 1172 415 Scale Reliability When we work with scales, it’s often a good idea to check the internal consistency of your scale. If the scale isn’t performing how it should be, that could critically impact the inferences you make from your data.\nTo check the internal consistency of our Big 5 scales, we will use the alpha() function from the psych package, which will give us Cronbach’s as well as a number of other indicators of internal consistency.\nHere’s the way you may have seen / done this in the past.\nalpha.C.2005 \u0026lt;- with(soep, psych::alpha(x = cbind(`Big 5__C_thorough.2005`, `Big 5__C_lazy.2005`, `Big 5__C_efficient.2005`))) ## Warning in psych::alpha(x = cbind(`Big 5__C_thorough.2005`, `Big 5__C_lazy.2005`, : Some items were negatively correlated with the total scale and probably ## should be reversed. ## To do this, run the function again with the \u0026#39;check.keys=TRUE\u0026#39; option ## Some items ( Big 5__C_lazy.2005 ) were negatively correlated with the total scale and ## probably should be reversed. ## To do this, run the function again with the \u0026#39;check.keys=TRUE\u0026#39; option But again, doing this 15 times would be quite a pain and would open you up to the possibility of a lot of copy and paste errors.\nSo instead, to do this, I’m going to use a mix of the tidyverse. At first glance, it may seem complex but as you move through other tutorials (particularly the purrr tutorial), it will begin to make much more sense.\n# short function to reshape data and run alpha alpha_fun \u0026lt;- function(df){ df %\u0026gt;% spread(scrap,value) %\u0026gt;% psych::alpha(.) } (alphas \u0026lt;- soep_long %\u0026gt;% filter(type == \u0026quot;Big 5\u0026quot;) %\u0026gt;% # filter out Big 5 select(Procedural__SID, item:value) %\u0026gt;% # get rid of extra columns group_by(item, year) %\u0026gt;% # group by construct and year nest() %\u0026gt;% # nest the data mutate(alpha_res = map(data, alpha_fun), # run alpha alpha = map(alpha_res, ~.$total[2])) %\u0026gt;% # get the alpha value unnest(alpha)) # pull it out of the list column ## # A tibble: 15 x 5 ## # Groups: item, year [15] ## item year data alpha_res std.alpha ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026lt;df[,3]\u0026gt;\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt; ## 1 C 2005 [31,117 × 3] \u0026lt;psych\u0026gt; 0.515 ## 2 C 2009 [30,728 × 3] \u0026lt;psych\u0026gt; 0.494 ## 3 C 2013 [28,496 × 3] \u0026lt;psych\u0026gt; 0.482 ## 4 E 2005 [31,188 × 3] \u0026lt;psych\u0026gt; 0.540 ## 5 E 2009 [30,770 × 3] \u0026lt;psych\u0026gt; 0.530 ## 6 E 2013 [28,532 × 3] \u0026lt;psych\u0026gt; 0.544 ## 7 A 2005 [31,184 × 3] \u0026lt;psych\u0026gt; 0.418 ## 8 A 2009 [30,796 × 3] \u0026lt;psych\u0026gt; 0.410 ## 9 A 2013 [28,529 × 3] \u0026lt;psych\u0026gt; 0.401 ## 10 O 2005 [31,091 × 3] \u0026lt;psych\u0026gt; 0.527 ## 11 O 2009 [30,722 × 3] \u0026lt;psych\u0026gt; 0.510 ## 12 O 2013 [28,451 × 3] \u0026lt;psych\u0026gt; 0.497 ## 13 N 2005 [31,162 × 3] \u0026lt;psych\u0026gt; 0.473 ## 14 N 2009 [30,802 × 3] \u0026lt;psych\u0026gt; 0.490 ## 15 N 2013 [28,536 × 3] \u0026lt;psych\u0026gt; 0.480  Zero-Order Correlations Finally, we often want to look at the zero-order correlation among study variables to make sure they are performing as we think they should.\nTo run the correlations, we will need to have our data in wide format, so we’re going to do a little bit of reshaping before we do.\nb5_soep_long %\u0026gt;% unite(tmp, item, year, sep = \u0026quot;_\u0026quot;) %\u0026gt;% spread(key = tmp, value = value) %\u0026gt;% select(-Procedural__SID) %\u0026gt;% cor(., use = \u0026quot;pairwise\u0026quot;) %\u0026gt;% round(., 2) ## DOB Sex A_2005 A_2009 A_2013 C_2005 C_2009 C_2013 E_2005 E_2009 ## DOB 1.00 0.00 -0.08 -0.07 -0.06 -0.13 -0.12 -0.14 0.10 0.12 ## Sex 0.00 1.00 0.18 0.17 0.18 0.05 0.07 0.09 0.08 0.08 ## A_2005 -0.08 0.18 1.00 0.50 0.50 0.32 0.20 0.19 0.10 0.06 ## A_2009 -0.07 0.17 0.50 1.00 0.55 0.19 0.28 0.18 0.05 0.08 ## A_2013 -0.06 0.18 0.50 0.55 1.00 0.18 0.19 0.29 0.04 0.06 ## C_2005 -0.13 0.05 0.32 0.19 0.18 1.00 0.52 0.48 0.19 0.10 ## C_2009 -0.12 0.07 0.20 0.28 0.19 0.52 1.00 0.55 0.12 0.16 ## C_2013 -0.14 0.09 0.19 0.18 0.29 0.48 0.55 1.00 0.13 0.14 ## E_2005 0.10 0.08 0.10 0.05 0.04 0.19 0.12 0.13 1.00 0.61 ## E_2009 0.12 0.08 0.06 0.08 0.06 0.10 0.16 0.14 0.61 1.00 ## E_2013 0.10 0.11 0.04 0.04 0.07 0.10 0.10 0.18 0.59 0.65 ## N_2005 0.06 -0.18 0.10 0.06 0.02 0.09 0.06 0.03 0.18 0.10 ## N_2009 0.03 -0.22 0.07 0.09 0.03 0.06 0.08 0.05 0.13 0.16 ## N_2013 0.02 -0.21 0.06 0.06 0.10 0.04 0.06 0.08 0.10 0.10 ## O_2005 0.11 0.06 0.12 0.09 0.07 0.17 0.12 0.08 0.40 0.29 ## O_2009 0.10 0.05 0.05 0.11 0.07 0.06 0.13 0.08 0.26 0.36 ## O_2013 0.05 0.07 0.08 0.09 0.13 0.07 0.08 0.15 0.24 0.28 ## E_2013 N_2005 N_2009 N_2013 O_2005 O_2009 O_2013 ## DOB 0.10 0.06 0.03 0.02 0.11 0.10 0.05 ## Sex 0.11 -0.18 -0.22 -0.21 0.06 0.05 0.07 ## A_2005 0.04 0.10 0.07 0.06 0.12 0.05 0.08 ## A_2009 0.04 0.06 0.09 0.06 0.09 0.11 0.09 ## A_2013 0.07 0.02 0.03 0.10 0.07 0.07 0.13 ## C_2005 0.10 0.09 0.06 0.04 0.17 0.06 0.07 ## C_2009 0.10 0.06 0.08 0.06 0.12 0.13 0.08 ## C_2013 0.18 0.03 0.05 0.08 0.08 0.08 0.15 ## E_2005 0.59 0.18 0.13 0.10 0.40 0.26 0.24 ## E_2009 0.65 0.10 0.16 0.10 0.29 0.36 0.28 ## E_2013 1.00 0.11 0.13 0.15 0.26 0.28 0.35 ## N_2005 0.11 1.00 0.55 0.53 0.09 0.08 0.06 ## N_2009 0.13 0.55 1.00 0.60 0.06 0.07 0.07 ## N_2013 0.15 0.53 0.60 1.00 0.05 0.05 0.05 ## O_2005 0.26 0.09 0.06 0.05 1.00 0.58 0.55 ## O_2009 0.28 0.08 0.07 0.05 0.58 1.00 0.61 ## O_2013 0.35 0.06 0.07 0.05 0.55 0.61 1.00 This is a lot of values and a little hard to make sense of, so as a bonus, I’m going to give you a little bit of more complex code that makes this more readable (and publishable ).\nr \u0026lt;- b5_soep_long %\u0026gt;% unite(tmp, item, year, sep = \u0026quot;_\u0026quot;) %\u0026gt;% spread(key = tmp, value = value) %\u0026gt;% select(-Procedural__SID, -DOB, -Sex) %\u0026gt;% cor(., use = \u0026quot;pairwise\u0026quot;) r[upper.tri(r, diag = T)] \u0026lt;- NA diag(r) \u0026lt;- (alphas %\u0026gt;% arrange(item, year))$std.alpha r %\u0026gt;% data.frame %\u0026gt;% rownames_to_column(\u0026quot;V1\u0026quot;) %\u0026gt;% gather(key = V2, value = r, na.rm = T, -V1) %\u0026gt;% separate(V1, c(\u0026quot;T1\u0026quot;, \u0026quot;Year1\u0026quot;), sep = \u0026quot;_\u0026quot;) %\u0026gt;% separate(V2, c(\u0026quot;T2\u0026quot;, \u0026quot;Year2\u0026quot;), sep = \u0026quot;_\u0026quot;) %\u0026gt;% mutate_at(vars(Year1), ~factor(., levels = c(2013, 2009, 2005))) %\u0026gt;% ggplot(aes(x = Year2, y = Year1, fill = r)) + geom_raster() + scale_fill_gradient2(low = \u0026quot;blue\u0026quot;, high = \u0026quot;red\u0026quot;, mid = \u0026quot;white\u0026quot;, midpoint = 0, limit = c(-1,1), space = \u0026quot;Lab\u0026quot;, name=\u0026quot;Correlations\u0026quot;) + geom_text(aes(label = round(r,2))) + facet_grid(T1 ~ T2) + theme_classic()  Figure 1: Correlations among Personality Indicators. Values on the diagonal represent Chronbach’s alpha for each scale in each year. Within-trait correlations represent test-retest correlations.    ","date":1565740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565820252,"objectID":"6db01486028d36148b869d8ed7e85679","permalink":"/workshops/workshop-1/","publishdate":"2019-08-14T00:00:00Z","relpermalink":"/workshops/workshop-1/","section":"Workshops","summary":"Data and workflow","tags":["workshop"],"title":"Workshop Week 1","type":"post"},{"authors":null,"categories":null,"content":" Instructor: Joshua Jackson\nOffice: 315B\nOffice hours: 1-2 Wednesday and by appointment\nAI: Emorie Beck Office hours: Wednesday 2-3\nCourse Description\nThis course covers modern methods of handling longitudinal, repeated measures. The class will introduce the rationale of measuring change and stability over time to study phenomena, as well as how within-person designs can increase statistical power and precision compared to more traditional designs. Most the course will use multi-level models and latent (growth) curve models to specify patterns of change across time. Additional topics include: visualization, measurement invariance, time-to- event models and power. PREREQ: Use of R will be required, Familiarity with MLM and/or Structural Equation Models.\nClass textbook\nReadings will be provided\nStructure of class\nEach class will cover a specific type of longitudinal model. During that class, I will provide an overview of the important considerations or motivation for this analysis. This will take the first half of the class or a little longer. The final hour of class will be devoted to walking through code and results. We’ll call this the workshop portion.\nGrading Grading consists of 3 aspects: 1. Weekly “pop” quiz (36% of grade - 12 @ 3% each). Quizes consist of 1-3 questions based on the previous lecture and reading.\nHomework take homes (64% of grade - 8 @ 8% each). Homeworks will be presented in the workshop portion of the class and due 1 week later.  Schedule\n  Week Date Topic Workshop Homework    1 8/29 Motivation, terms, concepts, and graphing Data Descriptives   2 9/5 Growth curve basics; MLM in R: packages and procedures tidyr Hw#1  3 9/12 Conditional (Level 1 and 2 predictors) MLM models purrr Hw#2  4 9/19 Polynomial, piecewise and spline models plotting redux Hw#3  5 9/26 Three level and group models brms Hw#4  6 10/03 Intensive data analysis/within person fluctuations p1 brms   7 10/10 Intensive data analysis/within person fluctuations p2 tidybayes Hw#5  8 10/17 SEM and simple path models lavaan   9 10/24 Latent Grown (curve) Models amelia Hw#6  10 10/31 MI and Second order Model semPlot   11 11/7 Multiple group models lavaan Hw#7  12 11/14 Class canceled    13 11/21 Flexible SEM models (LCM, STATE-TRAIT; ALT-SR)  Hw#8  14 11/28 Tofurkey day    15 12/5 Mixture Models      Other topics: Longitudinal mediation (and multilevel mediation), two wave data, experimental approaches\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4c565ba0724123eb589419cad7e99082","permalink":"/syllabus/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/syllabus/","section":"","summary":"Instructor: Joshua Jackson\nOffice: 315B\nOffice hours: 1-2 Wednesday and by appointment\nAI: Emorie Beck Office hours: Wednesday 2-3\nCourse Description\nThis course covers modern methods of handling longitudinal, repeated measures. The class will introduce the rationale of measuring change and stability over time to study phenomena, as well as how within-person designs can increase statistical power and precision compared to more traditional designs. Most the course will use multi-level models and latent (growth) curve models to specify patterns of change across time.","tags":null,"title":"Syllabus","type":"page"}]