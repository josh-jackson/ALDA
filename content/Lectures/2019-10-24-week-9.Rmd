---
title: 'Week 9 & 10'
author: ''
date: '2019-10-24'
slug: week-9
categories: []
tags: []
subtitle: ''
summary: 'growth models'
authors: []
lastmod: '2019-10-24T13:41:23-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup}

knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

```


# Types of longitudinal models other than growth models 

With SEM there are many different types of longitudinal models you can run. The basic ones are: 

1. Longitudinal CFA

2. Panel/cross lag/longitudinal path model/mediation

3. Growth models

4. Latent change/differene score models

5. Mixture or class based longitudinal models


Within these you can do multiple groups or change how the repeated measures are assessed. 

```{r}
library(lavaan)
long <- read.csv("~/Box/5165 Applied Longitudinal Data Analysis/SEM_workshop/longitudinal.csv")

summary(long)

```



```{r}
#Remember, we need wide data for SEM models. 
head(long)
```

Anyone have some comments on naming conventions for this dataset? 


## 1. Longitudinal CFA

Starting point in longitudinal analysis. Can be simply thought of as does this construct relate to itself across time? And, to the extent that it does not, is that due to changes in how the construct is measured over time? 

key questions: 
1. Should the correlations be the same across time? 
2. Should the error variances be correlated? 
3. Are the loadings the same across time? 


```{r}
long.cfa <- '
## define latent variables
Pos1 =~ PosAFF11 + PosAFF21 + PosAFF31
Pos2 =~ PosAFF12 + PosAFF22 + PosAFF32
Pos3 =~ PosAFF13 + PosAFF23 + PosAFF33
Neg1 =~ NegAFF11 + NegAFF21 + NegAFF31
Neg2 =~ NegAFF12 + NegAFF22 + NegAFF32
Neg3 =~ NegAFF13 + NegAFF23 + NegAFF33

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33

'

fit.long.cfa <- cfa(long.cfa, data=long, std.lv=TRUE)

summary(fit.long.cfa, standardized=TRUE, fit.measures=TRUE)
```


```{r}
library(semPlot)
semPaths(fit.long.cfa)
```


## Measurement Invariance (MI)

To meaningfully look at means, we need to have the means mean the same thing. In other words, without using the word mean, we need to make sure that the measurement of the construct is consistent across time. If it isn't, then what we may see as change actually reflect people responding to the indicators differently. For example, a common item on an extraversion scale is "Do you like to go to parties?" This is likely interpreted differently by a 20 year old compared to a 70 year old. This is due to what is normative, what parties look like that a typical 20 and 70 year old go to, etcetera. Another way to look at this is the item "2 x 3 = X, solve for X". The reasons that a 8 year old and a 18 year old get the item incorrect is likely for different reasons (ie knowledge vs not being careful). 

Maturation is the easiest way to see differences, but it also happens when you want to compare groups ie some anova design. This assumption is typically never critically examined.  

### types of MI
Configural (pattern). Typically always true with a decent measure of your construct. Can be tested through test statistics and eye-balling. Serves as default. 

Weak (metric/loading). Can be easily met. Not meeting this shows big problems, unless you are working with a really large dataset (where there is large power to find differences). 

Strong (Scalar/intercept). Need to meet this designation to run longitudinal models and look at means across time. 

Strict (residual/error variance). Not necessarily better than Strong, and does not need to be satisfied to use longitudinal models. Why might this not hold even if you are assessing the same construct? Hint: think of what residual variance is made up of. 

### Testing MI

#### configural (baseline)

```{r}
config <- '
## define latent variables
Pos1 =~ PosAFF11 + PosAFF21 + PosAFF31
Pos2 =~ PosAFF12 + PosAFF22 + PosAFF32
Pos3 =~ PosAFF13 + PosAFF23 + PosAFF33


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33


'

config <- cfa(config, data=long, meanstructure=TRUE, std.lv=TRUE)

summary(config, standardized=TRUE, fit.measures=TRUE)
```

#### Weak (constrain loadings) 

```{r}
weak <- '
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33


## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33


'

weak <- cfa(weak, data=long, meanstructure=TRUE, std.lv=TRUE)

summary(weak, standardized=TRUE, fit.measures=TRUE)


```


####  Strong (constrain loadings and intercepts)

```{r}
strong <- '
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33


## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33


## constrain intercepts across time
PosAFF11 ~ t1*1
PosAFF21 ~ t2*1
PosAFF31 ~ t3*1


PosAFF12 ~ t1*1
PosAFF22 ~ t2*1
PosAFF32 ~ t3*1


PosAFF13 ~ t1*1
PosAFF23 ~ t2*1
PosAFF33 ~ t3*1


## free latent means at later times (only set the scale once)
Pos2 ~ NA*1
Pos3 ~ NA*1'

strong <- cfa(strong, data=long, meanstructure=TRUE, std.lv=TRUE)

summary(strong, standardized=TRUE, fit.measures=TRUE)
```

#### Strict (loadings, intercept, residual variances)

```{r}
strict <- '
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33


## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

## equality of residuals 
PosAFF11 ~~ r*PosAFF11 
PosAFF12 ~~ r*PosAFF12
PosAFF13 ~~ r*PosAFF13

PosAFF21 ~~ r*PosAFF21 
PosAFF22 ~~ r*PosAFF22
PosAFF23 ~~ r*PosAFF23

PosAFF31 ~~ r*PosAFF31 
PosAFF32 ~~ r*PosAFF32
PosAFF33 ~~ r*PosAFF33


## constrain intercepts across time
PosAFF11 ~ t1*1
PosAFF21 ~ t2*1
PosAFF31 ~ t3*1


PosAFF12 ~ t1*1
PosAFF22 ~ t2*1
PosAFF32 ~ t3*1


PosAFF13 ~ t1*1
PosAFF23 ~ t2*1
PosAFF33 ~ t3*1


## free latent means at later times (only set the scale once)
Pos2 ~ NA*1
Pos3 ~ NA*1'

strict <- cfa(strict, data=long, meanstructure=TRUE, std.lv=TRUE)

summary(strict, standardized=TRUE, fit.measures=TRUE)
```


Note that there are other types of MI that we could investigate, depending on what we are interested in. We could look at equality of latent means and variances, as well as regressions, if they were in the model. 


### Comparing the models
Usually done through chi-square difference test. But this is a very sensitive test, especially with larger samples. Better to look at changes in CFI. If delta is .01 or greater than maybe it shows misfit. 

```{r}
##Compare configural and weak model
anova(config, weak)
```


```{r}
##Compare weak and strong model
anova(weak, strong)

fitmeasures(weak)['cfi']
fitmeasures(strong)['cfi']
fitmeasures(strict)['cfi']

```




## 2. Panel/cross lag/longitudinal path model/mediation

Instead of covariances, the key component of this is to examine structural relationships between constructs over time. This can extend to cross-construct associations such as mediational models. 

key concerns: 
1. Should the regressions be the same across time? 
2. Should the error variances be correlated? 
3. Are the loadings the same across time? 


Falling under this designation is the cross-lagged panel model (clpm) -- the type of two wave model that josh went off on last time. The concerns of clpms are assuaged when you have more than two assessment points, you measure your variables latently, and/or you account for mean structure.

Below is an example of two constructs modeled over time with three assessment points. Not yet a clpm. 


```{r}
long.path <- '
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33
Neg1 =~ L4*NegAFF11 + L5*NegAFF21 + L6*NegAFF31
Neg2 =~ L4*NegAFF12 + L5*NegAFF22 + L6*NegAFF32
Neg3 =~ L4*NegAFF13 + L5*NegAFF23 + L6*NegAFF33

## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3
Neg2 ~~ NA*Neg2
Neg3 ~~ NA*Neg3
Pos1 ~~ Neg1
Pos2 ~~ Neg2
Pos3 ~~ Neg3

## directional regression paths
Pos2 ~ Pos1
Pos3 ~ Pos2
Neg2 ~ Neg1
Neg3 ~ Neg2

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33
'

fit.long.path <- sem(long.path, data=long, std.lv=TRUE)

summary(fit.long.path, standardized=TRUE, fit.measures=TRUE)
```

```{r}
semPaths(fit.long.path, layout = "tree3")
## layout can also be done manually to get publications worthy plots

```


### Longitudinal Cross lagged panel model (clpm)

key concerns: 
1. Should the regressions (both cross lagged and autoregressive) be the same across time? 
2. Should the indicator error variances be correlated (within time or within construct)? 
3. Are the loadings the same across time? (more on this later)
4. Are the latent error variances the same or different? 
5. Are the latent error variances correlated the same or different across time? 
6. Are there more lagged effects? 


```{r}
long.cross <- '
## define latent variables
Pos1 =~ L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33
Neg1 =~ L4*NegAFF11 + L5*NegAFF21 + L6*NegAFF31
Neg2 =~ L4*NegAFF12 + L5*NegAFF22 + L6*NegAFF32
Neg3 =~ L4*NegAFF13 + L5*NegAFF23 + L6*NegAFF33

## free latent variances at later times (only set the scale once)
Pos2 ~~ NA*Pos2
Pos3 ~~ NA*Pos3
Neg2 ~~ NA*Neg2
Neg3 ~~ NA*Neg3

Pos1 ~~ Neg1
Pos2 ~~ Neg2
Pos3 ~~ Neg3

## directional regression paths
Pos2 ~ Pos1 + Neg1
Neg2 ~ Pos1 + Neg1
Pos3 ~ Pos2 + Neg2
Neg3 ~ Pos2 + Neg2

## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33
'

fit.long.cross <- sem(long.cross,data=long, std.lv=TRUE)

summary(fit.long.cross, standardized=TRUE, fit.measures=TRUE)

```

```{r}
semPaths(fit.long.cross)
```

```{r}
semPaths(fit.long.cross, layout = "tree3")
```



### Longitudinal mediation model

An extension of this model is to introduce indirect pathways. Indirect pathways are equivalent with mediational pathways. The calculation of indirect pathways are done through what is known as the tracing rules. The coefficients  along each path are multiplied together; those products are them summed across different tracings or paths. In short, one finds the sum of all compound paths between two variables. 

Maxwell, S. E., Cole, D. A., & Mitchell, M. A. (2011). Bias in cross-sectional analyses of longitudinal mediation: Partial and complete mediation under an autoregressive model. Multivariate Behavioral Research, 46(5), 816-841.

Selig, J. P., & Preacher, K. J. (2009). Mediation models for longitudinal data in developmental research. Research in Human Development, 6(2-3), 144-164.


Example below from a paper looking at whether extraverted people are happier because they connect and interact with more people. Extraversion and connection were measured latently. Extraversion was measured once, connection thrice, and subjective well being 4 times. 

```{r}
#Do Self-Reported Social Experiences Mediate the Effect of Extraversion on Life Satisfaction and Happiness?
#number close friends
library(readr)
TSS_sub <- read_csv("~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/TSS_sub.csv")


scon.model6<-'
# define extraversion
bfie =~ a1bfi01 + a1bfi06r + a1bfi11 + a1bfi16 + a1bfi21r + a1bfi26 + a1bfi31r + a1bfi36

# correlated residuals
a1bfi11 ~~  a1bfi16
a1bfi06r ~~ a1bfi21r + a1bfi31r
a1bfi21r ~~ a1bfi31r + a1bfi01

#define social connection at 4 waves
hconnect=~h1clrel + h1satfr + h1sosat + h1ced05
jconnect=~j1clrel + j1satfr + j1sosat + j1ced05
kconnect=~k1clrel + k1satfr + k1sosat + k1ced05
mconnect=~m1clrel + m1satfr + m1sosat + m1ced05

#correlate residuals
h1clrel ~~ j1clrel + k1clrel + m1clrel
j1clrel ~~ k1clrel + m1clrel
k1clrel ~~ m1clrel
h1satfr ~~ j1satfr + k1satfr + m1satfr
j1satfr ~~ k1satfr + m1satfr
k1satfr ~~ m1satfr 
h1sosat ~~ j1sosat + k1sosat + m1sosat 
j1sosat ~~ k1sosat + m1sosat 
k1sosat ~~ m1sosat
h1ced05 ~~ j1ced05 + k1ced05 + m1ced05
j1ced05 ~~ k1ced05 + m1ced05
k1ced05 ~~ m1ced05

# same time covariances between extraversion, connection, satisfaction
bfie~~a1swls
hconnect ~~ h1swls
jconnect ~~ j1swls
kconnect ~~ k1swls

#regressions to calculate indiret effects
hconnect ~ a1*bfie + d1*a1swls 
jconnect ~ a2*bfie + d2*h1swls + m1*hconnect
kconnect ~ a3*bfie + d3*j1swls + m2*jconnect
mconnect ~ a4*bfie + d4*k1swls + m3*kconnect
h1swls ~ y1*a1swls + c1*bfie 
j1swls ~ y2*h1swls + c2*bfie + b1*hconnect
k1swls ~ y3*j1swls + c3*bfie + b2*jconnect 
m1swls ~ y4*k1swls + c4*bfie + b3*kconnect

#effects 
# extraversion -> connect (a)
# connect ->  swb (b)
# extraversion -> swb (c)
# auto-regressive connection (m)
# auto-regressive swb (y)

ind:= a1*b1*y3*y4 + a1*m1*b2*y4 + a1*m1*m2*b3 + a2*b2*y4 + a2*m2*b3 + a3*b3
total:= ind + c4 + c3*y4 + c2*y3*y4 + c1*y2*y3*y4
'
scon62 <- sem(scon.model6, data=TSS_sub, missing = "ml", fixed.x = FALSE)
summary(scon62, standardized=T, fit.measures=TRUE)

# use se = "bootstrap" in the fit function to get bootstrapped se

```

We will come back to extensions of these panel models in later weeks, incorporating mean levels  as well as seperating between and within person sources. 

### Including covariates
Many times we would like to control for the influence of a variable. If we wanted to, how would we go about doing this with our panel model? 

First, you have to ask yourself is it a time varying covariate (tvc), akin to a level 1 predictor variable ie one that varies from wave to wave. Or is it a time invariate covariate that does not differ across time? 

Second, you have to ask where in the model you want to introduce covariates? The DV is tradiational in standard regression. But we could also control for with the IVs. But there are multiple IVs, do we control for them all? 

Third, do you want to control at the latent level or at the level of the indicators? 


It is potential defensible to: 1. only control for the DV or focal association. 2. only control for initial time. 3. control at all time points 4 control for only downstream timepoints ie those predicted by another variable. Each of these have different interpretation for how you think the covariates could influence the model. 

I would suggest 1. try to use covariates as closely as possible to your theoretical model and 2. not to over control. Remember that statistical models can be both too liberal (you get an asterisk, everyone gets an asterisk!) and too conservative (this one of many reasons why Josh dislikes correcting for type 1 error). The goal is to be accurate. Try them multiple ways. 


## 3. Growth models

The above models are well suited to address between subjects questions, but does not get at a within subjects questions at all. There are some that can separate these effects, which we will turn to in a few weeks. However, we will start with the most basic model, the model you are familiar with, the growth model. 

The implementation of growth models in an SEM framework is very similar to the HLM framework. The major differences is how time is treated. Here, time variables must be the same for everyone in that each assessment point must have a particular variable name associated with it. That is, time is considered categorical in SEM, whereas in MLM it could be treated continuously. This requirement also makes a differences in how our data need to be structured. Whereas previously we had a time variable, now we indirectly include time into our model by specifying when variables were assessed. This has the consequence of necessitating a wide format, as opposed to the long format of MLM. 

Other than time, the idea behind the growth model is exactly the same.


### Coding time
One key these models is how you code time. Because we are working with qualitative time rather than continuous everyone has to have the same time structure. 


Let's use the long dataset from the previous Bayesian workshop, taken from chapter 4 of Singer and Willet. It is a three wave longitudinal study of adolescents. We are looking at alcohol use during the previous year, measured from 0 - 7. COA is a variable indicating the child's parents are alcoholics.

```{r}
alcohol1_pp <- read_csv("alcohol1_pp.csv")
head(alcohol1_pp)
```

```{r}
library(tidyverse)
alcohol.wide <- alcohol1_pp %>% 
  select(-X1, -age_14, -ccoa) %>% 
  pivot_wider(names_from = "age", 
              names_prefix = "alcuse_",
              values_from  = alcuse) 
head(alcohol.wide)
```



```{r, echo = FALSE}
model.1 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16'
fit.1 <- growth(model.1, data=alcohol.wide)
summary(fit.1)
```


To fit a growth model within SEM we are going to create two latent variables: an intercept and a slope/trajectory. This is the same we did prior when we fit an MLM growth model with time as a predictor. 




```{r}

semPaths(fit.1)

```



How do we define these two latent factors? They are going to be made up of our repeated measures. We then add constraints on the loadings to force the latent variables to be interpreted how we want them to be interpreted. 

As before, the simplest way to think of the intercept is the initial value. While the slope indexes change. To obtain that, we will constrain the loadings to the intercept to be 1 for all repeated measures. This will obtain what is *constant* across time. 

But where is that constant located? Previously we could center or change our time variable to change the interpretation of the intercept. We do the same thing within SEM whereby we change the interpretation of intercept by changing how we define the slope parameter. As always, the intercept is defined as when X (or time in this case) equals zero. That is, the intercept is the mean of the DV when the predictor is 0, where we have time as the predictor. 

The slope loadings typically include 0, so as to make the intercept interpretable. However, it is up to you to define where zero goes and what the rest of the loadings are. A few rules: First, how you code the loadings represent the pattern of change you expect. 0,1,2 suggests a straight line that does not increase in speed. 0,1,5, suggests something different. Usually it is better to stick with linear change to start out with. 

Second, remember that this is just fancy regression. So the coefficients are interpreted as in regression whereas for a one unit change in the slope (time) corresponds to a coef change in your DV. Thus, you will change the magnitude of your slope parameter by choosing 0, .5, 1 versus 0,1,2 versus 0, 10, 20. As always, choose what makes sense to interpret. Changing the loadings (if they are just a multiple) will not change the substantive interpretation. 


### How do we do that in lavaan? 

```{r}
model.1 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16'
fit.1 <- growth(model.1, data=alcohol.wide)
summary(fit.1)
```

### Compare with MLM
```{r}
library(lme4)
fit1.mlm <- lmer(alcuse ~  age + (age | id), data = alcohol1_pp)
summary(fit1.mlm)
```

centered MLM
```{r}
fit1.mlm.c <- lmer(alcuse ~  age_14 + (age_14 | id), data = alcohol1_pp)
summary(fit1.mlm.c)
```


```{r}
semPaths(fit.1, 'est')
```



### we can rescale our time variable to be whatever we want

```{r}
model.1 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16'
fit.1 <- growth(model.1, data=alcohol.wide)
summary(fit.1)
```

This does not change our conclusions but it does change our numnbers. 


### Interpretation of variances

Intercept will reflect our fixed effects whereas our random effects are indexed by variances of the latent variables. These are interpreted the same as before. More variance means that more people differ from the fixed effect. 

In MLM we assumed that the residual variances were equal. Here we can better model if that is the case. Right now we are able to fit separate variances for each repeated measure. These residual variances are interpreted as what is left over or what is unique at each time point that cannot be explained by the trajectory. 


### constraining slope to be fixed only

As with MLM we have options to handle the inclusion of random effects.

```{r}
model.2 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16
               s ~~0*s'

fit.2 <- growth(model.2, data=alcohol.wide)
summary(fit.2)
           
```

### introducing covariates/predictors

```{r}
# a linear growth model with a time invariatnt covariate

model.3 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16
               
# regressions
    
    i ~ male + coa
    s ~ male + coa
'
fit.3 <- growth(model.3, data = alcohol.wide)
summary(fit.3)
```


```{r}
library(lme4)
fit3.mlm <- lmer(alcuse ~  age_14 + male*age_14 + age_14*coa + (age_14 | id), data = alcohol1_pp)
summary(fit3.mlm)
```

### centered predictors

```{r}

alcohol.wide$coa.c <- scale(alcohol.wide$coa, center=TRUE, scale = FALSE)
alcohol.wide$male.c <- scale(alcohol.wide$male, center=TRUE, scale = FALSE)


model.4 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16
               
# regressions
    
    i ~ male.c + coa.c
    s ~ male.c + coa.c
'
fit.4 <- growth(model.4, data = alcohol.wide)
summary(fit.4)
```


What is different what is the same? Notice that the main difference is the intercepts ie fixed effects.  



### introducing time varying covariates
```{r, eval = FALSE}
# a linear growth model with a time-varying covariate

model.5 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + .5*alcuse_15 + 1*alcuse_16
               
# regressions
    
    i ~ male.c + coa.c
    s ~ male.c + coa.c
    
# time-varying covariates

    alcuse_14 ~ c1
    alcuse_15 ~ c2
    alcuse_16 ~ c3

'
fit.5 <- growth(model.5, data = alcohol.wide)


```

How do we interpet tvcs? 


### latent basis model 


```{r}
model.6 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + alcuse_15 + 1*alcuse_16
              
'

fit.6 <- growth(model.6, data = alcohol.wide)
summary(fit.6)
```

Does not change the fit of the model nor the implied means, but it can change your parameters by changing the time scaling. 

### quadratic model

```{r, eval = FALSE}
model.6 <- '  i =~ 1*alcuse_14 + 1*alcuse_15 + 1*alcuse_16 
            s =~ 0*alcuse_14 + 1*alcuse_15 + 2*alcuse_16
            q =~ 0*alcuse_14 + 1*alcuse_15 + 4*alcuse_16  
'

fit.6 <- growth(model.6, data = alcohol.wide)
summary(fit.6)
```




## 3.b Second order growth model

Repeated measures are latent. Why would we want to do this? At least two reasons. 1. We can take advantage of the benefits of latent variables ie no measurement error. 2. we can impose constraints for MI across time. 

```{r}
sec.order <- '
## define latent variables
Pos1 =~ NA*PosAFF11 + L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ NA*PosAFF12 + L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ NA*PosAFF13 + L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33

## intercepts
PosAFF11 ~ t1*1
PosAFF21 ~ t2*1
PosAFF31 ~ t3*1

PosAFF12 ~ t1*1
PosAFF22 ~ t2*1
PosAFF32 ~ t3*1

PosAFF13 ~ t1*1
PosAFF23 ~ t2*1
PosAFF33 ~ t3*1


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33


## latent variable intercepts
Pos1 ~ 0*1
Pos2  ~ 0*1
Pos3  ~ 0*1

#model constraints for effect coding
## loadings must average to 1
L1 == 3 - L2 - L3
## means must average to 0
t1 == 0 - t2 - t3

i =~ 1*Pos1 + 1*Pos2 + 1*Pos3 
s =~ 0*Pos1 + 1*Pos2 + 2*Pos3 '


fit.sec.order <- growth(sec.order, data=long, missing = "ML")
summary(fit.sec.order, fit.measures=TRUE)

```

```{r}
semPaths(fit.sec.order)
```


## 3.c  Multivariate models

For all of the models this semester we have focused only on a single outcome. 

```{r}
mv.sec.order <- '
## define latent variables
Pos1 =~ NA*PosAFF11 + L1*PosAFF11 + L2*PosAFF21 + L3*PosAFF31
Pos2 =~ NA*PosAFF12 + L1*PosAFF12 + L2*PosAFF22 + L3*PosAFF32
Pos3 =~ NA*PosAFF13 + L1*PosAFF13 + L2*PosAFF23 + L3*PosAFF33

Neg1 =~ NA*NegAFF11 + L4*NegAFF11 + L5*NegAFF21 + L6*NegAFF31
Neg2 =~ NA*NegAFF12 + L4*NegAFF12 + L5*NegAFF22 + L6*NegAFF32
Neg3 =~ NA*NegAFF13 + L4*NegAFF13 + L5*NegAFF23 + L6*NegAFF33

## intercepts
PosAFF11 ~ t1*1
PosAFF21 ~ t2*1
PosAFF31 ~ t3*1

PosAFF12 ~ t1*1
PosAFF22 ~ t2*1
PosAFF32 ~ t3*1

PosAFF13 ~ t1*1
PosAFF23 ~ t2*1
PosAFF33 ~ t3*1

NegAFF11 ~ tt1*1
NegAFF21 ~ tt2*1
NegAFF31 ~ tt3*1

NegAFF12 ~ tt1*1
NegAFF22 ~ tt2*1
NegAFF32 ~ tt3*1

NegAFF13 ~ tt1*1
NegAFF23 ~ tt2*1
NegAFF33 ~ tt3*1


## correlated residuals across time
PosAFF11 ~~ PosAFF12 + PosAFF13
PosAFF12 ~~ PosAFF13
PosAFF21 ~~ PosAFF22 + PosAFF23
PosAFF22 ~~ PosAFF23
PosAFF31 ~~ PosAFF32 + PosAFF33
PosAFF32 ~~ PosAFF33

NegAFF11 ~~ NegAFF12 + NegAFF13
NegAFF12 ~~ NegAFF13
NegAFF21 ~~ NegAFF22 + NegAFF23
NegAFF22 ~~ NegAFF23
NegAFF31 ~~ NegAFF32 + NegAFF33
NegAFF32 ~~ NegAFF33

## latent variable intercepts
Pos1 ~ 0*1
Pos2  ~ 0*1
Pos3  ~ 0*1

Neg1 ~ 0*1
Neg2  ~ 0*1
Neg3  ~ 0*1

#model constraints for effect coding
## loadings must average to 1
L1 == 3 - L2 - L3
L4 == 3 - L5 - L6
## means must average to 0
t1 == 0 - t2 - t3
tt1 == 0 - tt2 - tt3

i.p =~ 1*Pos1 + 1*Pos2 + 1*Pos3 
s.p =~ 0*Pos1 + 1*Pos2 + 2*Pos3

i.n =~ 1*Neg1 + 1*Neg2 + 1*Neg3 
s.n =~ 0*Neg1 + 1*Neg2 + 2*Neg3'


mv.secondorder <- growth(mv.sec.order, data=long, missing = "ML")
summary(mv.secondorder, fit.measures=TRUE)

```


```{r}
semPaths(mv.secondorder)
```


## 3.d Factor of curves model
This type of model takes multiple slope terms and asks is there a latent variable associated with the change?


```{r, eval = FALSE}

i.a =~ 1*Pos1 + 1*Pos2 + 1*Pos3 
s.a =~ 0*Pos1 + 1*Pos2 + 2*Pos3

i.b =~ 1*Neg1 + 1*Neg2 + 1*Neg3 
s.b =~ 0*Neg1 + 1*Neg2 + 2*Neg3

i.c =~ 1*hap1 + 1*hap2 + 1*hap3 
s.c =~ 0*hap1 + 1*hap2 + 2*hap3

i.d =~ 1*Arou1 + 1*Arou2 + 1*Arou3 
s.d =~ 0*Arou1 + 1*Arou2 + 2*Arou3

#Note that there are no constraints on these

Intercept =~ i.a + i.b + i.c + i.d 
Slope =~ s.a + s.b + s.c + s.d 

'




```



##  4. Multiple groups


```{r}

group <- read_csv("~/Box/5165 Applied Longitudinal Data Analysis/Longitudinal/group.csv")
head(group)
```


```{r}
group1 <- 'Positive =~ P1 + P2 + P3
Negative =~ N1 + N2 + N3 

Positive ~~ 1*Positive
Negative ~~ 1*Negative

Positive ~~ Negative
'

fit.group.1 <- cfa(group1, data=group, std.lv=TRUE, group = "Grade")

summary(fit.group.1, standardized=TRUE, fit.measures=TRUE)

```


```{r}
semPaths(fit.group.1,'est', panelGroups=TRUE)
```


Need to put in labels to fix the different parameters across models

```{r}


group2 <- 'Positive =~ c(L1,L1)*P1 + c(L2,L2)*P2 + c(L3,L3)*P3
Negative =~ N1 + N2 + N3 

Positive ~~ 1*Positive
Negative ~~ 1*Negative

Positive ~~ Negative
'

fit.group.2 <- cfa(group2, data=group, std.lv=TRUE, group = "Grade")

summary(fit.group.2, standardized=TRUE, fit.measures=TRUE)


```

We could now test whether constraining the groups makes the fit worse (similar to how we did it for measurement invariance)

```{r}
anova(fit.group.1,fit.group.2)
```

What if you wanted to make everything the same for a particular type of parameter? Lavaan has an easier answer:

```{r}
group3 <- 'Positive =~ P1 + P2 + P3
Negative =~ N1 + N2 + N3 

Positive ~~ 1*Positive
Negative ~~ 1*Negative

Positive ~~ Negative
'

fit.group.3 <- cfa(group3, data=group, std.lv=TRUE, group = "Grade", group.equal = c("loadings", "intercepts"))

summary(fit.group.3, standardized=TRUE, fit.measures=TRUE)

```

What is nice is that the output will label what parameters are constrained. Go ahead and look at the lavaan tutorial for more (residual variances, latent means, etc)

### Measurement invariance revisited

Can you see how MI tests are just a special type of multiple group analysis? We could set up our data to have variables assessed at two different time points with the time demarcated by a group variable. The exact same findings from MI would be found with multiple group. It all depends on how you structure your data.  

### When to use 

For Longitudinal models you are faced with a choice: do you want to use group status to predict your slope, for example, or do you want to use multiple groups. The short answer is that it does not matter! The semi-longer answer is that including group as a predictor is simple while multiple groups analyses are more complex but also more flexible. Using one or the other depends on what your theory about where the group differences occur. If they occur in only the regression relationship indicating group mean differences then go with the easy option. If you think that the measurement model also may differ for your groups you have a more nuanced theory that necessitates multiple groups.


```{r}
Demo.growth$group <- as.numeric(cut_number(Demo.growth$x1,2))


model.7 <- '
  # intercept and slope with fixed coefficients
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4
  # regressions
    i ~ group
    s ~ group
'
fit.7 <- growth(model.7, data = Demo.growth)
summary(fit.7)

```




```{r}

model.8 <- '
  # intercept and slope with fixed coefficients
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4

'
fit.8 <- growth(model.8, data = Demo.growth, group = "group")
summary(fit.8)

```


Notice how the slope and the intercept differ between groups. Is this meaningful? To test we will have to do the multiple groups comparison. But before we do that, take a look at the relationship between the intercept and slope for the two groups and the regression of group onto slope and intercept. 


```{r}
model.9 <- '
  # intercept and slope with fixed coefficients
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4

'
fit.9 <- growth(model.9, data = Demo.growth, group = "group", group.equal = "means")
summary(fit.9)

```


Does constraining the latent means lead to a worse fit? 

```{r}
anova(fit.8, fit.9)
```

Yes it does! Buut, how to interpret this? It is asking whether the model where we constained both the intercept and the mean differ from one that we don't. What if we want to only look at the effect of slope? 


Testing just the slope difference without constraining anything else. 

```{r}
model.10 <- '
  # intercept and slope with fixed coefficients
    i =~ 1*t1 + 1*t2 + 1*t3 + 1*t4
    s =~ 0*t1 + 1*t2 + 2*t3 + 3*t4

s ~  c(m1, m1)*1

'
fit.10 <- growth(model.10, data = Demo.growth, group = "group")
summary(fit.10)

```

```{r}
anova(fit.8, fit.10)
```


What if you wanted to test differences in variance? 



## 5. Two wave assessments

There are a lot of pre-post designs. In fact, the recent nobel prize winners have a famous paper looking at difference in difference designs, basically looking at two group pre-post tests. Collecting longitudinal data is hard, so it makes sense that a lot of this type of data will be laying around. How do we analyze it? The first thing to notice is that we jumped right into MLM this semester, bypassing simple discussions. As a result, we can only really look at change with 3 or more waves. 

(As an aside, I do see people use two wave data with MLMs. It is often done incorrectly/problematically by using wave 1 as a level 2 covariate as opposed to using time as a level 1 predictor. This approach is related to residualized change, which we will discuss below).

How to measure change, or should we? https://www.gwern.net/docs/dnb/1970-cronbach.pdf This paper lays out some of the problems that occur with standard treatments of two wave assessments. 

The most basic two wave form of change is a difference score. However, many have said these are problematic. 
The issues are: 
1. hard to seperate measurement error from true change
2. unreliable estimate of change
3. initial level (or last level) may be driving change. How to account for? 

The second alternative is a standard residualized gain/change score where you regress time 2 onto time 1. This overcomes some of the issues raised about because we are being conservative about the error by "regressing to the mean" such that people with larger changes than average will have their change scores "shrunken" to the average, must like we do with MLMs. This also helps with accounting for starting values that may be responsible for the changes, as this is literally controlling for the initial level. 

The issues with this however are: 
1. it isn't true change, as you are implying people change similarly
2. it does not account for unreliability of change in a principled way
3. error, which should be random, is considered change and it is likely associated with T1. 


The way to address most of these concerns is to: 1. conceptualize and use change scores and 2. do so by measureing change latently, and thus error free. 3. give we are working with SEM (due to working with latent variables) we can now, statistically, seperate initial levels from change. 








## Planned missing data
## Missing data
## Power







