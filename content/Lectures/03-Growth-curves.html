---
title: "Week 2"
subtitle: "Growth Curves"
summary: "Growth Curves"
author: "Josh Jackson"
date: '2019-09-05'
type: post
output:
  blogdown::html_page:
    toc: true
---


<div id="TOC">
<ul>
<li><a href="#growth-curves">Growth curves</a><ul>
<li><a href="#between-person-models-and-cross-sectional-data">Between person models and cross sectional data</a></li>
<li><a href="#within-person-models-e.g.-2-level-models">Within person models e.g., 2-level models</a><ul>
<li><a href="#thinking-about-random-effects">Thinking about random effects</a></li>
<li><a href="#random-effects">Random effects</a></li>
<li><a href="#empty-model-equation">Empty model equation</a></li>
<li><a href="#putting-it-together">Putting it together</a></li>
<li><a href="#visualize-what-you-are-doing">Visualize what you are doing</a></li>
<li><a href="#icc">ICC</a></li>
</ul></li>
<li><a href="#adding-time">Adding time</a><ul>
<li><a href="#what-does-this-look-like-graphically">What does this look like graphically?</a></li>
<li><a href="#adding-a-random-slope">Adding a random slope?</a></li>
</ul></li>
<li><a href="#individual-level-random-effects">Individual level random effects</a><ul>
<li><a href="#calculation-of-individual-level-random-effects">Calculation of individual level random effects</a></li>
<li><a href="#how-are-these-random-effects-calculated">How are these random effects calculated?</a></li>
<li><a href="#random-effect-decomposition">Random effect decomposition</a></li>
</ul></li>
</ul></li>
</ul>
</div>

<div id="growth-curves" class="section level1">
<h1>Growth curves</h1>
<div id="between-person-models-and-cross-sectional-data" class="section level2">
<h2>Between person models and cross sectional data</h2>
<p>You already know this, but it gives us a chance to review regression</p>
<p><span class="math display">\[ {Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... +\epsilon_{i} \]</span></p>
<p><span class="math display">\[ \hat{Y}_{i} = b_{0} + b_{1}X_{1} + b_{2}X_{2} + b_{3}X_{3}+... \]</span></p>
<p>Parameters are considered fixed where one regression value corresponds to everyone. I.e., that association between X1 and Y is the same for everyone.</p>
<p>Each person has a Y, denoted by the subscript i, and each has a residual associated with them, also designated by i.</p>
<pre class="r"><code>library(readr)
example &lt;- read_csv(&quot;~/Box/5165 Applied Longitudinal Data Analysis/ALDA/example copy.csv&quot;)
example$ID &lt;- as.factor(example$ID)
# you can find the data on my github at: https://github.com/josh-jackson/ALDA/example%20copy.csv</code></pre>
<p>Lets look at some data. These data examine older adults who came into a study up to six times over a six year period. Multiple cognitive, psychiatric and imaging assessments were done. Let’s look at functional connectivity network called SMN7.</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ───────────────────────────────────────────────────────────────────────── tidyverse 1.2.1.9000 ──</code></pre>
<pre><code>## ✔ ggplot2 3.2.1           ✔ dplyr   0.8.3      
## ✔ tibble  2.1.3           ✔ stringr 1.4.0      
## ✔ tidyr   0.8.99.9000     ✔ forcats 0.4.0      
## ✔ purrr   0.3.2</code></pre>
<pre><code>## ── Conflicts ───────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(ggplot2)
gg1 &lt;- ggplot(example,
   aes(x = week, y = SMN7)) + geom_point() + stat_smooth(method = &quot;lm&quot;)   
print(gg1)</code></pre>
<p><img src="/Lectures/03-Growth-curves_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>What happens if we run a regression?</p>
<pre class="r"><code>regression &lt;- lm(SMN7 ~ week, data = example)
summary(regression)</code></pre>
<pre><code>## 
## Call:
## lm(formula = SMN7 ~ week, data = example)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.099294 -0.039929 -0.005938  0.032715  0.169885 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.100161   0.005261  19.039   &lt;2e-16 ***
## week        0.004087   0.002563   1.595    0.112    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.05562 on 214 degrees of freedom
##   (9 observations deleted due to missingness)
## Multiple R-squared:  0.01174,    Adjusted R-squared:  0.007124 
## F-statistic: 2.543 on 1 and 214 DF,  p-value: 0.1123</code></pre>
</div>
<div id="within-person-models-e.g.-2-level-models" class="section level2">
<h2>Within person models e.g., 2-level models</h2>
<p>We saw this last time where we can think of everyone being run in a separate regression model. Here the lines connect the dots of the same people across time.</p>
<pre class="r"><code>library(tidyverse)
gg2 &lt;- ggplot(example,
   aes(x = week, y = SMN7, group = ID)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE)   

gg3 &lt;- gg2 +  stat_smooth(data = example, aes(x = week, y = SMN7, group=1, colour=&quot;#990000&quot;), method = &quot;lm&quot;, size = 3, se=FALSE) + theme(legend.position = &quot;none&quot;)
print(gg3)</code></pre>
<p><img src="/Lectures/03-Growth-curves_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Each person has multiple assessments, so we need to distinguish between people and their assessments. In normal regression we wouldn’t think about this as everyone datapoint is assumed to be independent. However, this is not the case here. Failing to distinguish would lead to violation of independence, an important assumption of the standard regression model.</p>
<p>As seen in the graph above, what we have now is both individual level slopes as well as an average level slope. The average level slope is going to be the average of the individual level slopes, which will look like our average slope ignoring all dependencies. Same for the intercept.</p>
<p>One way to do this is to run separate regressions for each person. Then we could just pool (or average) together where people start and how much they change to get the average intercept (starting value) and trajectory (how much people change). We will see later that this is a somewhat poor approach.</p>
<pre><code>## Joining, by = &quot;ID&quot;</code></pre>
<pre class="r"><code>regressions &lt;- example2 %&gt;% 
  group_by(ID) %&gt;% 
  do(tidy(lm(SMN7 ~ week, data = .)))

head(regressions)</code></pre>
<pre><code>## # A tibble: 6 x 6
## # Groups:   ID [3]
##   ID    term        estimate std.error statistic  p.value
##   &lt;fct&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 67    (Intercept)  0.0921    0.0161       5.72   0.0292
## 2 67    week         0.00662   0.00657      1.01   0.420 
## 3 75    (Intercept)  0.126   NaN          NaN    NaN     
## 4 75    week         0.00771 NaN          NaN    NaN     
## 5 87    (Intercept)  0.0787  NaN          NaN    NaN     
## 6 87    week        -0.0227  NaN          NaN    NaN</code></pre>
<p>In addition to the average intercept and the average trajectory there is also the amount of variation around each of these estimates. Do people tend to change the same? Are there individual differences in the initial assessment?</p>
<p>This type of meaningful variation is lost when we have a between subjects only model that ignores the individual level. This variation will be called Random Effects (or variance estimates in SEM).</p>
<p>[Side note: note how some people do not have se estimates for their regression coefficients. The reason for this will impact our ability to fit longitudinal models later on.]</p>
<p>There is another important source of variation different from standard regression models. The within-subjects error that can be seen in the below graph. If we did not take people into account and just collapsed across people to get a between subjects assessment of change, this error would be confounded with individual differences in change. We will discuss this error more in depth later, but one way to think about our goal is to utilize our repeated assessments to make better predictions. A way to do that is to create additional buckets of explained variance, resulting in a smaller bucket of unexplained variance.</p>
<pre class="r"><code>example3 &lt;- example2 %&gt;% 
  filter(ID == &quot;67&quot;) </code></pre>
<pre class="r"><code>gg4 &lt;-  ggplot(example3, aes(x = week, y = SMN7, group = ID)) +  geom_point() + stat_smooth(method = &quot;lm&quot;)

gg4</code></pre>
<p><img src="/Lectures/03-Growth-curves_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div id="thinking-about-random-effects" class="section level3">
<h3>Thinking about random effects</h3>
</div>
<div id="random-effects" class="section level3">
<h3>Random effects</h3>
<p>Within subjects variability in either starting value or slope/trajectory is referenced in terms of random effects. How do we represent this in our equation? Easy, we just say that the typical regression parameters we have are not the same for everyone – that they are random (in contrast to fixed).</p>
<p>In general, when would we want to use random effects? If there is some sort of selection (random or not) of many possible values of the predictor (e.g., stimuli are 3 random depression drugs, three semi random chosen levels of a drug). With longitudinal data this is (random) people.</p>
<p>Side bar: Even in situations where these levels are not random (eg working with U.S. states) it is still useful to use MLM and we still call them random effects. To be consistent with language, random here can refer to as random from the population average, not randomly selected. When talking about “random effects” you can mean either of these definitions (and a few more). Luckily we can mostly ignore these semantic issues.</p>
<p>What is necessary for modeling random effects? For longitudinal models, there needs to be multiple assessments per your grouping category (people, schools, neighborhoods, trials).</p>
<p>We are assuming these random effects are sampled from some population and thus vary from group to group (or person to person). This means that your coefficients (like traditional regression coefficients) are estimates of some population parameter and thus have error associated with them. This error is not like like a standard residual, which represents error for your overall model. Nor is it like the standard error for a point estimate. Random effects can best be thought of as deviation of individual regression lines from the group regression line (though it technically is not this).</p>
<p>To facilitate the multiple assessments per person we will now use both i and j subscripts. We will see that the random effects are part of the overall error term in the model. Counterintuitively, the main focus of these types of models will be the fixed effects, with less attention paid to the random effects. That said, the random effects are necessary to account for dependency in the data. One can think about these models as normal fixed effects regressions, with the random effects there to account for the longitudinal nature of the data. They are made up of a number of standard regression equations, each for a single individual. Doing so side steps the trouble of having correlated errors, and thus allows us to interpret our findings without concern.</p>
<p>To facilitate adding random effects to our model it is helpful to think about two “levels” to our regression equation. We are going to put a regression equation within our regression equation. (Que Xzhibit joke). The first level will be the within-person model, in that it described how people differ across time. The second level will be the between person level. Note that these do not correspond to fixed or random effects. Instead they can be thought to model either within person differences or between person differences. Mastering thinking at these two levels will help make sense of these MLM models.</p>
</div>
<div id="empty-model-equation" class="section level3">
<h3>Empty model equation</h3>
<p>Let’s start with the most basic model and then expand from there.</p>
<p>Level 1 - within person
<span class="math display">\[ {Y}_{ij} = \beta_{0j}  +\varepsilon_{ij} \]</span></p>
<p>Note that we have multiple responses per individual j, noted with an i to refer to specific times.</p>
<p>Also note that the intercept has a subscript. In typical regression it does not. This suggests that not everyone has the same intercept.</p>
<p>The residuals at this level are thought of as measurement error OR as something that can be explained by time varying predictors.</p>
<p>Level 2 - between person
<span class="math display">\[ {\beta}_{0j} = \gamma_{00} + U_{0j} \]</span></p>
<p>Level 2 takes the intercept (or other parameter) at level 1 and breaks it down into an equation for each individual, j. An overall group average (the gamma) and a residual term specific to deviation around the intercept (see below).</p>
<p>And two variance components:
1. a random effect of the intercept
<span class="math display">\[ {U}_{0j} \sim \mathcal{N}(0, \tau_{00}^{2})  \]</span>
The subscript of the <span class="math inline">\(U_{0j}\)</span> refers to the number of the parameter where 0 is the intercept, 1 is the first regression coefficient, and so on. The second refers to the individual, j. So <span class="math inline">\(U_{0j}\)</span> refers to the intercept whereas <span class="math inline">\(U_{1j}\)</span> would refer to the random effect of the first regression coefficient.</p>
<p>The <span class="math inline">\(U_{0j}\)</span> random effect is said to be normally distributed with a mean of zero and a variance of <span class="math inline">\(\tau\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>the residual error term
<span class="math display">\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]</span>
Much like in normal regression there is an error term for all of the variation we cannot account for. What is unique here is that we took that normal variation and split it into two components. One that is attributable to variation around the intercept <span class="math inline">\({U}_{0j}\)</span> and a catch all residual.</li>
</ol>
<p>Technically this is not a growth model, nor one that is inherently longitudinal. However, it does serve as a nice starting point to identify random effects.</p>
</div>
<div id="putting-it-together" class="section level3">
<h3>Putting it together</h3>
<p><span class="math display">\[ {Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij} \]</span></p>
</div>
<div id="visualize-what-you-are-doing" class="section level3">
<h3>Visualize what you are doing</h3>
<p>Imagine the raw data plotted without knowing person j, how would <span class="math inline">\(\varepsilon_{i}\)</span> be calculated?</p>
<p>Now think about the data plotted again but with knowing each person has their own intercept. How would <span class="math inline">\(\varepsilon_{ij}\)</span> be calculated?</p>
<p>Finally, how is <span class="math inline">\(U_{0j}\)</span> calculated?</p>
</div>
<div id="icc" class="section level3">
<h3>ICC</h3>
<p>If the ICC is greater than zero, we are breaking standard regression assumptions.</p>
<p><span class="math display">\[\frac{U_{0j}}{U_{0j}+ \varepsilon_{ij}}\]</span></p>
<p>Is defined as % variation between over total variance.</p>
<p>ICC can also be interpreted as the average (or expected) correlation within a nested group, in this case a person. On other words, the ICC is the correlation between any person’s repeated measures (technically residuals).</p>
</div>
</div>
<div id="adding-time" class="section level2">
<h2>Adding time</h2>
<p>Here is the basic growth model where our predictor is a time variable</p>
<p>Level 1:</p>
<p><span class="math display">\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{ij} + \varepsilon_{ij} \]</span></p>
<p>Note how similar this looks like to a normal regression equation. Again, the differences are due to those pesky subscripts. Like before, think of this as a normal regression equation at the level of a person. Each person would have one of these equations with, in addition to a unique Y, X and residual, a unique <span class="math inline">\(\beta_{0}\)</span> and <span class="math inline">\(\beta_{1}\)</span>. Look above to those individual regressions we did at the start of this section.</p>
<p>Level 2:<br />
<span class="math display">\[ {\beta}_{0j} = \gamma_{00} + U_{0j}\]</span></p>
<p>Level 2 takes the parameters at level 1 and decomposes them into a fixed component that reflects that average and then the individual deviations around that fixed effect. <span class="math inline">\(U_{0j}\)</span> is not error in the traditional sense. It describes how much variation there is around that parameter. Do some people start higher while some start lower, for example.</p>
<p><span class="math display">\[ {\beta}_{1j} = \gamma_{10} \]</span></p>
<p>The new level 2 term refers to the first predictor in the level 1 regression equation ie the slope. This slope is fixed in that the level 2 equation only has a gamma term and no U residual term.</p>
<p>Putting it together:
<span class="math display">\[ {Y}_{ij} = \gamma_{00} + \gamma_{10} (X_{1j})+ U_{0j}  + \varepsilon_{ij} \]</span></p>
<p>Note that in computing a single individuals Y, it depends on the two fixed effects, the Xj, and the random effect for the intercept.</p>
<div id="what-does-this-look-like-graphically" class="section level3">
<h3>What does this look like graphically?</h3>
<p>And how does this differ from the random intercept model?</p>
<p>Can you draw out the sources of error? The random effects for each participant? The fixed effects?</p>
</div>
<div id="adding-a-random-slope" class="section level3">
<h3>Adding a random slope?</h3>
<p>What happens when we add a random slope?
Level 1:</p>
<p><span class="math display">\[ {Y}_{ij} = \beta_{0j}  + \beta_{1j}X_{1j} + \varepsilon_{ij} \]</span>
Level 2:<br />
<span class="math display">\[ {\beta}_{0j} = \gamma_{00} + U_{0j}\]</span></p>
<p><span class="math display">\[ {\beta}_{1j} = \gamma_{10} + U_{1j} \]</span></p>
<p>Putting it together:
<span class="math display">\[ {Y}_{ij} = \gamma_{00} + \gamma_{10}(X_{ij})+ U_{0j} + U_{1j}(X_{ij}) + \varepsilon_{ij} \]</span></p>
<p>Can think of a persons score divided up into a fixed component as well as the random component.</p>
<p>These random effects are likely related to one another. For example, if someone starts high on a construct they are then less likely to increase across time. This negative correlation can be seen in the residual structure, where the random effects are again normally distributed with a mean of zero, but this time one must also consider covariance in addition to variance.</p>
<p><span class="math display">\[ \begin{pmatrix} {U}_{0j} \\ {U}_{1j} \end{pmatrix}
\sim \mathcal{N} \begin{pmatrix} 
  0,     &amp; \tau_{00}^{2} &amp; \tau_{01}\\ 
  0, &amp; \tau_{01} &amp; \tau_{10}^{2}
\end{pmatrix} \]</span></p>
<p>Note that it is possible to have a different error structures, one where there is no relationship between the intercept and the slope, for example. We will discuss this more later in the semester. Right now just know that the default is to have correlated random effects.</p>
<p>We also have the within subject variance term that accounts for deviations that are not accounted for by time variable and other level 1 predictors.</p>
<p><span class="math display">\[ {R}_{ij} \sim \mathcal{N}(0, \sigma^{2})  \]</span></p>
<p>Note that it is possible to model these level 1 residuals with different structures. This specification implies that there is no correlation across an individuals residuals, once you account for level 1 predictors (ie growth trajectories). Having a specific level 1 autoregressive or other type of pattern is common in other treatments of longitudinal models (panel models) but is not necessary with growth models (but possible).</p>
<p>This is the basic format of the growth model. It will be expanded later on by adding variables to the level 1 model and to the level 2 model. Adding to the level 1 model is only possible with repeated variables.</p>
<p>Level 1 regression coefficients are added to the level 2 model. These coefficients are decomposed into a fixed effect, a random effect (possibly), and between person predictors. As with any regression model, each of these only have a single error term.</p>
</div>
</div>
<div id="individual-level-random-effects" class="section level2">
<h2>Individual level random effects</h2>
<div id="calculation-of-individual-level-random-effects" class="section level3">
<h3>Calculation of individual level random effects</h3>
<p>Random effects are often thought in terms of variance components. We can see this if we think of individual level regressions for each person where we then have a mean and a variance for both the intercept or the slope. The greater the variance around the intercept and the slope means that not everyone starts at the same position and not everyone changes at the same rate.</p>
<p>If you want to look at a specific person’s random effect you can think of it as a deviation from the fixed effect where subject 6’s intercept can be thought of as</p>
<p><span class="math display">\[ {\beta}_{06} = \gamma_{00} \pm U_{06}\]</span></p>
<p>e.g 2.2 = 3 - .8</p>
</div>
<div id="how-are-these-random-effects-calculated" class="section level3">
<h3>How are these random effects calculated?</h3>
<p>It isn’t as straightforward as calculating a slope for each person and then using the difference between that slope and the average slope. Instead, the estimates are partially pooled towards the overall mean of the sample, the fixed effect. We do this to get a better estimate of the parameters, the same way that using regression to predict y-hat given an X is better than binning X and calculating y-hat. More information = better.</p>
<p>Why not full pooling ie give everyone the same slope? Because it ignores individual differences in change. Often individual differences in (intraindividual) change is what we care about.</p>
<p>The result is that the variance of the change trajectories (using MLM) will be smaller than the variance of the fitted linear models. Trajectories are “regressed” towards the average trajectory under the assumption that extreme scores are extreme because of (measurement) error, not that people are actually extreme.</p>
<p>Can think about this in terms of creating an average for your intercept. Do you want the average to be the grand mean average, ignoring group? Do you want it to be the person average, ignoring that some people have more data points and thus are better assessed? No right answer, so maybe lets meet in the middle? This is sometimes called an empirical Bayes estimate.</p>
</div>
<div id="random-effect-decomposition" class="section level3">
<h3>Random effect decomposition</h3>
<p>Think of the original total variance in a scatter plot of our DVs. Adding random effects takes that variance and trims it down.</p>
<p>The intercept only MLM separates it into a level 1 variance (which at this stage is treated as error) and a level 2 random intercept variance.</p>
<p>Creating a random slopes model takes the Level 1 residual variance and creates a new “pile” of explained or accounted for variance.</p>
<p>We can then further explain the variance or reduce the pile by predictors at level 1 and level 2. Our goal isn’t necessarily to explain all of the variance but it is helpful to reduce the unexplained variance <span class="math inline">\(\varepsilon_{ij}\)</span> to improve model fit.</p>
</div>
</div>
</div>
