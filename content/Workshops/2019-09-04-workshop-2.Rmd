---
title: 'Workshop #2'
author: Josh
date: '2019-09-04'
summary: "tidyr and lmer"
slug: workshop-2
categories: []
tags: []
lastmod: '2019-09-04T12:00:19-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
type: post
output:
  blogdown::html_page:
    toc: true
---



```{r, echo = FALSE, message=FALSE}

library(sjPlot)
library(ggplot2)
library(tidyverse)
library(readr)

example <- read_csv("~/Box/5165 Applied Longitudinal Data Analysis/ALDA/example copy.csv")



```

```{r, echo = FALSE}
example$year <- example$week
```

# tidyr
## Wide and Long form

Depending on what type of analysis you want to perform you may need to restructure your data. I recommend the combination of tidyr and dplyr (among others) to restructure and manage your dataframes. The first decision you need to make is whether you want your data structured in a long or a wide format. There are multiple names to refer to these two types: multivariate vs univariate, person-level vs person-period, etc but they all refer to the same idea. How to structure your data depends on both what level of analysis (individual, dyad, household) and what type of analyses (MLM/SEM). Typically our focus is on individuals. 

Wide form is common among non-longitudinal data. It has one line per individual with all of their repeated measures in the same row, each with some name to distinguish which assessment wave the data came from. In general, this format is used for SEM. 

```{r, echo = FALSE, message=FALSE}
library(tidyverse)
wide<- tribble(
  ~ID, ~ext_1, ~ext_2, ~ext_3,
  1, 4, 4,4,
  2, 6, 5,4,
  3, 4,5,6
)
wide
```

In contrast, long format has a row per observation. Thus, participants likely have many rows, each one referring to a different assessment wave. There are fewer variables in this format which makes organization somewhat easier. Thus this has been referred to as "Tidy" data. Graphing with ggplot is facilitated when using tidy data such as being in the long format. 

```{r, echo = FALSE}

long<- tribble(
  ~ID, ~time, ~ext,
  1, 1,4,
  1, 2,4,
  1, 3,4,
  2, 1,6,
  2, 2,5,
  2, 3,4,
  3, 1,4,
  3, 2,5,
  3, 3,6
)
long
```


How do you go back and forth? We use the tidyr package! As of a few months ago, you would use the gather and the spread functions. Now these are being phased out and are being replaced by pivot_longer and pivot_wider. The functions work similar but the newer ones are a little more intuitive both in terms of remembering the correct function name as well as well adding more bells and whistles. 


Gather and pivot_longer goes from wide to long.

```{r}
library(tidyr)

wide_to_long <- wide %>% 
  gather(ext_1:ext_3,key = "time", value = "ext") 
wide_to_long

```

```{r}
library(tidyr)

wide_to_long.p <- wide %>% 
  pivot_longer(-ID, names_to = "time", values_to = "ext") 
wide_to_long.p

```

Note the similarities. The key in all three is to 1. identify which columns need to be reshaped. Here is it all of them besides ID. 2. we need to name the newly created variable that consists of the old column names (here time). 3. we need to name what those values represent (here levels of extraversion)

The separate function could be used to get only the assessment wave number. This might be useful when combining data together or for creating a common time metric for everyone.

```{r}
wide_to_long2 <- wide_to_long %>% 
  separate(time, into = c("omit", "wave"), sep = "_", convert = TRUE) %>%
  dplyr::select(-omit) %>% 
  arrange(ID)
wide_to_long2

# Note that the seperate function will identify non numeric characters and use that to seperate the values. You can omit the sep = function to check yourself. 
```


One issue that comes up here is that we have differing dates for each assessment. Ideally we would like to utilize that extra information. 


```{r, echo=FALSE}
wide.date<- tribble(
~ID, ~ext_1, ~ext_2, ~ext_3, ~date_1, ~date_2, ~date_3,
  1, 4, 4,4, '1/1/10', '5/1/10', '8/1/10',
  2, 6, 5,4, '1/6/10', '4/10/10', '9/1/10',
  3, 4,5,6, '1/8/10', '4/25/10', '9/13/10'
)
wide.date
```

How do we fix it? The same way we would with multiple variables we want to convert. Wave, along with ID helps us keep track of what variables go with which person at which time. Together, the two serve as a unique identifier. To better understand the code go through each line to see what the intervening data frame looks like. 


```{r}
long.date <- wide.date %>% 
  gather(-ID, key = "time", value = "value") %>% 
  separate(time, into = c("variable", "wave")) %>% 
  spread(variable,value) 
long.date

```

One difficulty of creating a wave variable is whether or not the variables are named in a manner such that 1) assessment wave is easily identifiable (e.g. does _a always refer to the first wave whereas _b always refer to the second?) and 2) if that is consistent across variables. Having a wave identifier for your variables is important/necessary. Having an easily selected one (ie at the end of the variable name, hopefully separated by an underscore or a period). If assessment wave separators are embedded within the variable name it will be harder to covert your data. Often, variable data is attached at the end of a name such as SWB_4 to refer to the fourth item in a scale. This may obscure wave identification as in SWB_A_4. A similar naming problem can occur with multiple reports e.g,. SWB_4_parent. I recommend putting wave identification last. The difficulties become partly moot when working in long format (read: entering data in) as opposed to wide. This also becomes moot when stored in a separate dataset. This is another reason why you should use codebooks! 

In the above code we used spread to go from long to wide as a means of creating a long dataset where there were multiple variables. Technically this is not a tidy dataset in that it comprises of both long and wide information, but it is the typical format used for MLM analyses. 

Going from long to wide uses spread or pivot_wider function. We will utilize this when converting our MLM models to SEM models, but try the code below to see what happens. 

```{r}
long_to_wide <- long %>% 
  spread(time, ext)
long_to_wide
```

Note that this is technically the dataframe format that we want. The problem is that our variable names are numeric. This often causes problems.  When working with tibbles use backticks ' to refer to the column e.g., select('1'). I'd recode into a more usable variable name. 





# lme4 
The basic function we will work with is lmer from the lme4 package

```{r}
library(lme4)
```

The package was developed to be similar to the lm function. The code will be similar to the formula for the combined model

Code for empty/null/intercept only model

```{r, eval= FALSE}

lmer(Y ~ 1 + (1 | subjects), data=example)
```

Level 1
  $$ {Y}_{ij} = \beta_{0j}  +\varepsilon_{ij} $$
  
 Level 2
 $$ {\beta}_{0j} = \gamma_{00} + U_{0j} $$
 
 
 Combined 
 $$ {Y}_{ij} = \gamma_{00} + U_{0j}  + \varepsilon_{ij} $$
 
1 is the way to reference the intercept. All additional fixed effects go outside the parentheses. Inside the parentheses are the random effects and residual terms. To the right of the vertical line is our level 1 residual term, which references the grouping variable. In this case, as with almost all longitudinal work, is the subject ID. To the left of the vertical line is the random effects we want to estimate. Right now this estimates only one random effect, one for the intercept.

It is possible to suppress a random intercept by putting a zero instead of a 1. If you do not put anything there the 1 is implied. 


```{r, eval = FALSE}
lmer(y ~ 1 + time + (1 + time | subjects), data=data)

lmer(y ~ time + (time | subjects), data=data)
# both are equivalent
```

### Example


```{r}
mod.1 <- lmer(SMN7 ~ 1 + (1 | ID), data=example)
summary(mod.1)
```





### How to calculate ICC? 


```{r}
0.001823/(0.001823 + 0.001302)
```


## Exploring beyond the summary


```{r}
class(mod.1)
```



### what do the random effects look like? 


```{r, message = FALSE}
library(sjPlot)
plot_model(mod.1, type = "re", sort.est = TRUE)
```


```{r}
head(ranef(mod.1))
```


```{r}
head(coef(mod.1))
```

```{r}
fixef(mod.1)
```

How do these relate? Lets calculate ID 6 intercept random effect
```{r}
#coef = fixef + raneff

# coef for ID = 6 is 0.04724795  
0.106972 -0.0597240676 

```


To get residuals and fitted scores
```{r}
library(broom)
example.aug<- augment(mod.1, data = example)


# .fitted	 = predicted values
# .resid	= residuals/errors
# .fixed	 = predicted values with no random effects

```


## Adding time to the MLM
### Fixed slope 
```{r}
mod.2f <- lmer(SMN7 ~ 1 + year + (1  | ID), data=example)
summary(mod.2f)
```
What does this look like graphically? 


### Random slope
```{r}
mod.2 <- lmer(SMN7 ~ 1 + year + (year  | ID), data=example)
summary(mod.2)
```

How does the intercept change from the random intercept only model? It may change because the intercept is now conditional on time ie after accounting for time. It is not the predicted outcome when time = 0. You can think of the previous intercept as the grand mean of person means. If our year variable here changed across time then there would be a larger change in the intercept. 

How do you interpret year? 

How did the random effects change? 

## Testing models

```{r}
anova(mod.2f, mod.2)
```

Why is there a 2df difference? 


also you can see the non-REML fit info here: 
```{r}
glance(mod.2f)
```


### Why treating time is so important

Time with a different scale. How do we interpret? And what changes? 

```{r}
example$year.n <- (example$year - 30)
  
mod.2n <- lmer(SMN7 ~ 1 + year.n + (year.n  | ID), data=example)
summary(mod.2n)
```
What happened? 



## Random effects 

### Calculation of random effect confidence interval

Conveys the predicted range around each fixed effect in which 95% of the sample individuals are predicted to fall. 

95% random effect = fixed effect plus minus 1.96 * random standard deviation

How to calculate? 
1. Intercept $$ \gamma_{00} \pm  1.96  *  \tau_{U_{0j}}  $$
```{r}
0.1193933 + (1.96 * 0.240217) 
0.1193933 - (1.96 * 0.240217) 
```


2. Slope $$ \gamma_{10} \pm  1.96  *  \tau_{U_{1j}}  $$
```{r}
0.0004891 + (1.96 * 0.007745) 
0.0004891 - (1.96 * 0.007745) 
```



###Individual level random effects 

Are the intercept random effects the same as the model with only the intercept? Why or why not? 


```{r}
head(ranef(mod.1))
```

```{r}
head(ranef(mod.2))
```


## Using simulations to get better estimates of confidence around our estimates

```{r}
library(broom.mixed)
random_params <- tidy(mod.2,  effects = "ran_vals", conf.int=TRUE)
head(random_params)
```



```{r}
library(merTools)
FEsim(mod.2)
```

```{r}
re.sim <- REsim(mod.2)
head(re.sim)
```


This can be used to create CIs for each individual random effect (and fixed effect). What is the confidence interval around person 6's intercept estimate compared to person 2000 who has 25 repeated measurements? 

### Caterpillar plots

Look through these different methods of getting random effects. Note that they are not all exactly the same. 


caterpillar plots
```{r}
p1 <- plotREsim(re.sim)
p1
```


### Density of individual random effects

```{r}
p1.gg1 <- re.sim %>% 
  filter(term == "(Intercept)") 

ggplot(p1.gg1, aes(mean)) +
  geom_density()
```



```{r}
p1.gg2 <- re.sim %>% 
  filter(term == "year") 


ggplot(p1.gg2, aes(mean)) +
  geom_density()
```



## Comparing to a standard linear model


```{r}
lm.1 <- lm(SMN7 ~ 1 + year, data = example)
summary(lm.1)
```



### Comparing models

LRT 


Parametric bootstrap for CIs

```{r, eval= FALSE}
confint(mod.1, method="boot", nsim=1000)
summary(mod.1)

# uses SDs of random effects
# sigma = residual standard error
```

Comparing two models. fit the reduced model, then repeatedly simulate from it and compute the differences between the deviance of the reduced and the full model for each simulated data set. Compare this null distribution to the observed deviance difference. 

This procedure is implemented in the pbkrtest package.

```{r}
library(pbkrtest)
#pb <- PBmodcomp(mod.2,mod.2r)
```




## Predictions and prediction intervals

Predict function is deterministic and uses only the fixed effects (i.e. does not include random effects in the predictions). It does not do prediction in the typical sense where you are predicting *new* individual's scores. 

Simulate is non-deterministic because it samples random effect values for all subjects and then samples from the conditional distribution. Simulation is needed to create true predictions. 




### Predictions and prediction intervals
Predict function is deterministic and uses only the fixed effects (i.e. does not include random effects in the predictions). It does not do prediction in the typical sense where you are predicting *new* individual's scores. 

Simulate is non-deterministic because it samples random effect values for all subjects and then samples from the conditional distribution. Simulation is needed to create true predictions. 

Short of a fully Bayesian analysis, bootstrapping is the gold-standard for deriving prediction intervals/bands (ie where would a new person score given X), but the time required is typically high.

In order to generate a proper prediction (for either a new person or a new observation within a person), a prediction must account for three sources of uncertainty in mixed models:

1. the residual (observation-level) variance,
2. the uncertainty in the fixed coefficients, and
3. the uncertainty in the variance parameters for the random effects

Does so by:
1. extracting the fixed and random coefficients
2. takes n draws from the multivariate normal distribution of the fixed and random coefficients (separately)
3. calculates the linear predictor for each row in newdata based on these draws, and
4. incorporates the residual variation  
then: 
5. returns newdata with the lower and upper limits of the prediction interval and the mean or median of the simulated predictions



```{r, warning=FALSE}
library(merTools)
# see also their shiny app: shinyMer(mod.1)

PI <- predictInterval(merMod = mod.2, newdata = example, level = 0.9, n.sims = 100, stat = "median", include.resid.var = TRUE)
head(PI)
```

Nice for bringing in confidence bands around your prediction (And we might use this later)


Broom offers the fitted (predicted) values already if you just want to plot your trajectory. But note that these are not typical prediction intervals (what happens if you get a new participant with a certain value of X). The bands fit in ggplot are for predicted $\mu$|X


Broom offers the fitted (predicted) values already if you just want to plot your trajectory. But note that these are not typical prediction intervals (what happens if you get a new participant with a certain value of X). The bands fit in ggplot are for predicted $\mu$|X

```{r}
P.gg <- ggplot(example.aug, aes(x= year, y = .fitted)) + geom_point() + stat_smooth(method = "lm")   

P.gg

```


Can also explicitly simulate new data (rather than rely on another function to do so), which will be useful for power calculations later. In the simulated data, the subject means are different from the means in the original data because simulate samples by-subject random effect values using the variance components in the fitted model. 


```{r, eval = FALSE}
sim.1<- simulate(mod.2)
head(sim.1)
```
